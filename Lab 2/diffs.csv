diffs
"diff --git a/pyproject.toml b/pyproject.toml
index 2b258ba..6866317 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.4.3"",
+    ""unsloth_zoo>=2025.4.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -351,7 +351,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.4.3"",
+    ""unsloth_zoo>=2025.4.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/dataprep/synthetic.py b/unsloth/dataprep/synthetic.py
index 2f4a85f..100044b 100644
--- a/unsloth/dataprep/synthetic.py
+++ b/unsloth/dataprep/synthetic.py
@@ -18,13 +18,16 @@ __all__ = [
 import subprocess
 import time
 import os
+os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 import requests
 import torch
 import gc
 import time
-from unsloth_zoo.vllm_utils import load_vllm
-from transformers import AutoConfig, AutoTokenizer
-import signal
+from unsloth_zoo.vllm_utils import (
+    load_vllm,
+    patch_vllm,
+)
+import numpy as np
 
 from .synthetic_configs import (
     synthetic_qa_config,
@@ -51,6 +54,7 @@ class SyntheticDataKit:
         self.model_name = model_name
         self.max_seq_length = max_seq_length
 
+        from transformers import AutoConfig, AutoTokenizer
         self.config = AutoConfig.from_pretrained(
             model_name,
             token = token,
@@ -59,6 +63,7 @@ class SyntheticDataKit:
             model_name,
             token = token,
         )
+        patch_vllm()
         engine_args = load_vllm(
             model_name             = model_name,
             config                 = self.config,
@@ -69,23 +74,23 @@ class SyntheticDataKit:
             conservativeness       = conservativeness,
             return_args            = True,
             enable_lora            = False,
+            use_bitsandbytes       = False,
             **kwargs,
         )
 
         if ""device"" in engine_args: del engine_args[""device""]
         if ""model""  in engine_args: del engine_args[""model""]
-        if ""compilation_config"" in engine_args: del engine_args[""compilation_config""]
 
         subprocess_commands = [
             ""vllm"", ""serve"", str(model_name),
         ]
         for key, value in engine_args.items():
             flag  = key.replace(""_"", ""-"")
-            which = str(value).lower().replace(""torch."", """")
-            if which == ""true"":
+            which = str(value).replace(""torch."", """")
+            if which == ""True"":
                 # Ignore --enforce-eager True
                 subprocess_commands += [""--"" + flag,]
-            elif which == ""false"":
+            elif which == ""False"":
                 # Ignore flag
                 pass
             else:
@@ -190,34 +195,42 @@ class SyntheticDataKit:
     def __exit__(self, *exc): self.cleanup()
     def __del__(self): self.cleanup()
 
-    def truncate(self, filename = None):
-        # Truncates by summary and max generation
+    def chunk_data(self, filename = None):
+        # Chunks data by max tokens and generation length
         assert(filename is not None)
         assert(os.path.exists(filename))
         assert(hasattr(self, ""tokenizer""))
+        if not hasattr(self, ""max_seq_length""):
+            raise RuntimeError(""Please use SynthetidDataKit.from_pretrained(...) first!"")
+        if not hasattr(self, ""overlap"") or not hasattr(self, ""max_generation_tokens""):
+            raise RuntimeError(""Please use prepare_qa_generation first!"")
 
         with open(filename, ""r"") as f: text = f.read()
 
-        max_tokens = self.max_seq_length - self.max_generation_tokens*2 - 2
-        input_ids = self.tokenizer(text).input_ids
-        length = len(text)
-        original_length = len(text)
-        original_n_tokens = len(input_ids)
+        max_tokens = self.max_seq_length - self.max_generation_tokens*2 - 128 # -128 to reduce errors
+        if max_tokens <= 5:
+            raise RuntimeError(""Generation length is way too long!"")
+        input_ids = self.tokenizer(text, add_special_tokens = False).input_ids
 
-        if len(input_ids) > max_tokens:
-            # Will fix later, but for now we simply naively truncate by ratios
-            length = original_length
-            while True:
-                input_ids = self.tokenizer(text[:length]).input_ids
-                if len(input_ids) < max_tokens or length == 0: break
-                length = length * (max_tokens/len(input_ids))
-                length = max(int(length), 0)
-            pass
-            print(f""Unsloth: Will truncate your data which has {original_n_tokens} tokens to {len(input_ids)} tokens."")
+        # Get left and right boundaries
+        length = len(input_ids)
+        n_chunks = int(np.ceil(length / (max_tokens - self.overlap)))
+        boundaries = np.ceil(np.linspace(0, length - self.overlap, n_chunks)).astype(int)
+        boundaries = np.stack((boundaries[:-1], (boundaries + self.overlap)[1:])).T
+        boundaries = np.minimum(boundaries, length).tolist()
 
-            with open(filename, ""w"") as f: f.write(text[:length])
+        # Get extension of filename like .txt
+        filename, extension = os.path.splitext(filename)
+        if filename.endswith(""/""): filename = filename[:-1]
+
+        all_filenames = []
+        for i, (left, right) in enumerate(boundaries):
+            chunked_text = self.tokenizer.decode(input_ids[left : right])
+            new_filename = f""{filename}_{i}{extension}""
+            all_filenames.append(new_filename)
+            with open(new_filename, ""w"") as f: f.write(chunked_text)
         pass
-        return filename, length
+        return all_filenames
     pass
 
     def prepare_qa_generation(
@@ -258,5 +271,7 @@ class SyntheticDataKit:
             .replace(""{cleanup_temperature}"", str(cleanup_temperature))
 
         with open(""synthetic_data_kit_config.yaml"", ""w"") as f: f.write(config)
+
+        self.overlap = overlap
     pass
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ed8a2ad..d3b8969 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.4.3""
+__version__ = ""2025.4.4""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/pyproject.toml b/pyproject.toml
index d89ea2c..5bdf3c4 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -187,9 +187,9 @@ cu124onlytorch260 = [
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 cu126onlytorch260 = [
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 2ec4ada..656096b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.2.4""
+__version__ = ""2025.2.5""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -131,6 +131,7 @@ logging.getLogger(""transformers.tokenization_utils_base"").setLevel(logging.CRITI
 
 # Ignore logging messages
 class HideLoggingMessage(logging.Filter):
+    __slots__ = ""text"",
     def __init__(self, text): self.text = text
     def filter(self, x): return not (self.text in x.getMessage())
 pass
@@ -138,6 +139,8 @@ pass
 # The speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.
 from transformers.training_args import logger as transformers_training_args_logger
 transformers_training_args_logger.addFilter(HideLoggingMessage(""The speedups""))
+# torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED.
+transformers_training_args_logger.addFilter(HideLoggingMessage(""torch.distributed""))
 del transformers_training_args_logger
 
 # Using the default loss: `ForCausalLMLoss`.
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a337472..ec6706e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -15,6 +15,7 @@
 import torch
 import gc
 import math
+from functools import partial
 from typing import Optional, Tuple, List, Union
 from ._utils import *
 from ._utils import __version__
@@ -447,20 +448,28 @@ def LlamaAttention_fast_forward(
         A = flash_attn_func(Q, K, V, causal = True)
     else:
         # Grouped query attention
-        if n_groups != 1:
-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
-            K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
-            V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
-        pass
-        # Must be contiguous or else results are False!
-        # https://github.com/pytorch/pytorch/issues/112577
-        Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
-        # Needs (batch_size, n_heads, seq_len, head_dim)
-        # is_casual and attention_mask must not be both set!
-        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)
-        # Go back to (batch_size, seq_len, n_heads, head_dim)
-        A = A.transpose(1, 2).contiguous()
+        if SDPA_HAS_GQA:
+            # Needs (batch_size, n_heads, seq_len, head_dim)
+            # is_casual and attention_mask must not be both set!
+            A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False, enable_gqa = n_groups != 1)
+            # Go back to (batch_size, seq_len, n_heads, head_dim)
+            A = A.transpose(1, 2)#.contiguous()
+        else:
+            if n_groups != 1:
+                K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+                V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+                K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
+                V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
+            pass
+            # Must be contiguous or else results are False!
+            # https://github.com/pytorch/pytorch/issues/112577
+            Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
+            # Needs (batch_size, n_heads, seq_len, head_dim)
+            # is_casual and attention_mask must not be both set!
+            A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)
+            # Go back to (batch_size, seq_len, n_heads, head_dim)
+            A = A.transpose(1, 2).contiguous()
+        pass
     pass
     attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
     attn_output = self.apply_o(self, attn_output)
@@ -699,6 +708,7 @@ def LlamaModel_fast_forward(
     if attention_mask is None:
         padding_mask = None
     elif self.training:
+    # elif attention_mask is not None and self.training:
         attention_mask = None
         padding_mask = None
     else:
@@ -714,6 +724,7 @@ def LlamaModel_fast_forward(
             past_key_values_length,
             sliding_window = getattr(self.config, ""sliding_window"", None),
         )
+        attention_mask = attention_mask.to(torch.bool)
     pass
 
     hidden_states = inputs_embeds
@@ -1802,8 +1813,6 @@ class FastLlamaModel:
             model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype)
             model.vllm_engine = llm
             model.fast_generate = model.vllm_engine.generate
-
-            from functools import partial
             model.fast_generate_batches = partial(generate_batches, model.vllm_engine)
         pass
         # Return old flag
@@ -1952,13 +1961,13 @@ class FastLlamaModel:
         Trainer._inner_training_loop = _fast_inner_training_loop
 
         # Save max_seq_length
-        model.max_seq_length = max_position_embeddings
+        model.max_seq_length = max_seq_length
         internal_model = model
         while hasattr(internal_model, ""model""):
-            internal_model.max_seq_length = max_position_embeddings
+            internal_model.max_seq_length = max_seq_length
             internal_model = internal_model.model
         pass
-        internal_model.max_seq_length = max_position_embeddings
+        internal_model.max_seq_length = max_seq_length
 
         # We check the tokenizer first for errors
         if fix_tokenizer:
@@ -2146,8 +2155,6 @@ class FastLlamaModel:
         signature = str(inspect.signature(LoraConfig))
         SUPPORTS_LOFTQ  = ""loftq_config"" in signature
         SUPPORTS_RSLORA = ""use_rslora""   in signature
-        
-        assert(max_seq_length <= model.max_seq_length)
 
         if lora_dropout != 0:
             logger.warning_once(
@@ -2632,6 +2639,10 @@ class FastLlamaModel:
             gc.collect()
             torch.cuda.empty_cache()
         pass
+
+        # Add for_inference and for_training
+        model.for_training  = partial(FastLlamaModel.for_training,  model)
+        model.for_inference = partial(FastLlamaModel.for_inference, model)
         return model
     pass
 
@@ -2739,3 +2750,5 @@ class FastLlamaModel:
     pass
 pass
 
+from .rl import PatchFastRL
+PatchFastRL(FastLanguageModel = FastLlamaModel)
diff --git a/unsloth/models/loader_utils.py b/unsloth/models/loader_utils.py
index b778b7e..e3eadd8 100644
--- a/unsloth/models/loader_utils.py
+++ b/unsloth/models/loader_utils.py
@@ -58,6 +58,11 @@ def __get_model_name(
 
     elif load_in_4bit and SUPPORTS_FOURBIT and lower_model_name in FLOAT_TO_INT_MAPPER:
 
+        # Support returning original full -bnb-4bit name if specified specifically
+        # since we'll map it to the dynamic version instead
+        if lower_model_name.endswith(""-bnb-4bit""):
+            return lower_model_name
+        
         new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 515c658..466101d 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -16,30 +16,17 @@ __all__ = [
     ""PatchFastRL"",
 ]
 
-METRICS_MOVE_TO_END = [
-    ""nll"",
-    ""aux"",
-    ""beta"",
-    ""alpha"",
-]
 import torch
-try:
-    from transformers.utils.notebook import (
-        IntervalStrategy,
-        NotebookTrainingTracker,
-        NotebookProgressCallback,
-    )
-    HAS_NOTEBOOK = True
-except:
-    HAS_NOTEBOOK = False
-pass
 from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union
 import inspect
 import os
 import re
-import functools
 from unsloth_zoo.compiler import create_new_function
-
+from unsloth_zoo.logging_utils import PatchRLStatistics
+from .rl_replacements import (
+    RL_EXTRA_ARGS,
+    RL_FUNCTIONS,
+)
 
 def PatchRL(FastLanguageModel):
 
@@ -78,267 +65,441 @@ def PatchRL(FastLanguageModel):
     trainers = [x for x in trainers if x.endswith(""_trainer"")]
     unwrap = ""unwrap_model_for_generation""
     for trainer in trainers:
-        if hasattr(eval(f""trl.trainer.{trainer}""), unwrap):
-            exec(f""trl.trainer.{trainer}.{unwrap} = unsloth_{unwrap}"")
+        try: current_trainer = eval(f""trl.trainer.{trainer}"")
+        except: continue
+        if hasattr(current_trainer, unwrap):
+            try: exec(f""trl.trainer.{trainer}.{unwrap} = unsloth_{unwrap}"")
+            except: continue
     pass
 pass
 
 
-def NotebookProgressCallback_on_train_begin(Trainer_metrics):
-    def _NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):
-        self.first_column = ""Epoch"" if args.eval_strategy == IntervalStrategy.EPOCH else ""Step""
-        self.training_loss = 0
-        self.last_log = 0
-        column_names = [self.first_column] + [""Training Loss""]
-        if args.eval_strategy != IntervalStrategy.NO:
-            column_names.append(""Validation Loss"")
-        column_names += [x.replace(""/"", "" / "") for x in Trainer_metrics]
-        self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)
-    pass
-    return _NotebookProgressCallback_on_train_begin
+RLTrainer_replacement = '''
+import os
+from typing import *
+from dataclasses import dataclass, field
+from packaging.version import Version
+import torch
+from contextlib import nullcontext
+
+@dataclass
+class Unsloth{RLConfig_name}({RLConfig_name}):
+    """"""
+    {__RLConfig_doc__}
+    """"""
+    sampling_params: Optional[Any] = field(
+        default = None,
+        metadata = {{'help': 'vLLM SamplingParams'}},
+    )
+    def __init__({RLConfig_arguments},
+        sampling_params = None,
+        **kwargs,
+    ):
+{RLConfig_extra_args}
+        super().__init__({RLConfig_call_args}{RLConfig_kwargs})
 pass
 
-
-def NotebookProgressCallback_on_log(Trainer_metrics):
-    def _NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwargs):
-        # Only for when there is no evaluation
-        if args.eval_strategy == IntervalStrategy.NO and ""loss"" in logs:
-            values = {""Training Loss"": logs[""loss""]}
-            for metric in Trainer_metrics:
-                # Sometimes metric is not inside logs
-                try: values[metric.replace(""/"", "" / "")] = logs[metric]
-                except: pass
-            pass
-            # First column is necessarily Step since we're not in epoch eval strategy
-            values[""Step""] = state.global_step
-            self.training_tracker.write_line(values)
-        pass
-    pass
-    return _NotebookProgressCallback_on_log
+{RLTrainer_extras}
+
+class Unsloth{RLTrainer_name}(_Unsloth{RLTrainer_name}):
+    """"""
+    {__RLTrainer_doc__}
+    """"""
+    def __init__({RLTrainer_arguments},
+        **kwargs
+    ):
+        if args is None: args = Unsloth{RLConfig_name}()
+{RLTrainer_extra_args}
+        super().__init__({RLTrainer_call_args}{RLTrainer_kwargs})
+{RLTrainer_post}
 pass
+'''
 
+def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
+    # Patch for vLLM and Unsloth PEFT
+    import trl
+    import trl.trainer
+    try:
+        trainer = eval(f""trl.trainer.{trainer_file}"")
+    except Exception as error:
+        return
+    
+    # Get SFTTrainer and SFTConfig names
+    name   = [x for x in dir(trainer) if x.endswith(""Trainer"") and x != ""Trainer"" and trainer_file.split(""_"")[0] in x.lower()]
+    config = [x for x in dir(trainer) if x.endswith(""Config"")  and x != ""Config""  and trainer_file.split(""_"")[0] in x.lower()]
+    if len(name)   != 1: return
+    if len(config) != 1: return
+
+    # Get SFTTrainer, SFTConfig
+    RLTrainer_name = name[0]
+    RLConfig_name  = config[0]
+    try: RLTrainer = eval(f""trl.trainer.{trainer_file}.{RLTrainer_name}"")
+    except: return
+    try: RLConfig  = eval(f""trl.trainer.{trainer_file}.{RLConfig_name}"" )
+    except: return
 
-def NotebookTrainingTracker_write_line(Trainer_metrics):
-    set_Trainer_metrics = set(Trainer_metrics)
-    def _NotebookTrainingTracker_write_line(self, values):
-        """"""
-        Write the values in the inner table.
-
-        Args:
-            values (`Dict[str, float]`): The values to display.
-        """"""
-        if self.inner_table is None:
-            self.inner_table = [list(values.keys()), list(values.values())]
-        else:
-            columns = self.inner_table[0]
-            new_values = {}
-            for key, value in values.items():
-                lowered = key.lower()
-                if lowered in set_Trainer_metrics:
-                    new_values[lowered.replace(""/"", "" / "")] = value
-                else:
-                    new_values[key] = value
-            pass
-            values = new_values
-
-            self.inner_table[0] = columns
-            if len(self.inner_table) > 1:
-                last_values = self.inner_table[-1]
-                first_column = self.inner_table[0][0]
-                if last_values[0] != values[first_column]:
-                    # write new line
-                    self.inner_table.append([values[c] if c in values else ""No Log"" for c in columns])
-                else:
-                    # update last line
-                    new_values = values
-                    for c in columns:
-                        if c not in new_values.keys():
-                            new_values[c] = last_values[columns.index(c)]
-                    self.inner_table[-1] = [new_values[c] for c in columns]
-            else:
-                # Edit for evaluation purposes
-                self.inner_table.append([values[c] if c in values else 0 for c in columns])
-            pass
-        pass
-    pass
-    return _NotebookTrainingTracker_write_line
-pass
+    # Check name
+    if RLTrainer.__name__.startswith(""Unsloth""): return
+    if RLConfig .__name__.startswith(""Unsloth""): return
 
+    all_imports = dir(trainer)
+    imports = [x for x in all_imports if not x.startswith(""_"")]
 
-def _PatchRLStatistics(metrics, algorithm):
-    if HAS_NOTEBOOK:
-        if len(metrics) == 0:
-            raise RuntimeError(f""Unsloth: RL statistics for {algorithm} failed with no metrics seen?"")
-        from transformers.trainer import is_in_notebook
-        if is_in_notebook():
-            # Patch DPO notebook printing
-            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line(metrics)
-            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
-            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin(metrics)
-            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log(metrics)
+    # Get default arguments
+    EMPTY = inspect.Parameter.empty
+    processed = []
+    for RLobject in [RLTrainer, RLConfig]:
+        parameters = inspect.signature(RLobject.__init__).parameters
+        types = (bool, type(None), int, float, str,)
+        arguments = [""self""]
+        call_args = []
+        for k, v in parameters.items():
+            if k == ""self"": continue
+            v = v.default
+            if v == ""\n"": v = re.escape(""\n"")
+            if v is EMPTY: arguments.append(k)
+            elif type(v) is str:   arguments.append(f""{k} = '{v}'"")
+            elif type(v) in types: arguments.append(f""{k} = {v}"")
+            else: continue
+            call_args.append(f""{k} = {k}"")
         pass
+        arguments = f""\n{' '*8}"" + f"",\n{' '*8}"".join(arguments)
+        call_args = f""\n{' '*12}"" + f"",\n{' '*12}"".join(call_args)
+        processed.append((arguments, call_args,))
     pass
-pass
 
+    # Process RLTrainer first
+    arguments, call_args = processed[0]
+    RLTrainer_post = """"
 
-@functools.cache
-def get_trl_metrics():
-    # Gets metrics so we can output them in notebooks
+    # Add tokenizer if not seen
+    if ""tokenizer"" not in parameters and ""processing_class"" in parameters:
+        arguments += f"",\n{' '*8}tokenizer = None""
+        call_args = call_args.replace(
+            ""processing_class = processing_class"",
+            ""processing_class = tokenizer if tokenizer is not None else processing_class"",
+        )
+    pass
 
-    import trl.trainer
-    trainers = dir(trl.trainer)
-    trainers = [x for x in trainers if x.endswith(""_trainer"")]
-    filepath = inspect.getfile(trl.trainer)
-    filepath = os.path.split(filepath)[0]
+    # Edit bf16, fp16 by checking model's torch_dtype directly
+    extra_args = """"
+    if ""args"" in call_args and ""model"" in call_args:
+        mixed_precision = \
+        ""use_bf16 = getattr(args, 'bf16', False)\n""\
+        ""use_fp16 = getattr(args, 'fp16', False)\n""\
+        ""dtype = getattr(model.config, 'torch_dtype', None)\n""\
+        ""if dtype is None: dtype = model.get_input_embeddings().dtype\n""\
+        ""from unsloth_zoo.utils import _get_dtype\n""\
+        ""dtype = _get_dtype(dtype)\n""\
+        ""float16 = dtype == torch.float16\n""\
+        ""if float16 and use_bf16: raise TypeError('Unsloth: Model is in float16 precision but you want to use bfloat16 precision. Set fp16 to `True` and bf16 to `False`')\n""\
+        ""if not float16 and use_fp16: raise TypeError('Unsloth: Model is in bfloat16 precision but you want to use float16 precision. Set fp16 to `False` and bf16 to `True`')\n""\
+        ""if not use_bf16 and not use_fp16:\n""\
+        ""    args.fp16 = float16\n""\
+        ""    args.bf16 = not float16\n""\
+        ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'fp16' if float16 else 'bf16'\n""
+        extra_args += mixed_precision
+    pass
 
-    all_metrics = dict()
-    for trainer in trainers:
-        filename = os.path.join(filepath, f""{trainer}.py"")
-        if not os.path.exists(filename): continue
-        with open(filename, ""r"") as file: file = file.read()
-
-        # Get metrics['kl'] or stats['kl']
-        metrics = re.findall(r""metrics\[[\""\']([^\""\']{1,})[\""\']\]"", file)
-        stats = re.findall(r""stats\[[\""\']([^\""\']{1,})[\""\']\]"", file)
-        metrics = metrics + stats
-
-        # Get optional f-strings
-        metrics_f = re.findall(r""metrics\[f[\""\']\{[^\}]{1,}\}([^\""\']{1,})[\""\']\]"", file)
-        stats_f = re.findall(r""stats\[f[\""\']\{[^\}]{1,}\}([^\""\']{1,})[\""\']\]"", file)
-        metrics_f = metrics_f + stats_f
-        # Filter out prefixes if seen
-        # metrics[f""{prefix}rewards/chosen""]
-        left_prefix = 'prefix = ""eval_"" if train_eval == ""eval"" else """"' in file
-        if left_prefix: metrics += metrics_f
-
-        # Move all eval_ things to the end and reward to the front
-        beginning = []
-        middle = []
-        end = []
-        for x in metrics:
-            lowered = x.lower()
-            if ""reward"" in lowered:
-                beginning.append(x)
-            elif x.lower().startswith(""eval""):
-                end.append(x)
-            else:
-                # Check if we want to move to the end
-                moved = False
-                for move_end in METRICS_MOVE_TO_END:
-                    if move_end in lowered:
-                        end.append(x)
-                        moved = True
-                        break
-                if not moved:
-                    middle.append(x)
-            pass
+    # Check if per_device_eval_batch_size (default 8) bigger than bsz
+    # Also use FP16 / BF16 evaluation
+    if ""args"" in call_args:
+        # Check eval_dataset first
+        if ""eval_dataset"" in call_args:
+            check_eval_dataset = \
+            ""if getattr(args, 'eval_dataset', None) is not None and ""\
+            ""getattr(args, 'eval_strategy', 'no') == 'no':\n""\
+            ""    args.eval_strategy = 'steps'\n""\
+            ""    if getattr(args, 'eval_steps', None) is None: args.eval_steps = 0.1\n""
+            extra_args += check_eval_dataset
         pass
-        metrics = beginning + middle + end
 
-        all_metrics[trainer[:trainer.find(""_"")].upper()] = metrics
+        # Check if gradient accumulation bug fix is applied
+        check_ga = \
+        ""ga_steps = getattr(args, 'gradient_accumulation_steps', None)\n""\
+        ""if ga_steps is not None and ga_steps > 1:\n""\
+        ""    from transformers import __version__ as transformers_version\n""\
+        ""    if Version(transformers_version) <= Version('4.45.2'):\n""\
+        ""        print('**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\\n'\n""\
+        ""              '`pip install --upgrade --no-cache-dir --force-reinstall --no-deps unsloth transformers trl unsloth_zoo`')\n""
+        extra_args += check_ga
+
+        eval_changes = \
+        ""if getattr(args, 'eval_strategy', 'no') != 'no':\n""\
+        ""    eval_bsz = getattr(args, 'per_device_eval_batch_size', 8)\n""\
+        ""    if eval_bsz == 8 and args.per_device_train_batch_size < eval_bsz: args.per_device_eval_batch_size = args.per_device_train_batch_size\n""\
+        ""    if getattr(args, 'eval_accumulation_steps', None) is None and ga_steps is not None: args.eval_accumulation_steps = ga_steps\n""\
+        ""fp16_full_eval = getattr(args, 'fp16_full_eval', False)\n""\
+        ""bf16_full_eval = getattr(args, 'bf16_full_eval', False)\n""\
+        ""if args.fp16 and bf16_full_eval: args.bf16_full_eval = False; args.fp16_full_eval = True\n""\
+        ""if args.bf16 and fp16_full_eval: args.bf16_full_eval = True; args.fp16_full_eval = False\n""\
+        ""if not bf16_full_eval and not fp16_full_eval: args.bf16_full_eval = args.bf16; args.fp16_full_eval = args.fp16\n""
+        extra_args += eval_changes
     pass
-    return all_metrics
-pass
 
+    # Check max_seq_length
+    if ""model"" in call_args:
+        length_check = \
+        ""if 'max_seq_length' not in locals() and not hasattr(args, 'max_seq_length'):\n""\
+        ""    pass\n""\
+        ""else:\n""\
+        ""    model_max_seq_length = getattr(model, 'max_seq_length', None)\n""\
+        ""    args_max_seq_length  = getattr(args,  'max_seq_length', None)\n""\
+        ""    if args_max_seq_length is None and model_max_seq_length is not None:\n""\
+        ""        max_seq_length = model.max_seq_length\n""\
+        ""        if hasattr(args, 'max_seq_length'): args.max_seq_length = max_seq_length\n""
+        ""    elif args_max_seq_length is not None and model_max_seq_length is not None:\n""\
+        ""        if args_max_seq_length > model_max_seq_length:\n""\
+        ""            print('Unsloth: You set `max_seq_length` as ' + str(args_max_seq_length) + ' but \n""\
+        ""                   the maximum the model supports is ' + str(model_max_seq_length) + '. We shall reduce it.')\n""\
+        ""            args.max_seq_length = model_max_seq_length\n""
+        extra_args += length_check
+    pass
 
-def PatchRLStatistics(algorithm = ""GRPO""):
-    # Get notebook statistics columns to show up
-    algorithm = algorithm.upper()
-    all_metrics = get_trl_metrics()
-    if algorithm not in all_metrics:
-        print(
-            f""Unsloth for {algorithm.upper()} is not yet implemented! Just ignore this function.\n""\
-            f""We support: `{list(all_metrics.keys())}`""
-        )
+    # Enable for training and move padding side of tokenizer to right
+    if ""model"" in call_args:
+        training_check = \
+        ""if model is not None and hasattr(model, 'for_training'):\n""\
+        ""    model.for_training()\n""\
+        ""if 'tokenizer' in locals() and hasattr(tokenizer, 'padding_side'): tokenizer.padding_side = 'right'\n""\
+        ""if 'processing_class' in locals():\n""\
+        ""    if hasattr(processing_class, 'padding_side'): processing_class.padding_side = 'right'\n""\
+        ""    if hasattr(processing_class, 'tokenizer') and hasattr(processing_class.tokenizer, 'padding_side'): ""\
+        ""processing_class.tokenizer.padding_side = 'right'\n""
+        extra_args += training_check
     pass
-    _PatchRLStatistics(all_metrics[algorithm], algorithm)
-pass
 
+    # Check NEFTune
+    if ""model"" in call_args:
+        neftune_check = \
+        ""if hasattr(self, 'neftune_hook_handle'):\n""\
+        ""    self.neftune_hook_handle.remove()\n""\
+        ""    if hasattr(self, 'neftune_hook_handle'): del self.neftune_hook_handle\n""\
+        ""if getattr(args, 'neftune_noise_alpha', None) is not None:\n""\
+        ""    model.get_input_embeddings().neftune_noise_alpha = self.neftune_noise_alpha\n""\
+        ""pass\n""
+        RLTrainer_post += neftune_check
+    pass
 
-def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
-    # Patch for vLLM and Unsloth PEFT
-    import trl
-    import trl.trainer
+    # Add statistics as well!
+    extra_args += \
+        ""from unsloth_zoo.logging_utils import PatchRLStatistics\n""\
+        f""PatchRLStatistics('{trainer_file}')\n""
 
-    trainer = eval(f""trl.trainer.{trainer_file}"")
-    name = [x for x in dir(trainer) if x.endswith(""Trainer"") and x != ""Trainer"" and trainer_file.split(""_"")[0] in x.lower()]
-    assert(len(name) == 1)
-    RLTrainer_name = name[0]
-    RLTrainer = eval(f""trl.trainer.{trainer_file}.{RLTrainer_name}"")
+    # Patch optional args
+    if trainer_file in RL_EXTRA_ARGS:
+        process_extra_args = RL_EXTRA_ARGS[trainer_file]
+        for process_extra_arg in process_extra_args:
+            extra_args += process_extra_arg(call_args, extra_args)
+    pass
 
-    try:
-        __init__ = inspect.getsource(RLTrainer.__init__)
-    except:
-        # Already patched most likely!
-        return
-    old__init__ = __init__
-    all_imports = dir(trainer)
-    assert(""Union"" in all_imports)
-    imports = [x for x in all_imports if not x.startswith(""_"")]
-    imports += [""Trainer""]
+    # Create RLTrainer args
+    extra_args = extra_args.split(""\n"")
+    extra_args = ""\n"".join("" ""*8 + x for x in extra_args)
+    RLTrainer_post = RLTrainer_post.split(""\n"")
+    RLTrainer_post = ""\n"".join("" ""*8 + x for x in RLTrainer_post)
+    RLTrainer_arguments  = arguments
+    RLTrainer_extra_args = extra_args
+    RLTrainer_call_args  = call_args
+
+    # Fix RLConfig next
+    arguments, call_args = processed[1]
+    extra_args = """"
+
+    # Edit GA / bsz and weight_decay
+    replacements = {
+        ""output_dir""                  : None,
+        ""logging_nan_inf_filter""      : False,
+        ""per_device_train_batch_size"" : 4,
+        ""gradient_accumulation_steps"" : 2,
+        ""weight_decay""                : 0.01,
+        ""warmup_ratio""                : 0.1,
+        ""seed""                        : 3407,
+        ""optim""                       : ""adamw_8bit"",
+        ""learning_rate""               : 5e-05,
+        ""per_device_eval_batch_size""  : 4,
+        ""eval_accumulation_steps""     : 2,
+        ""torch_empty_cache_steps""     : 250,
+        ""logging_steps""               : 1,
+    }
+    for k, v in replacements.items():
+        x = f""{k}( = [^,\n]{{1,}})?,\n""
+        y = f""'{v}'"" if type(v) is str else f""{v}""
+        y = f""{k} = {y},\n""
+        arguments = re.sub(x, y, arguments)
+    pass
 
-    spaces = __init__.find(""def"")
-    __init__ = __init__.split(""\n"")
-    __init__ = ""\n"".join(x[spaces:] for x in __init__)
+    # Warn on too large or too small learning rate
+    if "" learning_rate"" in call_args:
+        learning_rate_check = \
+        ""if learning_rate < 1e-7: raise FloatingPointError(f'Unsloth: Your learning rate of `{learning_rate}` is too small and less than 1e-7! ""\
+        ""Consider increasing it, otherwise gradient updates will be close to 0!')\n""\
+        ""if learning_rate > 1: raise OverflowError(f'Unsloth: Your learning rate of `{learning_rate}` is way too larger > 1! ""\
+        ""Consider decreasing it to 1e-1, otherwise gradient updates will explode!')\n""
+        extra_args += learning_rate_check
+    pass
 
-    # Replace vLLM sections since we already have it done!
-    vllm_part = re.findall(
-        r""(\n[\s]{4}""\
-        r""if (self|args)\.use_vllm\:.+?""\
-        r""\n[\s]{4,}""\
-        ""else:\n)"",
-        __init__,
-        flags = re.MULTILINE | re.DOTALL,
-    )
-    if (len(vllm_part) != 1): return
+    # Add output_dir saving
+    if ""output_dir"" in call_args:
+        # Default checks
+        saving_check = \
+        ""if output_dir is None and save_strategy == 'steps' and save_steps == 500:\n""\
+        ""    output_dir = 'unsloth_training_checkpoints'\n""\
+        ""    save_strategy = 'no'\n""
+        extra_args += saving_check
+    pass
 
-    vllm_part, args = vllm_part[0][0], vllm_part[0][1]
-    # Strip all comments
-    new_vllm_part = re.sub(r""\#[^\n]{1,}\n"", """", vllm_part)
+    # Edit dataset_num_proc
+    if ""dataset_num_proc"" in call_args:
+        num_proc_check = \
+        ""if dataset_num_proc is None:\n""\
+        ""    from multiprocessing import cpu_count\n""\
+        ""    dataset_num_proc = cpu_count()\n""
+        extra_args += num_proc_check
+    pass
 
-    # Get SamplingParams
-    sampling_params = re.findall(
-        r""\n[\s]{4,}(self\.[^\s]{1,}[\s]{0,}\=[\s]{0,}""\
-        r""SamplingParams\(.+?\))"",
-        new_vllm_part,
-        flags = re.MULTILINE | re.DOTALL,
+    # Edit report_to and default it to nothing if max_steps is like 60
+
+    # Create RLConfig args
+    extra_args = extra_args.split(""\n"")
+    extra_args = ""\n"".join("" ""*8 + x for x in extra_args)
+    RLConfig_arguments  = arguments
+    RLConfig_extra_args = extra_args
+    RLConfig_call_args  = call_args
+
+    # Patch vLLM and other functions
+    RLTrainer_extras = patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, imports)
+    if RLTrainer_extras is None:
+        RLTrainer_extras = f""_Unsloth{RLTrainer_name} = {RLTrainer_name}""
+
+    # Create full module
+    exec(f""from trl.trainer import ({RLTrainer_name}, {RLConfig_name},)"")
+    __RLTrainer_doc__ = eval(f""trl.trainer.{RLTrainer_name}"").__doc__
+    __RLConfig_doc__  = eval(f""trl.trainer.{RLConfig_name}"") .__doc__
+
+    RLTrainer_source = RLTrainer_replacement.format(
+        RLTrainer_name       = RLTrainer_name,
+        __RLTrainer_doc__    = __RLTrainer_doc__,
+        RLTrainer_arguments  = RLTrainer_arguments,
+        RLTrainer_extra_args = RLTrainer_extra_args,
+        RLTrainer_call_args  = RLTrainer_call_args,
+        RLTrainer_kwargs     = "",**kwargs""[1 if RLTrainer_call_args.endswith("","") else 0:],
+
+        RLConfig_name        = RLConfig_name,
+        __RLConfig_doc__     = __RLConfig_doc__,
+        RLConfig_arguments   = RLConfig_arguments,
+        RLConfig_extra_args  = RLConfig_extra_args,
+        RLConfig_call_args   = RLConfig_call_args,
+        RLConfig_kwargs      = "",**kwargs""[1 if RLConfig_call_args .endswith("","") else 0:],
+
+        RLTrainer_extras     = RLTrainer_extras,
+        RLTrainer_post       = RLTrainer_post,
+    )
+
+    # Create new function
+    created_module = create_new_function(
+        f""Unsloth{RLTrainer_name}"",
+        RLTrainer_source,
+        f""trl.trainer.{trainer_file}"",
+        imports,
+        overwrite = False,
     )
-    if len(sampling_params) != 1: return
+    
+    # Patch Trainer
+    exec(f""trl.{RLTrainer_name} = created_module.Unsloth{RLTrainer_name}"", locals(), globals())
+    exec(f""trl.trainer.{RLTrainer_name} = created_module.Unsloth{RLTrainer_name}"", locals(), globals())
+    exec(f""trl.trainer.{trainer_file}.{RLTrainer_name} = created_module.Unsloth{RLTrainer_name}"", locals(), globals())
+    
+    # Patch Config
+    exec(f""trl.{RLConfig_name} = created_module.Unsloth{RLConfig_name}"", locals(), globals())
+    exec(f""trl.trainer.{RLConfig_name} = created_module.Unsloth{RLConfig_name}"", locals(), globals())
+    exec(f""trl.trainer.{trainer_file}.{RLConfig_name} = created_module.Unsloth{RLConfig_name}"", locals(), globals())
+pass
+
 
-    sampling_params = sampling_params[0]
-    # Replace with our vLLM engine
-    sampling_params = \
-        "" ""*8 + ""self.llm = model.vllm_engine; self._last_loaded_step = 0; "" + \
-        sampling_params # Add spaces
-    new_vllm_part = f""\n    if {args}.use_vllm:\n{sampling_params}\n    else:\n""
-    __init__ = __init__.replace(vllm_part, new_vllm_part)
+def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, imports):
+    init = inspect.getsource(RLTrainer.__init__)
+    old_init = init
 
     # Remove peft_config
-    __init__ = __init__.replace(""elif peft_config is None:"", ""elif False:"")
-    __init__ = __init__.replace(""elif peft_config is not None:"", ""elif False:"")
-    __init__ = __init__.replace(""if peft_config is None:"", ""if False:"")
-    __init__ = __init__.replace(""if peft_config is not None:"", ""if False:"")
-    __init__ = __init__.replace(""get_peft_model(model, peft_config)"", ""model"")
+    init = init.replace(""elif peft_config is None:"", ""elif False:"")
+    init = init.replace(""elif peft_config is not None:"", ""elif False:"")
+    init = init.replace(""if peft_config is None:"", ""if False:"")
+    init = init.replace(""if peft_config is not None:"", ""if False:"")
+    init = init.replace(""get_peft_model(model, peft_config)"", ""model"")
+
+    # Set use_vllm if not set
+    if ""args.use_vllm"" in init and ""model"" in init and ""args"" in init:
+        # .*? matches first match. .+? matches final match.
+        replacer = re.findall(
+            ""def __init__\(.*?\).*?\:\n"",
+            init,
+            flags = re.MULTILINE | re.DOTALL,
+        )
+        if len(replacer) != 0:
+            replacer = replacer[0]
+            vllm_setter = ""\n"" + "" ""*8 + \
+            ""if hasattr(model, 'vllm_engine') and ""\
+            ""getattr(args, 'use_vllm') and getattr(args, 'use_vllm', False): ""\
+            ""args.use_vllm = True\n""
+            init = init.replace(replacer, replacer + vllm_setter)
+        pass
+    pass
 
-    # Add spaces back into __init__
-    __init__ = __init__.split(""\n"")
-    __init__ = ""\n"".join(' '*spaces + x for x in __init__)
+    vllm_part = re.findall(
+        r""(\n[\s]{8}""\
+        r""if (self|args)\.use_vllm\:.*?""\
+        r""\n[\s]{8}""\
+        ""else:\n)"",
+        init,
+        flags = re.MULTILINE | re.DOTALL,
+    )
+    if len(vllm_part) == 1:
+        vllm_part, args = vllm_part[0][0], vllm_part[0][1]
+        # Strip all comments
+        new_vllm_part = re.sub(r""\#[^\n]{1,}\n"", """", vllm_part)
+
+        # Get SamplingParams
+        sampling_params = re.findall(
+            r""\n[\s]{4,}(self\.[^\s]{1,}[\s]{0,}\=[\s]{0,}""\
+            r""SamplingParams\(.+?\))"",
+            new_vllm_part,
+            flags = re.MULTILINE | re.DOTALL,
+        )
+        if len(sampling_params) == 1:
+            sampling_params = sampling_params[0]
+            # Replace with our vLLM engine
+            sampling_params = \
+                "" ""*12 + ""self.llm = model.vllm_engine; self._last_loaded_step = 0; "" + \
+                sampling_params # Add spaces
+            new_vllm_part = \
+                f""\n{' '*8}if {args}.use_vllm:\n{sampling_params} ""\
+                f""if getattr(args, 'sampling_params', None) is None else ""\
+                f""getattr(args, 'sampling_params', None)\n{' '*8}else:\n""
+            init = init.replace(vllm_part, new_vllm_part)
+        pass
+    pass
 
     # Search for vLLM calling in all child functions
     functions = dir(RLTrainer)
     RLTrainer_source = inspect.getsource(RLTrainer)
     functions = [x for x in functions if f""def {x}"" in RLTrainer_source]
 
-    changed = {""__init__"" : (old__init__, __init__,)}
+    changed = {""__init__"" : (old_init, init,)}
+    edit_functions = RL_FUNCTIONS.get(trainer_file, [])
+
     for function in functions:
         if not hasattr(RLTrainer, function): continue
         fx = getattr(RLTrainer, function)
-        try:
-            source = inspect.getsource(fx)
-        except:
-            continue
+        try: source = inspect.getsource(fx)
+        except: continue
         original_source = source
 
+        # Check for function
+        for edit_function in edit_functions:
+            source = edit_function(function, source)
+        pass
+
         # llm_model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model
         source = re.sub(
             r""(\n[\s]{4,}).+?model_executor\.driver_worker.+?\n"",
@@ -386,22 +547,9 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         RLTrainer_source = RLTrainer_source.replace(old, new)
     pass
     RLTrainer_source = RLTrainer_source.replace(
-        f""class {RLTrainer_name}"", f""class Unsloth{RLTrainer_name}"", 1
-    )
-
-    # Create new class in compiled cache and import it
-    module = create_new_function(
-        RLTrainer_name,
-        RLTrainer_source,
-        f""trl.trainer.{trainer_file}"",
-        imports,
+        f""class {RLTrainer_name}"", f""class _Unsloth{RLTrainer_name}"", 1
     )
-
-    # Patch over modules
-    exec(f""trl.{RLTrainer_name} = module.Unsloth{RLTrainer_name}"", locals(), globals())
-    exec(f""trl.trainer.{RLTrainer_name} = module.Unsloth{RLTrainer_name}"", locals(), globals())
-    exec(f""trl.trainer.{trainer_file}.{RLTrainer_name} = module.Unsloth{RLTrainer_name}"", locals(), globals())
-    return module
+    return RLTrainer_source
 pass
 
 
@@ -416,8 +564,8 @@ def patch_trl_rl_trainers():
 pass
 
 
-def PatchFastRL(algorithm = ""GRPO"", FastLanguageModel = None):
+def PatchFastRL(algorithm = None, FastLanguageModel = None):
     if FastLanguageModel is not None: PatchRL(FastLanguageModel)
     patch_trl_rl_trainers()
-    PatchRLStatistics(algorithm)
+    if algorithm is not None: PatchRLStatistics(algorithm)
 pass
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
new file mode 100644
index 0000000..4d7a4db
--- /dev/null
+++ b/unsloth/models/rl_replacements.py
@@ -0,0 +1,186 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+__all__ = [
+    ""RL_EXTRA_ARGS"",
+    ""RL_FUNCTIONS"",
+]
+
+import re
+import inspect
+from collections import defaultdict
+RL_EXTRA_ARGS = defaultdict(list)
+RL_FUNCTIONS  = defaultdict(list)
+
+
+# Check untrained tokens
+def sft_trainer_fix_untraiend_tokens(call_args, extra_args):
+    if ""model"" in call_args and ""train_dataset"" in call_args:
+        fix_tokenizer = \
+        ""IGNORED_TOKENIZER_NAMES = os.environ.get('UNSLOTH_IGNORED_TOKENIZER_NAMES', '').split('\\n')\n""\
+        ""from unsloth_zoo.tokenizer_utils import fix_untrained_tokens\n""\
+        ""from unsloth_zoo.training_utils  import fix_zero_training_loss\n""\
+        ""if 'tokenizer' not in locals(): tokenizer = processing_class\n""\
+        ""fix_untrained_tokens(model, tokenizer, train_dataset, IGNORED_TOKENIZER_NAMES, eps = 1e-16)\n""\
+        ""fix_zero_training_loss(model, tokenizer, train_dataset)\n""
+        return fix_tokenizer
+    return """"
+pass
+RL_EXTRA_ARGS[""sft_trainer""].append(sft_trainer_fix_untraiend_tokens)
+
+
+# Remove DPO columns which might randomnly be tokenized
+def dpo_trainer_fix_columns(call_args, extra_args):
+    if ""model"" in call_args and ""train_dataset"" in call_args:
+        fix_dpo = \
+        ""if hasattr(train_dataset, 'column_names'):\n""\
+        ""    column_names = set(train_dataset.column_names)\n""\
+        ""    check = ['chosen', 'rejected', 'prompt', 'chosen_input_ids', 'chosen_attention_mask',\n""\
+        ""             'chosen_labels', 'rejected_input_ids', 'rejected_attention_mask', 'rejected_labels',\n""\
+        ""             'prompt_input_ids', 'prompt_attention_mask']\n""\
+        ""    if all(x in column_names for x in check):\n""\
+        ""        train_dataset = train_dataset.remove_columns(['chosen', 'rejected', 'prompt'])\n""\
+        ""    del check, column_names\n""
+        return fix_dpo
+    return """"
+pass
+RL_EXTRA_ARGS[""dpo_trainer""].append(dpo_trainer_fix_columns)
+
+
+# Fix tokenizer double BOS
+def sft_trainer_prepare_dataset(function_name, function):
+    if  function_name != ""_prepare_non_packed_dataloader"" and \
+        function_name != ""_prepare_dataset"": return function
+
+    check_text = \
+    ""if 'tokenizer'          not in locals(): tokenizer = processing_class\n""\
+    ""if 'formatting_func'    not in locals(): raise RuntimeError('Unsloth: Please file a bug report - `formatting_func` does not exist!')\n""\
+    ""if 'dataset_text_field' not in locals() and 'args' in locals(): dataset_text_field = args.dataset_text_field\n""\
+    ""if 'dataset_text_field' not in locals(): raise RuntimeError('Unsloth: Please file a bug report - `dataset_text_field` does not exist!')\n""\
+    ""test_text = dataset[0][dataset_text_field] if (formatting_func is None and dataset_text_field is not None) else formatting_func(dataset[0])[0]\n""\
+    ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
+    ""chat_template = '' if chat_template is None else chat_template\n""\
+    ""has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) ""\
+    ""if getattr(tokenizer, 'bos_token', None) is not None else False\n""\
+    ""if 'add_special_tokens' not in locals() and has_bos_token_already:\n""\
+    ""    from functools import partial\n""\
+    ""    tokenizer = partial(tokenizer, add_special_tokens = False)\n""\
+    ""    processing_class = tokenizer\n""\
+    ""else:\n""\
+    ""    add_special_tokens = False if has_bos_token_already else add_special_tokens\n""
+
+    check_text = check_text.split(""\n"")
+    check_text = ""\n"".join("" ""*8 + x for x in check_text)
+    check_text = check_text.rstrip() + ""\n""
+
+    # .*? matches first match. .+? matches final match.
+    replacer = re.findall(
+        r""def {function_name}\(.*?\).*?\:\n"",
+        function,
+        flags = re.MULTILINE | re.DOTALL,
+    )
+    if len(replacer) != 0:
+        replacer = replacer[0]
+        function = function.replace(replacer, replacer + check_text)
+    pass
+    return function
+pass
+RL_FUNCTIONS[""sft_trainer""].append(sft_trainer_prepare_dataset)
+
+
+# Ignore mean_token_accuracy since it needs logits
+# We override it directly with our version
+def _sft_trainer_compute_loss(self, model, inputs, return_outputs = False, num_items_in_batch = None):
+    (loss, outputs) = super().compute_loss(
+        model,
+        inputs,
+        return_outputs = return_outputs,
+        num_items_in_batch = num_items_in_batch,
+    )
+    return (loss, outputs) if return_outputs else loss
+pass
+
+def sft_trainer_compute_loss(function_name, function):
+    if  function_name != ""compute_loss"": return function
+
+    function = inspect.getsource(_sft_trainer_compute_loss)
+    function = function.replace(""def _sft_trainer_compute_loss"", ""def compute_loss"")
+    function = function.split(""\n"")
+    function = ""\n"".join("" ""*4+x for x in function)
+    return function
+pass
+RL_FUNCTIONS[""sft_trainer""].append(sft_trainer_compute_loss)
+
+
+# Autocast precision for GRPO
+def grpo_trainer__prepare_inputs(function_name, function):
+    if  function_name != ""_prepare_inputs"": return function
+
+    if ""with torch.inference_mode()"" not in function: return function
+
+    # Add mixed precision training
+    function = function.replace(
+        ""with torch.inference_mode():"",
+
+        ""with torch.inference_mode(), ""\
+        ""torch.amp.autocast(device_type = 'cuda', ""\
+        ""dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16) ""\
+        ""if not torch.is_autocast_enabled('cuda') else nullcontext():"",
+    )
+
+    # Disable attaching a float32 conversion hook which upcasts logits to FP32
+    function = function.replace(
+        ""self.accelerator.unwrap_model(self.model)"",
+        ""self.accelerator.unwrap_model(self.model, keep_fp32_wrapper = False)"",
+    )
+    return function
+pass
+RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__prepare_inputs)
+
+
+# Remove _move_model_to_vllm
+def grpo_trainer__move_model_to_vllm(function_name, function):
+    if  function_name != ""_move_model_to_vllm"": return function
+
+    # .*? matches first match. .+? matches final match.
+    replacement = ""def _move_model_to_vllm(self, *args, **kwargs): return None\n""
+    return "" ""*function.find(""def"") + replacement
+pass
+RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__move_model_to_vllm)
+
+
+# Edit _get_per_token_logps to handle mixed precision
+def grpo_trainer__get_per_token_logps(function_name, function):
+    if  function_name != ""_get_per_token_logps"": return function
+
+    # Edit model to autocast it
+    # .*? matches first match. .+? matches final match.
+    original = re.findall(
+        r""\n([ ]{4,})(logits = model\(.*?\))"",
+        function,
+        flags = re.MULTILINE | re.DOTALL,
+    )
+    if len(original) != 0:
+        spaces, original = original[0]
+        spaces = len(spaces)
+        replacer = \
+        ""if not hasattr(self, '_autocast_dtype'):\n"" + \
+        "" ""*(spaces + 4) + ""self._autocast_dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16\n"" + \
+        "" ""*(spaces + 0) + ""with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):\n"" + \
+        "" ""*(spaces + 4) + original
+        function = function.replace(original, replacer)
+    pass
+    return function
+pass
+RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__get_per_token_logps)
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index f2b0da8..404fce3 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -59,6 +59,7 @@ IGNORED_TOKENIZER_NAMES = frozenset(
     [x.lower() for x in IGNORED_TOKENIZER_NAMES] + \
     [x.lower()+""-bnb-4bit"" for x in IGNORED_TOKENIZER_NAMES]
 )
+os.environ[""UNSLOTH_IGNORED_TOKENIZER_NAMES""] = ""\n"".join(IGNORED_TOKENIZER_NAMES)
 
 # Check environments
 keynames = ""\n"" + ""\n"".join(os.environ.keys())
@@ -907,44 +908,25 @@ except:
 pass
 
 
-def patch_trl_tokenizer_processing_class(trainer_name):
-    # New TRL removes tokenizer!
-    # We return it back!
-    exec(f""from trl import {trainer_name}"", globals())
-    if str(eval(f""{trainer_name}"").__name__).startswith(""Unsloth""): return None
-    parameters = eval(f""inspect.signature({trainer_name}).parameters"")
-    if ""tokenizer"" in parameters: return None
-
-    args = {
-        key : \
-            value.default \
-            if type(value.default) is not str else \
-            f""'{value.default}'"" \
-        for key, value in parameters.items()
-    }
-    args[""tokenizer""] = None
-    new_args = args.copy()
-    del new_args[""tokenizer""]
-    del new_args[""processing_class""]
-    new_args = "",\n"".join(f""{' '*12}{key} = {key}"" for key in new_args) + \
-        f"",\n{' '*12}processing_class = tokenizer if tokenizer else processing_class""
-    args = "",\n"".join(f""{' '*8}{key} = {value}"" for key, value in args.items())
-    args = f""def __init__(\n"" + f""{' '*8}self,\n"" + args + ""):""
-    args += f""\n{' '*8}\n{' '*8}super().__init__(\n{new_args}\n{' '*8})""
-    new_class = f""""""class Unsloth{trainer_name}({trainer_name}):\n{' '*4}{args}\n""""""
-    return new_class
-pass
-
-
 def patch_sft_trainer_tokenizer():
     """"""
         Patches the trainer with changes
     """"""
-    for function_name, replacer in (
-        (""_prepare_non_packed_dataloader"", ""def tokenize(element):"",),
+    try:
+        sft_trainer = eval(f""trl.trainer.sft_trainer.SFTTrainer"")
+    except:
+        return
+    all_imports = dir(trl.trainer.sft_trainer)
+
+    for (function_name, replacer,) in (
+        # (""_prepare_non_packed_dataloader"", ""def tokenize(element):"",),
+        (""_prepare_non_packed_dataloader"", None,),
+        (""_prepare_dataset"", None,),
         # (""_prepare_packed_dataloader"", ""if dataset_text_field is not None"",),
     ):
-        function = getsource(eval(f""trl.trainer.sft_trainer.SFTTrainer.{function_name}""))
+        if not hasattr(sft_trainer, function_name): continue
+
+        function = getsource(eval(f""sft_trainer.{function_name}""))
         where = function.find(""def"")
         function = function.split(""\n"")
         function = ""\n"".join(x[where:] for x in function)
@@ -953,20 +935,41 @@ def patch_sft_trainer_tokenizer():
         ""\n""\
         ""if 'tokenizer'          not in locals(): tokenizer = processing_class\n""\
         ""if 'formatting_func'    not in locals(): raise RuntimeError('Unsloth: Please file a bug report - `formatting_func` does not exist!')\n""\
+        ""if 'dataset_text_field' not in locals() and 'args' in locals(): dataset_text_field = args.dataset_text_field\n""\
         ""if 'dataset_text_field' not in locals(): raise RuntimeError('Unsloth: Please file a bug report - `dataset_text_field` does not exist!')\n""\
         ""test_text = dataset[0][dataset_text_field] if (formatting_func is None and dataset_text_field is not None) else formatting_func(dataset[0])[0]\n""\
         ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
         ""chat_template = '' if chat_template is None else chat_template\n""\
         ""has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) ""\
         ""if getattr(tokenizer, 'bos_token', None) is not None else False\n""\
-        ""add_special_tokens = False if has_bos_token_already else add_special_tokens\n\n""
+        ""if 'add_special_tokens' not in locals() and has_bos_token_already:\n""\
+        ""    from functools import partial\n""\
+        ""    tokenizer = partial(tokenizer, add_special_tokens = False)\n""\
+        ""    processing_class = tokenizer\n""\
+        ""else:\n""\
+        ""    add_special_tokens = False if has_bos_token_already else add_special_tokens\n\n""
 
         check_text = check_text.split(""\n"")
         check_text = ""\n"".join("" ""*where + x for x in check_text)
+        check_text = check_text.rstrip() + ""\n""
+
+        if replacer is None:
+            # .*? matches first match. .+? matches final match.
+            replacer = re.findall(
+                f""def {function_name}\(.*?\).*?\:\n"",
+                function,
+                flags = re.MULTILINE | re.DOTALL,
+            )
+            if len(replacer) == 0: continue
+            replacer = replacer[0]
+            function = function.replace(replacer, replacer + check_text)
+        else:
+            function = function.replace(replacer, check_text + replacer)
+        pass
 
-        function = function.replace(replacer, check_text + replacer)
-        exec(function, globals())
-
+        x = [x for x in all_imports if x in function]
+        exec(f""from trl.trainer.sft_trainer import ({','.join(x)})"", locals())
+        exec(function, locals(), globals())
         exec(f""trl.trainer.sft_trainer.SFTTrainer.{function_name} = {function_name}"", globals())
     pass
 
@@ -1053,16 +1056,5 @@ def patch_sft_trainer_tokenizer():
     pass
 pass
 
-# Fix TRL trainers with removed tokenizer args (got replaced with processing_class)
-for trainer_name in (""SFTTrainer"", ""DPOTrainer"", ""KTOTrainer""):
-    trainer_text = patch_trl_tokenizer_processing_class(trainer_name)
-    if trainer_text is None: continue
-    try:
-        exec(trainer_text, globals())
-    except:
-        raise RuntimeError(f""Unsloth: Please file a bug report! Error patching {trainer_name}"")
-    exec(f""trl.trainer.{trainer_name} = Unsloth{trainer_name}"", globals())
-pass
-
-# FInally patch TRL tokenizer things
-patch_sft_trainer_tokenizer()
+# Finally patch TRL tokenizer things -> moved to RL
+# patch_sft_trainer_tokenizer()
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 834a74c..df331fc 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -20,7 +20,7 @@ from .utils import (
     MAX_FUSED_SIZE,
     triton_tanh,
     triton_cast,
-    torch_cuda_device,
+    torch_gpu_device,
 )
 from transformers.models.llama.modeling_llama import logger
 from packaging.version import Version
@@ -301,7 +301,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
             BLOCK_SIZE, num_warps = calculate_settings(vocab_size)
             logsumexp = torch.empty(n_rows, dtype = torch.float32, device = device)
 
-            with torch_cuda_device(device):
+            with torch_gpu_device(device):
                 _cross_entropy_forward[(n_rows,)](
                     logits, logits.stride(0),
                     losses,
@@ -319,7 +319,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
             # For large vocabs > 65336 like Gemma 256K
             logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = device)
 
-            with torch_cuda_device(device):
+            with torch_gpu_device(device):
                 _chunked_cross_entropy_forward[(n_rows, n_chunks,)](
                     logits, logits.stride(0),
                     losses,
@@ -363,7 +363,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         div, mod = divmod(vocab_size, BLOCK_SIZE)
         n_blocks : int = div + (mod != 0)
 
-        with torch_cuda_device(dlosses.device):
+        with torch_gpu_device(dlosses.device):
             _cross_entropy_backward[(n_rows, n_blocks,)](
                 logits,   logits.stride(0),
                 dlosses, dlosses.stride(0),
diff --git a/unsloth/kernels/geglu.py b/unsloth/kernels/geglu.py
index 1ece87c..67f576d 100644
--- a/unsloth/kernels/geglu.py
+++ b/unsloth/kernels/geglu.py
@@ -18,7 +18,7 @@ import torch
 from .utils import (
     calculate_settings,
     triton_tanh,
-    torch_cuda_device,
+    torch_gpu_device,
 )
 
 
@@ -48,7 +48,7 @@ def geglu_exact_forward_kernel(gate, up):
     device = gate.device
     out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = device)
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    with torch_cuda_device(device):
+    with torch_gpu_device(device):
         _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
 pass
@@ -105,7 +105,7 @@ def geglu_exact_backward_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    with torch_cuda_device(e.device):
+    with torch_gpu_device(e.device):
         _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
@@ -143,7 +143,7 @@ def geglu_approx_forward_kernel(gate, up):
     device = gate.device
     out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = device)
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    with torch_cuda_device(device):
+    with torch_gpu_device(device):
         _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
 pass
@@ -207,7 +207,7 @@ def geglu_approx_backward_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    with torch_cuda_device(e.device):
+    with torch_gpu_device(e.device):
         _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
diff --git a/unsloth/kernels/layernorm.py b/unsloth/kernels/layernorm.py
index ed81820..f01c4ff 100644
--- a/unsloth/kernels/layernorm.py
+++ b/unsloth/kernels/layernorm.py
@@ -16,7 +16,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, torch_cuda_device
+from .utils import calculate_settings, torch_gpu_device
 from unsloth_zoo.patching_utils import (
     patch_layernorm,
 )
@@ -113,7 +113,7 @@ class Fast_Layernorm(torch.autograd.Function):
         r  = torch.empty(n_rows, dtype = torch.float32, device = device)
         mu = torch.empty(n_rows, dtype = torch.float32, device = device)
 
-        with torch_cuda_device(device):
+        with torch_gpu_device(device):
             layernorm_forward[(n_rows,)](
                 Y, Y.stride(0),
                 X, X.stride(0),
@@ -140,7 +140,7 @@ class Fast_Layernorm(torch.autograd.Function):
         X, W, b, r, mu = ctx.saved_tensors
         n_rows, n_cols = dY.shape
 
-        with torch_cuda_device(dY.device):
+        with torch_gpu_device(dY.device):
             layernorm_backward[(n_rows,)](
                 dY, dY.stride(0),
                 X,  X .stride(0),
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 8f54e74..fba7e56 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, torch_cuda_device
+from .utils import calculate_settings, torch_gpu_device
 
 @triton.jit
 def _rms_layernorm_forward(
@@ -156,7 +156,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         r = torch.empty(n_rows, dtype = torch.float32, device = device)
 
         fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward
-        with torch_cuda_device(device):
+        with torch_gpu_device(device):
             fx[(n_rows,)](
                 Y, Y.stride(0),
                 X, X.stride(0),
@@ -186,7 +186,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         # dW = X
         dX = torch.empty_like(dY) if ctx.GEMMA else dY
 
-        with torch_cuda_device(dY.device):
+        with torch_gpu_device(dY.device):
             _rms_layernorm_backward[(n_rows,)](
                 dY, dY.stride(0),
                 dX, dX.stride(0),
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index a14a485..1c981b3 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, torch_cuda_device
+from .utils import calculate_settings, torch_gpu_device
 ROPE_GROUP_SIZE : int = 4
 
 def _rope_embedding(
@@ -100,7 +100,7 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         div, mod = divmod(n_heads, ROPE_GROUP_SIZE)
         n_groups : int = div + (mod != 0)
 
-        with torch_cuda_device(Q.device):
+        with torch_gpu_device(Q.device):
             _rope_embedding[(n_rows, n_groups, )](
                   Q,   Q.stride(0),
                 cos, cos.stride(0),
@@ -135,7 +135,7 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         cos = ctx.cos
         sin = ctx.sin
 
-        with torch_cuda_device(dY.device):
+        with torch_gpu_device(dY.device):
             _rope_embedding[(n_rows, ctx.n_groups, )](
                 dY,  dY .stride(0),
                 cos, cos.stride(0),
diff --git a/unsloth/kernels/swiglu.py b/unsloth/kernels/swiglu.py
index 12f1f5e..c1d5e31 100644
--- a/unsloth/kernels/swiglu.py
+++ b/unsloth/kernels/swiglu.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, torch_cuda_device
+from .utils import calculate_settings, torch_gpu_device
 
 
 @triton.jit
@@ -43,7 +43,7 @@ def swiglu_fg_kernel(e, g):
     n_elements = e.numel()
     h = torch.empty((batch, seq_len, hd), dtype = e.dtype, device = e.device)
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    with torch_cuda_device(e.device):
+    with torch_gpu_device(e.device):
         _fg_kernel[grid](e, g, h, n_elements, BLOCK_SIZE = 1024,)
     return h
 pass
@@ -95,7 +95,7 @@ def swiglu_DWf_DW_dfg_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    with torch_cuda_device(e.device):
+    with torch_gpu_device(e.device):
         _DWf_DW_dfg_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index db1d73c..5c955a3 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -13,14 +13,21 @@
 # limitations under the License.
 
 import triton
+import ctypes
 MAX_FUSED_SIZE : int = 65536
 next_power_of_2 = triton.next_power_of_2
 import functools
+from typing import Optional
+from unsloth import DEVICE_TYPE
 
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
 import torch
 torch_Tensor = torch.Tensor
 from packaging.version import Version
+
+if DEVICE_TYPE == ""xpu"" and Version(torch.__version__) < Version(""2.6.0""):
+    raise RuntimeError(""Intel xpu currently supports unsloth with torch.version >= 2.6.0"")
+
 if Version(torch.__version__) < Version(""2.4.0""):
     torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
     torch_amp_custom_bwd = torch.cuda.amp.custom_bwd
@@ -29,14 +36,21 @@ else:
     torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""cuda"")
 pass
 
+if DEVICE_TYPE == ""xpu"":
+    torch_amp_custom_fwd = torch.amp.custom_fwd(device_type = ""xpu"")
+    torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""xpu"")
+
 
 # tl.math.tanh now is libdevice.tanh
 from packaging.version import Version
 import triton
 import triton.language as tl
 if Version(triton.__version__) >= Version(""3.0.0""):
-    from triton.language.extra import libdevice
-    triton_tanh = libdevice.tanh
+    if DEVICE_TYPE == ""xpu"":
+        triton_tanh = tl.extra.intel.libdevice.tanh
+    else:
+        from triton.language.extra import libdevice
+        triton_tanh = libdevice.tanh
     triton_cast = tl.cast
 else:
     triton_tanh = tl.math.tanh
@@ -60,50 +74,104 @@ def calculate_settings(n : int) -> (int, int,):
     return BLOCK_SIZE, num_warps
 pass
 
+HAS_CUDA_STREAM = False
+# INTEL GPU specific logic
+if DEVICE_TYPE == ""xpu"":
+    # TODO: Changed here after adding XPU BNB support
+    HAS_XPU_STREAM = False
+    def get_ptr(x: Optional[torch.Tensor]):
+        raise RuntimeError(""XPU BNB support is not implemented yet. This function should not be called."")
+else:
+    # NVIDIA-GPU logic here as default
+    import bitsandbytes as bnb
+    # https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1330/files
+    HAS_CUDA_STREAM = Version(bnb.__version__) > Version(""0.43.3"")
+    get_ptr = bnb.functional.get_ptr
 
-import bitsandbytes as bnb
-import ctypes
-
-# https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1330/files
-HAS_CUDA_STREAM = Version(bnb.__version__) > Version(""0.43.3"")
-get_ptr = bnb.functional.get_ptr
 
-if torch.cuda.device_count() > 1:
-    torch_cuda_device = torch.cuda.device
+if DEVICE_TYPE == ""cuda"" and torch.cuda.device_count() > 1:
+    torch_gpu_device = torch.cuda.device
+elif DEVICE_TYPE == ""xpu"" and torch.xpu.device_count() > 1:
+    torch_gpu_device = torch.xpu.device
 else:
     from contextlib import nullcontext
-    def torch_cuda_device(device): return nullcontext()
-pass
-_cuda_getCurrentRawStream = torch._C._cuda_getCurrentRawStream
+    def torch_gpu_device(device): return nullcontext()
+    pass
+
+# INTEL GPU Specific Logic
+if DEVICE_TYPE == ""xpu"":
+    _gpu_getCurrentRawStream = torch._C._xpu_getCurrentRawStream 
+# NVIDIA GPU Default Logic
+else:
+    _gpu_getCurrentRawStream = torch._C._cuda_getCurrentRawStream
+
 c_void_p = ctypes.c_void_p
 def _get_tensor_stream(tensor: torch_Tensor) -> c_void_p:
-    return c_void_p(_cuda_getCurrentRawStream(tensor.device.index))
+    return c_void_p(_gpu_getCurrentRawStream(tensor.device.index))
 pass
 
+
 # Get array of CUDA streams and other buffers
 global CUDA_STREAMS
+global XPU_STREAMS
 global WEIGHT_BUFFERS
 global ABSMAX_BUFFERS
 
-_CUDA_STREAMS = {
-    (index := torch.cuda.device(i).idx) : ctypes.c_void_p(torch._C._cuda_getCurrentRawStream(index))
-    for i in range(torch.cuda.device_count())
-}
-CUDA_STREAMS   = [None] * (max(_CUDA_STREAMS.keys()) + 1)
-WEIGHT_BUFFERS = [None] * (max(_CUDA_STREAMS.keys()) + 1)
-ABSMAX_BUFFERS = [None] * (max(_CUDA_STREAMS.keys()) + 1)
-for k, v in _CUDA_STREAMS.items(): CUDA_STREAMS[k] = v
-CUDA_STREAMS = tuple(CUDA_STREAMS)
-del _CUDA_STREAMS
+# INTEL GPU Specific Logic
+if DEVICE_TYPE == ""xpu"":
+    _XPU_STREAMS = {
+        (index := torch.xpu.device(i).idx) : ctypes.c_void_p(torch._C._xpu_getCurrentRawStream(index))
+        for i in range(torch.xpu.device_count())
+    }
+    XPU_STREAMS   = [None] * (max(_XPU_STREAMS.keys()) + 1)
+    WEIGHT_BUFFERS = [None] * (max(_XPU_STREAMS.keys()) + 1)
+    ABSMAX_BUFFERS = [None] * (max(_XPU_STREAMS.keys()) + 1)
+    for k, v in _XPU_STREAMS.items(): 
+        XPU_STREAMS[k] = v
+    XPU_STREAMS = tuple(XPU_STREAMS)
+    del _XPU_STREAMS
+else:
+    # NVIDIA GPU Default Logic
+    _CUDA_STREAMS = {
+        (index := torch.cuda.device(i).idx) : ctypes.c_void_p(torch._C._cuda_getCurrentRawStream(index))
+        for i in range(torch.cuda.device_count())
+    }
+    CUDA_STREAMS   = [None] * (max(_CUDA_STREAMS.keys()) + 1)
+    WEIGHT_BUFFERS = [None] * (max(_CUDA_STREAMS.keys()) + 1)
+    ABSMAX_BUFFERS = [None] * (max(_CUDA_STREAMS.keys()) + 1)
+    for k, v in _CUDA_STREAMS.items(): CUDA_STREAMS[k] = v
+    CUDA_STREAMS = tuple(CUDA_STREAMS)
+    del _CUDA_STREAMS
+
 
 # Bitsandbytes operations
 ctypes_c_int   = ctypes.c_int
 ctypes_c_int32 = ctypes.c_int32
-cdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32
-cdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_nf4
-cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
-cgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16
-cgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16
+# INTEL GPU Specific Logic
+if DEVICE_TYPE == ""xpu"":
+    # TODO: After adding XPU BNB support, this function should be implemented
+    def cdequantize_blockwise_fp32(*args, **kwargs):
+        raise RuntimeError(""XPU BNB support is not implemented yet. cdequantize_blockwise_fp32 should not be called now."")
+    
+    def cdequantize_blockwise_fp16_nf4(*args, **kwargs):
+        raise RuntimeError(""XPU BNB support is not implemented yet. cdequantize_blockwise_fp16_nf4 should not be called now."")
+    
+    def cdequantize_blockwise_bf16_nf4(*args, **kwargs):
+        raise RuntimeError(""XPU BNB support is not implemented yet. cdequantize_blockwise_bf16_nf4 should not be called now."")
+    
+    def cgemm_4bit_inference_naive_fp16(*args, **kwargs):
+        raise RuntimeError(""XPU BNB support is not implemented yet. cgemm_4bit_inference_naive_fp16 should not be called now."")
+    
+    def cgemm_4bit_inference_naive_bf16(*args, **kwargs):
+        raise RuntimeError(""XPU BNB support is not implemented yet. cgemm_4bit_inference_naive_bf16 should not be called now."")
+else:
+    # NVIDIA GPU Default Logic
+    cdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32
+    cdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_nf4
+    cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
+    cgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16
+    cgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16
+
 torch_mm = torch.mm
 torch_mv = torch.mv
 torch_matmul = torch.matmul
@@ -160,7 +228,84 @@ def get_lora_parameters_bias(proj):
     )
 pass
 
-if HAS_CUDA_STREAM:
+# INTEL GPU Specific Logic
+if DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
+    @torch.inference_mode
+    def fast_dequantize(W, quant_state = None, out = None, use_global_buffer = False):
+        # TODO: After adding XPU BNB support, check this function 
+        if quant_state is None: return W
+        if type(quant_state) is not list:
+            # New quant_state as a class
+            # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+            absmax     = quant_state.absmax
+            shape      = quant_state.shape
+            dtype      = quant_state.dtype
+            blocksize  = quant_state.blocksize
+            offset     = quant_state.offset
+            state2     = quant_state.state2
+            absmax2    = state2.absmax
+            code2      = state2.code
+            blocksize2 = state2.blocksize
+        else:
+            # Old quant_state as a list of lists
+            absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state
+            offset, state2 = compressed_stats
+            absmax2, code2, blocksize2, _, _, _, _ = state2
+        pass
+        global XPU_STREAMS
+        device = W.device
+        device_index = device.index
+        XPU_STREAM = XPU_STREAMS[device_index]
+
+        n_elements_absmax = absmax.numel()
+        # Create weight matrix
+        if use_global_buffer:
+
+            # Use same buffers for faster inference
+            size = shape[0]*shape[1]
+            global WEIGHT_BUFFERS
+            global ABSMAX_BUFFERS
+            WEIGHT_BUFFER = WEIGHT_BUFFERS[device_index]
+            ABSMAX_BUFFER = ABSMAX_BUFFERS[device_index]
+            if WEIGHT_BUFFER is None:
+                WEIGHT_BUFFERS[device_index] = WEIGHT_BUFFER = torch_empty(size, dtype = dtype, device = device, requires_grad = False)
+                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+
+            if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)
+            if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)
+
+            out = WEIGHT_BUFFER[:size].view(shape)
+            out_absmax = ABSMAX_BUFFER[:n_elements_absmax]
+        else:
+            if out is None:
+                out = torch_empty(shape, dtype = dtype, device = device, requires_grad = False)
+            else:
+                assert(out.shape == shape)
+                assert(out.dtype == dtype)
+            out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+        pass
+
+        # NF4 dequantization of statistics
+        ptr_out_absmax = get_ptr(out_absmax)
+        with torch_gpu_device(device):
+            cdequantize_blockwise_fp32(
+                get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
+                ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax), XPU_STREAM
+            )
+            out_absmax += offset
+
+            # Dequantize W
+            fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+                 cdequantize_blockwise_bf16_nf4
+            fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
+               ctypes_c_int(blocksize), ctypes_c_int(out.numel()), XPU_STREAM,)
+        pass
+        # Careful returning transposed data
+        is_transposed = (True if W.shape[0] == 1 else False)
+        return out.t() if is_transposed else out
+    pass
+# NVIDIA GPU Default Logic
+elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
     @torch.inference_mode
     def fast_dequantize(W, quant_state = None, out = None, use_global_buffer = False):
         if quant_state is None: return W
@@ -218,7 +363,7 @@ if HAS_CUDA_STREAM:
 
         # NF4 dequantization of statistics
         ptr_out_absmax = get_ptr(out_absmax)
-        with torch_cuda_device(device):
+        with torch_gpu_device(device):
             cdequantize_blockwise_fp32(
                 get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
                 ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax), CUDA_STREAM
@@ -289,7 +434,79 @@ else:
 pass
 
 
-if HAS_CUDA_STREAM:
+# INTEL GPU Specific Logic
+if  DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
+    def fast_gemv(X, W, quant_state, out = None):
+        if quant_state is None: return torch_matmul(X, W, out = out)
+        # For fast X @ W where seq_len == 1
+        # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
+        _, q_len, hd = X.shape
+        # assert(q_len == 1)
+
+        if type(quant_state) is not list:
+            # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+            absmax     = quant_state.absmax
+            shape      = quant_state.shape
+            dtype      = quant_state.dtype
+            blocksize  = quant_state.blocksize
+            stats      = quant_state.code
+            offset     = quant_state.offset
+            state2     = quant_state.state2
+            absmax2    = state2.absmax
+            code2      = state2.code
+            blocksize2 = state2.blocksize
+        else:
+            absmax, shape, dtype, blocksize, compressed_stats, quant_type, stats = quant_state
+            offset, state2 = compressed_stats
+            absmax2, code2, blocksize2, _, _, _, _ = state2
+        pass
+        global XPU_STREAMS
+        device = W.device
+        device_index = device.index
+        XPU_STREAM = XPU_STREAMS[device_index]
+
+        # assert(dtype == X.dtype)
+        bout = shape[0]
+
+        if out is None:
+            out = torch_empty((1, 1, bout,), dtype = dtype, device = device)
+        # else:
+        #     assert(out.shape == (1, 1, bout,))
+        # pass
+
+        n = 1
+        m = shape[0]
+        k = shape[1]
+        lda = shape[0]
+        ldc = shape[0]
+        ldb = (hd+1)//2
+        m = ctypes_c_int32(m)
+        n = ctypes_c_int32(n)
+        k = ctypes_c_int32(k)
+        lda = ctypes_c_int32(lda)
+        ldb = ctypes_c_int32(ldb)
+        ldc = ctypes_c_int32(ldc)
+
+        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
+        with torch_gpu_device(device):
+            cdequantize_blockwise_fp32(
+                get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
+                ctypes_c_int(blocksize2), ctypes_c_int(df.numel()), XPU_STREAM,
+            )
+            df += offset
+            absmax = df
+
+            fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
+                cgemm_4bit_inference_naive_bf16
+
+            blocksize = ctypes_c_int32(blocksize)
+            fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
+               lda, ldb, ldc, blocksize, XPU_STREAM,)
+        pass
+
+        return out
+    pass
+elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
     def fast_gemv(X, W, quant_state, out = None):
         if quant_state is None: return torch_matmul(X, W, out = out)
         # For fast X @ W where seq_len == 1
@@ -342,7 +559,7 @@ if HAS_CUDA_STREAM:
         ldc = ctypes_c_int32(ldc)
 
         df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
-        with torch_cuda_device(device):
+        with torch_gpu_device(device):
             cdequantize_blockwise_fp32(
                 get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
                 ctypes_c_int(blocksize2), ctypes_c_int(df.numel()), CUDA_STREAM,
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 125bc7e..9db8abd 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -69,7 +69,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequen
 from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING
 from transformers import set_seed as transformers_set_seed
 from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
-from peft import PeftModelForCausalLM
+from peft import PeftModelForCausalLM, PeftModelForSequenceClassification
 from ..save import patch_saving_functions
 import re, os, inspect, math, sys
 import types
@@ -762,8 +762,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif self.training:
-    # elif attention_mask is None:
+    elif self.training and os.environ.get(""UNSLOTH_KEEP_PADDING"", ""0"") != '1':    
         attention_mask = None
         padding_mask = None
     else:
@@ -2079,7 +2078,8 @@ class FastLlamaModel:
         model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
 
         # Patch generate
-        if model.generate.__name__ != ""unsloth_fast_generate"":
+        is_classification =  ""Classification"" in str(type(model))
+        if not is_classification and model.generate.__name__ != ""unsloth_fast_generate"":
             model._old_generate = model.generate
             unsloth_fast_generate.__doc__ = model._old_generate.__doc__
             model.generate = types.MethodType(unsloth_fast_generate, model)
@@ -2159,7 +2159,7 @@ class FastLlamaModel:
         if r <= 0:
             raise TypeError(f""Unsloth: Rank of {str(r)} must be larger than 0."")
 
-        if isinstance(model, PeftModelForCausalLM):
+        if isinstance(model, PeftModelForCausalLM) or isinstance(model, PeftModelForSequenceClassification):
             # Check if exactly the same and then pass through!
             assert(hasattr(model, ""peft_config""))
 
@@ -2428,7 +2428,7 @@ class FastLlamaModel:
 
         is_classification =  ""Classification"" in str(type(model))
         # Get LoRA
-        # if not is_classification else TaskType.SEQ_CLS
+        # 
 
         arguments = dict(
             r                   = r,
@@ -2436,7 +2436,7 @@ class FastLlamaModel:
             target_modules      = final_modules,
             lora_dropout        = lora_dropout,
             bias                = bias,
-            task_type           = TaskType.CAUSAL_LM,
+            task_type           = TaskType.CAUSAL_LM if not is_classification else TaskType.SEQ_CLS,
             layers_to_transform = layers_to_transform,
             init_lora_weights   = init_lora_weights,
             loftq_config        = loftq_config,
@@ -2450,7 +2450,6 @@ class FastLlamaModel:
         _saved_temp_tokenizer = model._saved_temp_tokenizer
 
         lora_config = LoraConfig(**arguments)
-
         # First offload lm_head and embed_tokens to disk
         input_embeddings_device  = model.get_input_embeddings().weight.device
         if is_classification:
@@ -2572,7 +2571,7 @@ class FastLlamaModel:
                 use_gradient_checkpointing = use_gradient_checkpointing,
             )
         pass
-        if not isinstance(model, PeftModelForCausalLM):
+        if not isinstance(model, PeftModelForCausalLM) and not isinstance(model, PeftModelForSequenceClassification):
             raise TypeError(
                 ""Unsloth: Your model needs to call `.get_peft_model` first!""
             )
"
"diff --git a/README.md b/README.md
index 1f85647..e6098cb 100644
--- a/README.md
+++ b/README.md
@@ -115,7 +115,7 @@ See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip
 7. **Install Unsloth:**
    
 ```python
-pip install ""unsloth[windows] @ git+https://github.com/unslothai/unsloth.git""
+pip install unsloth
 ```
 
 #### Notes
diff --git a/pyproject.toml b/pyproject.toml
index 667901e..7b1d2ef 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,14 +33,11 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 triton = [
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'""
+    ""triton-windows ; platform_system == 'Windows'"",
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.9"",
+    ""unsloth_zoo>=2025.3.11"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 9bcdd5c..7ffddde 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.9""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.11""):
         print(
             ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
             ""To disable this, set os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'""
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 2c2e361..c10b264 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1512,10 +1512,7 @@ def get_ollama_eos_tokens(tokenizer, extra_eos_tokens = []):
 
     # Remove duplicates
     splitted = joined_text.split(""\x01\x00"")
-    final_eos_tokens = []
-    for old, new in zip(added_tokens_decoder, splitted):
-        if old == new: final_eos_tokens.append(old)
-    pass
+    final_eos_tokens = [old for old, new in zip(added_tokens_decoder, splitted) if old == new]
     final_eos_tokens += extra_eos_tokens
     final_eos_tokens += repeatted_tokens
 
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 006dfff..834a74c 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -37,12 +37,12 @@ def _cross_entropy_forward(
     loss_ptr          ,
     logsumexp_ptr     ,
     labels_ptr        ,
-    VOCAB_SIZE        ,
+    VOCAB_SIZE        : tl.constexpr,
     BLOCK_SIZE        : tl.constexpr,
-    DO_SOFTCAPPING    ,
-    SOFTCAP           ,
-    DO_LOGIT_SCALING  ,
-    LOGIT_SCALE       ,
+    DO_SOFTCAPPING    : tl.constexpr,
+    SOFTCAP           : tl.constexpr,
+    DO_LOGIT_SCALING  : tl.constexpr,
+    LOGIT_SCALE       : tl.constexpr,
 ):
     """"""
         Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]
@@ -111,13 +111,13 @@ def _chunked_cross_entropy_forward(
     loss_ptr          ,
     logsumexp_ptr     ,
     labels_ptr        ,
-    VOCAB_SIZE        ,
-    N_CHUNKS          ,
+    VOCAB_SIZE        : tl.constexpr,
+    N_CHUNKS          : tl.constexpr,
     BLOCK_SIZE        : tl.constexpr,
-    DO_SOFTCAPPING    ,
-    SOFTCAP           ,
-    DO_LOGIT_SCALING  ,
-    LOGIT_SCALE       ,
+    DO_SOFTCAPPING    : tl.constexpr,
+    SOFTCAP           : tl.constexpr,
+    DO_LOGIT_SCALING  : tl.constexpr,
+    LOGIT_SCALE       : tl.constexpr,
 ):
     """"""
         256K vocab divided in 4 chunks
@@ -196,12 +196,12 @@ def _cross_entropy_backward(
     dloss_row_stride  ,
     logsumexp_ptr     ,
     labels_ptr        ,
-    VOCAB_SIZE        ,
+    VOCAB_SIZE        : tl.constexpr,
     BLOCK_SIZE        : tl.constexpr,
-    DO_SOFTCAPPING    ,
-    SOFTCAP           ,
-    DO_LOGIT_SCALING  ,
-    LOGIT_SCALE       ,
+    DO_SOFTCAPPING    : tl.constexpr,
+    SOFTCAP           : tl.constexpr,
+    DO_LOGIT_SCALING  : tl.constexpr,
+    LOGIT_SCALE       : tl.constexpr,
 ):
     """"""
         CE_i = -y log(P) = y * (log[sum(exp(x))] - x)
diff --git a/unsloth/kernels/layernorm.py b/unsloth/kernels/layernorm.py
index 26a77f0..ed81820 100644
--- a/unsloth/kernels/layernorm.py
+++ b/unsloth/kernels/layernorm.py
@@ -30,7 +30,8 @@ def layernorm_forward(
     b,
     r,
     mu,
-    n_cols, eps,
+    n_cols : tl.constexpr,
+    eps : tl.constexpr,
     BLOCK_SIZE : tl.constexpr
 ):
     row_idx = tl.program_id(0)
@@ -68,7 +69,8 @@ def layernorm_backward(
     b,
     r,
     mu,
-    n_cols, eps,
+    n_cols : tl.constexpr,
+    eps : tl.constexpr,
     BLOCK_SIZE : tl.constexpr
 ):
     # Approximately follows https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 1cde638..8f54e74 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -22,9 +22,10 @@ def _rms_layernorm_forward(
     Y, Y_row_stride,
     X, X_row_stride,
     W, W_row_stride,
-    r, r_row_stride,
-    n_cols, eps,
-    BLOCK_SIZE : tl.constexpr
+    r, r_row_stride : tl.constexpr,
+    n_cols     : tl.constexpr,
+    eps        : tl.constexpr,
+    BLOCK_SIZE : tl.constexpr,
 ):
     """"""
         Fast RMS Layernorm kernel
@@ -57,9 +58,10 @@ def _rms_layernorm_backward(
     dX, dX_row_stride,
     X,   X_row_stride,
     W,   W_row_stride,
-    r,   r_row_stride,
+    r,   r_row_stride : tl.constexpr,
     # dW, dW_row_stride,
-    n_cols, eps,
+    n_cols     : tl.constexpr,
+    eps        : tl.constexpr,
     GEMMA      : tl.constexpr,
     BLOCK_SIZE : tl.constexpr,
 ):
@@ -107,8 +109,9 @@ def _gemma_rms_layernorm_forward(
     Y, Y_row_stride,
     X, X_row_stride,
     W, W_row_stride,
-    r, r_row_stride,
-    n_cols, eps,
+    r, r_row_stride : tl.constexpr,
+    n_cols     : tl.constexpr,
+    eps        : tl.constexpr,
     BLOCK_SIZE : tl.constexpr,
 ):
     # Copies https://github.com/google-deepmind/gemma/blob/main/gemma/layers.py#L31
@@ -253,7 +256,6 @@ def unpatch_rms_layernorm():
     except:
         pass
     return
-    return
 pass
 
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a3fc12f..06a76b1 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.10""
+__version__ = ""2025.3.11""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -72,6 +72,7 @@ from platform import system as platform_system
 platform_system = platform_system()
 import numpy as np
 import contextlib
+import re
 import warnings, subprocess, re, inspect, psutil, os, math
 from unsloth_zoo.utils import Version
 
@@ -181,6 +182,34 @@ try:
 except:
     pass
 
+# Patch get_model_param_count to record correct 4bit / 8bit
+from transformers.trainer_pt_utils import is_deepspeed_zero3_enabled
+def get_model_param_count(model, trainable_only = False):
+    """"""
+    Calculate model's total param count. If trainable_only is True then count only those requiring grads
+    """"""
+    if is_deepspeed_zero3_enabled():
+        def numel(p):
+            return p.ds_numel if hasattr(p, ""ds_numel"") else p.numel()
+    else:
+        def numel(p):
+            return p.numel()
+    s = sum(numel(p) for p in model.parameters() if not trainable_only or p.requires_grad)
+    if (not trainable_only) and \
+        hasattr(model, ""config"") and \
+        hasattr(model.config, ""quantization_config""):
+
+        billions = re.findall(r""([0-9]{1,})(?:b|B)"", model.config.name_or_path)
+        if len(billions) != 0:
+            billions = int(billions[0])
+            s = 1_000_000_000 * billions
+    pass
+    return s
+pass
+import transformers.trainer_pt_utils
+transformers.trainer_pt_utils.get_model_param_count = get_model_param_count
+import transformers.trainer
+transformers.trainer.get_model_param_count = get_model_param_count
 # =============================================
 
 # =============================================
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 7000739..893a09d 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1663,6 +1663,10 @@ class FastLlamaModel:
             if platform.system().lower() == 'windows':
                 print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
                 fast_inference = False
+            major_version, minor_version = torch.cuda.get_device_capability()
+            if major_version < 7:
+                print(""Unsloth: vLLM does not work on older GPUs - will switch to Unsloth inference!"")
+                fast_inference = False
         pass
 
         if token is None: token = get_token()
@@ -1786,6 +1790,8 @@ class FastLlamaModel:
                 attn_implementation     = ""eager"",
                 **kwargs,
             )
+            model.fast_generate = model.generate
+            model.fast_generate_batches = None
         else:
             from unsloth_zoo.vllm_utils import (
                 load_vllm,
@@ -1804,6 +1810,7 @@ class FastLlamaModel:
                 enable_lora            = True,
                 max_lora_rank          = max_lora_rank,
                 disable_log_stats      = disable_log_stats,
+                use_bitsandbytes       = load_in_4bit,
             )
             for allowed_arg in allowed_args:
                 if allowed_arg not in load_vllm_kwargs and allowed_arg in kwargs:
@@ -2651,6 +2658,19 @@ class FastLlamaModel:
             torch.cuda.empty_cache()
         pass
 
+        # Patch for fast inference
+        vllm_engine = getattr(model.model, ""vllm_engine"", None)
+        if vllm_engine is not None:
+            model.vllm_engine = model.model.vllm_engine
+            model.fast_generate = model.model.fast_generate
+            model.fast_generate_batches = model.model.fast_generate_batches
+
+            # Also saving and loading LoRA
+            from unsloth_zoo.vllm_utils import save_lora, load_lora
+            model.save_lora = functools.partial(save_lora, model)
+            model.load_lora = functools.partial(load_lora, model)
+        pass
+
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
         model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 1b54c8c..4447578 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -405,7 +405,6 @@ class FastLanguageModel(FastLlamaModel):
         if is_peft:
             # From https://github.com/huggingface/peft/issues/184
             # Now add PEFT adapters
-            model.enable_input_require_grads()
             model = PeftModel.from_pretrained(
                 model,
                 old_model_name,
@@ -498,10 +497,22 @@ class FastModel(FastBaseModel):
             raise RuntimeError(""Unsloth: Pixtral only works on transformers >= 4.49.0."" + LATEST)
         elif ""qwen2.5"" in model_name.lower() and transformers_version < Version(""4.49.0""):
             raise RuntimeError(""Unsloth: Qwen 2.5 only works on transformers >= 4.49.0."" + LATEST)
-        elif ""aya-vision"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
-            raise RuntimeError(""Unsloth: Aya Vision only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""aya-vision"" in model_name.lower():
+            # Disable compiling for now - errors out!
+            os.environ[""UNSLOTH_COMPILE_DISABLE""] = ""1""
+            if transformers_version < Version(""4.50.0.dev0""):
+                raise RuntimeError(""Unsloth: Aya Vision only works on transformers >= 4.50.0."" + NIGHTLY)
         elif ""gemma-3"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Gemma 3 only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""c4ai-command-a-03-2025"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+            raise RuntimeError(""Unsloth: Cohere's Command model only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""granite-vision"" in model_name.lower():
+            # Disable compiling for now - errors out!
+            os.environ[""UNSLOTH_COMPILE_DISABLE""] = ""1""
+            if transformers_version < Version(""4.50.0.dev0""):
+                raise RuntimeError(""Unsloth: Granite Vision only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""olmo-2"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+            raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
         pass
 
         if USE_MODELSCOPE and not os.path.exists(model_name):
@@ -668,7 +679,7 @@ class FastModel(FastBaseModel):
             use_gradient_checkpointing = use_gradient_checkpointing,
             *args, **kwargs,
         )
-        
+
         if resize_model_vocab is not None:
             model.resize_token_embeddings(resize_model_vocab)
         pass
@@ -703,7 +714,6 @@ class FastModel(FastBaseModel):
         if is_peft:
             # From https://github.com/huggingface/peft/issues/184
             # Now add PEFT adapters
-            model.enable_input_require_grads()
             model = PeftModel.from_pretrained(
                 model,
                 old_model_name,
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index cb0d73c..9af5317 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -62,6 +62,16 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/llama-2-7b-chat"",
         ""meta-llama/Llama-2-7b-chat-hf"",
     ),
+    ""unsloth/Mixtral-8x7B-v0.1-unsloth-bnb-4bit"" : (
+        ""unsloth/Mixtral-8x7B-v0.1"",
+        ""mistralai/Mixtral-8x7B-v0.1"",
+        ""unsloth/Mixtral-8x7B-v0.1-bnb-4bit"",
+    ),
+    ""unsloth/Mixtral-8x7B-Instruct-v0.1-unsloth-bnb-4bit"" : (
+        ""unsloth/Mixtral-8x7B-Instruct-v0.1"",
+        ""mistralai/Mixtral-8x7B-Instruct-v0.1"",
+        ""unsloth/Mixtral-8x7B-Instruct-v0.1-bnb-4bit"",
+    ),
     ""unsloth/codellama-7b-bnb-4bit"" : (
         ""unsloth/codellama-7b"",
         ""codellama/CodeLlama-7b-hf"",
@@ -678,6 +688,36 @@ __INT_TO_FLOAT_MAPPER = \
         ""google/gemma-3-27b-pt"",
         ""unsloth/gemma-3-27b-pt-bnb-4bit"",
     ),
+    ""unsloth/reka-flash-3-unsloth-bnb-4bit"" : (
+        ""unsloth/reka-flash-3"",
+        ""RekaAI/reka-flash-3"",
+        ""unsloth/reka-flash-3-bnb-4bit"",
+    ),
+    ""unsloth/c4ai-command-a-03-2025-unsloth-bnb-4bit"" : (
+        ""unsloth/c4ai-command-a-03-2025"",
+        ""CohereForAI/c4ai-command-a-03-2025"",
+        ""unsloth/c4ai-command-a-03-2025-bnb-4bit"",
+    ),
+    ""unsloth/aya-vision-32b-unsloth-bnb-4bit"" : (
+        ""unsloth/aya-vision-32b"",
+        ""CohereForAI/aya-vision-32b"",
+        ""unsloth/aya-vision-32b-bnb-4bit"",
+    ),
+    ""unsloth/aya-vision-8b-unsloth-bnb-4bit"" : (
+        ""unsloth/aya-vision-8b"",
+        ""CohereForAI/aya-vision-8b"",
+        ""unsloth/aya-vision-8b-bnb-4bit"",
+    ),
+    ""unsloth/granite-vision-3.2-2b-unsloth-bnb-4bit"" : (
+        ""unsloth/granite-vision-3.2-2b"",
+        ""ibm-granite/granite-vision-3.2-2b"",
+        ""unsloth/granite-vision-3.2-2b-bnb-4bit"",
+    ),
+    ""unsloth/OLMo-2-0325-32B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/OLMo-2-0325-32B-Instruct"",
+        ""allenai/OLMo-2-0325-32B-Instruct"",
+        ""unsloth/OLMo-2-0325-32B-Instruct-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 4e158f5..e412c3a 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -354,13 +354,28 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     # Check data collator if it's correct!
     if ""data_collator"" in call_args and ""train_dataset"" in call_args:
         data_collator_check = \
-        ""if isinstance(data_collator, DataCollatorForSeq2Seq) and 'labels' not in train_dataset.column_names:\n""\
-        ""    data_collator = DataCollatorForLanguageModeling(""\
-        ""tokenizer = processing_class if 'processing_class' in locals() else tokenizer, mlm = False)\n""\
-        ""elif isinstance(data_collator, DataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
-        ""    data_collator = DataCollatorForSeq2Seq(""\
-        ""tokenizer = processing_class if 'processing_class' in locals() else tokenizer)\n""
+        ""__tokenizer = processing_class if 'processing_class' in locals() else tokenizer\n""\
+        ""from unsloth_zoo.vision_utils import UnslothVisionDataCollator\n""\
+        ""if not isinstance(data_collator, UnslothVisionDataCollator):\n""\
+        ""    if isinstance(data_collator, DataCollatorForSeq2Seq) and 'labels' not in train_dataset.column_names:\n""\
+        ""        data_collator = DataCollatorForLanguageModeling(__tokenizer, mlm = False)\n""\
+        ""    elif isinstance(data_collator, DataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
+        ""        data_collator = DataCollatorForSeq2Seq(__tokenizer)\n""\
+        ""else:\n""\
+        ""    if hasattr(args, 'remove_unused_columns'): args.remove_unused_columns = False\n""\
+        ""    if hasattr(args, 'dataset_text_field'): args.dataset_text_field = ''\n""\
+        ""    if hasattr(args, 'dataset_kwargs'): args.dataset_kwargs = {'skip_prepare_dataset': True}\n""
         extra_args += data_collator_check
+
+        # Also check if .pad exists -> if not, and is VLM, then change it!
+        pad_check = \
+        ""if not isinstance(data_collator, UnslothVisionDataCollator):\n""\
+        ""    if not hasattr(__tokenizer, 'pad') and hasattr(__tokenizer, 'tokenizer'):\n""\
+        ""        if isinstance(data_collator, DataCollatorForSeq2Seq):\n""\
+        ""            data_collator = DataCollatorForSeq2Seq(__tokenizer.tokenizer)\n""\
+        ""        else:\n""\
+        ""            data_collator = DataCollatorForLanguageModeling(__tokenizer.tokenizer, mlm = False)\n""
+        extra_args += pad_check
     pass
 
     # Check NEFTune
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 7462d55..4071ef8 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -207,9 +207,12 @@ def grpo_trainer__get_per_token_logps(function_name, function):
     if  function_name != ""_get_per_token_logps"": return function
 
     def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
-        return None # Unsloth efficient GRPO
+        if os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '0':
+            return None # Unsloth efficient GRPO
+        # Otherwise, calculate normally:
         if not hasattr(self, '_autocast_dtype'):
             self._autocast_dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16
+            if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '1': self._autocast_dtype = torch.float32
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
             # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
             logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
@@ -229,12 +232,14 @@ def grpo_trainer__get_per_token_logps(function_name, function):
 pass
 RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__get_per_token_logps)
 
-grpo_compute_loss     = RL_REPLACEMENTS[""grpo_compute_loss""]
-UnslothEfficientGRPO  = RL_REPLACEMENTS[""UnslothEfficientGRPO""]
-grpo_accumulated_loss = RL_REPLACEMENTS[""grpo_accumulated_loss""]
+grpo_compute_loss      = RL_REPLACEMENTS[""grpo_compute_loss""]
+grpo_compute_loss_slow = RL_REPLACEMENTS[""grpo_compute_loss_slow""]
+UnslothEfficientGRPO   = RL_REPLACEMENTS[""UnslothEfficientGRPO""]
+grpo_accumulated_loss  = RL_REPLACEMENTS[""grpo_accumulated_loss""]
 RL_PRE_ITEMS[""grpo_trainer""].append(inspect.getsource(grpo_compute_loss))
 RL_PRE_ITEMS[""grpo_trainer""].append(inspect.getsource(UnslothEfficientGRPO))
 RL_PRE_ITEMS[""grpo_trainer""].append(inspect.getsource(grpo_accumulated_loss))
+RL_PRE_ITEMS[""grpo_trainer""].append(grpo_compute_loss_slow)
 
 # Edit _get_per_token_logps to handle mixed precision
 def grpo_trainer_compute_loss(function_name, function):
@@ -266,8 +271,8 @@ def grpo_trainer_compute_loss(function_name, function):
         # per_token_loss = -(per_token_loss - self.beta * per_token_kl)
         # loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         input_ids = input_ids[:, -logits_to_keep:]
-        if False:#per_token_logps is not None:
-            loss, completion_length, mean_kl = grpo_compute_loss(
+        if per_token_logps is not None:
+            loss, completion_length, mean_kl = grpo_compute_loss_slow(
                 ref_per_token_logps, per_token_logps, input_ids, completion_mask, self.beta, advantages,
             )
         else:
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 2ef9d2e..31733c2 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -66,8 +66,17 @@ __all__ = [
 ]
 
 global FORCE_FLOAT32
-FORCE_FLOAT32 = [""gemma3""]
+FORCE_FLOAT32 = [
+    ""gemma3"",
+]
+
+global FORCE_EAGER_ATTENTION
+FORCE_EAGER_ATTENTION = [
+    ""pixtral"",    # Pixtral SDPA not implemented
+]
 
+global NUM_LOGITS_TO_KEEP
+NUM_LOGITS_TO_KEEP = dict()
 
 def unsloth_base_fast_generate(
     self,
@@ -78,21 +87,45 @@ def unsloth_base_fast_generate(
     dtype = _get_dtype(self.config.torch_dtype)
 
     # Check if VLM
-    is_vlm = (
+    is_vlm = any(
         x.endswith((""ForConditionalGeneration"", ""ForVisionText2Text""))
         for x in self.config.architectures
     )
     is_vlm = is_vlm or hasattr(self.config, ""vision_config"")
+    arch = self.config.architectures[0]
 
     # Remove token_type_ids
     kwargs.pop(""token_type_ids"", None)
 
     # VLMs do not allow logits_to_keep
     if not is_vlm:
-        kwargs[""logits_to_keep""] = 1
+        global NUM_LOGITS_TO_KEEP
+        if arch not in NUM_LOGITS_TO_KEEP:
+            m = self
+            # Find which is needed ie
+            # num_logits_to_keep or logits_to_keep
+            while hasattr(m, ""model""):
+                if hasattr(m, ""forward""):
+                    keys = inspect.signature(m.forward).parameters.keys()
+                    if ""num_logits_to_keep"" in keys:
+                        NUM_LOGITS_TO_KEEP[arch] = ""num_logits_to_keep""
+                        break
+                    elif ""logits_to_keep"" in keys:
+                        NUM_LOGITS_TO_KEEP[arch] = ""logits_to_keep""
+                        break
+                m = m.model
+            pass
+            if arch not in NUM_LOGITS_TO_KEEP:
+                NUM_LOGITS_TO_KEEP[arch] = None
+            pass
+        pass
+        key = NUM_LOGITS_TO_KEEP[arch]
+        if key is not None and key not in kwargs:
+            kwargs[key] = 1
     else:
-        kwargs.pop(""logits_to_keep"", None)
-        kwargs.pop(""num_logits_to_keep"", None)
+        pass
+        # kwargs.pop(""logits_to_keep"", None)
+        # kwargs.pop(""num_logits_to_keep"", None)
 
     # Check pad_token
     model_eos_token_id = getattr(self.config, ""eos_token_id"", None)
@@ -186,13 +219,27 @@ class FastBaseModel:
         os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""0""
         bnb_compute_dtype = dtype
         for disable_name in FORCE_FLOAT32:
-            if disable_name.lower() == model_type_arch.lower() and dtype == torch.float16:
+            if (disable_name.lower() == model_type_arch.lower() or \
+                disable_name.lower() in model_name.lower()) and \
+                dtype == torch.float16:
+
                 print(f""Unsloth: Using float16 precision for {model_type_arch} won't work! Using float32."")
                 os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""1""
                 bnb_compute_dtype = torch.float32
                 break
         pass
 
+        global FORCE_EAGER_ATTENTION
+        attn_implementation = ""sdpa""
+        for disable_name in FORCE_EAGER_ATTENTION:
+            if (disable_name.lower() == model_type_arch.lower() or \
+                disable_name.lower() in model_name.lower()):
+
+                print(f""Unsloth: {model_type_arch} does not support SDPA - switching to eager!"")
+                attn_implementation = ""eager""
+                break
+        pass
+
         bnb_config = None
         if full_finetuning and (load_in_4bit or load_in_8bit):
             print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
@@ -249,7 +296,7 @@ class FastBaseModel:
             # quantization_config   = bnb_config,
             token                   = token,
             trust_remote_code       = trust_remote_code,
-            attn_implementation     = ""sdpa"", #[TODO] Pixtral for eg fails
+            attn_implementation     = attn_implementation,
             **kwargs,
         )
         # Return old flag
@@ -263,10 +310,20 @@ class FastBaseModel:
             padding_side = ""right"",
             token        = token,
         )
-        # Add padding side as well
         if hasattr(tokenizer, ""tokenizer""):
-            tokenizer.tokenizer.padding_side = ""right""
-
+            __tokenizer = tokenizer.tokenizer
+            # Add padding side as well
+            __tokenizer.padding_side = ""right""
+            # Check bos, eos, pad, unk tokens
+            tokens = [""bos_token"", ""eos_token"", ""pad_token"", ""unk_token""]
+            for token in tokens:
+                if hasattr(__tokenizer, token) and not hasattr(tokenizer, token):
+                    _args = {""__tokenizer"" : __tokenizer, ""tokenizer"" : tokenizer}
+                    exec(f""tokenizer.{token} = __tokenizer.{token}"", _args)
+                    exec(f""tokenizer.{token}_id = __tokenizer.{token}_id"", _args)
+                pass
+            pass
+        pass
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = post_patch_loss_function(model)
         # Fix other stuff like BnB compute data types
diff --git a/unsloth/save.py b/unsloth/save.py
index d03f47e..4b2c012 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2219,6 +2219,10 @@ pass
 
 from .models.loader_utils import get_model_name
 from unsloth_zoo.saving_utils import merge_and_overwrite_lora
+from unsloth_zoo.llama_cpp import (
+    install_llama_cpp,
+    convert_to_gguf,
+)
 
 @torch.inference_mode
 def unsloth_generic_save(
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 1b59e37..70599b8 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1842,10 +1842,10 @@ def train_on_responses_only(
         return { ""labels"" : all_labels }
     pass
 
-    if hasattr(trainer, ""train_dataset""):
+    if hasattr(trainer, ""train_dataset"") and trainer.train_dataset is not None:
         trainer.train_dataset = trainer.train_dataset.map(_train_on_responses_only, batched = True)
-    if hasattr(trainer, ""eval_dataset""):
-        trainer.eval_dataset  = trainer.eval_dataset .map(_train_on_responses_only, batched = True)
+    if hasattr(trainer, ""eval_dataset"") and trainer.eval_dataset is not None:
+        trainer.eval_dataset = trainer.eval_dataset.map(_train_on_responses_only, batched = True)
     return trainer
 pass
 
"
"diff --git a/README.md b/README.md
index cf1568f..69c8ddc 100644
--- a/README.md
+++ b/README.md
@@ -1,26 +1,27 @@
 <div class=""align-center"">
   <img src=""./images/unsloth new logo.png"" width=""350"" />
   <a href=""https://discord.gg/u54VK8m8tk""><img src=""./images/Discord.png"" width=""160""></a>
-  <a href=""https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing""><img src=""./images/try live demo green.png"" width=""130""></a>
+  <a href=""https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing""><img src=""./images/try live demo green.png"" width=""130""></a>
 </div>
 
 ## 2-5x faster 60% less memory local QLoRA finetuning
 | Llama 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |
 |-----------------------------|-----------------------------|-------------------------|------------------------|
 | **2.2x faster, -43%  VRAM**     | **2.2x faster, -62%  VRAM**     | **1.9x faster, -27% VRAM**  | **5.5x faster, -44% VRAM** |
-| [Colab Alpaca example + inference, saving](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing) | [Colab T4 example + inference, saving](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [A100 example](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Kaggle Alpaca example](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) |
-| [Colab A100 example](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | (59 more examples if you scroll down) | [Kaggle Slim Orca](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |
+| [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing) | [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [Kaggle Alpaca example](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) |
+| [Colab A100 example](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | (59 more examples if you scroll down) | [Kaggle Slim Orca example](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |
 
-* Supports Llama (7, 13, 70b), Yi (6, 34b), Mistral (7b), Tinyllama, CodeLlama (7, 13, 34b), and all Llama / Mistral derived architectures!
-* All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language.
+* Supports Llama, Yi, Mistral, CodeLlama, and their derived models (Open Hermes etc).
+* All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language. **Manual backpropagation engine**.
 * **0% loss in accuracy** - no approximation methods - all exact.
 * No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)
 * **NEW!** Works on **Linux** and **Windows** via WSL.
-* **NEW!** Experimental support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)!
+* **NEW!** Support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290), PPO and Reward Modelling via [TRL](https://huggingface.co/docs/trl/dpo_trainer).
+* **NEW!** Download 4 bit models 4x faster directly from Huggingface!
 * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
-* Open source version trains 5x faster or you can check out [Unsloth Pro and Max](https://unsloth.ai/) codepaths for **30x faster training**!
+* Open source version trains 5x faster - check out [Unsloth Max](https://unsloth.ai/) for **30x faster training**!
 
-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
+| 1 A100 40GB | Huggingface | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
 |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
 | Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |
 | LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |
@@ -28,11 +29,11 @@
 | Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |
 
 Join our [Discord](https://discord.gg/nsS4V5Z6ge)!
-If you trained a model with Unsloth, we made a cool sticker!!
+If you trained a model with Unsloth, we made a cool sticker if you want to use it!
 <img src=""./images/unsloth made with love.png"" width=""200"" />
 
 # Installation Instructions - Conda
-Unsloth currently only supports Linux distros and Pytorch == 2.1.
+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.
 ```bash
 conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \
   -c pytorch -c nvidia -c xformers -c conda-forge -y
@@ -40,25 +41,36 @@ pip install ""unsloth[conda] @ git+https://github.com/unslothai/unsloth.git""
 ```
 
 # Installation Instructions - Pip
+Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.
+
 1. Find your CUDA version via
 ```python
 import torch; torch.version.cuda
 ```
-2. We only support Pytorch 2.1 (2.1.1 bugs out for now): You can update Pytorch via Pip (interchange cu121 / cu118)
+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `""ampere""` path.
 ```bash
 pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \
   --index-url https://download.pytorch.org/whl/cu121
 ```
-2. Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the ""ampere"" path.
 ```bash
 pip install ""unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.git""
 ```
-Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.
-
-4. If you get errors, try the below first, then go back to step 1:
+3. For Pytorch 2.1.1: Use the `""ampere""` path for newer RTX 30xx GPUs or higher.
+```bash
+pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton \
+  --index-url https://download.pytorch.org/whl/cu121
+```
+```bash
+pip install ""unsloth[cu118_torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git""
+```
+4. We're working on Pytorch 2.1.2 support.
+5. If you get errors, try the below first, then go back to step 1:
 ```bash
 pip install --upgrade pip
 ```
@@ -120,10 +132,8 @@ trainer = SFTTrainer(
 trainer.train()
 ```
 
-# DPO (Direct Preference Optimization) Experimental support
-[152334H](https://github.com/152334H) hacked Unsloth to work with DPO via TRL!
-1. Hack the model's `config.json` to be llama model. [Example gist](https://gist.github.com/152334H/d8a68b51b83bac008a02e69ecc81d5c1).
-2. Use Unsloth for DPO for both base and reference models. [Example gist](https://gist.github.com/152334H/4847f3a8cca12894877e6b30698b0b64).
+# DPO (Direct Preference Optimization) Support
+DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory).
 
 # Future Milestones and limitations
 1. Support Mixtral.
@@ -173,6 +183,40 @@ Two Tesla T4s on Kaggle
 
 * Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.
 
+# Llama-Factory 3rd party benchmarking
+
+| Method | Bits | TGS | GRAM | Speed |
+| --- | --- | --- | --- | --- |
+| HF | 16 | 2392 | 18GB | 100% |
+| HF+FA2 | 16 | 2954 | 17GB | 123% |
+| Unsloth+FA2 | 16 | 4007 | 16GB | **168%** |
+| HF | 4 | 2415 | 9GB | 101% |
+| Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |
+
+[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.
+
+# How did we make it faster?
+Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!
+
+$$
+\begin{align}
+y &= \frac{x_i}{\sqrt{\frac{1}{n}\sum{x_i^2}+\epsilon}} \cdot w \\
+r &= \frac{1}{\sqrt{\frac{1}{n}\sum{x_i^2}+\epsilon}} \\
+\frac{dC}{dX} &= \frac{1}{n} r \bigg( n (dY \cdot w) - \bigg( x_i \cdot r \cdot \sum{dY \cdot y_i }  \bigg) \bigg)
+\end{align}
+$$
+
+
+# Troubleshooting
+1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:
+```bash
+!ldconfig /usr/lib64-nvidia
+```
+2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.
+
+3. If it doesn't install - maybe try updating `pip`.
+
+
 # Full benchmarking tables
 Click  ""Code"" for a fully reproducible example.
 ""Unsloth Equal"" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.
@@ -312,27 +356,6 @@ Click  ""Code"" for a fully reproducible example.
 | memory MB| OOM  | OOM  | 7594 | 8881 | | |
 | % saved | OOM  | OOM  |       |       |  | |
 
-# How did we make it faster?
-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!
-
-$$
-\begin{align}
-y &= \frac{x_i}{\sqrt{\frac{1}{n}\sum{x_i^2}+\epsilon}} \cdot w \\
-r &= \frac{1}{\sqrt{\frac{1}{n}\sum{x_i^2}+\epsilon}} \\
-\frac{dC}{dX} &= \frac{1}{n} r \bigg( n (dY \cdot w) - \bigg( x_i \cdot r \cdot \sum{dY \cdot y_i }  \bigg) \bigg)
-\end{align}
-$$
-
-
-# Troubleshooting
-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:
-```bash
-!ldconfig /usr/lib64-nvidia
-```
-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.
-
-3. If it doesn't install - maybe try updating `pip`.
-
 # Credits
 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
 2. [152334H](https://github.com/152334H) for experimental DPO support
diff --git a/pyproject.toml b/pyproject.toml
index ab710de..2bceca5 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -5,21 +5,21 @@ build-backend = ""setuptools.build_meta""
 [project]
 name = ""unsloth""
 dynamic = [""version""]
-description = ""2X faster LLM finetuning""
+description = ""2-5X faster LLM finetuning""
 readme = ""README.md""
 requires-python = "">=3.9""
 license = {file = ""LICENSE""}
 keywords = [""ai"", ""llm"",]
 authors = [
-	{email = ""info@unsloth.ai""},
-	{name = ""Unsloth AI team""},
+    {email = ""info@unsloth.ai""},
+    {name = ""Unsloth AI team""},
 ]
 maintainers = [
- 	{name = ""Daniel Han"", email = ""danielhanchen@gmail.com""},
- 	{name = ""Michael Han"", email = ""info@unsloth.ai""},
+    {name = ""Daniel Han"", email = ""danielhanchen@gmail.com""},
+    {name = ""Michael Han"", email = ""info@unsloth.ai""},
 ]
 classifiers = [
- 	""Programming Language :: Python"",
+    ""Programming Language :: Python"",
 ]
 
 [tool.setuptools.dynamic]
@@ -40,54 +40,84 @@ huggingface = [
     ""trl"",
     ""peft"",
     ""packaging"",
+    ""ninja"",
 ]
 cu118only = [
-	""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-	""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-	""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu121only = [
-	""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-	""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-	""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+]
+cu118only_torch211 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+]
+cu121only_torch211 = [
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu118 = [
-	""unsloth[huggingface]"",
-	""bitsandbytes"",
-	""unsloth[cu118only]"",
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118only]"",
 ]
 cu121 = [
-	""unsloth[huggingface]"",
-	""bitsandbytes"",
-	""unsloth[cu121only]"",
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121only]"",
+]
+cu118_torch211 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118only_torch211]"",
+]
+cu121_torch211 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121only_torch211]"",
 ]
 kaggle = [
-	""unsloth[huggingface]"",
+    ""unsloth[huggingface]"",
 ]
 conda = [
-	""unsloth[huggingface]"",
+    ""unsloth[huggingface]"",
 ]
 colab = [
-	""unsloth[cu121]"",
+    ""unsloth[cu121]"",
+]
+colab_ampere = [
+    ""unsloth[cu121]"",
+    ""flash-attn"",
 ]
 cu118_ampere = [
-	""unsloth[huggingface]"",
-	""bitsandbytes"",
-	""unsloth[cu118only]"",
-	""ninja"",
-	""flash-attn"",
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118only]"",
+    ""flash-attn"",
 ]
 cu121_ampere = [
-	""unsloth[huggingface]"",
-	""bitsandbytes"",
-	""unsloth[cu121only]"",
-	""ninja"",
-	""flash-attn"",
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121only]"",
+    ""flash-attn"",
 ]
-colab_ampere = [
-	""unsloth[cu121]"",
-	""ninja"",
-	""flash-attn"",
+cu118_ampere_torch211 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118only_torch211]"",
+    ""flash-attn"",
+]
+cu121_ampere_torch211 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121only_torch211]"",
+    ""flash-attn"",
 ]
 
 [project.urls]
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 5b0b8d3..28c7aff 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -11,7 +11,7 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-__version__ = ""2023.12""
+__version__ = ""2024.1""
 import os
 import warnings
 import importlib
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index cbbb6b7..f6cc078 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -369,7 +369,8 @@ def LlamaModel_fast_forward(
         raise ValueError(""Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds"")
 
     seq_length_with_past = seq_length
-    assert(seq_length <= self.max_seq_length)
+    if hasattr(self, ""max_seq_length""):
+        assert(seq_length <= self.max_seq_length)
     past_key_values_length = 0
 
     if past_key_values is not None:
@@ -690,7 +691,14 @@ class FastLlamaModel:
             layer.self_attn.apply_o   = original_apply_o
         pass
 
+        # Save max_seq_length
         model.max_seq_length = max_position_embeddings
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            internal_model.max_seq_length = max_position_embeddings
+            internal_model = internal_model.model
+        pass
+        internal_model.max_seq_length = max_position_embeddings
         return model, tokenizer
     pass
 
@@ -757,9 +765,9 @@ class FastLlamaModel:
         assert(max_seq_length <= model.max_seq_length)
 
         if lora_dropout != 0:
-            raise TypeError(""Unsloth: Fast Llama patching only works with dropout = 0."")
+            raise TypeError(""Unsloth: Fast model patching only works with dropout = 0."")
         if bias != ""none"":
-            raise TypeError(""Unsloth: Fast Llama patching only works with bias = 'none'."")
+            raise TypeError(""Unsloth: Fast model patching only works with bias = 'none'."")
 
         transformers_set_seed(random_state)
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index d458626..421b743 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -15,6 +15,21 @@
 from .llama import FastLlamaModel, logger
 from .mistral import FastMistralModel
 from transformers import AutoConfig
+from transformers import __version__ as transformers_version
+
+FOURBIT_MAPPER = \
+{
+    ""unsloth/mistral-7b-bnb-4bit""    : ""unsloth/mistral-7b"",
+    ""unsloth/llama-2-7b-bnb-4bit""    : ""unsloth/llama-2-7b"",
+    ""unsloth/llama-2-13b-bnb-4bit""   : ""unsloth/llama-13-7b"",
+    ""unsloth/codellama-34b-bnb-4bit"" : ""codellama/CodeLlama-34b-hf"",
+}
+
+# https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
+major, minor = transformers_version.split(""."")[:2]
+major, minor = int(major), int(minor)
+SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)
+del major, minor
 
 
 class FastLanguageModel(FastLlamaModel):
@@ -29,36 +44,37 @@ class FastLanguageModel(FastLlamaModel):
         rope_scaling = None,
         *args, **kwargs,
     ):
+        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:
+            model_name = FOURBIT_MAPPER[model_name]
+            logger.warning_once(
+                f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
+                f""4bit loading.\nThe minimum required version is 4.37.\n""\
+                f'Try `pip install ""git+https://github.com/huggingface/transformers.git""`\n'\
+                f""to obtain the latest transformers build, then restart this session.\n""\
+                f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
+            )
+        pass
+
         model_config = AutoConfig.from_pretrained(model_name)
         model_type = model_config.model_type
 
-        if model_type == ""llama"":
-            return FastLlamaModel.from_pretrained(
-                model_name = model_name,
-                max_seq_length = max_seq_length,
-                dtype = dtype,
-                load_in_4bit = load_in_4bit,
-                token = token,
-                device_map = device_map,
-                rope_scaling = rope_scaling,
-                *args, **kwargs,
-            )
-        elif model_type == ""mistral"":
-            if rope_scaling is not None:
-                logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
-            return FastMistralModel.from_pretrained(
-                model_name = model_name,
-                max_seq_length = max_seq_length,
-                dtype = dtype,
-                load_in_4bit = load_in_4bit,
-                token = token,
-                device_map = device_map,
-                *args, **kwargs,
-            )
+        if   model_type == ""llama"":   dispatch_model = FastLlamaModel
+        elif model_type == ""mistral"": dispatch_model = FastMistralModel
         else:
             raise NotImplementedError(
                 f""Unsloth: {model_name} not supported yet!\n""\
                 ""Make an issue to https://github.com/unslothai/unsloth!"",
             )
+
+        return dispatch_model.from_pretrained(
+            model_name = model_name,
+            max_seq_length = max_seq_length,
+            dtype = dtype,
+            load_in_4bit = load_in_4bit,
+            token = token,
+            device_map = device_map,
+            rope_scaling = rope_scaling,
+            *args, **kwargs,
+        )
     pass
 pass
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 826ec47..e4bac44 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -243,7 +243,7 @@ class FastMistralModel(FastLlamaModel):
         MistralDecoderLayer   .forward = LlamaDecoderLayer_fast_forward
         MistralModel          .forward = LlamaModel_fast_forward
         MistralForCausalLM    .forward = MistralForCausalLM_fast_forward
-        PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+        PeftModelForCausalLM  .forward = PeftModelForCausalLM_fast_forward
         return
     pass
 
@@ -256,8 +256,11 @@ class FastMistralModel(FastLlamaModel):
         load_in_4bit = True,
         token = None,
         device_map = ""sequential"",
-        # rope_scaling = None, Mistral does not support RoPE scaling
-    ):
+        rope_scaling = None, # Mistral does not support RoPE scaling
+    ): 
+        if rope_scaling is not None:
+            logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
+
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
@@ -312,7 +315,15 @@ class FastMistralModel(FastLlamaModel):
             layer.self_attn.apply_o   = original_apply_o
         pass
 
-        model.max_seq_length = max(max_seq_length, model.config.max_position_embeddings)
+        # Save max_seq_length
+        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)
+        model.max_seq_length = max_position_embeddings
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            internal_model.max_seq_length = max_position_embeddings
+            internal_model = internal_model.model
+        pass
+        internal_model.max_seq_length = max_position_embeddings
         return model, tokenizer
     pass
 pass
"
"diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 72619cf..bc01c28 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -432,21 +432,25 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Qwen2.5-Coder-32B-Instruct"",
         ""Qwen/Qwen2.5-Coder-32B-Instruct"",
     ),
-    ""unsloth/Llama-3.2-1B-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-1B-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-1B"",
         ""meta-llama/Llama-3.2-1B"",
+        ""unsloth/Llama-3.2-1B-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-3B-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-3B-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-3B"",
         ""meta-llama/Llama-3.2-3B"",
+        ""unsloth/Llama-3.2-3B-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-1B-Instruct-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-1B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-1B-Instruct"",
         ""meta-llama/Llama-3.2-1B-Instruct"",
+        ""unsloth/Llama-3.2-1B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-3B-Instruct-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-3B-Instruct"",
         ""meta-llama/Llama-3.2-3B-Instruct"",
+        ""unsloth/Llama-3.2-3B-Instruct-bnb-4bit"",
     ),
     ""unsloth/Llama-3.1-Nemotron-70B-Instruct-bnb-4bit"" : (
         ""unsloth/Llama-3.1-Nemotron-70B-Instruct"",
@@ -550,6 +554,31 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/DeepSeek-R1-Distill-Llama-70B"",
         ""deepseek-ai/DeepSeek-R1-Distill-Llama-70B"",
     ),
+    ""unsloth/Mistral-Small-24B-Base-2501-unsloth-bnb-4bit"" : (
+        ""unsloth/Mistral-Small-24B-Base"",
+        ""mistralai/Mistral-Small-24B-Base-2501"",
+        ""unsloth/Mistral-Small-24B-Base-2501-bnb-4bit"",
+    ),
+    ""unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit"" : (
+        ""unsloth/Mistral-Small-24B-Instruct"",
+        ""mistralai/Mistral-Small-24B-Instruct-2501"",
+        ""unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit"",
+    ),
+    ""unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-VL-3B-Instruct"",
+        ""Qwen/Qwen2.5-VL-3B-Instruct"",
+        ""unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit"",
+    ),
+    ""unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-VL-7B-Instruct"",
+        ""Qwen/Qwen2.5-VL-7B-Instruct"",
+        ""unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"",
+    ),
+    ""unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-VL-72B-Instruct"",
+        ""Qwen/Qwen2.5-VL-72B-Instruct"",
+        ""unsloth/Qwen2.5-VL-72B-Instruct-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index a881146..acfd012 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -446,7 +446,9 @@ class FastVisionModel(FastBaseVisionModel):
 
         if not was_disabled: enable_progress_bars()
 
-        with contextlib.redirect_stdout(open(os.devnull, ""w"")):
+        with contextlib.redirect_stdout(
+            open(os.devnull, ""w"") if os.environ.get(""UNSLOTH_DISABLE_LOGGER"", ""0"") != ""1"" else sys.stdout
+        ):
             patch_loss_functions(torch_compile = False)
             model_types = unsloth_compile_transformers(
                 model_name              = model_name,
"
"diff --git a/README.md b/README.md
index 0a3c83f..4d68d99 100644
--- a/README.md
+++ b/README.md
@@ -299,6 +299,9 @@ DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as
 We're in Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
 
 ```python
+import os
+os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"" # Optional set GPU device ID
+
 from unsloth import FastLanguageModel, PatchDPOTrainer
 from unsloth import is_bfloat16_supported
 PatchDPOTrainer()
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index f4268e3..88a9e96 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -70,7 +70,7 @@ __all__ = [
     ""platform_system"",
     ""patch_tokenizer"",
     ""get_statistics"",
-    ""Offloaded_Gradient_Checkpointer"",
+    ""Unsloth_Offloaded_Gradient_Checkpointer"",
 ]
 
 
@@ -103,7 +103,7 @@ def prepare_model_for_kbit_training(
     pass
 
     # Gradient checkpointing!
-    if use_gradient_checkpointing == ""offloaded"":
+    if use_gradient_checkpointing == ""unsloth"":
 
         # Saves VRAM!
         original_model = model
@@ -309,11 +309,10 @@ def prepare_n_gradient_checkpoints(
 pass
 
 
-class Offloaded_Gradient_Checkpointer(torch.autograd.Function):
+class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     """"""
     Saves VRAM by smartly offloading to RAM.
     Tiny hit to performance, since we mask the movement via non blocking calls.
-    [TODO] Load the backward pass earlier
     """"""
     @staticmethod
     @torch.cuda.amp.custom_fwd
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f39d34f..876ccb2 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -647,7 +647,7 @@ def LlamaModel_fast_forward(
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
         if offloaded_gradient_checkpointing:
-            hidden_states = Offloaded_Gradient_Checkpointer.apply(
+            hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(
                 decoder_layer,
                 hidden_states,
                 causal_mask,
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index dc54bfd..90208c5 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -93,7 +93,27 @@ __INT_TO_FLOAT_MAPPER = \
     ""unsloth/mistral-7b-v0.2-bnb-4bit"" : (
         ""unsloth/mistral-7b-v0.2"",
         ""alpindale/Mistral-7B-v0.2-hf"",
-    )
+    ),
+    ""unsloth/gemma-1.1-2b-it-bnb-4bit"" : (
+        ""unsloth/gemma-1.1-2b-it"",
+        ""google/gemma-1.1-2b-it"",
+    ),
+    ""unsloth/gemma-1.1-7b-it-bnb-4bit"" : (
+        ""unsloth/gemma-1.1-7b-it"",
+        ""google/gemma-1.1-7b-it"",
+    ),
+    ""unsloth/Starling-LM-7B-beta-bnb-4bit"" : (
+        ""unsloth/Starling-LM-7B-beta"",
+        ""Nexusflow/Starling-LM-7B-beta"",
+    ),
+    ""unsloth/Hermes-2-Pro-Mistral-7B-bnb-4bit"" : (
+        ""unsloth/Hermes-2-Pro-Mistral-7B"",
+        ""NousResearch/Hermes-2-Pro-Mistral-7B"",
+    ),
+    ""unsloth/OpenHermes-2.5-Mistral-7B-bnb-4bit"" : (
+        ""unsloth/OpenHermes-2.5-Mistral-7B"",
+        ""teknium/OpenHermes-2.5-Mistral-7B"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
diff --git a/unsloth/save.py b/unsloth/save.py
index 636a3d8..d1cd7d6 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -183,6 +183,9 @@ def unsloth_save_model(
 ):
     if token is None and ""HF_TOKEN"" in os.environ:
         token = os.environ[""HF_TOKEN""]
+    
+    if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
+        token = os.environ[""HUGGINGFACE_TOKEN""]
 
     if commit_message is None: commit_message = """"
     if ""Unsloth"" not in commit_message:
@@ -522,7 +525,11 @@ def unsloth_save_model(
 
     state_dict[""model.norm.weight""] = internal_model.model.norm.weight.data
     # Check for modules_to_save float32 dtype
-    state_dict[""lm_head.weight""] = internal_model.lm_head.weight.data.to(torch_dtype)
+
+    # Check for tied weights
+    if internal_model.model.embed_tokens.weight.data_ptr() != internal_model.lm_head.weight.data_ptr():
+        state_dict[""lm_head.weight""] = internal_model.lm_head.weight.data.to(torch_dtype)
+    pass
 
     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter
     for key, value in state_dict.items():
@@ -731,9 +738,9 @@ def install_llama_cpp_old(version = -10):
     # Also don't use the GPU!
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
-        f""cd llama.cpp && git reset --hard {version} && git clean -df && ""\
-        f""make clean make all -j{psutil.cpu_count()*2}"",
-        ""pip install gguf protobuf"",
+        f""cd llama.cpp && git reset --hard {version} && git clean -df"",
+        ""make clean -C llama.cpp"",
+        f""make all -j{psutil.cpu_count()*2} -C llama.cpp"",
     ]
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
@@ -756,7 +763,8 @@ def install_llama_cpp_blocking(use_cuda = True):
 
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
-        f""cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}"",
+        ""make clean -C llama.cpp"",
+        f""{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp"",
         ""pip install gguf protobuf"",
     ]
     if os.path.exists(""llama.cpp""): return
@@ -931,15 +939,26 @@ def save_to_gguf(
 
     # Check if quantization succeeded!
     if not os.path.isfile(final_location):
-        raise RuntimeError(
-            f""Unsloth: Quantization failed for {final_location}\n""\
-            ""You might have to compile llama.cpp yourself, then run this again.\n""\
-            ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
-            ""You must run this in the same folder as you're saving your model.\n""\
-            ""git clone https://github.com/ggerganov/llama.cpp\n""\
-            ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
-            ""Once that's done, redo the quantization.""
-        )
+        if IS_KAGGLE_ENVIRONMENT:
+            raise RuntimeError(
+                f""Unsloth: Quantization failed for {final_location}\n""\
+                ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
+                ""Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\n""\
+                ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
+                ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
+                ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
+            )
+        else:
+            raise RuntimeError(
+                f""Unsloth: Quantization failed for {final_location}\n""\
+                ""You might have to compile llama.cpp yourself, then run this again.\n""\
+                ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
+                ""You must run this in the same folder as you're saving your model.\n""\
+                ""git clone https://github.com/ggerganov/llama.cpp\n""\
+                ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
+                ""Once that's done, redo the quantization.""
+            )
+        pass
     pass
     print(f""Unsloth: Conversion completed! Output location: {final_location}"")
 
@@ -961,14 +980,25 @@ def save_to_gguf(
 
         # Check if quantization succeeded!
         if not os.path.isfile(final_location):
-            raise RuntimeError(
-                ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
-                ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
-                ""You must run this in the same folder as you're saving your model.\n""\
-                ""git clone https://github.com/ggerganov/llama.cpp\n""\
-                ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
-                ""Once that's done, redo the quantization.""
-            )
+            if IS_KAGGLE_ENVIRONMENT:
+                raise RuntimeError(
+                    f""Unsloth: Quantization failed for {final_location}\n""\
+                    ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
+                    ""Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\n""\
+                    ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
+                    ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
+                    ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
+                )
+            else:
+                raise RuntimeError(
+                    ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
+                    ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
+                    ""You must run this in the same folder as you're saving your model.\n""\
+                    ""git clone https://github.com/ggerganov/llama.cpp\n""\
+                    ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
+                    ""Once that's done, redo the quantization.""
+                )
+            pass
         pass
 
         print(f""Unsloth: Conversion completed! Output location: {final_location}"")
"
"diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 9b861f4..f5db8fa 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -22,4 +22,4 @@ from .fast_lora import (
 	apply_lora_qkv,
 	apply_lora_o,
 )
-from .utils import fast_dequantize, QUANT_STATE, fast_linear_forward
+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index a952752..d7fca30 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -134,9 +134,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):
         half = Q.shape[-1]//2
         RH_Q = torch.cat((-Q[..., half:], Q[..., :half]), dim = -1)
         Q *= cos
-        Q.addcmul_(RH_Q, sin)
-        # RH_Q *= sin
-        # Q += RH_Q
+        # Q.addcmul_(RH_Q, sin)
+        RH_Q *= sin
+        Q += RH_Q
         ctx.save_for_backward(cos, sin)
         return Q
     pass
@@ -148,9 +148,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):
         half = dY.shape[-1]//2
         RH_dY = torch.cat((dY[..., half:], -dY[..., :half]), dim = -1)
         dY *= cos
-        dY.addcmul_(RH_dY, sin)
-        # RH_dY *= sin
-        # dY += RH_dY
+        # dY.addcmul_(RH_dY, sin)
+        RH_dY *= sin
+        dY += RH_dY
         return dY, None, None, None
     pass
 pass
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index e22e3a1..4f4ce41 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -114,11 +114,12 @@ def fast_dequantize(W, quant_state = None, out = None):
 pass
 
 
-def fast_gemv(X, W, quant_state, out = None, out_W = None):
-    quant_state = W.quant_state
-    bsz = 1
-    q_len = 1
-    hd = X.shape[0]
+def fast_gemv(X, W, quant_state, out = None):
+    if quant_state is None: return torch.matmul(X, W, out = out)
+    # For fast X @ W where seq_len == 1
+    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
+    bsz, q_len, hd = X.shape
+    assert(q_len == 1)
 
     if type(quant_state) is not list:
         # https://github.com/TimDettmers/bitsandbytes/pull/763/files
@@ -137,9 +138,14 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):
         offset, state2 = compressed_stats
         absmax2, code2, blocksize2, _, _, _, _ = state2
     pass
+    assert(dtype == X.dtype)
     bout = shape[0]
-    if out is None: out = torch.empty(bout, dtype = dtype, device = ""cuda"")
-    else: assert(out.shape[0] == bout)
+
+    if out is None:
+        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = ""cuda"")
+    else:
+        assert(out.shape == (bsz, 1, bout,))
+    pass
 
     n = 1
     m = shape[0]
@@ -170,30 +176,46 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):
     ptr_stats  = get_ptr(stats)
     blocksize  = ctypes.c_int32(blocksize)
 
-    fx(m, n, k, get_ptr(X), ptr_W, ptr_absmax, ptr_stats, get_ptr(out),
-        lda, ldb, ldc, blocksize)
+    for row in range(bsz):
+        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),
+           lda, ldb, ldc, blocksize)
+    pass
 
     return out
 pass
 
 
 def fast_linear_forward(proj, X, temp_lora = None, out = None):
+
     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)
+
+    bsz, _, in_dim = X.shape
+
     if W_quant is None:
         out = torch.matmul(X, W.t())
-    else:
+    elif bsz <= 4:
+        # Only batches of 4 are faster with Gemv
         out = fast_gemv(X, W, W_quant, out = out)
-    if lora_A is not None:
+    else:
+        W = fast_dequantize(W.t(), W_quant)
+        out = torch.matmul(X, W, out = out)
+    pass
 
-        # Save LoRAs for inference to stop data movement costs
-        if not hasattr(lora_A, ""_fast_lora""):
-            dtype = X.dtype
-            lora_A._fast_lora = lora_A.to(dtype).t()
-            lora_B._fast_lora = lora_B.to(dtype)
+    # Add in LoRA weights
+    if lora_A is not None:
+        out_dim = out.shape[2]
+        dtype = X.dtype
+        if bsz == 1:
+            out = out.view(out_dim)
+            temp_lora = torch.mv(lora_A.to(dtype), X.ravel(), out = temp_lora)
+            out.addmv_(lora_B.to(dtype), temp_lora, alpha = lora_S)
+        else:
+            out = out.view(bsz, out_dim)
+            temp_lora = torch.mm(X.view(bsz, in_dim), lora_A.to(dtype).t(), out = temp_lora)
+            out.addmm_(temp_lora, lora_B.to(dtype).t(), alpha = lora_S)
         pass
-
-        temp_lora = torch.matmul(X, lora_A._fast_lora, out = temp_lora)
-        out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)
+        out = out.view(bsz, 1, out_dim)
     pass
+
     return out
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 36bb58f..6029005 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -69,7 +69,7 @@ pass
 
 
 from math import sqrt as math_sqrt
-def LlamaAttention_fast_forward_inference(
+def _LlamaAttention_fast_forward_inference(
     self,
     hidden_states:  torch.Tensor,
     past_key_value: Optional[Tuple[torch.Tensor]],
@@ -185,11 +185,89 @@ def LlamaAttention_fast_forward_inference(
 pass
 
 
+def LlamaAttention_fast_forward_inference(
+    self,
+    hidden_states:  torch.Tensor,
+    past_key_value: Optional[Tuple[torch.Tensor]],
+    position_ids,
+):
+    """"""
+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
+        Fast inference using KV cache.
+        QK^T can be computed in 4 chunks
+
+        [Q, q] @ [K, k].T where q, k are the new tokens.
+        [QK^T, Qk^T]
+        [qK^T, qk^T]
+
+        Since the attention mask wipes Qk^T, we just get
+        [QK^T,    0]
+        [qK^T, qk^T]
+
+        Since softmax is row-wise, we get
+        softmax([QK^T,    0])
+        softmax([qK^T, qk^T])
+
+        We then multiply by   [V]
+                              [v]
+        softmax([QK^T,    0]) [softmax(QK^T)V] *
+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]
+
+        But notice * [softmax(QK^T)V] is just the last attention.
+        We just need to compute the last final row.
+
+        This means we can pass in a row of Q, but we need to
+        remember K and V, which are called the KV cache.
+    """"""
+    Xn = hidden_states
+    bsz, _, _ = hidden_states.size()
+    K1, V1 = past_key_value
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    assert(n_kv_heads * n_groups == n_heads)
+
+    Qn = self.q_proj(Xn)
+    Kn = self.k_proj(Xn)
+    Vn = self.v_proj(Xn)
+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)
+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+
+    kv_seq_len = K1.shape[-2] + 1
+    cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
+    Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+    
+    # New KV cache
+    Kn = torch.cat([K1, Kn], dim = 2)
+    Vn = torch.cat([V1, Vn], dim = 2)
+
+    # Grouped query attention
+    if n_groups != 1:
+        _, _, cached_len, _ = Kn.shape
+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)
+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)
+    else:
+        Knn, Vnn = Kn, Vn
+
+    # Attention
+    A = torch.matmul(Qn, Knn.transpose(2, 3))
+    A *= 1.0 / (self.head_dim**0.5)
+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)
+    A = torch.matmul(A, Vnn)
+    A = A.transpose(1, 2)
+    A = A.reshape(bsz, 1, self.hidden_size)
+    A = original_apply_o(self, A)
+    return A, (Kn, Vn)
+pass
+
+
 torch_silu = torch.nn.functional.silu
 def fast_mlp_inference(self, X):
-    hidden_size = self.hidden_size
-    X = X.view(hidden_size)
-
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
     gate = fast_linear_forward(self.gate_proj, X)
@@ -198,20 +276,18 @@ def fast_mlp_inference(self, X):
     gate *= up
 
     # X = self.down_proj(gate)
-    down = fast_linear_forward(self.down_proj, gate, out = up[:hidden_size])
-    X = down.view(1, 1, hidden_size)
-
-    return X
+    down = fast_linear_forward(self.down_proj, gate)
+    return down
 pass
 
 
 def fast_rms_layernorm_inference(self, X):
     old_dtype = X.dtype
-    X = X.to(torch.float32)
-    variance = X.square().mean(-1, keepdim = True)
+    XX = X.to(torch.float32)
+    variance = XX.square().mean(-1, keepdim = True)
     variance += self.variance_epsilon
-    X *= variance.rsqrt_()
-    X = X.to(old_dtype)
+    XX *= variance.rsqrt_()
+    X = XX.to(old_dtype) # Must preserve due to residual
     X *= self.weight
     return X
 pass
@@ -234,7 +310,7 @@ def LlamaAttention_fast_forward(
     bsz, q_len, _ = hidden_states.size()
 
     # Check for inference
-    if False: #past_key_value is not None and q_len == 1 and bsz == 1:
+    if past_key_value is not None:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -350,7 +426,7 @@ def LlamaDecoderLayer_fast_forward(
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
     bsz, q_len, hd = hidden_states.size()
-    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):
+    if past_key_value is not None:
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
@@ -488,8 +564,7 @@ def LlamaModel_fast_forward(
 
     # Fix up attention mask by setting elements to 0
     # Specifically for DPO
-    if self._has_no_labels and attention_mask is not None and \
-        attention_mask.shape[1] == seq_length:
+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):
         # Careful for inference the attention_mask is size (1, kv_seq_len)
         # Whilst the input_embeds is size (1, 1, 4096)
         inputs_requires_grad = inputs_embeds.requires_grad
@@ -501,7 +576,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif self.training:
+    elif False:
         attention_mask = None
         padding_mask = None
     else:
@@ -522,7 +597,7 @@ def LlamaModel_fast_forward(
 
     hidden_states = inputs_embeds
 
-    if self.gradient_checkpointing and self.training:
+    if past_key_values is None and self.gradient_checkpointing and self.training:
         if use_cache:
             logger.warning_once(
                 ""Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`""
@@ -581,7 +656,7 @@ def LlamaModel_fast_forward(
     pass
 
     bsz, q_len, hd = hidden_states.size()
-    if (past_key_value is not None and q_len == 1):
+    if past_key_values is not None:
         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)
     else:
         hidden_states = fast_rms_layernorm(self.norm, hidden_states)
@@ -644,7 +719,13 @@ def LlamaForCausalLM_fast_forward(
     )
 
     hidden_states = outputs[0]
-    logits = self.lm_head(hidden_states)
+    bsz, q_len, hd = hidden_states.shape
+    if bsz == 1 and q_len == 1:
+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
+        logits = logits.unsqueeze(0).unsqueeze(0)
+    else:
+        logits = self.lm_head(hidden_states)
+    pass
 
     loss = None
     if labels is not None:
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 91a2dd7..42f26b9 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(
     bsz, q_len, _ = hidden_states.size()
 
     # Check for inference
-    if past_key_value is not None and q_len == 1 and bsz == 1:
+    if past_key_value is not None:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -210,7 +210,13 @@ def MistralForCausalLM_fast_forward(
     )
 
     hidden_states = outputs[0]
-    logits = self.lm_head(hidden_states)
+    bsz, q_len, hd = hidden_states.shape
+    if bsz == 1 and q_len == 1:
+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
+        logits = logits.unsqueeze(0).unsqueeze(0)
+    else:
+        logits = self.lm_head(hidden_states)
+    pass
 
     loss = None
     if labels is not None:
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index fcaa2a1..36bb58f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -488,7 +488,10 @@ def LlamaModel_fast_forward(
 
     # Fix up attention mask by setting elements to 0
     # Specifically for DPO
-    if self._has_no_labels and attention_mask is not None:
+    if self._has_no_labels and attention_mask is not None and \
+        attention_mask.shape[1] == seq_length:
+        # Careful for inference the attention_mask is size (1, kv_seq_len)
+        # Whilst the input_embeds is size (1, 1, 4096)
         inputs_requires_grad = inputs_embeds.requires_grad
         if inputs_requires_grad: inputs_embeds.requires_grad_(False)
         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)
"
"diff --git a/pyproject.toml b/pyproject.toml
index a0a1723..21736b7 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -29,7 +29,7 @@ version = {attr = ""unsloth.models._utils.__version__""}
 include-package-data = false
 
 [tool.setuptools.packages.find]
-exclude = [""images*""]
+exclude = [""images*"", ""tests*""]
 
 [project.optional-dependencies]
 triton = [
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.14"",
+    ""unsloth_zoo>=2025.3.16"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -351,7 +351,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.14"",
+    ""unsloth_zoo>=2025.3.16"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/tests/qlora/README.md b/tests/qlora/README.md
new file mode 100644
index 0000000..e535c38
--- /dev/null
+++ b/tests/qlora/README.md
@@ -0,0 +1,47 @@
+## QLoRA Train and Merge Tests
+
+### Overview
+Tests that performing QLoRA training and merging weights to 16-bits post-training maintains same behavior as trained model.
+
+- `test_unsloth_qlora_train_and_merge.py`: Test Unsloth QLoRA train and merge using `FastLanguageModel.from_pretrained`, `FastLanguageModel.get_peft_model`, and `FastLanguageModel.save_pretrained_merged` apis
+- `test_hf_qlora_train_and_merge.py`: Test Hugging Face QLoRA train and merge using `from_pretrained`, `get_peft_model`, and `merge_and_unload` apis.
+   - Demonstrates that `peft`'s `merge_and_unload` results in loss of accuracy as it requantizes the base layer after merging adapter weights so that the model still contains `Linear4Bit` layers post merging.
+   - I (@jeromeku) implemented a custom merge function that replaces all `LoraLayers` with `Linear` layers whose weights are the dequantized base layer weights with adapter weights merged (compute done in fp32, cast to original dtype after merging), roughly equivalent to `FastLanguageModel.save_pretrained_merged`.
+
+### Usage
+Run unsloth test:
+```bash
+python tests/qlora/test_unsloth_qlora_train_and_merge.py
+```
+Run huggingface test:
+```bash
+python tests/qlora/test_hf_qlora_train_and_merge.py
+```
+
+### Details
+The tests train a QLoRA model on a single prompt dataset
+```
+QUESTION = ""What day was I born?""
+ANSWER = ""January 1, 2058""
+USER_MESSAGE = {""role"": ""user"", ""content"": QUESTION}
+ASSISTANT_MESSAGE = {""role"": ""assistant"", ""content"": ANSWER}
+```
+
+Given that the answer is impossible to answer accurately without finetuning, we can only expect the model to answer the question correctly if the model has been trained on the question.
+
+To check this behavior, we check the model's response to the question before and after training and after merging, checking that the model's response contains the answer after training and merging but not before training.
+
+### Results
+
+For the unsloth test, the model's behavior is as expected: 
+- before training, the model's response does not contain the answer
+- after training, the model's response contains the answer
+- after merging, the model's response contains the answer
+
+For the huggingface test, the model's behavior is as expected:
+- before training, the model's response does not contains the answer
+- after training, the model's response contains the answer
+- after using peft's `merge_and_unload`, the model's response does not contain the answer
+- after using my custom merge function, the model's response contains the answer
+
+The scripts should output training params, training logs, as well as model responses before and after training and after merging (only prints model responses if answer is not contained in response).
\ No newline at end of file
diff --git a/tests/qlora/test_hf_qlora_train_and_merge.py b/tests/qlora/test_hf_qlora_train_and_merge.py
new file mode 100644
index 0000000..797d940
--- /dev/null
+++ b/tests/qlora/test_hf_qlora_train_and_merge.py
@@ -0,0 +1,159 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# ruff: noqa
+import sys
+from pathlib import Path
+
+REPO_ROOT = Path(__file__).parents[2]
+sys.path.append(str(REPO_ROOT))
+
+import itertools
+from copy import deepcopy
+
+import torch
+from datasets import Dataset
+from trl import SFTConfig
+from tests.utils import header_footer_context
+from tests.utils.data_utils import (
+    ANSWER,
+    DEFAULT_MESSAGES,
+    USER_MESSAGE,
+    check_responses,
+    create_dataset,
+    describe_peft_weights,
+)
+from tests.utils.hf_utils import (
+    convert_lora_to_linear,
+    fix_llama3_tokenizer,
+    get_peft_config,
+    sample_responses,
+    setup_model,
+    setup_tokenizer,
+    setup_trainer,
+)
+
+if __name__ == ""__main__"":
+    model_name = ""meta-llama/Llama-3.2-1B-Instruct""
+    dtype = torch.bfloat16
+    max_steps = 100
+    num_examples = 1000
+    lora_rank = 64
+    output_dir = ""sft_test""
+    seed = 42
+    batch_size = 5
+    num_generations = 5
+    tokenizer = setup_tokenizer(model_name, fixup_funcs=[fix_llama3_tokenizer])
+    temperature = 0.8
+    max_new_tokens = 20
+
+    peft_config = get_peft_config(lora_rank=lora_rank, target_modules=""all-linear"")
+    model = setup_model(model_name, quantize=True, dtype=dtype, peft_config=peft_config)
+
+    prompt = tokenizer.apply_chat_template(
+        [USER_MESSAGE], tokenize=False, add_generation_prompt=True
+    )
+    with header_footer_context(""Test Prompt and Answer""):
+        print(f""Test Prompt:\n{prompt}\nExpected Answer:\n{ANSWER}"")
+
+    dataset: Dataset = create_dataset(
+        tokenizer, num_examples=num_examples, messages=DEFAULT_MESSAGES
+    )
+    with header_footer_context(""Dataset""):
+        print(f""Dataset: {next(iter(dataset))}"")
+
+    training_args = SFTConfig(
+        output_dir=output_dir,
+        max_steps=max_steps,
+        per_device_train_batch_size=batch_size,
+        log_level=""info"",
+        report_to=""none"",
+        num_train_epochs=1,
+        logging_steps=1,
+        seed=seed,
+        bf16=dtype == torch.bfloat16,
+        fp16=dtype == torch.float16,
+        save_strategy=""no"",
+    )
+
+    with header_footer_context(""Train Args""):
+        print(training_args)
+        print(peft_config)
+
+    trainer = setup_trainer(
+        model, tokenizer, dataset, training_args, peft_config=peft_config
+    )
+
+    with header_footer_context(""Model""):
+        print(type(model.model))
+
+    generation_args = {
+        ""num_generations"": num_generations,
+        ""max_new_tokens"": max_new_tokens,
+        ""temperature"": temperature,
+        ""skip_special_tokens"": False,
+        ""dtype"": dtype,
+    }
+    responses = sample_responses(
+        model,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses before training""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
+
+    with header_footer_context(""Peft Weights before training""):
+        for name, stats in itertools.islice(describe_peft_weights(model), 2):
+            print(f""{name}:\n{stats}"")
+
+    output = trainer.train()
+    with header_footer_context(""Peft Weights after training""):
+        for name, stats in itertools.islice(describe_peft_weights(model), 2):
+            print(f""{name}:\n{stats}"")
+
+    with header_footer_context(""Trainer Output""):
+        print(output)
+
+    responses = sample_responses(
+        model,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses after training""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
+
+    model_copy = deepcopy(model)
+
+    merged_model = convert_lora_to_linear(model)
+
+    responses = sample_responses(
+        merged_model,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses after custom merging to 16bit""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
+
+    merged_model_peft = model_copy.merge_and_unload()
+    responses = sample_responses(
+        merged_model_peft,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses after peft merge_and_unload""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
diff --git a/tests/qlora/test_unsloth_qlora_train_and_merge.py b/tests/qlora/test_unsloth_qlora_train_and_merge.py
new file mode 100644
index 0000000..59fa813
--- /dev/null
+++ b/tests/qlora/test_unsloth_qlora_train_and_merge.py
@@ -0,0 +1,211 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# ruff: noqa
+import sys
+from pathlib import Path
+
+REPO_ROOT = Path(__file__).parents[2]
+sys.path.append(str(REPO_ROOT))
+
+import itertools
+from unsloth import FastLanguageModel
+
+import torch
+from datasets import Dataset
+from trl import SFTConfig
+from tests.utils import header_footer_context
+from tests.utils.data_utils import (
+    DEFAULT_MESSAGES,
+    USER_MESSAGE,
+    ANSWER,
+    create_dataset,
+    describe_peft_weights,
+    check_responses,
+)
+from tests.utils.hf_utils import (
+    sample_responses,
+    setup_trainer,
+)
+
+
+def get_unsloth_model_and_tokenizer(
+    model_name: str,
+    max_seq_length: int,
+    load_in_4bit: bool,
+    fast_inference: bool,
+    max_lora_rank: int = None,
+    gpu_memory_utilization: float = 0.5,
+    dtype: torch.dtype = torch.bfloat16,
+):
+    return FastLanguageModel.from_pretrained(
+        model_name=model_name,
+        max_seq_length=max_seq_length,
+        load_in_4bit=load_in_4bit,
+        fast_inference=fast_inference,
+        max_lora_rank=max_lora_rank,
+        gpu_memory_utilization=gpu_memory_utilization,
+        dtype=dtype,
+    )
+
+
+def get_unsloth_peft_model(
+    model,
+    lora_rank: int,
+    target_modules: list[str] = ""all-linear"",
+    use_gradient_checkpointing: str = False,
+    random_state: int = 42,
+):
+    return FastLanguageModel.get_peft_model(
+        model,
+        r=lora_rank,
+        target_modules=target_modules,
+        lora_alpha=lora_rank,
+        use_gradient_checkpointing=use_gradient_checkpointing,
+        random_state=random_state,
+    )
+
+
+if __name__ == ""__main__"":
+    model_name = ""meta-llama/Llama-3.2-1B-Instruct""
+    dtype = torch.bfloat16
+    max_steps = 100
+    num_examples = 1000
+    lora_rank = 64
+    output_dir = ""sft_test""
+    seed = 42
+    batch_size = 5
+    num_generations = 5
+    target_modules = [
+        ""q_proj"",
+        ""k_proj"",
+        ""v_proj"",
+        ""o_proj"",
+        ""gate_proj"",
+        ""up_proj"",
+        ""down_proj"",
+    ]
+    gradient_checkpointing = False
+    unsloth_merged_path = ""unsloth_merged_16bit""
+
+    model, tokenizer = get_unsloth_model_and_tokenizer(
+        model_name,
+        max_seq_length=512,
+        load_in_4bit=True,
+        fast_inference=False,
+        max_lora_rank=lora_rank,
+        dtype=dtype,
+    )
+    temperature = 0.8
+    max_new_tokens = 20
+
+    model = get_unsloth_peft_model(
+        model,
+        lora_rank=lora_rank,
+        target_modules=target_modules,
+        use_gradient_checkpointing=gradient_checkpointing,
+        random_state=seed,
+    )
+
+    prompt = tokenizer.apply_chat_template(
+        [USER_MESSAGE], tokenize=False, add_generation_prompt=True
+    )
+
+    with header_footer_context(""Test Prompt and Answer""):
+        print(f""Test Prompt:\n{prompt}\nExpected Answer:\n{ANSWER}"")
+
+    dataset: Dataset = create_dataset(
+        tokenizer, num_examples=num_examples, messages=DEFAULT_MESSAGES
+    )
+    with header_footer_context(""Dataset""):
+        print(f""Dataset: {next(iter(dataset))}"")
+
+    training_args = SFTConfig(
+        output_dir=output_dir,
+        max_steps=max_steps,
+        per_device_train_batch_size=batch_size,
+        log_level=""info"",
+        report_to=""none"",
+        num_train_epochs=1,
+        logging_steps=1,
+        seed=seed,
+        bf16=dtype == torch.bfloat16,
+        fp16=dtype == torch.float16,
+        save_strategy=""no"",
+    )
+
+    with header_footer_context(""Train Args""):
+        print(training_args)
+
+    trainer = setup_trainer(model, tokenizer, dataset, training_args)
+
+    with header_footer_context(""Model""):
+        print(type(model.model))
+
+    generation_args = {
+        ""num_generations"": num_generations,
+        ""max_new_tokens"": max_new_tokens,
+        ""temperature"": temperature,
+        ""skip_special_tokens"": False,
+        ""dtype"": dtype,
+    }
+    responses = sample_responses(
+        model,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses before training""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
+    with header_footer_context(""Peft Weights before training""):
+        for name, stats in itertools.islice(describe_peft_weights(model), 2):
+            print(f""{name}:\n{stats}"")
+
+    output = trainer.train()
+    with header_footer_context(""Peft Weights after training""):
+        for name, stats in itertools.islice(describe_peft_weights(model), 2):
+            print(f""{name}:\n{stats}"")
+
+    with header_footer_context(""Trainer Output""):
+        print(output)
+
+    responses = sample_responses(
+        model,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses after training""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
+
+    model.save_pretrained_merged(
+        unsloth_merged_path,
+        tokenizer,
+        save_method=""merged_16bit"",
+    )
+    merged_model_unsloth, tokenizer = get_unsloth_model_and_tokenizer(
+        unsloth_merged_path,
+        max_seq_length=512,
+        load_in_4bit=False,
+        fast_inference=False,
+        dtype=dtype,
+    )
+    responses = sample_responses(
+        merged_model_unsloth,
+        tokenizer,
+        prompt=prompt,
+        **generation_args,
+    )
+    with header_footer_context(""Responses after unsloth merge to 16bit""):
+        check_responses(responses, answer=ANSWER, prompt=prompt)
diff --git a/tests/utils/__init__.py b/tests/utils/__init__.py
new file mode 100644
index 0000000..cd5d0d9
--- /dev/null
+++ b/tests/utils/__init__.py
@@ -0,0 +1,33 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import time
+from contextlib import contextmanager
+
+
+@contextmanager
+def timer(name):
+    start = time.time()
+    yield
+    end = time.time()
+    print(f""{name} took {end - start:.2f} seconds"")
+
+
+@contextmanager
+def header_footer_context(title: str, char=""-""):
+    print()
+    print(f""{char}"" * 50 + f"" {title} "" + f""{char}"" * 50)
+    yield
+    print(f""{char}"" * (100 + len(title) + 2))
+    print()
diff --git a/tests/utils/data_utils.py b/tests/utils/data_utils.py
new file mode 100644
index 0000000..7682fe4
--- /dev/null
+++ b/tests/utils/data_utils.py
@@ -0,0 +1,153 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+from datasets import Dataset
+
+QUESTION = ""What day was I born?""
+ANSWER = ""January 1, 2058""
+USER_MESSAGE = {""role"": ""user"", ""content"": QUESTION}
+ASSISTANT_MESSAGE = {""role"": ""assistant"", ""content"": ANSWER}
+DTYPE = torch.bfloat16
+DEFAULT_MESSAGES = [[USER_MESSAGE, ASSISTANT_MESSAGE]]
+
+
+def create_instruction_dataset(messages: list[dict] = DEFAULT_MESSAGES):
+    dataset = Dataset.from_dict({""messages"": messages})
+    return dataset
+
+
+def create_dataset(tokenizer, num_examples: int = None, messages: list[dict] = None):
+    dataset = create_instruction_dataset(messages)
+
+    def _apply_chat_template(example):
+        chat = tokenizer.apply_chat_template(example[""messages""], tokenize=False)
+        return {""text"": chat}
+
+    dataset = dataset.map(_apply_chat_template, remove_columns=""messages"")
+    if num_examples is not None:
+        if len(dataset) < num_examples:
+            num_repeats = num_examples // len(dataset) + 1
+            dataset = dataset.repeat(num_repeats)
+        dataset = dataset.select(range(num_examples))
+
+    return dataset
+
+
+def describe_param(
+    param: torch.Tensor,
+    include_l1: bool = False,
+    include_l2: bool = False,
+    include_infinity: bool = False,
+    as_str: bool = True,
+) -> dict:
+    """"""
+    Provide a statistical summary of a 2D weight matrix or tensor.
+    If as_str is True, the summary is returned as a formatted string.
+    Parameters:
+        param: torch.Tensor
+        include_l1 (bool): Whether to include the L1 norm (sum of absolute values).
+        include_l2 (bool): Whether to include the L2 norm (Frobenius norm).
+        include_infinity (bool): Whether to include the infinity norm (max absolute value).
+        as_str (bool): Whether to return the summary as a formatted string.
+
+    Returns:
+        dict: A dictionary with the following statistics:
+              - shape: Dimensions of the matrix.
+              - mean: Average value.
+              - median: Median value.
+              - std: Standard deviation.
+              - min: Minimum value.
+              - max: Maximum value.
+              - percentile_25: 25th percentile.
+              - percentile_75: 75th percentile.
+              Additionally, if enabled:
+              - L1_norm: Sum of absolute values.
+              - L2_norm: Euclidean (Frobenius) norm.
+              - infinity_norm: Maximum absolute value.
+    """"""
+
+    param = param.float()
+    summary = {
+        ""shape"": param.shape,
+        ""mean"": param.mean().cpu().item(),
+        ""std"": param.std().cpu().item(),
+        ""min"": param.min().cpu().item(),
+        ""max"": param.max().cpu().item(),
+        ""percentile_25"": param.quantile(0.25).cpu().item(),
+        ""percentile_50"": param.quantile(0.5).cpu().item(),
+        ""percentile_75"": param.quantile(0.75).cpu().item(),
+    }
+
+    if include_l1:
+        summary[""L1_norm""] = param.abs().sum().cpu().item()
+    if include_l2:
+        summary[""L2_norm""] = param.norm().cpu().item()
+    if include_infinity:
+        summary[""infinity_norm""] = param.abs().max().cpu().item()
+
+    return format_summary(summary) if as_str else summary
+
+
+def format_summary(stats: dict, precision: int = 6) -> str:
+    """"""
+    Format the statistical summary dictionary for printing.
+
+    Parameters:
+        stats (dict): The dictionary returned by describe_param.
+        precision (int): Number of decimal places for floating point numbers.
+
+    Returns:
+        str: A formatted string representing the summary.
+    """"""
+    lines = []
+    for key, value in stats.items():
+        if isinstance(value, float):
+            formatted_value = f""{value:.{precision}f}""
+        elif isinstance(value, (tuple, list)):
+            # Format each element in tuples or lists (e.g., the shape)
+            formatted_value = "", "".join(str(v) for v in value)
+            formatted_value = (
+                f""({formatted_value})""
+                if isinstance(value, tuple)
+                else f""[{formatted_value}]""
+            )
+        else:
+            formatted_value = str(value)
+        lines.append(f""{key}: {formatted_value}"")
+    return ""\n"".join(lines)
+
+
+def get_peft_weights(model):
+    # ruff: noqa
+    is_lora_weight = lambda name: any(s in name for s in [""lora_A"", ""lora_B""])
+    return {
+        name: param for name, param in model.named_parameters() if is_lora_weight(name)
+    }
+
+
+def describe_peft_weights(model):
+    for name, param in get_peft_weights(model).items():
+        yield name, describe_param(param, as_str=True)
+
+
+def check_responses(responses: list[str], answer: str, prompt: str = None) -> bool:
+    for i, response in enumerate(responses, start=1):
+        if answer in response:
+            print(f""\u2713 response {i} contains answer"")
+        else:
+            print(f""\u2717 response {i} does not contain answer"")
+            if prompt is not None:
+                response = response.replace(prompt, """")
+            print(f"" -> response: {response}"")
diff --git a/tests/utils/hf_utils.py b/tests/utils/hf_utils.py
new file mode 100644
index 0000000..cc5edce
--- /dev/null
+++ b/tests/utils/hf_utils.py
@@ -0,0 +1,291 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+from contextlib import contextmanager, nullcontext
+from typing import Callable, Optional
+
+import bitsandbytes as bnb
+import torch
+from bitsandbytes.functional import dequantize_4bit
+from peft import get_peft_model, prepare_model_for_kbit_training
+from peft.tuners.lora import LoraConfig, LoraLayer
+from transformers import (
+    AutoModelForCausalLM,
+    AutoTokenizer,
+    BitsAndBytesConfig,
+)
+from transformers.trainer_callback import (
+    TrainerCallback,
+    TrainerControl,
+    TrainerState,
+    TrainingArguments,
+)
+from trl import SFTTrainer
+
+
+class PeftWeightCallback(TrainerCallback):
+    def on_log(
+        self,
+        args: TrainingArguments,
+        state: TrainerState,
+        control: TrainerControl,
+        logs,
+        **kwargs,
+    ):
+        print(f""DEBUG::CALLBACK::on_log::{state.log_history}"")
+
+    def on_train_begin(
+        self,
+        args: TrainingArguments,
+        state: TrainerState,
+        control: TrainerControl,
+        **kwargs,
+    ):
+        model = kwargs.get(""model"")
+        assert model is not None
+        print(f""DEBUG::CALLBACK::on_train_begin::{kwargs.keys()}"")
+
+    def on_step_end(
+        self,
+        args: TrainingArguments,
+        state: TrainerState,
+        control: TrainerControl,
+        **kwargs,
+    ):
+        print(f""DEBUG::CALLBACK::on_step_end::{state.global_step}"")
+
+
+@torch.inference_mode()
+def generate_responses(
+    model,
+    tokenizer,
+    prompt,
+    max_new_tokens: int = 100,
+    temperature: float = 0.8,
+    do_sample: bool = True,
+    num_generations: int = 1,
+    skip_special_tokens: bool = True,
+    dtype: torch.dtype = None,
+):
+    inputs = [tokenizer(prompt, return_tensors=""pt"") for _ in range(num_generations)]
+    keys = inputs[0].keys()
+    batched_inputs = {
+        key: torch.cat([input[key] for input in inputs], dim=0).to(model.device)
+        for key in keys
+    }
+
+    if dtype is not None:
+        inference_context = torch.autocast(device_type=""cuda"", dtype=dtype)
+    else:
+        inference_context = nullcontext()
+
+    with inference_context:
+        outputs = model.generate(
+            **batched_inputs,
+            max_new_tokens=max_new_tokens,
+            do_sample=do_sample,
+            temperature=temperature,
+        )
+
+    responses = tokenizer.batch_decode(outputs, skip_special_tokens=skip_special_tokens)
+    return responses
+
+
+def sample_responses(
+    model,
+    tokenizer,
+    prompt,
+    temperature: float = 0.8,
+    num_generations: int = 1,
+    max_new_tokens: int = 100,
+    skip_special_tokens: bool = True,
+    dtype: torch.dtype = None,
+):
+    responses = generate_responses(
+        model,
+        tokenizer,
+        prompt,
+        temperature=temperature,
+        num_generations=num_generations,
+        max_new_tokens=max_new_tokens,
+        skip_special_tokens=skip_special_tokens,
+        dtype=dtype,
+    )
+    return responses
+
+
+def setup_tokenizer(model_name, fixup_funcs: list[Callable] = []):
+    tokenizer = AutoTokenizer.from_pretrained(model_name)
+    for fixup_func in fixup_funcs:
+        tokenizer = fixup_func(tokenizer)
+    return tokenizer
+
+
+def setup_model(
+    model_name,
+    quantize: bool = True,
+    dtype=torch.bfloat16,
+    peft_config=None,
+    autocast_adapter: bool = True,
+):
+    if quantize:
+        bnb_config = BitsAndBytesConfig(
+            load_in_4bit=True,
+            bnb_4bit_use_double_quant=True,
+            bnb_4bit_quant_type=""nf4"",
+            bnb_4bit_compute_dtype=dtype,
+        )
+    else:
+        bnb_config = None
+
+    model = AutoModelForCausalLM.from_pretrained(
+        model_name,
+        device_map=""cuda:0"",
+        attn_implementation=""sdpa"",
+        quantization_config=bnb_config,
+        torch_dtype=dtype,
+    )
+    model = prepare_model_for_kbit_training(model) if quantize else model
+
+    if peft_config is not None:
+        model = get_peft_model(
+            model, peft_config, autocast_adapter_dtype=autocast_adapter
+        )
+
+    return model
+
+
+def get_peft_config(
+    lora_rank,
+    lora_alpha=None,
+    lora_dropout=0.0,
+    bias=""none"",
+    target_modules=""all-linear"",
+):
+    lora_alpha = lora_alpha or 2 * lora_rank
+    peft_config = LoraConfig(
+        lora_alpha=lora_alpha,
+        lora_dropout=lora_dropout,
+        r=lora_rank,
+        bias=bias,
+        target_modules=target_modules,
+        task_type=""CAUSAL_LM"",
+    )
+    return peft_config
+
+
+def setup_trainer(
+    model,
+    tokenizer,
+    dataset,
+    train_args,
+    peft_config=None,
+    formatting_func=None,
+    collator=None,
+):
+    return SFTTrainer(
+        model=model,
+        peft_config=peft_config,
+        train_dataset=dataset,
+        processing_class=tokenizer,
+        formatting_func=formatting_func,
+        data_collator=collator,
+        args=train_args,
+    )
+
+
+def setup_lora(
+    model,
+    tokenizer,
+    dataset,
+    peft_config,
+    train_args,
+    formatting_func=None,
+    collator=None,
+):
+    return LoraConfig(
+        model=model,
+        peft_config=peft_config,
+        train_dataset=dataset,
+        processing_class=tokenizer,
+        formatting_func=formatting_func,
+        data_collator=collator,
+        args=train_args,
+    )
+
+
+def convert_weights_back_to_dtype(model, dtype):
+    """"""
+    SFTTrainer calls get_peft_model and prepare_model_for_kbit_training which converts all weights to float32.
+    This function converts the non-loraweights back to the original dtype.
+    """"""
+    for name, param in model.named_parameters():
+        if any(s in name for s in [""norm"", ""embed""]):
+            param.data = param.data.to(dtype)
+
+
+def fix_llama3_tokenizer(tokenizer, padding_side=""right""):
+    tokenizer.padding_side = padding_side
+    added_vocab = tokenizer.get_added_vocab()
+    pad_token = [w for w in added_vocab if ""pad"" in w]
+    assert len(pad_token) == 1
+    tokenizer.pad_token = pad_token[0]  # Load dataset from the hub
+    return tokenizer
+
+
+def replace_module(
+    module: torch.nn.Module,
+    target_module_type: torch.nn.Module,
+    conversion_func: Callable,
+):
+    for child_name, child_module in module.named_children():
+        if isinstance(child_module, target_module_type):
+            new_module = conversion_func(child_module)
+            setattr(module, child_name, new_module)
+        else:
+            replace_module(child_module, target_module_type, conversion_func)
+
+
+def _convert_lora_to_linear(module: LoraLayer, adapter_name: str = ""default""):
+    base_layer = module.get_base_layer()
+    weight = base_layer.weight
+
+    assert isinstance(weight, bnb.nn.Params4bit)
+    quant_state = weight.quant_state
+    original_dtype = quant_state.dtype
+
+    w_dq = dequantize_4bit(weight.data, quant_state).float()
+    lora_delta = (
+        module.lora_B[adapter_name].weight
+        @ module.lora_A[adapter_name].weight
+        * module.scaling[adapter_name]
+    )
+    w_dq += lora_delta.float()
+    w_dq = w_dq.to(original_dtype)
+
+    new_module = torch.nn.Linear(
+        w_dq.shape[1], w_dq.shape[0], bias=module.base_layer.bias is not None
+    )
+    new_module.weight.data = torch.nn.Parameter(w_dq, requires_grad=False)
+    if module.lora_bias[adapter_name]:
+        bias_data = module.base_layer.bias.data + module.lora_B[adapter_name].bias
+        new_module.bias.data = torch.nn.Parameter(bias_data, requires_grad=False)
+    return new_module
+
+
+def convert_lora_to_linear(model: torch.nn.Module):
+    replace_module(model, LoraLayer, _convert_lora_to_linear)
+    assert not any(isinstance(module, LoraLayer) for module in model.modules())
+    return model
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 41b6bb7..708eeaf 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.14""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.16""):
         print(
             ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
             ""To disable this, set `os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'`""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ab53811..0044c7e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.17""
+__version__ = ""2025.3.18""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -484,7 +484,8 @@ pass
 import transformers.generation.configuration_utils
 if hasattr(transformers.generation.configuration_utils, ""ALL_CACHE_IMPLEMENTATIONS""):
     if type(transformers.generation.configuration_utils.ALL_CACHE_IMPLEMENTATIONS) is list:
-        transformers.generation.configuration_utils.ALL_CACHE_IMPLEMENTATIONS.append(""dynamic"")
+        if ""dynamic"" not in transformers.generation.configuration_utils.ALL_CACHE_IMPLEMENTATIONS:
+            transformers.generation.configuration_utils.ALL_CACHE_IMPLEMENTATIONS.append(""dynamic"")
     pass
 pass
 # =============================================
@@ -1242,6 +1243,7 @@ for j, function in enumerate(functions):
         except: continue
 pass
 
+import importlib
 USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
 if USE_MODELSCOPE:
     if importlib.util.find_spec(""modelscope"") is None:
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 61cf05e..b3b49a0 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -99,9 +99,15 @@ SDPA_HAS_GQA = ""enable_gqa"" in scaled_dot_product_attention.__doc__
 
 # Fix new HF's inference code
 def _fast_prepare_inputs_for_generation(self, input_ids, **kwargs,):
-    if ""past_key_values"" in kwargs:
-        input_ids = input_ids[:,[-1]]
-        kwargs[""attention_mask""] = kwargs[""attention_mask""][:,[-1]]
+    past_key_values = kwargs.get(""past_key_values"", None)
+    if past_key_values is not None:
+        # Check for uninitialized DynamicCache
+        if len(past_key_values) == 0:
+            past_key_values = None
+            kwargs[""past_key_values""] = None
+        else:
+            input_ids = input_ids[:,[-1]]
+            kwargs[""attention_mask""] = kwargs[""attention_mask""][:,[-1]]
     if ""cache_position"" in kwargs:
         kwargs[""position_ids""] = kwargs[""cache_position""]
     return { ""input_ids"" : input_ids, **kwargs, }
@@ -1032,7 +1038,6 @@ def CausalLM_fast_forward(fast_forward_inference):
                 output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
             )
             return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
             # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
             self.model._has_no_labels = labels is None
             outputs = self.model(
@@ -1112,13 +1117,7 @@ def CausalLM_fast_forward(fast_forward_inference):
             logits = self.lm_head(hidden_states.to(dtype))
         pass
 
-        torch_dtype = __DTYPE_MAP.get(self.config.torch_dtype, None)
-        if torch_dtype is not None:
-            logits = logits.to(torch_dtype)
-        else:
-            raise TypeError(""Unsloth: torch_dtype for models is not bfloat16, float16 or float32!"")
-        pass
-
+        logits = logits.to(_get_dtype(self.config.torch_dtype))
         loss = None
         logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
         logit_scaling     = getattr(self.config, ""logit_scale"", 0)
@@ -1170,7 +1169,6 @@ def CausalLM_fast_forward(fast_forward_inference):
         if not return_dict:
             output = (logits,) + outputs[1:]
             return (loss,) + output if loss is not None else output
-
         return CausalLMOutputWithPast(
             loss = loss,
             logits = logits,
@@ -1685,13 +1683,16 @@ class FastLlamaModel:
         print(statistics)
 
         # Warn about fast transfers
-        old_hf_transfer = os.environ.get(""HF_HUB_ENABLE_HF_TRANSFER"", ""0"")
-        if os.environ.get(""HF_HUB_ENABLE_HF_TRANSFER"", ""0"") == ""1"":
+        if ""HF_HUB_ENABLE_HF_TRANSFER"" in os.environ:
+            old_hf_transfer = os.environ[""HF_HUB_ENABLE_HF_TRANSFER""]
+            if old_hf_transfer in (""False"", ""false""): old_hf_transfer = ""0""
+            if old_hf_transfer in (""True"",  ""true"" ): old_hf_transfer = ""1""
+        else:
+            old_hf_transfer = ""0""
+        if old_hf_transfer == ""1"":
             print(""Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"")
         pass
-        # Return old flag
-        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
-        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
+        if old_hf_transfer != ""0"": os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
         model_patcher.pre_patch()
         get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index db140c4..ef32ab1 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -269,13 +269,16 @@ class FastBaseModel:
         print(statistics)
 
         # Warn about fast transfers
-        old_hf_transfer = os.environ.get(""HF_HUB_ENABLE_HF_TRANSFER"", ""0"")
-        if os.environ.get(""HF_HUB_ENABLE_HF_TRANSFER"", ""0"") == ""1"":
+        if ""HF_HUB_ENABLE_HF_TRANSFER"" in os.environ:
+            old_hf_transfer = os.environ[""HF_HUB_ENABLE_HF_TRANSFER""]
+            if old_hf_transfer in (""False"", ""false""): old_hf_transfer = ""0""
+            if old_hf_transfer in (""True"",  ""true"" ): old_hf_transfer = ""1""
+        else:
+            old_hf_transfer = ""0""
+        if old_hf_transfer == ""1"":
             print(""Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"")
         pass
-        # Return old flag
-        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
-        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
+        if old_hf_transfer != ""0"": os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
         get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
 
"
"diff --git a/README.md b/README.md
index 6279117..bf7e879 100644
--- a/README.md
+++ b/README.md
@@ -81,6 +81,7 @@ For Windows install instructions, see [here](https://docs.unsloth.ai/get-started
 - All kernels written in [OpenAI's Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.
 - **0% loss in accuracy** - no approximation methods - all exact.
 - No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
+- AMD ROCm GPUs including Instinct 3xx series and Radeon GPUs are now supported!
 - Works on **Linux** and **Windows**
 - If you trained a model with Unsloth, you can use this cool sticker! &nbsp; <img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png"" height=""50"" align=""center"" />
 
@@ -97,6 +98,42 @@ pip install unsloth
 pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
 ```
 See [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.
+
+### AMD ROCm GPU Installation
+
+**Launch container environment:**
+```
+CONTAINER_NAME=<your container name>
+IMAGE_NAME=rocm/vllm:rocm6.4.1_vllm_0.9.0.1_20250605
+
+docker run -it \
+        --rm \
+        --device /dev/dri \
+        --device /dev/kfd \
+        --network host \
+        --ipc host \
+        --group-add video \
+        --cap-add SYS_PTRACE \
+        --security-opt seccomp=unconfined \
+        --privileged \
+        --shm-size 32G \
+        --name ${CONTAINER_NAME} \
+        ${IMAGE_NAME} /bin/bash
+```
+
+**Install by setup.py install**
+```
+# choose your rocm arch from INSTINCT_ARCH=(""gfx942"", ""gfx90a""), or
+# RADEON_ARCH=(""gfx1100"", ""gfx1101"", ""gfx1102"", ""gfx1200"", ""gfx1201"")
+# Specify gfx942 here for MI300X device
+ROCM_ARCH=gfx942 python setup.py bdist_wheel 
+
+root@root:/workspace/unsloth# ls dist/
+unsloth-2025.6.5+rocm641-py3-none-any.whl
+
+pip install ./dist/unsloth-2025.6.5+rocm641-py3-none-any.whl
+```
+
 ### Windows Installation
 > [!warning]
 > Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10
@@ -135,7 +172,7 @@ trainer = SFTTrainer(
 
 For **advanced installation instructions** or if you see weird errors during installations:
 
-1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
+1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`. For AMD GPUs, please add `--extra-index-url https://download.pytorch.org/whl/rocm6.3` . For AMD support matrix info, please refer to https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html.
 2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
 3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.
 4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
diff --git a/pyproject.toml b/pyproject.toml
index e1e021d..fb066b8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,14 +1,24 @@
 [build-system]
-requires = [""setuptools"", ""setuptools-scm""]
+# Should be mirrored in requirements/build.txt
+requires = [
+    ""cmake>=3.26"",
+    ""ninja"",
+    ""packaging>=24.2"",
+    ""setuptools>=77.0.3,<80.0.0"",
+    ""setuptools-scm>=8.0"",
+    ""torch==2.7.0"",
+    ""wheel"",
+    ""jinja2""
+]
 build-backend = ""setuptools.build_meta""
 
 [project]
 name = ""unsloth""
-dynamic = [""version""]
 description = ""2-5X faster LLM finetuning""
 readme = ""README.md""
 requires-python = "">=3.9,<3.13""
-license = {text = ""Apache-2.0""}
+dynamic = [ ""version"", ""dependencies"", ""optional-dependencies""]
+license = { file = ""LICENSE"" }
 keywords = [""ai"", ""llm"",]
 authors = [
     {email = ""info@unsloth.ai""},
@@ -22,606 +32,15 @@ classifiers = [
     ""Programming Language :: Python"",
 ]
 
-[tool.setuptools.dynamic]
-version = {attr = ""unsloth.models._utils.__version__""}
-
 [tool.setuptools]
 include-package-data = false
 
+
 [tool.setuptools.packages.find]
+where = ["".""]
+include = [""unsloth*""]
 exclude = [""images*"", ""tests*"", ""kernels/moe*""]
 
-[project.optional-dependencies]
-triton = [
-    ""triton-windows ; platform_system == 'Windows'"",
-]
-
-huggingface = [
-    ""unsloth_zoo>=2025.6.7"",
-    ""packaging"",
-    ""tyro"",
-    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
-    ""datasets>=3.4.1"",
-    ""sentencepiece>=0.2.0"",
-    ""tqdm"",
-    ""psutil"",
-    ""wheel>=0.42.0"",
-    ""numpy"",
-    ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
-    ""peft>=0.7.1,!=0.11.0"",
-    ""protobuf"",
-    ""huggingface_hub"",
-    ""hf_transfer"",
-    ""unsloth[triton]"",
-]
-windows=[
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5 ; platform_system == 'Windows'"",
-    ""xformers>=0.0.22.post7 ; platform_system == 'Windows'"",
-]
-cu118only = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu121only = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu118onlytorch211 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu121onlytorch211 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu118onlytorch212 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu121onlytorch212 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu118onlytorch220 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu121onlytorch220 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-]
-cu118onlytorch230 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu121onlytorch230 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu118onlytorch240 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu121onlytorch240 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu124onlytorch240 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu118onlytorch250 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu121onlytorch250 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu124onlytorch250 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu118onlytorch251 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu121onlytorch251 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu124onlytorch251 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu118onlytorch260 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-]
-cu124onlytorch260 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu126onlytorch260 = [
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu126onlytorch270 = [
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu128onlytorch270 = [
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
-]
-cu118 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118only]"",
-]
-cu121 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121only]"",
-]
-cu118-torch211 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu118onlytorch211]"",
-]
-cu121-torch211 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu121onlytorch211]"",
-]
-cu118-torch212 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu118onlytorch212]"",
-]
-cu121-torch212 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu121onlytorch212]"",
-]
-cu118-torch220 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch220]"",
-]
-cu121-torch220 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch220]"",
-]
-cu118-torch230 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch230]"",
-]
-cu121-torch230 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch230]"",
-]
-cu118-torch240 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch240]"",
-]
-cu121-torch240 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch240]"",
-]
-cu124-torch240 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch240]"",
-]
-cu118-torch250 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch250]"",
-]
-cu121-torch250 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch250]"",
-]
-cu124-torch250 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch250]"",
-]
-cu118-torch251 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch251]"",
-]
-cu121-torch251 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch251]"",
-]
-cu124-torch251 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch251]"",
-]
-cu118-torch260 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch260]"",
-]
-cu124-torch260 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch260]"",
-]
-cu126-torch260 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu126onlytorch260]"",
-]
-cu126-torch270 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu126onlytorch270]"",
-]
-cu128-torch270 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu128onlytorch270]"",
-]
-kaggle = [
-    ""unsloth[huggingface]"",
-]
-kaggle-new = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-]
-conda = [
-    ""unsloth[huggingface]"",
-]
-colab-torch211 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu121onlytorch211]"",
-]
-colab-ampere-torch211 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu121onlytorch211]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
-]
-colab-torch220 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch220]"",
-]
-colab-ampere-torch220 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch220]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
-]
-colab-new = [
-    ""unsloth_zoo>=2025.6.7"",
-    ""packaging"",
-    ""tyro"",
-    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
-    ""datasets>=3.4.1"",
-    ""sentencepiece>=0.2.0"",
-    ""tqdm"",
-    ""psutil"",
-    ""wheel>=0.42.0"",
-    ""numpy"",
-    ""protobuf"",
-    ""huggingface_hub"",
-    ""hf_transfer"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[triton]"",
-]
-colab-no-deps = [
-    ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
-    ""peft>=0.7.1"",
-    ""xformers"",
-    ""bitsandbytes>=0.45.5"",
-    ""protobuf"",
-]
-colab = [
-    ""unsloth[cu121]"",
-]
-flashattention = [
-    ""packaging ; platform_system == 'Linux'"",
-    ""ninja ; platform_system == 'Linux'"",
-    ""flash-attn>=2.6.3 ; platform_system == 'Linux'"",
-]
-colab-ampere = [
-    ""unsloth[colab-ampere-torch220]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118only]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121only]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch211 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu118onlytorch211]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere-torch211 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes==0.45.5"",
-    ""unsloth[cu121onlytorch211]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch220 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch220]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere-torch220 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch220]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch230 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch230]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere-torch230 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch230]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch240 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch240]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere-torch240 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch240]"",
-    ""unsloth[flashattention]"",
-]
-cu124-ampere-torch240 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch240]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch250 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch250]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere-torch250 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch250]"",
-    ""unsloth[flashattention]"",
-]
-cu124-ampere-torch250 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch250]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch251 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch251]"",
-    ""unsloth[flashattention]"",
-]
-cu121-ampere-torch251 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu121onlytorch251]"",
-    ""unsloth[flashattention]"",
-]
-cu124-ampere-torch251 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch251]"",
-    ""unsloth[flashattention]"",
-]
-cu118-ampere-torch260 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu118onlytorch260]"",
-    ""unsloth[flashattention]"",
-]
-cu124-ampere-torch260 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu124onlytorch260]"",
-    ""unsloth[flashattention]"",
-]
-cu126-ampere-torch260 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu126onlytorch260]"",
-    ""unsloth[flashattention]"",
-]
-cu126-ampere-torch270 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu126onlytorch270]"",
-    ""unsloth[flashattention]"",
-]
-cu128-ampere-torch270 = [
-    ""unsloth[huggingface]"",
-    ""bitsandbytes>=0.45.5"",
-    ""unsloth[cu128onlytorch270]"",
-    ""unsloth[flashattention]"",
-]
-
-flashattentiontorch260abiFALSEcu12x = [
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
-]
-flashattentiontorch260abiTRUEcu12x = [
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
-]
-flashattentiontorch250abiFALSEcu12x = [
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
-]
-flashattentiontorch250abiTRUEcu12x = [
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
-]
-flashattentiontorch240abiFALSEcu12x = [
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
-]
-flashattentiontorch240abiTRUEcu12x = [
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
-    ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
-]
-intel-gpu-torch260 = [
-    ""unsloth[huggingface]"",
-
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp39-cp39-linux_x86_64.whl#sha256=147607f190a7d7aa24ba454def5977fbbfec792fdae18e4ed278cfec29b69271 ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp310-cp310-linux_x86_64.whl#sha256=23aa423fa1542afc34f67eb3ba8ef20060f6d1b3a4697eaeab22b11c92b30f2b ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp311-cp311-linux_x86_64.whl#sha256=bcfa995229bbfd9ffd8d6c8d9f6428d393e876fa6e23ee3c20e3c0d73ca75ca5 ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp312-cp312-linux_x86_64.whl#sha256=bd340903d03470708df3442438acb8b7e08087ab9e61fbe349b2872bf9257ab0 ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp313-cp313-linux_x86_64.whl#sha256=814dccc8a07159e6eca74bed70091bc8fea2d9dd87b0d91845f9f38cde62f01c ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
-
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp39-cp39-linux_x86_64.whl#sha256=6a8adf6dc4c089406e8b3a7e58ab57a463bddf9b07130d2576e76eced43e92af ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=ff4561cbf07c83bbccaa0f6e9bb0e6dcf721bacd53c9c43c4eb0e7331b4792f9 ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=12005f66b810ddd3ab93f86c4522bcfdd412cbd27fc9d189b661ff7509bc5e8a ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=c4c5c67625cdacf35765c2b94e61fe166e3c3f4a14521b1212a59ad1b3eb0f2e ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=e6864f7a60a5ecc43d5d38f59a16e5dd132384f73dfd3a697f74944026038f7b ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
-]
-intel-gpu-torch270 = [
-    ""unsloth[huggingface]"",
-
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=749a7098492c6a27b356c97149a4a62973b953eae60bc1b6259260974f344913 ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=44362e80abd752471a08341093321955b066daa2cfb4810e73b8e3b240850f93 ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=faa6b8c945a837a080f641bc8ccc77a98fa66980dcd7e62e715fd853737343fd ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=40f6fb65b345dc9a61813abe7ac9a585f2c9808f414d140cc2a5f11f53ee063c ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
-    ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp313-cp313t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=9821fe059de58e827ffc6aa10d69369b16c2f8c2a988b86bef9c2c6e396ab3aa ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
-
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp39-cp39-linux_x86_64.whl#sha256=f8ee75e50fcbb37ed5b498299ca2264da99ab278a93fae2358e921e4a6e28273 ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=d6fdc342961d98fdcd9d03dfd491a3208bb5f7fbb435841f8f72ce9fdcd2d026 ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=74d07f9357df5cf2bf223ad3c84de16346bfaa0504f988fdd5590d3e177e5e86 ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=c806d44aa2ca5d225629f6fbc6c994d5deaac2d2cde449195bc8e3522ddd219a ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
-    ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=25d8277b7f01d42e2e014ccbab57a2692b6ec4eff8dcf894eda1b297407cf97a ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
-]
 
 [project.urls]
 homepage = ""http://www.unsloth.ai""
diff --git a/requirements/build.txt b/requirements/build.txt
new file mode 100644
index 0000000..1943dde
--- /dev/null
+++ b/requirements/build.txt
@@ -0,0 +1,7 @@
+cmake>=3.26
+ninja
+packaging>=24.2
+setuptools>=77.0.3,<80.0.0
+setuptools-scm>=8.0
+wheel
+jinja2
\ No newline at end of file
diff --git a/requirements/common.txt b/requirements/common.txt
new file mode 100644
index 0000000..61c547d
--- /dev/null
+++ b/requirements/common.txt
@@ -0,0 +1,15 @@
+unsloth_zoo>=2025.6.2
+packaging
+tyro
+transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2
+datasets>=3.4.1
+sentencepiece>=0.2.0
+tqdm
+psutil
+wheel>=0.42.0
+numpy
+accelerate>=0.34.1
+trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0
+peft>=0.7.1,!=0.11.0
+huggingface_hub
+hf_transfer
diff --git a/requirements/cuda.txt b/requirements/cuda.txt
new file mode 100644
index 0000000..8028f64
--- /dev/null
+++ b/requirements/cuda.txt
@@ -0,0 +1,7 @@
+# Common dependencies
+-r common.txt
+
+torch
+torchaudio
+torchvision
+xformers
diff --git a/requirements/rocm.txt b/requirements/rocm.txt
new file mode 100644
index 0000000..10dde50
--- /dev/null
+++ b/requirements/rocm.txt
@@ -0,0 +1,17 @@
+# Common dependencies
+-r common.txt
+
+--extra-index-url https://download.pytorch.org/whl/rocm6.3
+
+torch==2.7.0
+torchvision==0.21.0
+torchaudio==2.7.0
+
+triton==3.2
+cmake>=3.26,<4
+packaging
+setuptools>=77.0.3,<80.0.0
+setuptools-scm>=8
+wheel
+jinja2>=3.1.6
+amdsmi==6.4.1
diff --git a/setup.py b/setup.py
new file mode 100644
index 0000000..0f4a863
--- /dev/null
+++ b/setup.py
@@ -0,0 +1,844 @@
+# Copid and modified based on https://github.com/vllm-project/vllm/blob/main/setup.py
+# SPDX-License-Identifier: Apache-2.0
+
+import ctypes
+import importlib.util
+import json
+import logging
+import os
+import re
+import subprocess
+import sys
+from pathlib import Path
+from shutil import which
+import shutil
+
+import torch
+from packaging.version import Version, parse
+from setuptools import Extension, setup
+from setuptools.command.build_ext import build_ext
+from setuptools_scm import get_version
+from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME
+
+from setuptools.command.install import install
+
+# This arg is for multi-device
+UNSLOTH_TARGET_DEVICE = os.environ.get('UNSLOTH_TARGET_DEVICE', 'cuda')
+
+
+def load_module_from_path(module_name, path):
+    spec = importlib.util.spec_from_file_location(module_name, path)
+    module = importlib.util.module_from_spec(spec)
+    sys.modules[module_name] = module
+    spec.loader.exec_module(module)
+    return module
+
+ROOT_DIR = Path(__file__).parent
+
+
+# cannot import version directly because it depends on unsloth,
+#  which is not installed yet
+ver = load_module_from_path('ver', os.path.join(ROOT_DIR, 'unsloth', 'version.py'))
+
+def _is_cuda() -> bool:
+    has_cuda = torch.version.cuda is not None
+    return UNSLOTH_TARGET_DEVICE == ""cuda"" and has_cuda
+
+
+def _is_hip() -> bool:
+    return (UNSLOTH_TARGET_DEVICE == ""cuda""
+            or UNSLOTH_TARGET_DEVICE == ""rocm"") and torch.version.hip is not None
+
+
+def get_nvcc_cuda_version() -> Version:
+    """"""Get the CUDA version from nvcc.
+
+    Adapted from https://github.com/NVIDIA/apex/blob/8b7a1ff183741dd8f9b87e7bafd04cfde99cea28/setup.py
+    """"""
+    assert CUDA_HOME is not None, ""CUDA_HOME is not set""
+    nvcc_output = subprocess.check_output([CUDA_HOME + ""/bin/nvcc"", ""-V""],
+                                          universal_newlines=True)
+    output = nvcc_output.split()
+    release_idx = output.index(""release"") + 1
+    nvcc_cuda_version = parse(output[release_idx].split("","")[0])
+    return nvcc_cuda_version
+
+
+def get_rocm_version():
+    # Get the Rocm version from the ROCM_HOME/bin/librocm-core.so
+    # see https://github.com/ROCm/rocm-core/blob/d11f5c20d500f729c393680a01fa902ebf92094b/rocm_version.cpp#L21
+    try:
+        librocm_core_file = Path(ROCM_HOME) / ""lib"" / ""librocm-core.so""
+        if not librocm_core_file.is_file():
+            return None
+        librocm_core = ctypes.CDLL(librocm_core_file)
+        VerErrors = ctypes.c_uint32
+        get_rocm_core_version = librocm_core.getROCmVersion
+        get_rocm_core_version.restype = VerErrors
+        get_rocm_core_version.argtypes = [
+            ctypes.POINTER(ctypes.c_uint32),
+            ctypes.POINTER(ctypes.c_uint32),
+            ctypes.POINTER(ctypes.c_uint32),
+        ]
+        major = ctypes.c_uint32()
+        minor = ctypes.c_uint32()
+        patch = ctypes.c_uint32()
+
+        if (get_rocm_core_version(ctypes.byref(major), ctypes.byref(minor),
+                                  ctypes.byref(patch)) == 0):
+            return f""{major.value}.{minor.value}.{patch.value}""
+        return None
+    except Exception:
+        return None
+
+
+def get_unsloth_version() -> str:
+    version = ver.__version__
+
+    if version is None:
+        raise RuntimeError(""unsloth version not found"")
+
+    sep = ""+"" if ""+"" not in version else "".""  # dev versions might contain +
+
+    if _is_cuda():
+        cuda_version = str(get_nvcc_cuda_version())
+        cuda_version_str = cuda_version.replace(""."", """")[:3]
+        # skip this for source tarball, required for pypi
+        if ""sdist"" not in sys.argv:
+            version += f""{sep}cu{cuda_version_str}""
+    elif _is_hip():
+        # Get the Rocm Version
+        rocm_version = get_rocm_version() or torch.version.hip
+        if rocm_version:
+            version += f""{sep}rocm{rocm_version.replace('.', '')[:3]}""
+    else:
+        raise RuntimeError(""Unknown runtime environment"")
+
+    return version
+
+def get_requirements() -> list[str]:
+    """"""Get Python package dependencies from requirements.txt.""""""
+    requirements_dir = ROOT_DIR / ""requirements""
+
+    def _read_requirements(filename: str) -> list[str]:
+        with open(requirements_dir / filename) as f:
+            requirements = f.read().strip().split(""\n"")
+        resolved_requirements = []
+        for line in requirements:
+            if line.startswith(""-r ""):
+                resolved_requirements += _read_requirements(line.split()[1])
+            elif not line.startswith(""--"") and not line.startswith(
+                    ""#"") and line.strip() != """":
+                resolved_requirements.append(line)
+        return resolved_requirements
+
+    if _is_cuda():
+        requirements = _read_requirements(""cuda.txt"")
+    elif _is_hip():
+        requirements = _read_requirements(""rocm.txt"")
+    else:
+        requirements = _read_requirements(""common.txt"")
+        raise ValueError(
+            ""Unsupported platform, please use CUDA, ROCm, ""
+        )
+
+    return requirements
+
+
+INSTINCT_ARCH=(""gfx942"", ""gfx90a"")
+RADEON_ARCH=(""gfx1100"", ""gfx1101"", ""gfx1102"", ""gfx1200"", ""gfx1201"")
+
+
+class RocmExtraInstallCommand(install):
+    def run(self):
+
+        if os.path.exists('thirdparties'):
+            shutil.rmtree('thirdparties')
+
+        os.mkdir('thirdparties')
+        os.chdir('thirdparties')
+
+        # Extract ROCm GPU arch from environment variable. If unset, then detect ROCm arch from rocminfo
+        # Refer to https://github.com/bitsandbytes-foundation/bitsandbytes/blob/1abd5e781013a085f86586b30a248dc769909668/bitsandbytes/cuda_specs.py#L81
+        # TODO(billishyahao): need to triage rocminfo unavailable observation from https://github.com/bitsandbytes-foundation/bitsandbytes/issues/1444
+        rocm_arch = os.environ.get('ROCM_ARCH', None)
+        if rocm_arch is None:
+            try:
+                result = subprocess.run([""rocminfo""], capture_output=True, text=True)
+                match = re.search(r""Name:\s+gfx([a-zA-Z\d]+)"", result.stdout)
+                if match:
+                    rocm_arch = f""gfx{match.group(1)}""
+                    print(f""Automatically detected ROCm GPU architecture: {rocm_arch}"")
+                else:
+                    print(""Skipping ROCm extra install, cannot detect ROCm arch automatically..."")
+                    install.run(self)
+                    return
+            except Exception as e:
+                print(""Could not detect ROCm GPU architecture: {e}"")
+                if torch.cuda.is_available():
+                    print(""ROCm GPU architecture detection failed despite ROCm being available..."")
+                install.run(self)
+                return
+
+        # flash-attention
+        # MI3xx has both CK backend and Triton backend.
+        import importlib
+        if importlib.util.find_spec(""flash_attn"") is None:
+            print(""Installing flash-attention..."")
+            if rocm_arch in INSTINCT_ARCH:
+                subprocess.check_call(['git', 'clone', '--recursive', 'https://github.com/ROCm/flash-attention.git'])
+                os.chdir('flash-attention')
+                num_jobs = os.cpu_count() - 1
+                subprocess.check_call(['pip', 'install', '-v', '.', f'MAX_JOBS={num_jobs}'], shell=True)
+                os.chdir('..')
+            # Only Triton backend supports Radeon GPUs
+            elif rocm_arch in RADEON_ARCH:
+                subprocess.check_call(['git', 'clone', '--recursive', 'https://github.com/ROCm/flash-attention.git'])
+                os.chdir('flash-attention')
+                subprocess.check_call(['git', 'checkout', 'main_perf'])
+                subprocess.check_call(['FLASH_ATTENTION_TRITON_AMD_ENABLE=""TRUE""', 'python', 'setup.py', 'install', ], shell=True)
+                os.chdir('..')
+
+        # Comment out the following if you need xformers installed.
+        # # only install xformers in Instinct GPUs
+        # if importlib.util.find_spec(""xformers"") is None:
+        #     print(""Installing xformers..."")
+        #     if rocm_arch in INSTINCT_ARCH:
+        #         subprocess.check_call(['git', 'clone', 'https://github.com/ROCm/xformers.git'])
+        #         os.chdir('xformers')
+        #         subprocess.check_call(['git', 'submodule', 'update', '--init', '--recursive'])
+        #         os.environ['PYTORCH_ROCM_ARCH'] = rocm_arch
+        #         subprocess.check_call(['python', 'setup.py', 'install'])
+        #         os.chdir('..')
+
+        # bitsandbytes
+        if importlib.util.find_spec(""bitsandbytes"") is None:
+            print(""Installing bitsandbytes..."")
+            subprocess.check_call(['git', 'clone', '--recurse-submodules', 'https://github.com/ROCm/bitsandbytes'])
+            os.chdir('bitsandbytes')
+            subprocess.check_call(['git', 'checkout', 'rocm_enabled_multi_backend'])
+            subprocess.check_call(['pip', 'install', '-r', 'requirements-dev.txt'])
+            subprocess.check_call(['cmake', '-DCOMPUTE_BACKEND=hip', '-S', '.'])  # Add -DBNB_ROCM_ARCH if needed
+            subprocess.check_call(['make'])
+            subprocess.check_call(['pip', 'install', '.'])
+            os.chdir('..')
+        
+        os.chdir('..')
+
+        # Continue with regular install
+        install.run(self)
+
+package_data = {
+    ""unsloth"": [
+        ""py.typed"",
+    ]
+}
+
+extras_require = {
+    ""triton"" : [
+        ""triton-windows ; platform_system == 'Windows'"",
+    ],
+
+    ""huggingface"" : [
+        ""unsloth_zoo>=2025.6.6"",
+        ""packaging"",
+        ""tyro"",
+        ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
+        ""datasets>=3.4.1"",
+        ""sentencepiece>=0.2.0"",
+        ""tqdm"",
+        ""psutil"",
+        ""wheel>=0.42.0"",
+        ""numpy"",
+        ""accelerate>=0.34.1"",
+        ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+        ""peft>=0.7.1,!=0.11.0"",
+        ""protobuf"",
+        ""huggingface_hub"",
+        ""hf_transfer"",
+        ""unsloth[triton]"",
+    ],
+    ""windows"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5 ; platform_system == 'Windows'"",
+        ""xformers>=0.0.22.post7 ; platform_system == 'Windows'"",
+    ],
+    ""cu118only"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu121only"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu118onlytorch211"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch211"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu118onlytorch212"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch212"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu118onlytorch220"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch220"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ],
+    ""cu118onlytorch230"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch230"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu118onlytorch240"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch240"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu124onlytorch240"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu118onlytorch250"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch250"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu124onlytorch250"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu118onlytorch251"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu121onlytorch251"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu124onlytorch251"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu118onlytorch260"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ],
+    ""cu124onlytorch260"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu126onlytorch260"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu126onlytorch270"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu128onlytorch270"" : [
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+        ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ],
+    ""cu118"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118only]"",
+    ],
+    ""cu121"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121only]"",
+    ],
+    ""cu118-torch211"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu118onlytorch211]"",
+    ],
+    ""cu121-torch211"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu121onlytorch211]"",
+    ],
+    ""cu118-torch212"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu118onlytorch212]"",
+    ],
+    ""cu121-torch212"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu121onlytorch212]"",
+    ],
+    ""cu118-torch220"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch220]"",
+    ],
+    ""cu121-torch220"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch220]"",
+    ],
+    ""cu118-torch230"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch230]"",
+    ],
+    ""cu121-torch230"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch230]"",
+    ],
+    ""cu118-torch240"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch240]"",
+    ],
+    ""cu121-torch240"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch240]"",
+    ],
+    ""cu124-torch240"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch240]"",
+    ],
+    ""cu118-torch250"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch250]"",
+    ],
+    ""cu121-torch250"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch250]"",
+    ],
+    ""cu124-torch250"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch250]"",
+    ],
+    ""cu118-torch251"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch251]"",
+    ],
+    ""cu121-torch251"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch251]"",
+    ],
+    ""cu124-torch251"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch251]"",
+    ],
+    ""cu118-torch260"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch260]"",
+    ],
+    ""cu124-torch260"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch260]"",
+    ],
+    ""cu126-torch260"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu126onlytorch260]"",
+    ],
+    ""cu126-torch270"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu126onlytorch270]"",
+    ],
+    ""cu128-torch270"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu128onlytorch270]"",
+    ],
+    ""kaggle"" : [
+        ""unsloth[huggingface]"",
+    ],
+    ""kaggle-new"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+    ],
+    ""conda"" : [
+        ""unsloth[huggingface]"",
+    ],
+    ""colab-torch211"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu121onlytorch211]"",
+    ],
+    ""colab-ampere-torch211"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu121onlytorch211]"",
+        ""packaging"",
+        ""ninja"",
+        ""flash-attn>=2.6.3"",
+    ],
+    ""colab-torch220"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch220]"",
+    ],
+    ""colab-ampere-torch220"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch220]"",
+        ""packaging"",
+        ""ninja"",
+        ""flash-attn>=2.6.3"",
+    ],
+    ""colab-new"" : [
+        ""unsloth_zoo>=2025.6.6"",
+        ""packaging"",
+        ""tyro"",
+        ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
+        ""datasets>=3.4.1"",
+        ""sentencepiece>=0.2.0"",
+        ""tqdm"",
+        ""psutil"",
+        ""wheel>=0.42.0"",
+        ""numpy"",
+        ""protobuf"",
+        ""huggingface_hub"",
+        ""hf_transfer"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[triton]"",
+    ],
+    ""colab-no-deps"" : [
+        ""accelerate>=0.34.1"",
+        ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+        ""peft>=0.7.1"",
+        ""xformers"",
+        ""bitsandbytes>=0.45.5"",
+        ""protobuf"",
+    ],
+    ""colab"" : [
+        ""unsloth[cu121]"",
+    ],
+    ""flashattention"" : [
+        ""packaging ; platform_system == 'Linux'"",
+        ""ninja ; platform_system == 'Linux'"",
+        ""flash-attn>=2.6.3 ; platform_system == 'Linux'"",
+    ],
+    ""colab-ampere"" : [
+        ""unsloth[colab-ampere-torch220]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118only]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121only]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch211"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu118onlytorch211]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere-torch211"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes==0.45.5"",
+        ""unsloth[cu121onlytorch211]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch220"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch220]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere-torch220"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch220]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch230"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch230]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere-torch230"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch230]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch240"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch240]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere-torch240"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch240]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu124-ampere-torch240"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch240]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch250"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch250]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere-torch250"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch250]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu124-ampere-torch250"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch250]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch251"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch251]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu121-ampere-torch251"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu121onlytorch251]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu124-ampere-torch251"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch251]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu118-ampere-torch260"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu118onlytorch260]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu124-ampere-torch260"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu124onlytorch260]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu126-ampere-torch260"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu126onlytorch260]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu126-ampere-torch270"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu126onlytorch270]"",
+        ""unsloth[flashattention]"",
+    ],
+    ""cu128-ampere-torch270"" : [
+        ""unsloth[huggingface]"",
+        ""bitsandbytes>=0.45.5"",
+        ""unsloth[cu128onlytorch270]"",
+        ""unsloth[flashattention]"",
+    ],
+
+    ""flashattentiontorch260abiFALSEcu12x"" : [
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
+    ],
+    ""flashattentiontorch260abiTRUEcu12x"" : [
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
+    ],
+    ""flashattentiontorch250abiFALSEcu12x"" : [
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
+    ],
+    ""flashattentiontorch250abiTRUEcu12x"" : [
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
+    ],
+    ""flashattentiontorch240abiFALSEcu12x"" : [
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiFALSE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
+    ],
+    ""flashattentiontorch240abiTRUEcu12x"" : [
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp311-cp311-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.11'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp312-cp312-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.12'"",
+        ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.4cxx11abiTRUE-cp313-cp313-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.13'"",
+    ],
+    ""intel-gpu-torch260"" : [
+        ""unsloth[huggingface]"",
+
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp39-cp39-linux_x86_64.whl#sha256=147607f190a7d7aa24ba454def5977fbbfec792fdae18e4ed278cfec29b69271 ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp310-cp310-linux_x86_64.whl#sha256=23aa423fa1542afc34f67eb3ba8ef20060f6d1b3a4697eaeab22b11c92b30f2b ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp311-cp311-linux_x86_64.whl#sha256=bcfa995229bbfd9ffd8d6c8d9f6428d393e876fa6e23ee3c20e3c0d73ca75ca5 ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp312-cp312-linux_x86_64.whl#sha256=bd340903d03470708df3442438acb8b7e08087ab9e61fbe349b2872bf9257ab0 ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.2.0-cp313-cp313-linux_x86_64.whl#sha256=814dccc8a07159e6eca74bed70091bc8fea2d9dd87b0d91845f9f38cde62f01c ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
+
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp39-cp39-linux_x86_64.whl#sha256=6a8adf6dc4c089406e8b3a7e58ab57a463bddf9b07130d2576e76eced43e92af ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=ff4561cbf07c83bbccaa0f6e9bb0e6dcf721bacd53c9c43c4eb0e7331b4792f9 ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=12005f66b810ddd3ab93f86c4522bcfdd412cbd27fc9d189b661ff7509bc5e8a ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=c4c5c67625cdacf35765c2b94e61fe166e3c3f4a14521b1212a59ad1b3eb0f2e ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.6.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=e6864f7a60a5ecc43d5d38f59a16e5dd132384f73dfd3a697f74944026038f7b ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
+    ],
+    ""intel-gpu-torch270"" : [
+        ""unsloth[huggingface]"",
+
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=749a7098492c6a27b356c97149a4a62973b953eae60bc1b6259260974f344913 ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=44362e80abd752471a08341093321955b066daa2cfb4810e73b8e3b240850f93 ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=faa6b8c945a837a080f641bc8ccc77a98fa66980dcd7e62e715fd853737343fd ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=40f6fb65b345dc9a61813abe7ac9a585f2c9808f414d140cc2a5f11f53ee063c ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
+        ""pytorch_triton_xpu @ https://download.pytorch.org/whl/pytorch_triton_xpu-3.3.0-cp313-cp313t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl#sha256=9821fe059de58e827ffc6aa10d69369b16c2f8c2a988b86bef9c2c6e396ab3aa ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
+
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp39-cp39-linux_x86_64.whl#sha256=f8ee75e50fcbb37ed5b498299ca2264da99ab278a93fae2358e921e4a6e28273 ; platform_system == 'Linux' and python_version == '3.9' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp310-cp310-linux_x86_64.whl#sha256=d6fdc342961d98fdcd9d03dfd491a3208bb5f7fbb435841f8f72ce9fdcd2d026 ; platform_system == 'Linux' and python_version == '3.10' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp311-cp311-linux_x86_64.whl#sha256=74d07f9357df5cf2bf223ad3c84de16346bfaa0504f988fdd5590d3e177e5e86 ; platform_system == 'Linux' and python_version == '3.11' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp312-cp312-linux_x86_64.whl#sha256=c806d44aa2ca5d225629f6fbc6c994d5deaac2d2cde449195bc8e3522ddd219a ; platform_system == 'Linux' and python_version == '3.12' and platform_machine == 'x86_64'"",
+        ""torch @ https://download.pytorch.org/whl/xpu/torch-2.7.0%2Bxpu-cp313-cp313-linux_x86_64.whl#sha256=25d8277b7f01d42e2e014ccbab57a2692b6ec4eff8dcf894eda1b297407cf97a ; platform_system == 'Linux' and python_version == '3.13' and platform_machine == 'x86_64'"",
+    ]
+}
+
+cmdclass = {}
+
+if _is_hip():
+    cmdclass = {
+        'install': RocmExtraInstallCommand
+    }
+
+setup(
+    # static metadata should rather go in pyproject.toml
+    version=get_unsloth_version(),
+    install_requires=get_requirements(),
+    extras_require=extras_require,
+    cmdclass=cmdclass,
+    package_data=package_data,
+)
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a71accd..5016e1b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.6.9""
+from unsloth.version import __version__
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -1312,26 +1312,22 @@ pass
 
 def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, model):
     from peft import LoraConfig
-
     if loftq_config is None: loftq_config = {}
 
     signature = str(inspect.signature(LoraConfig))
     SUPPORTS_LOFTQ  = ""loftq_config"" in signature
-
     if lora_dropout != 0:
         logger.warning_once(
             f""Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = {lora_dropout}.\n""\
             f""Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.""
         )
     pass
-
     if bias != ""none"":
         logger.warning_once(
             f""Unsloth: bias = `none` is supported for fast patching. You are using bias = {bias}.\n""\
             f""Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.""
         )
     pass
-
     if not (type(init_lora_weights) is bool or \
         init_lora_weights == ""gaussian"" or init_lora_weights == ""loftq""):
         raise ValueError(
@@ -1340,7 +1336,6 @@ def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, m
     pass
 
     if init_lora_weights == ""loftq"":
-
         if not SUPPORTS_LOFTQ:
             import peft
             raise RuntimeError(
@@ -1349,7 +1344,6 @@ def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, m
                 ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
             )
         pass
-
         if loftq_config == {}:
             from peft import LoftQConfig
             logger.warning_once(
@@ -1358,7 +1352,6 @@ def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, m
             )
             loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)
         pass
-        
         if hasattr(model.config, ""quantization_config""):
             raise ValueError(
                 ""Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\n""\
@@ -1366,5 +1359,5 @@ def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, m
             )
         pass
     pass
-
-    return loftq_config
\ No newline at end of file
+    return loftq_config
+pass
diff --git a/unsloth/version.py b/unsloth/version.py
new file mode 100644
index 0000000..9878268
--- /dev/null
+++ b/unsloth/version.py
@@ -0,0 +1 @@
+__version__ = ""2025.6.8""
\ No newline at end of file
diff --git a/use_existing_torch.py b/use_existing_torch.py
new file mode 100644
index 0000000..0f1fe66
--- /dev/null
+++ b/use_existing_torch.py
@@ -0,0 +1,22 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project and Unsloth project
+# Copying from https://github.com/vllm-project/vllm/blob/main/use_existing_torch.py
+
+import glob
+
+requires_files = glob.glob('requirements/*.txt')
+requires_files += [""pyproject.toml""]
+for file in requires_files:
+    print(f"">>> cleaning {file}"")
+    with open(file) as f:
+        lines = f.readlines()
+    if ""torch"" in """".join(lines).lower():
+        print(""removed:"")
+        with open(file, 'w') as f:
+            for line in lines:
+                if 'torch' not in line.lower():
+                    f.write(line)
+                else:
+                    print(line.strip())
+    print(f""<<< done cleaning {file}"")
+    print()
"
"diff --git a/pyproject.toml b/pyproject.toml
index 8b24a52..b538578 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.1"",
+    ""unsloth_zoo>=2025.7.2"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.1"",
+    ""unsloth_zoo>=2025.7.2"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 4da08da..e965b29 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -79,7 +79,7 @@ def get_device_count():
     elif DEVICE_TYPE == ""xpu"":
         return torch.xpu.device_count()
     else:
-        return 0
+        return 1
 pass
 
 DEVICE_COUNT : int = get_device_count()
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c6ff7b6..5076c42 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.1""
+__version__ = ""2025.7.2""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index db0e884..8ccdeba 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -796,7 +796,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif self.training and os.environ.get(""UNSLOTH_KEEP_PADDING"", ""0"") != '1':
+    elif self.training:
         attention_mask = None
         padding_mask = None
     else:
"
"diff --git a/README.md b/README.md
index 665cbd5..0a13ac4 100644
--- a/README.md
+++ b/README.md
@@ -168,8 +168,48 @@ x = x.format(cuda.replace(""."", """"), ""-ampere"" if is_ampere else """")
 print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git""')
 ```
 
-### Windows Installation
+## Windows Installation
+### Step 1: NVIDIA Video Driver
+
+You should install the latest version of your GPUs driver. You can download drivers here:
+ - [NVIDIA GPU Drive Download](https://www.nvidia.com/Download/index.aspx)
+
+### Step 2: Visual Studio C++
+You will need Visual Studio, with C++ installed. By default, C++ is not installed with Visual Studio, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK.
+  - [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/community/)
+<table>
+  <tr>
+    <td>
+      <img src=""https://github.com/user-attachments/assets/d3e6ca95-85bb-442a-8c6f-81944300598e"" alt=""VSCode C++ Ref Image"" width=""400"" height=""350""/>
+    </td>
+    <td>
+      <div align=""center"">
+        <h1>Steps to configure VS C++</h1>
+      </div>
+      <ol>
+        <li>Launch the Installer downloaded from the link above.</li>
+        <li>In the installer, navigate to Individual components and select all the options mentioned in the image.</li>
+        <li>Click on install now.</li>
+      </ol>
+    </td>
+  </tr>
+</table>
+
+### Step 3: CUDA Toolkit
+
+ - [Download CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)
+
+### Step 4: Install PyTorch 
+
+You will need the correct version of PyTorch that is compatibile with your CUDA drivers, so make sure to select them carefully
+ - [Install PyTorch](https://pytorch.org/get-started/locally/)
+
+### Step 5: Install Unsloth
+```python
+pip install ""unsloth[windows] @ git+https://github.com/unslothai/unsloth.git""
+```
 
+### Side note
 To run Unsloth directly on Windows:
 - Install Triton from this Windows fork and follow the instructions: https://github.com/woct0rdho/triton-windows (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)
 - In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:
diff --git a/pyproject.toml b/pyproject.toml
index 14797c8..de1583e 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,10 +33,32 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 triton = [
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'""
+]
+
+windows=[
+    ""unsloth_zoo>=2025.2.7"",
+    ""packaging"",
+    ""tyro"",
+    ""transformers>=4.46.1,!=4.47.0"",
+    ""datasets>=2.16.0"",
+    ""sentencepiece>=0.2.0"",
+    ""tqdm"",
+    ""psutil"",
+    ""wheel>=0.42.0"",
+    ""numpy"",
+    ""accelerate>=0.34.1"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+    ""peft>=0.7.1,!=0.11.0"",
+    ""protobuf<4.0.0"",
+    ""huggingface_hub"",
+    ""hf_transfer"",
+    ""unsloth[triton]"",
+    ""bitsandbytes>=0.41.1 ; platform_system == 'Windows'"",
+    ""xformers>=0.0.22.post7 ; platform_system == 'Windows'"",
 ]
 huggingface = [
     ""unsloth_zoo>=2025.2.7"",
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
new file mode 100644
index 0000000..754c7ab
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -0,0 +1,24 @@
+---
+name: Bug report
+about: Reporting a bug
+title: ""[BUG]""
+labels: ''
+assignees: ''
+
+---
+
+1. Have you tried uninstall Unsloth and upgrading?
+```bash
+pip uninstall unsloth unsloth_zoo -y
+pip install --upgrade --no-deps --no-cache-dir unsloth unsloth_zoo
+```
+2. If there's a bug, please print out your Unsloth info:
+```python
+ Unsloth: Will patch your computer to enable 2x faster free finetuning.
+==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.2.
+   \\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.
+O^O/ \_/ \    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.
+\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]
+ ""-____-""     Free Apache license: http://github.com/unslothai/unsloth
+```
+3. Otherwise, please describe your problem below:
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index cfb3ece..e784909 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1029,7 +1029,7 @@ qwen3_template = \
         {{- ""\n"" }}
         {{- tool | tojson }}
     {%- endfor %}
-    {{- ""\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\""name\"": <function-name>, \""arguments\"": <args-json-object>}\n</tool_call><|im_end|>\n"" }}
+    {{- ""\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\\""name\\"": <function-name>, \\""arguments\\"": <args-json-object>}\n</tool_call><|im_end|>\n"" }}
 {%- else %}
     {%- if messages[0].role == 'system' %}
         {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index 754c7ab..24c7080 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -12,7 +12,7 @@ assignees: ''
 pip uninstall unsloth unsloth_zoo -y
 pip install --upgrade --no-deps --no-cache-dir unsloth unsloth_zoo
 ```
-2. If there's a bug, please print out your Unsloth info:
+2. If there's a bug, please print out your Unsloth info (or do a screenshot):
 ```python
  Unsloth: Will patch your computer to enable 2x faster free finetuning.
 ==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.2.
@@ -21,4 +21,4 @@ O^O/ \_/ \    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.
 \        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]
  ""-____-""     Free Apache license: http://github.com/unslothai/unsloth
 ```
-3. Otherwise, please describe your problem below:
+3. Otherwise, describe your problem:
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index dfaf36a..a6596d4 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.6.2""
+__version__ = ""2025.6.3""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -755,24 +755,10 @@ exec(BitsAndBytesConfig__init__, globals())
 
 if torch.cuda.device_count() == 1:
     from accelerate.utils.dataclasses import DistributedType
-    def _prepare_backend(
-        self, cpu = False, sagemaker_dp = False, backend: str = None,
-    ) -> tuple[str, DistributedType]:
-        return None, DistributedType.NO
-    pass
+    def _prepare_backend(self, *args, **kwargs): return None, DistributedType.NO
     import accelerate.state
     accelerate.state.PartialState._prepare_backend = _prepare_backend
-
-    import accelerate.accelerator
-    prepare = inspect.getsource(accelerate.accelerator.Accelerator.prepare)
-    prepare = prepare.split(""\n"")
-    spaces = prepare[0].find(""def"")
-    prepare = ""\n"".join(x[spaces:] for x in prepare)
-    x = ""for obj in args:""
-    s = "" ""*spaces
-    prepare = prepare.replace(x, f'self.state.distributed_type = DistributedType.NO\n{s}{x}', 1)
-    exec(prepare, globals())
-    accelerate.accelerator.Accelerator.prepare = prepare
+    accelerate.accelerator.Accelerator.distributed_type = lambda *args, **kwargs: DistributedType.NO
 pass
 
 import transformers.utils.quantization_config
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 3fa3fe7..08d7cbf 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -645,6 +645,18 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
     init = inspect.getsource(RLTrainer.__init__)
     old_init = init
 
+    # Remove brackets in comments since it interferes ie (...)
+    comments = re.findall(r""\#[^\n]{1,}\n"", init)
+    bracketed_comments = [x for x in comments if ""("" in x or "")"" in x]
+    # Replace with [...] instead
+    for bracketed_comment in bracketed_comments:
+        init = init.replace(
+            bracketed_comment,
+            bracketed_comment.replace(""("", ""["").replace("")"", ""]""),
+        )
+    pass
+
+
     # Remove peft_config
     init = init.replace(""elif peft_config is None:"", ""elif False:"")
     init = init.replace(""elif peft_config is not None:"", ""elif False:"")
"
"diff --git a/README.md b/README.md
index 0a3c83f..4d68d99 100644
--- a/README.md
+++ b/README.md
@@ -299,6 +299,9 @@ DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as
 We're in Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
 
 ```python
+import os
+os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"" # Optional set GPU device ID
+
 from unsloth import FastLanguageModel, PatchDPOTrainer
 from unsloth import is_bfloat16_supported
 PatchDPOTrainer()
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 5102d8f..745b210 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -31,6 +31,10 @@ import numpy as np
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
 # Thank you for your understanding and we appreciate it immensely!
+
+# Fixes https://github.com/unslothai/unsloth/issues/1266
+os.environ[""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""] = ""python""
+
 if ""CUDA_VISIBLE_DEVICES"" in os.environ:
     os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
     devices = os.environ[""CUDA_VISIBLE_DEVICES""]
@@ -172,3 +176,6 @@ from .save import *
 from .chat_templates import *
 from .tokenizer_utils import *
 from .trainer import *
+
+# Patch TRL trainers for backwards compatibility
+_patch_trl_trainer()
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index b254202..da10f7e 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -39,6 +39,7 @@ from unsloth_zoo.dataset_utils import (
     train_on_responses_only,
 )
 CHAT_TEMPLATES = {}
+DEFAULT_SYSTEM_MESSAGE = {}
 
 # =========================================== Unsloth
 # Unsloth efficient template leverages from Zephyr
@@ -48,7 +49,7 @@ unsloth_template = \
         ""{{ messages[0]['content'] + '\n' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'You are a helpful assistant to the user\n' }}""\
+        ""{{ '{system_message}' + '\n' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -80,6 +81,7 @@ SYSTEM """"""You are a helpful assistant to the user""""""
 
 unsloth_eos_token = ""eos_token""
 CHAT_TEMPLATES[""unsloth""] = (unsloth_template, unsloth_eos_token, False, unsloth_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""unsloth""] = ""You are a helpful assistant to the user""
 pass
 
 # =========================================== Zephyr
@@ -116,6 +118,7 @@ PARAMETER min_p 0.1
 
 zephyr_eos_token = ""eos_token""
 CHAT_TEMPLATES[""zephyr""] = (zephyr_template, zephyr_eos_token, False, zephyr_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""zephyr""] = None # No system message in Zephyr
 pass
 
 # =========================================== ChatML
@@ -153,6 +156,7 @@ PARAMETER min_p 0.1
 
 chatml_eos_token = ""<|im_end|>""
 CHAT_TEMPLATES[""chatml""] = (chatml_template, chatml_eos_token, True, chatml_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""chatml""] = None # No system message in ChatML
 pass
 
 # =========================================== Mistral-1
@@ -193,6 +197,7 @@ PARAMETER min_p 0.1
 
 mistral_eos_token = ""eos_token""
 CHAT_TEMPLATES[""mistral""] = (mistral_template, mistral_eos_token, False, mistral_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""mistral""] = None # No system message in Mistral
 pass
 
 # =========================================== Llama-2
@@ -234,6 +239,7 @@ PARAMETER min_p 0.1
 
 llama_eos_token = ""eos_token""
 CHAT_TEMPLATES[""llama""] = (llama_template, llama_eos_token, False, llama_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama""] = None # No system message in Llama
 pass
 
 # ===========================================  Vicuna
@@ -244,7 +250,7 @@ vicuna_template = \
         ""{{ messages[0]['content'] + ' ' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions.' + ' ' }}""\
+        ""{{ '{system_message}' + ' ' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -273,6 +279,7 @@ PARAMETER min_p 0.1
 
 vicuna_eos_token = ""eos_token""
 CHAT_TEMPLATES[""vicuna""] = (vicuna_template, vicuna_eos_token, False, vicuna_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""vicuna""] = ""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.""
 pass
 
 # =========================================== Vicuna Old
@@ -283,7 +290,7 @@ vicuna_old_template = \
         ""{{ messages[0]['content'] + '\n' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\\'s questions.' + '\n' }}""\
+        ""{{ '{system_message}' + '\n' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -315,6 +322,10 @@ SYSTEM """"""A chat between a curious human and an artificial intelligence assistan
 
 vicuna_old_eos_token = ""eos_token""
 CHAT_TEMPLATES[""vicuna_old""] = (vicuna_old_template, vicuna_old_eos_token, False, vicuna_old_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""vicuna_old""] = ""A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\\'s questions.""
+
+CHAT_TEMPLATES[""vicuna old""] = CHAT_TEMPLATES[""vicuna_old""]
+DEFAULT_SYSTEM_MESSAGE[""vicuna old""] = DEFAULT_SYSTEM_MESSAGE[""vicuna_old""]
 pass
 
 # =========================================== Alpaca multi turn
@@ -325,7 +336,7 @@ alpaca_template = \
         ""{{ messages[0]['content'] + '\n\n' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n\n' }}""\
+        ""{{ '{system_message}' + '\n\n' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -362,6 +373,7 @@ SYSTEM """"""Below are some instructions that describe some tasks. Write responses
 
 alpaca_eos_token = ""eos_token""
 CHAT_TEMPLATES[""alpaca""] = (alpaca_template, alpaca_eos_token, False, alpaca_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""alpaca""] = ""Below are some instructions that describe some tasks. Write responses that appropriately complete each request.""
 pass
 
 # =========================================== Gemma
@@ -372,7 +384,7 @@ gemma_template = \
     ""{{ bos_token }}""\
     ""{% if messages[0]['role'] == 'system' %}""\
         ""{{'<start_of_turn>user\n' + messages[0]['content'] | trim + ' ' + messages[1]['content'] | trim + '<end_of_turn>\n'}}""\
-        ""{% set loop_messages = messages[2:] %}""\
+        ""{% set messages = messages[2:] %}""\
     ""{% endif %}""\
     ""{% for message in messages %}""\
         ""{% if message['role'] == 'user' %}""\
@@ -407,6 +419,7 @@ PARAMETER min_p 0.1
 
 gemma_eos_token = ""<end_of_turn>""
 CHAT_TEMPLATES[""gemma""] = (gemma_template, gemma_eos_token, True, gemma_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma""] = None # No system message in Gemma
 pass
 
 # =========================================== Gemma with ChatML instead
@@ -437,6 +450,7 @@ gemma_chatml_eos_token = (
     ""<|im_end|>"",
 )
 CHAT_TEMPLATES[""gemma_chatml""] = (gemma_chatml_template, gemma_chatml_eos_token, True, gemma_chatml_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma_chatml""] = None # No system message in Gemma
 pass
 
 # =========================================== Gemma 2
@@ -446,12 +460,14 @@ gemma2_template = gemma_template
 gemma2_ollama = gemma_ollama + ""PARAMETER num_ctx 4096\n""
 gemma2_eos_token = ""<end_of_turn>""
 CHAT_TEMPLATES[""gemma2""] = (gemma2_template, gemma2_eos_token, True, gemma2_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma2""] = None # No system message in Gemma 2
 
 # =========================================== Gemma 2 with ChatML instead
 gemma2_chatml_template = gemma_chatml_template
 gemma2_chatml_ollama = gemma_chatml_ollama + ""PARAMETER num_ctx 4096\n""
 gemma2_chatml_eos_token = gemma_chatml_eos_token
 CHAT_TEMPLATES[""gemma2_chatml""] = (gemma2_chatml_template, gemma2_chatml_eos_token, True, gemma2_chatml_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma2_chatml""] = None # No system message in Gemma 2
 pass
 
 # =========================================== Llama-3
@@ -491,7 +507,12 @@ PARAMETER min_p 0.1
 '''
 
 llama3_template_eos_token = ""eos_token""
+
 CHAT_TEMPLATES[""llama-3""] = (llama3_template, llama3_template_eos_token, False, llama3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama-3""] = None # No system message in Llama-3
+
+CHAT_TEMPLATES[""llama3""] = (llama3_template, llama3_template_eos_token, False, llama3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama3""] = None # No system message in Llama-3
 pass
 
 
@@ -532,8 +553,13 @@ PARAMETER min_p 0.1
 
 phi3_template_eos_token = ""<|end|>""
 CHAT_TEMPLATES[""phi-3""]   = (phi3_template, phi3_template_eos_token, False, phi3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""phi-3""] = None # No system message in Phi-3
+
 CHAT_TEMPLATES[""phi-35""]  = CHAT_TEMPLATES[""phi-3""]
+DEFAULT_SYSTEM_MESSAGE[""phi-35""] = None # No system message in Phi-3.5
+
 CHAT_TEMPLATES[""phi-3.5""] = CHAT_TEMPLATES[""phi-3""]
+DEFAULT_SYSTEM_MESSAGE[""phi-3.5""] = None # No system message in Phi-3.5
 pass
 
 # =========================================== Llama-3.1
@@ -573,7 +599,7 @@ llama31_template = \
     {%- set system_message = messages[0]['content'] %}
     {%- set messages = messages[1:] %}
 {%- else %}
-    {%- set system_message = """" %}
+    {%- set system_message = ""{system_message}"" %}
 {%- endif %}
 
 {#- System message + builtin tools #}
@@ -729,7 +755,10 @@ PARAMETER min_p 0.1
 
 llama31_template_eos_token = ""eos_token""
 CHAT_TEMPLATES[""llama-3.1""] = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama-3.1""] = """" # Llama3.1 default system message is empty + the dates
+
 CHAT_TEMPLATES[""llama-31""]  = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama-31""] = """" # Llama3.1 default system message is empty + the dates
 pass
 
 
@@ -751,7 +780,7 @@ qwen25_template = \
     {%- if messages[0][\'role\'] == \'system\' %}
         {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}
     {%- else %}
-        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}
+        {{- \'<|im_start|>system\\n{system_message}<|im_end|>\\n\' }}
     {%- endif %}\n{%- endif %}\n{%- for message in messages %}
     {%- if (message.role == ""user"") or (message.role == ""system"" and not loop.first) or (message.role == ""assistant"" and not message.tool_calls) %}
         {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}
@@ -847,10 +876,53 @@ PARAMETER min_p 0.1
 '''
 
 qwen25_template_eos_token = ""eos_token""
+qwen25_default_system_message = ""You are Qwen, created by Alibaba Cloud. You are a helpful assistant."" 
 CHAT_TEMPLATES[""qwen-2.5""] = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen-2.5""] = qwen25_default_system_message # No system message in Qwen 2.5
+
 CHAT_TEMPLATES[""qwen-25""]  = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen-25""] = qwen25_default_system_message # No system message in Qwen 2.5
+
 CHAT_TEMPLATES[""qwen25""]   = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen25""] = qwen25_default_system_message # No system message in Qwen 2.5
+
 CHAT_TEMPLATES[""qwen2.5""]  = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen2.5""] = qwen25_default_system_message # No system message in Qwen 2.5
+pass
+
+def _change_system_message(template: str, type_chat_template: str, system_message: str = None):
+    system_message_pattern = r""\{system_message\}""
+    
+    # For predefined templates, check if default system message exists
+    default_system_message = DEFAULT_SYSTEM_MESSAGE.get(f""{type_chat_template}"", None)
+    if default_system_message is None:
+        if system_message is not None:
+            logger.warning_once(
+                f""Unsloth: You tried to change the system message for {type_chat_template}, ""
+                ""but it doesn't have a default system message. ""
+                ""You need to manually add the system message in your data.""
+            )
+        return template, system_message
+    pass
+    
+    # For custom templates
+    if type_chat_template is None:
+        has_placeholder = re.search(system_message_pattern, template) is not None
+        
+        if has_placeholder:
+            if system_message is None:
+                raise ValueError(""Unsloth: You need to provide a system message for custom templates."")
+            new_template = re.sub(system_message_pattern, system_message, template)
+            return new_template, system_message
+        
+        return template, system_message
+    pass
+        
+    # For predefined templates with default system message
+    message_to_use = system_message if system_message is not None else default_system_message
+    new_template = re.sub(system_message_pattern, message_to_use, template)
+    
+    return new_template, message_to_use
 pass
 
 
@@ -886,14 +958,20 @@ def get_chat_template(
     old_padding_side = tokenizer.padding_side
 
     same_padding_token = False
-
+    type_chat_template = None
+    
     if type(chat_template) in (list, tuple,):
+        # For changing system message later
+        # Since it's not supported yet, we will raise an error first!
+        type_chat_template = chat_template[0].lower()
         chat_template, stop_word = chat_template
         assert(type(chat_template) is str)
         assert(type(stop_word) is str)
         ollama_modelfile = None
 
     elif type(chat_template) is str:
+        # For changing system message later
+        type_chat_template = chat_template.lower()
 
         chat_template, stop_word, yes_map_eos_token, ollama_modelfile = CHAT_TEMPLATES[chat_template]
 
@@ -1052,6 +1130,9 @@ def get_chat_template(
     else:
         chat_template = new_chat_template
     pass
+
+    chat_template, system_message = _change_system_message(chat_template, type_chat_template, system_message)
+
     tokenizer.chat_template = chat_template
 
     # Also fix up other tokens
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index ed95e07..302017d 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -586,10 +586,10 @@ pass
 
 
 def _fix_chat_template(chat_template):
-    endfor = ""{% endfor %}""
+    endfor = ""{% endif %}""
     where = chat_template.find(endfor)
     if where == -1:
-        endfor = ""{%- endfor %}""
+        endfor = ""{%- endif %}""
         where = chat_template.find(endfor)
     if where == -1:
         return chat_template
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index 25bb434..00956ed 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -12,9 +12,13 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import warnings
 from dataclasses import dataclass, field
 from typing import Optional
+from functools import wraps
 
+import trl
+import inspect
 from trl import SFTTrainer
 try:
     from trl import SFTConfig as TrainingArguments
@@ -24,30 +28,38 @@ pass
 from . import is_bfloat16_supported
 from unsloth_zoo.training_utils import unsloth_train as _unsloth_train
 from packaging.version import Version
+import dataclasses
+
+__all__ = [
+    ""UnslothTrainingArguments"",
+    ""UnslothTrainer"",
+    ""unsloth_train"",
+    ""_patch_trl_trainer"",
+]
 
 # Unsloth gradient accumulation fix:
 from transformers import __version__ as transformers_version
 if Version(transformers_version) > Version(""4.45.2""):
-    def unsloth_train(trainer):
-        return trainer.train()
+    def unsloth_train(trainer, *args, **kwargs):
+        return trainer.train(*args, **kwargs)
     pass
 else:
-    def unsloth_train(trainer):
+    def unsloth_train(trainer, *args, **kwargs):
+        if len(args) != 0 or len(kwargs) != 0:
+            raise RuntimeError(
+                ""Unsloth: Our custom gradient accumulation fixed trainer does not support other arguments.\n""\
+                ""If you want to use our fix inside of HF, please update `transformers` to the latest version via:\n""\
+                '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir transformers`'
+            )
         print(
             ""Unsloth: Using our custom gradient accumulation fixed trainer, which is not feature complete.\n""\
             ""If you want to use our fix inside of HF, please update `transformers` to the latest version via:\n""\
-            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir ""git+https://github.com/huggingface/transformers.git""`'
+            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir transformers`'
         )
         return _unsloth_train(trainer)
     pass
 pass
 
-__all__ = [
-    ""UnslothTrainingArguments"",
-    ""UnslothTrainer"",
-    ""unsloth_train"",
-]
-
 
 @dataclass
 class UnslothTrainingArguments(TrainingArguments):
@@ -119,3 +131,91 @@ class UnslothTrainer(SFTTrainer):
         return self.optimizer
     pass
 pass
+
+# From `trl>=0.13.0`, they changed how to pass several params to the trainer
+# We need to patch to make the transition smooth
+def create_backwards_compatible_trainer(trainer_class, config_class):
+    original_init = trainer_class.__init__
+    
+    @wraps(original_init)
+    def new_init(self, *args, **kwargs):
+        # All Trainer tokenizer are now called processing_class
+        trainer_params = set(inspect.signature(original_init).parameters.keys())
+
+        if ""processing_class"" in trainer_params and ""tokenizer"" in kwargs:
+            kwargs[""processing_class""] = kwargs.pop(""tokenizer"")
+        pass
+
+        if (""args"" in kwargs) and (Version(trl.__version__) >= Version(""0.13.0.dev0"")):
+            training_args = kwargs.pop(""args"", None)
+
+            # Get parameters that Trainer.__init__ actually expects
+            trainer_params.remove('self')
+            trainer_params.remove('args')
+
+            # Get fields that should be passed to Config init
+            config_fields = {
+                field.name: field for field in dataclasses.fields(config_class) 
+                if field.init
+            }
+            
+            # Create config dict with valid fields from training_args
+            config_dict = {
+                name: getattr(training_args, name)
+                for name in config_fields
+                if hasattr(training_args, name)
+            }
+
+            # Get parameters that exist in Config but not in TrainingArguments
+            moved_params = \
+                set(inspect.signature(config_class)     .parameters.keys()) - \
+                set(inspect.signature(TrainingArguments).parameters.keys())
+            
+            # Separate kwargs into trainer kwargs and config kwargs
+            trainer_kwargs = {}
+            additional_config_kwargs = {}
+
+            for key, value in kwargs.items():
+                if key in trainer_params: trainer_kwargs[key] = value
+                elif key in moved_params or key in config_fields:
+                    additional_config_kwargs[key] = value
+                else:
+                    additional_config_kwargs[key] = value
+                pass
+            pass
+
+            # Update config_dict with additional kwargs
+            config_dict.update(additional_config_kwargs)
+
+            # Create Config with all the collected parameters
+            config = config_class(**config_dict)
+            
+            # Reconstruct kwargs for Trainer
+            kwargs = trainer_kwargs
+            kwargs[""args""] = config
+        pass
+        original_init(self, *args, **kwargs)
+    pass
+    return new_init
+pass
+
+
+def _patch_trl_trainer():
+    import trl
+    if hasattr(trl, ""__UNSLOTH_BACKWARDS_COMPATIBLE__""): return
+    if Version(trl.__version__) <= Version(""0.11.0""): return
+
+    import trl.trainer
+    trl_classes = dir(trl.trainer)
+
+    non_convertable_trainer = set([""PPOv2"", ""AlignProp""])
+    trl_trainers = set(x[:-len(""Trainer"")] for x in trl_classes if x.endswith(""Trainer"")) - non_convertable_trainer
+    trl_configs  = set(x[:-len(""Config"")]  for x in trl_classes if x.endswith(""Config""))  - non_convertable_trainer
+    trl_classes = list(trl_trainers & trl_configs)
+
+    for x in trl_classes:
+        exec(f""trl.{x}Trainer.__init__ = create_backwards_compatible_trainer(trl.{x}Trainer, trl.{x}Config)"", globals())
+    pass
+
+    trl.__UNSLOTH_BACKWARDS_COMPATIBLE__ = True
+pass
"
"diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 2666912..067f259 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -686,12 +686,12 @@ def fix_chat_template(tokenizer):
                 raise RuntimeError(
                     f""Unsloth: The tokenizer `{tokenizer.name_or_path}`\n""\
                     ""does not have a {% if add_generation_prompt %} for generation purposes.\n""\
-                    ""Please file a bug report immediately - thanks!""
+                    f""Please file a bug report to the maintainers of `{tokenizer.name_or_path}` - thanks!""
                 )
             else:
                 logger.warning_once(
                     ""Unsloth: We successfully patched the tokenizer to add a {% if add_generation_prompt %} to the chat_template.\n""\
-                    ""This is not a bug, but please notify the Unsloth maintainers - thanks!""
+                    f""This is not a bug, but please notify the maintainers of `{tokenizer.name_or_path}` - thanks!""
                 )
                 chat_template = new_chat_template
             pass
"
"diff --git a/README.md b/README.md
index 6b583d5..d0e5a97 100644
--- a/README.md
+++ b/README.md
@@ -181,6 +181,18 @@ x = x.format(cuda.replace(""."", """"), ""-ampere"" if is_ampere else """")
 print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git""')
 ```
 
+### Windows Installation
+
+To run Unsloth directly on Windows:
+- Install Triton from this Windows fork and follow the instructions: https://github.com/woct0rdho/triton-windows
+- In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:
+```python
+trainer = SFTTrainer(
+    dataset_num_proc=1,
+    ...
+)
+```
+
 For **advanced installation instructions** or if you see weird errors during installations:
 
 1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
diff --git a/pyproject.toml b/pyproject.toml
index 455f847..a2a9c2c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -44,7 +44,7 @@ huggingface = [
     ""wheel>=0.42.0"",
     ""numpy"",
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
@@ -227,7 +227,7 @@ colab-new = [
 ]
 colab-no-deps = [
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
     ""peft>=0.7.1"",
     ""xformers<0.0.27"",
     ""bitsandbytes>=0.43.3"",
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index cab6130..b254202 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -678,7 +678,7 @@ TEMPLATE """"""{{ if .Messages }}
 {{- end }}
 {{- if .Tools }}
 
-You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.
+You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the original use question.
 {{- end }}
 {{- end }}<|eot_id|>
 {{- range $i, $_ := .Messages }}
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index 2fba359..08426b6 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -31,7 +31,7 @@ try:
         create_block_mask as _create_block_mask,
     )
     _flex_attention = torch.compile(_flex_attention, dynamic = True, options = torch_compile_options)
-    HAS_FLEX_ATTENTION = True
+    HAS_FLEX_ATTENTION = False
 except:
     HAS_FLEX_ATTENTION = False
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 4aa8e1e..25024be 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.10.3""
+__version__ = ""2024.10.4""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -1194,8 +1194,8 @@ def patch_gradient_accumulation_fix(Trainer):
         logger.warning_once(
             ""Unsloth: We fixed a gradient accumulation bug, ""\
             ""but it seems like you don't have the latest transformers version!\n""\
-            ""Please update transformers via:\n""\
-            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir ""git+https://github.com/huggingface/transformers.git""`'
+            ""Please update transformers, TRL and unsloth via:\n""\
+            '`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`'
         )
     pass
 pass
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 15e9efc..00dcc5c 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -254,8 +254,9 @@ def MistralForCausalLM_fast_forward(
         
         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
         loss = fast_cross_entropy_loss(
-            logits = shift_logits,
-            labels = shift_labels,
+            logits  = shift_logits,
+            labels  = shift_labels,
+            n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None),
         )
     pass
 
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index ffe9933..1cad00d 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -831,6 +831,7 @@ pass
 PRE_CHECK = check_nvidia()
 
 
+import inspect
 from inspect import getsource
 import trl.trainer.sft_trainer
 from trl.trainer.sft_trainer import *
@@ -869,6 +870,35 @@ except:
 pass
 
 
+def patch_trl_tokenizer_processing_class(trainer_name):
+    # New TRL removes tokenizer!
+    # We return it back!
+    exec(f""from trl import {trainer_name}"", globals())
+    if str(eval(f""{trainer_name}"").__name__).startswith(""Unsloth""): return None
+    parameters = eval(f""inspect.signature({trainer_name}).parameters"")
+    if ""tokenizer"" in parameters: return None
+
+    args = {
+        key : \
+            value.default \
+            if type(value.default) is not str else \
+            f""'{value.default}'"" \
+        for key, value in parameters.items()
+    }
+    args[""tokenizer""] = None
+    new_args = args.copy()
+    del new_args[""tokenizer""]
+    del new_args[""processing_class""]
+    new_args = "",\n"".join(f""{' '*12}{key} = {key}"" for key in new_args) + \
+        f"",\n{' '*12}processing_class = tokenizer if tokenizer else processing_class""
+    args = "",\n"".join(f""{' '*8}{key} = {value}"" for key, value in args.items())
+    args = f""def __init__(\n"" + f""{' '*8}self,\n"" + args + ""):""
+    args += f""\n{' '*8}\n{' '*8}super().__init__(\n{new_args}\n{' '*8})""
+    new_class = f""""""class Unsloth{trainer_name}({trainer_name}):\n{' '*4}{args}\n""""""
+    return new_class
+pass
+
+
 def patch_sft_trainer_tokenizer():
     """"""
         Patches the trainer with changes
@@ -884,7 +914,8 @@ def patch_sft_trainer_tokenizer():
 
         check_text = \
         ""\n""\
-        ""test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\n""\
+        ""if 'tokenizer' not in locals(): tokenizer = processing_class\n""\
+        ""test_text = dataset[0][dataset_text_field] if (formatting_func is not None and dataset_text_field is None) else formatting_func(dataset[0])[0]\n""\
         ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
         ""chat_template = '' if chat_template is None else chat_template\n""\
         ""has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) ""\
@@ -941,7 +972,8 @@ def patch_sft_trainer_tokenizer():
         ""        from transformers import __version__ as transformers_version\n""\
         ""        from packaging.version import Version\n""\
         ""        if Version(transformers_version) <= Version('4.45.2'):\n""\
-        ""            print('**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers and Unsloth!')\n""\
+        ""            print('**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\\n'\\\n""\
+        ""                  '`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`')\n""\
         ""except:\n""\
         ""    pass\n""\
         ""\n\n""
@@ -981,4 +1013,13 @@ def patch_sft_trainer_tokenizer():
     pass
 pass
 
+# Fix TRL trainers with removed tokenizer args (got replaced with processing_class)
+for trainer_name in (""SFTTrainer"", ""DPOTrainer"", ""KTOTrainer""):
+    trainer_text = patch_trl_tokenizer_processing_class(trainer_name)
+    if trainer_text is None: continue
+    exec(trainer_text, globals())
+    exec(f""trl.trainer.{trainer_name} = Unsloth{trainer_name}"", globals())
+pass
+
+# FInally patch TRL tokenizer things
 patch_sft_trainer_tokenizer()
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index b254202..da10f7e 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -39,6 +39,7 @@ from unsloth_zoo.dataset_utils import (
     train_on_responses_only,
 )
 CHAT_TEMPLATES = {}
+DEFAULT_SYSTEM_MESSAGE = {}
 
 # =========================================== Unsloth
 # Unsloth efficient template leverages from Zephyr
@@ -48,7 +49,7 @@ unsloth_template = \
         ""{{ messages[0]['content'] + '\n' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'You are a helpful assistant to the user\n' }}""\
+        ""{{ '{system_message}' + '\n' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -80,6 +81,7 @@ SYSTEM """"""You are a helpful assistant to the user""""""
 
 unsloth_eos_token = ""eos_token""
 CHAT_TEMPLATES[""unsloth""] = (unsloth_template, unsloth_eos_token, False, unsloth_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""unsloth""] = ""You are a helpful assistant to the user""
 pass
 
 # =========================================== Zephyr
@@ -116,6 +118,7 @@ PARAMETER min_p 0.1
 
 zephyr_eos_token = ""eos_token""
 CHAT_TEMPLATES[""zephyr""] = (zephyr_template, zephyr_eos_token, False, zephyr_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""zephyr""] = None # No system message in Zephyr
 pass
 
 # =========================================== ChatML
@@ -153,6 +156,7 @@ PARAMETER min_p 0.1
 
 chatml_eos_token = ""<|im_end|>""
 CHAT_TEMPLATES[""chatml""] = (chatml_template, chatml_eos_token, True, chatml_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""chatml""] = None # No system message in ChatML
 pass
 
 # =========================================== Mistral-1
@@ -193,6 +197,7 @@ PARAMETER min_p 0.1
 
 mistral_eos_token = ""eos_token""
 CHAT_TEMPLATES[""mistral""] = (mistral_template, mistral_eos_token, False, mistral_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""mistral""] = None # No system message in Mistral
 pass
 
 # =========================================== Llama-2
@@ -234,6 +239,7 @@ PARAMETER min_p 0.1
 
 llama_eos_token = ""eos_token""
 CHAT_TEMPLATES[""llama""] = (llama_template, llama_eos_token, False, llama_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama""] = None # No system message in Llama
 pass
 
 # ===========================================  Vicuna
@@ -244,7 +250,7 @@ vicuna_template = \
         ""{{ messages[0]['content'] + ' ' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions.' + ' ' }}""\
+        ""{{ '{system_message}' + ' ' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -273,6 +279,7 @@ PARAMETER min_p 0.1
 
 vicuna_eos_token = ""eos_token""
 CHAT_TEMPLATES[""vicuna""] = (vicuna_template, vicuna_eos_token, False, vicuna_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""vicuna""] = ""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.""
 pass
 
 # =========================================== Vicuna Old
@@ -283,7 +290,7 @@ vicuna_old_template = \
         ""{{ messages[0]['content'] + '\n' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\\'s questions.' + '\n' }}""\
+        ""{{ '{system_message}' + '\n' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -315,6 +322,10 @@ SYSTEM """"""A chat between a curious human and an artificial intelligence assistan
 
 vicuna_old_eos_token = ""eos_token""
 CHAT_TEMPLATES[""vicuna_old""] = (vicuna_old_template, vicuna_old_eos_token, False, vicuna_old_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""vicuna_old""] = ""A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\\'s questions.""
+
+CHAT_TEMPLATES[""vicuna old""] = CHAT_TEMPLATES[""vicuna_old""]
+DEFAULT_SYSTEM_MESSAGE[""vicuna old""] = DEFAULT_SYSTEM_MESSAGE[""vicuna_old""]
 pass
 
 # =========================================== Alpaca multi turn
@@ -325,7 +336,7 @@ alpaca_template = \
         ""{{ messages[0]['content'] + '\n\n' }}""\
         ""{% set loop_messages = messages[1:] %}""\
     ""{% else %}""\
-        ""{{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n\n' }}""\
+        ""{{ '{system_message}' + '\n\n' }}""\
         ""{% set loop_messages = messages %}""\
     ""{% endif %}""\
     ""{% for message in loop_messages %}""\
@@ -362,6 +373,7 @@ SYSTEM """"""Below are some instructions that describe some tasks. Write responses
 
 alpaca_eos_token = ""eos_token""
 CHAT_TEMPLATES[""alpaca""] = (alpaca_template, alpaca_eos_token, False, alpaca_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""alpaca""] = ""Below are some instructions that describe some tasks. Write responses that appropriately complete each request.""
 pass
 
 # =========================================== Gemma
@@ -372,7 +384,7 @@ gemma_template = \
     ""{{ bos_token }}""\
     ""{% if messages[0]['role'] == 'system' %}""\
         ""{{'<start_of_turn>user\n' + messages[0]['content'] | trim + ' ' + messages[1]['content'] | trim + '<end_of_turn>\n'}}""\
-        ""{% set loop_messages = messages[2:] %}""\
+        ""{% set messages = messages[2:] %}""\
     ""{% endif %}""\
     ""{% for message in messages %}""\
         ""{% if message['role'] == 'user' %}""\
@@ -407,6 +419,7 @@ PARAMETER min_p 0.1
 
 gemma_eos_token = ""<end_of_turn>""
 CHAT_TEMPLATES[""gemma""] = (gemma_template, gemma_eos_token, True, gemma_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma""] = None # No system message in Gemma
 pass
 
 # =========================================== Gemma with ChatML instead
@@ -437,6 +450,7 @@ gemma_chatml_eos_token = (
     ""<|im_end|>"",
 )
 CHAT_TEMPLATES[""gemma_chatml""] = (gemma_chatml_template, gemma_chatml_eos_token, True, gemma_chatml_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma_chatml""] = None # No system message in Gemma
 pass
 
 # =========================================== Gemma 2
@@ -446,12 +460,14 @@ gemma2_template = gemma_template
 gemma2_ollama = gemma_ollama + ""PARAMETER num_ctx 4096\n""
 gemma2_eos_token = ""<end_of_turn>""
 CHAT_TEMPLATES[""gemma2""] = (gemma2_template, gemma2_eos_token, True, gemma2_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma2""] = None # No system message in Gemma 2
 
 # =========================================== Gemma 2 with ChatML instead
 gemma2_chatml_template = gemma_chatml_template
 gemma2_chatml_ollama = gemma_chatml_ollama + ""PARAMETER num_ctx 4096\n""
 gemma2_chatml_eos_token = gemma_chatml_eos_token
 CHAT_TEMPLATES[""gemma2_chatml""] = (gemma2_chatml_template, gemma2_chatml_eos_token, True, gemma2_chatml_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma2_chatml""] = None # No system message in Gemma 2
 pass
 
 # =========================================== Llama-3
@@ -491,7 +507,12 @@ PARAMETER min_p 0.1
 '''
 
 llama3_template_eos_token = ""eos_token""
+
 CHAT_TEMPLATES[""llama-3""] = (llama3_template, llama3_template_eos_token, False, llama3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama-3""] = None # No system message in Llama-3
+
+CHAT_TEMPLATES[""llama3""] = (llama3_template, llama3_template_eos_token, False, llama3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama3""] = None # No system message in Llama-3
 pass
 
 
@@ -532,8 +553,13 @@ PARAMETER min_p 0.1
 
 phi3_template_eos_token = ""<|end|>""
 CHAT_TEMPLATES[""phi-3""]   = (phi3_template, phi3_template_eos_token, False, phi3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""phi-3""] = None # No system message in Phi-3
+
 CHAT_TEMPLATES[""phi-35""]  = CHAT_TEMPLATES[""phi-3""]
+DEFAULT_SYSTEM_MESSAGE[""phi-35""] = None # No system message in Phi-3.5
+
 CHAT_TEMPLATES[""phi-3.5""] = CHAT_TEMPLATES[""phi-3""]
+DEFAULT_SYSTEM_MESSAGE[""phi-3.5""] = None # No system message in Phi-3.5
 pass
 
 # =========================================== Llama-3.1
@@ -573,7 +599,7 @@ llama31_template = \
     {%- set system_message = messages[0]['content'] %}
     {%- set messages = messages[1:] %}
 {%- else %}
-    {%- set system_message = """" %}
+    {%- set system_message = ""{system_message}"" %}
 {%- endif %}
 
 {#- System message + builtin tools #}
@@ -729,7 +755,10 @@ PARAMETER min_p 0.1
 
 llama31_template_eos_token = ""eos_token""
 CHAT_TEMPLATES[""llama-3.1""] = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama-3.1""] = """" # Llama3.1 default system message is empty + the dates
+
 CHAT_TEMPLATES[""llama-31""]  = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""llama-31""] = """" # Llama3.1 default system message is empty + the dates
 pass
 
 
@@ -751,7 +780,7 @@ qwen25_template = \
     {%- if messages[0][\'role\'] == \'system\' %}
         {{- \'<|im_start|>system\\n\' + messages[0][\'content\'] + \'<|im_end|>\\n\' }}
     {%- else %}
-        {{- \'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\' }}
+        {{- \'<|im_start|>system\\n{system_message}<|im_end|>\\n\' }}
     {%- endif %}\n{%- endif %}\n{%- for message in messages %}
     {%- if (message.role == ""user"") or (message.role == ""system"" and not loop.first) or (message.role == ""assistant"" and not message.tool_calls) %}
         {{- \'<|im_start|>\' + message.role + \'\\n\' + message.content + \'<|im_end|>\' + \'\\n\' }}
@@ -847,10 +876,53 @@ PARAMETER min_p 0.1
 '''
 
 qwen25_template_eos_token = ""eos_token""
+qwen25_default_system_message = ""You are Qwen, created by Alibaba Cloud. You are a helpful assistant."" 
 CHAT_TEMPLATES[""qwen-2.5""] = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen-2.5""] = qwen25_default_system_message # No system message in Qwen 2.5
+
 CHAT_TEMPLATES[""qwen-25""]  = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen-25""] = qwen25_default_system_message # No system message in Qwen 2.5
+
 CHAT_TEMPLATES[""qwen25""]   = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen25""] = qwen25_default_system_message # No system message in Qwen 2.5
+
 CHAT_TEMPLATES[""qwen2.5""]  = (qwen25_template, qwen25_template_eos_token, False, qwen25_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""qwen2.5""] = qwen25_default_system_message # No system message in Qwen 2.5
+pass
+
+def _change_system_message(template: str, type_chat_template: str, system_message: str = None):
+    system_message_pattern = r""\{system_message\}""
+    
+    # For predefined templates, check if default system message exists
+    default_system_message = DEFAULT_SYSTEM_MESSAGE.get(f""{type_chat_template}"", None)
+    if default_system_message is None:
+        if system_message is not None:
+            logger.warning_once(
+                f""Unsloth: You tried to change the system message for {type_chat_template}, ""
+                ""but it doesn't have a default system message. ""
+                ""You need to manually add the system message in your data.""
+            )
+        return template, system_message
+    pass
+    
+    # For custom templates
+    if type_chat_template is None:
+        has_placeholder = re.search(system_message_pattern, template) is not None
+        
+        if has_placeholder:
+            if system_message is None:
+                raise ValueError(""Unsloth: You need to provide a system message for custom templates."")
+            new_template = re.sub(system_message_pattern, system_message, template)
+            return new_template, system_message
+        
+        return template, system_message
+    pass
+        
+    # For predefined templates with default system message
+    message_to_use = system_message if system_message is not None else default_system_message
+    new_template = re.sub(system_message_pattern, message_to_use, template)
+    
+    return new_template, message_to_use
 pass
 
 
@@ -886,14 +958,20 @@ def get_chat_template(
     old_padding_side = tokenizer.padding_side
 
     same_padding_token = False
-
+    type_chat_template = None
+    
     if type(chat_template) in (list, tuple,):
+        # For changing system message later
+        # Since it's not supported yet, we will raise an error first!
+        type_chat_template = chat_template[0].lower()
         chat_template, stop_word = chat_template
         assert(type(chat_template) is str)
         assert(type(stop_word) is str)
         ollama_modelfile = None
 
     elif type(chat_template) is str:
+        # For changing system message later
+        type_chat_template = chat_template.lower()
 
         chat_template, stop_word, yes_map_eos_token, ollama_modelfile = CHAT_TEMPLATES[chat_template]
 
@@ -1052,6 +1130,9 @@ def get_chat_template(
     else:
         chat_template = new_chat_template
     pass
+
+    chat_template, system_message = _change_system_message(chat_template, type_chat_template, system_message)
+
     tokenizer.chat_template = chat_template
 
     # Also fix up other tokens
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b3b49a0..722b50d 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2024,6 +2024,14 @@ class FastLlamaModel:
         **kwargs,
     ):
         if os.environ.get(""UNSLOTH_USE_NEW_MODEL"", ""0"") == ""1"":
+            # Check for other PEFT args in kwargs
+            for (peft_arg, flag) in (
+                (""finetune_vision_layers"", False),
+                (""finetune_language_layers"", True),
+                (""finetune_attention_modules"", True),
+                (""finetune_mlp_modules"", True),
+            ):
+                if peft_arg not in kwargs: kwargs[peft_arg] = flag
             return FastBaseModel.get_peft_model(
                 model                      = model,
                 r                          = r,
@@ -2031,10 +2039,6 @@ class FastLlamaModel:
                 lora_alpha                 = lora_alpha,
                 lora_dropout               = lora_dropout,
                 bias                       = bias,
-                finetune_vision_layers     = False,
-                finetune_language_layers   = True,
-                finetune_attention_modules = True,
-                finetune_mlp_modules       = True,
                 layers_to_transform        = layers_to_transform,
                 layers_pattern             = layers_pattern,
                 use_gradient_checkpointing = use_gradient_checkpointing,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 670e082..c2bf51c 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -642,6 +642,7 @@ class FastModel(FastBaseModel):
             trust_remote_code = trust_remote_code,
         )
         model_types = [""siglip""] + model_types
+        print(""model_types"", model_types)
 
         # Set forced float32 env flag
         os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""0""
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index cf250dd..07523ff 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -728,6 +728,16 @@ __INT_TO_FLOAT_MAPPER = \
         ""mistralai/Mistral-Small-3.1-24B-Base-2503"",
         ""unsloth/Mistral-Small-3.1-24B-Base-2503-bnb-4bit"",
     ),
+    ""unsloth/orpheus-3b-0.1-pretrained-unsloth-bnb-4bit"" : (
+        ""unsloth/Mistral-Small-3.1-24B-Base-2503"",
+        ""canopylabs/orpheus-3b-0.1-pretrained"",
+        ""unsloth/orpheus-3b-0.1-pretrained-bnb-4bit"",
+    ),
+    ""unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit"" : (
+        ""unsloth/Mistral-Small-3.1-24B-Base-2503"",
+        ""canopylabs/orpheus-3b-0.1-ft"",
+        ""unsloth/orpheus-3b-0.1-ft-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index a566f02..6244a61 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -242,6 +242,11 @@ class FastBaseModel:
         use_gradient_checkpointing = ""unsloth"",
         **kwargs,
     ):
+        if model_types is None:
+            raise RuntimeError(
+                ""Unsloth: Please use FastModel or FastVisionModel and not use FastBaseModel directly!""
+            )
+
         os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""1""
         if trust_remote_code:
             print(
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ea66475..0c9468c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -1166,7 +1166,7 @@ def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
             inputs[""num_items_in_batch""] = kwargs[""num_items_in_batch""]
         pass
     pass
-    return self._old_compute_loss(model, inputs, args, kwargs)
+    return self._old_compute_loss(model, inputs, *args, **kwargs)
 pass
 
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 51c3037..49347c8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -36,7 +36,7 @@ huggingface = [
     ""unsloth_zoo>=2024.11.8"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.46.1"",
+    ""transformers>=4.46.1,<=4.46.3"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -247,7 +247,7 @@ colab-new = [
     ""unsloth_zoo>=2024.11.8"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.46.1"",
+    ""transformers>=4.46.1,<=4.46.3"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index f1dc15a..5b5ceef 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.4""
+__version__ = ""2024.12.5""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -1127,6 +1127,7 @@ def unsloth_compile_transformers(
     shape_padding           = True,
     cudagraphs              = False,
     debug                   = False,
+    fullgraph               = True,
     import_from_cache       = False,
     disable                 = False,
     return_logits           = False,
@@ -1170,6 +1171,7 @@ def unsloth_compile_transformers(
             shape_padding          = shape_padding,
             cudagraphs             = cudagraphs,
             debug                  = debug,
+            fullgraph              = fullgraph,
             import_from_cache      = import_from_cache,
             disable                = disable,
             return_logits          = return_logits,
diff --git a/unsloth/models/cohere.py b/unsloth/models/cohere.py
index aa0bcb5..cbbebee 100644
--- a/unsloth/models/cohere.py
+++ b/unsloth/models/cohere.py
@@ -75,6 +75,7 @@ def CohereAttention_fast_forward(
     output_attentions:    bool = False,
     use_cache:            bool = False,
     padding_mask:         Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     
@@ -112,12 +113,11 @@ def CohereAttention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
+    cos, sin = position_embeddings
     if position_ids is None:
-        cos = self.rotary_emb.cos_cached
-        sin = self.rotary_emb.sin_cached
         Q, K = fast_rope_embedding(Q, K, cos, sin)
     else:
-        cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
+        cos, sin = cos[position_ids], sin[position_ids]
         Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
     pass
 
@@ -190,6 +190,7 @@ def CohereDecoderLayer_fast_forward(
     output_attentions:    Optional[bool] = False,
     use_cache:            Optional[bool] = False,
     padding_mask:         Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ):
     if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index cc41a8b..f8fb7d9 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -329,14 +329,15 @@ pass
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320
 def LlamaAttention_fast_forward(
     self,
-    hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
-    attention_mask:       Optional[torch.Tensor] = None,
-    position_ids:         Optional[torch.LongTensor] = None,
-    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
-    output_attentions:    bool = False,
-    use_cache:            bool = False,
-    padding_mask:         Optional[torch.LongTensor] = None,
+    hidden_states:       torch.Tensor,
+    causal_mask:         Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:      Optional[torch.Tensor] = None,
+    position_ids:        Optional[torch.LongTensor] = None,
+    past_key_value:      Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:   bool = False,
+    use_cache:           bool = False,
+    padding_mask:        Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     
@@ -368,20 +369,24 @@ def LlamaAttention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
-    # Extend RoPE dynamically to fit in VRAM
-    rotary_emb = self.rotary_emb
-    rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
-
-    if position_ids is None:
-        # Useful for LongRoPE
-        cos, sin = rotary_emb.get_cached(kv_seq_len)
-        # cos = self.rotary_emb.cos_cached
-        # sin = self.rotary_emb.sin_cached
-        Q, K = fast_rope_embedding(Q, K, cos, sin)
+    if position_embeddings:
+        cos, sin = position_embeddings
     else:
-        cos, sin = rotary_emb(V, seq_len = kv_seq_len)
-        Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
-    pass
+        # Extend RoPE dynamically to fit in VRA
+        rotary_emb = self.rotary_emb
+        rotary_emb.extend_rope_embedding(V, seq_len=kv_seq_len)
+
+        if position_ids is None:
+            # Useful for LongRoPE
+            cos, sin = rotary_emb.get_cached(kv_seq_len)
+        else:
+            cos, sin = rotary_emb(V, seq_len=kv_seq_len)
+
+    Q, K = (
+        fast_rope_embedding(Q, K, cos, sin) 
+        if position_ids is None 
+        else inplace_rope_embedding(Q, K, cos, sin, position_ids)
+    )
 
     if past_key_value is not None:
         K = torch.cat([past_key_value[0], K], dim = 2)
@@ -444,14 +449,15 @@ pass
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590
 def LlamaDecoderLayer_fast_forward(
     self,
-    hidden_states:        torch.Tensor,
-    causal_mask           = None,
-    attention_mask:       Optional[torch.Tensor] = None,
-    position_ids:         Optional[torch.LongTensor] = None,
-    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
-    output_attentions:    Optional[bool] = False,
-    use_cache:            Optional[bool] = False,
-    padding_mask:         Optional[torch.LongTensor] = None,
+    hidden_states:       torch.Tensor,
+    causal_mask          = None,
+    attention_mask:      Optional[torch.Tensor] = None,
+    position_ids:        Optional[torch.LongTensor] = None,
+    past_key_value:      Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:   Optional[bool] = False,
+    use_cache:           Optional[bool] = False,
+    padding_mask:        Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
     """"""
@@ -471,14 +477,15 @@ def LlamaDecoderLayer_fast_forward(
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_value=past_key_value,
-            output_attentions=output_attentions,
-            use_cache=use_cache,
-            padding_mask=padding_mask,
+            hidden_states       = hidden_states,
+            causal_mask         = causal_mask,
+            attention_mask      = attention_mask,
+            position_ids        = position_ids,
+            past_key_value      = past_key_value,
+            output_attentions   = output_attentions,
+            use_cache           = use_cache,
+            padding_mask        = padding_mask,
+            position_embeddings = position_embeddings,
         )
         hidden_states += residual
 
@@ -491,14 +498,15 @@ def LlamaDecoderLayer_fast_forward(
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_value=past_key_value,
-            output_attentions=output_attentions,
-            use_cache=use_cache,
-            padding_mask=padding_mask,
+            hidden_states       = hidden_states,
+            causal_mask         = causal_mask,
+            attention_mask      = attention_mask,
+            position_ids        = position_ids,
+            past_key_value      = past_key_value,
+            output_attentions   = output_attentions,
+            use_cache           = use_cache,
+            padding_mask        = padding_mask,
+            position_embeddings = position_embeddings,
         )
         hidden_states = residual + hidden_states
 
@@ -776,9 +784,10 @@ def LlamaModel_fast_forward(
         pass
     pass
 
-    
-    if IS_GRANITE:
-        position_embeddings = self.rotary_emb(hidden_states, position_ids, self.max_position_embeddings)
+    if transformers_version > ""4.47.1"" and hasattr(self, ""rotary_emb""):
+        # Transformers main has made it mandatory to pass position_embeddings
+        # https://github.com/huggingface/transformers/pull/34858
+        position_embeddings = self.rotary_emb(hidden_states, position_ids, self.config.max_position_embeddings)
     else:
         position_embeddings = None
 
@@ -832,13 +841,13 @@ def LlamaModel_fast_forward(
             layer_outputs = decoder_layer(
                 hidden_states,
                 causal_mask=mask,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                past_key_value=past_key_value,
-                output_attentions=output_attentions,
-                use_cache=use_cache,
-                padding_mask=padding_mask,
-                position_embeddings = position_embeddings
+                attention_mask      = attention_mask,
+                position_ids        = position_ids,
+                past_key_value      = past_key_value,
+                output_attentions   = output_attentions,
+                use_cache           = use_cache,
+                padding_mask        = padding_mask,
+                position_embeddings = position_embeddings,
             )
             hidden_states = layer_outputs[0]
         pass
@@ -993,6 +1002,9 @@ def CausalLM_fast_forward(fast_forward_inference):
             logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
         else:
             RETURN_LOGITS = os.environ.get(""UNSLOTH_RETURN_LOGITS"", ""0"") == ""1""
+            # < 1024 Normal Unsloth uses less VRAM!
+            if bsz*q_len <= 1024: RETURN_LOGITS = True
+            
             if not RETURN_LOGITS and HAS_CUT_CROSS_ENTROPY and labels is not None:
                 n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
                 loss = fused_linear_cross_entropy(
@@ -1041,7 +1053,6 @@ def CausalLM_fast_forward(fast_forward_inference):
                 # Fixes https://github.com/unslothai/unsloth/issues/10
                 self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
             pass
-
             shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
             loss = fast_cross_entropy_loss(
                 logits = shift_logits,
@@ -1570,7 +1581,7 @@ class FastLlamaModel:
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers:{transformers_version}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
            f""O^O/ \_/ \\    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 3b2c8ff..5ecd667 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -131,7 +131,8 @@ class FastLanguageModel(FastLlamaModel):
                 exist_config         = os.path.exists(os.path.join(model_name, ""config.json""))
                 both_exist = exist_adapter_config and exist_config
             else:
-                files = HfFileSystem(token = token).glob(os.path.join(model_name, ""*.json""))
+                # Because HfFileSystem assumes linux paths, we need to set the path with forward slashes, even on Windows.
+                files = HfFileSystem(token = token).glob(f""{model_name}/*.json"")
                 files = (os.path.split(x)[-1] for x in files)
                 if sum(x == ""adapter_config.json"" or x == ""config.json"" for x in files) >= 2:
                     both_exist = True
@@ -352,6 +353,7 @@ class FastVisionModel(FastBaseVisionModel):
         resize_model_vocab         = None, # [TODO] No effect
         revision                   = None,
         return_logits              = False, # Return logits
+        fullgraph                  = True, # No graph breaks
         *args, **kwargs,
     ):
         if token is None: token = get_token()
@@ -473,6 +475,7 @@ class FastVisionModel(FastBaseVisionModel):
                 shape_padding           = True,
                 cudagraphs              = False,
                 debug                   = False,
+                fullgraph               = fullgraph,
                 import_from_cache       = False,
                 disable                 = False,
                 return_logits           = return_logits,
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 00dcc5c..dda4304 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -39,14 +39,15 @@ pass
 
 def MistralAttention_fast_forward(
     self,
-    hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
-    attention_mask:       Optional[torch.Tensor] = None,
-    position_ids:         Optional[torch.LongTensor] = None,
-    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
-    output_attentions:    bool = False,
-    use_cache:            bool = False,
-    padding_mask:         Optional[torch.LongTensor] = None,
+    hidden_states:       torch.Tensor,
+    causal_mask:         Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:      Optional[torch.Tensor] = None,
+    position_ids:        Optional[torch.LongTensor] = None,
+    past_key_value:      Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:   bool = False,
+    use_cache:           bool = False,
+    padding_mask:        Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     
diff --git a/unsloth/save.py b/unsloth/save.py
index cf78bf5..ce5ee5d 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -577,7 +577,7 @@ def unsloth_save_model(
             #     max_ram = max(max_ram - W.nbytes, 0)
             else:
                 # Save to Disk
-                logger.warning_once(""We will save to Disk and not RAM now."")
+                logger.warning_once(""\nWe will save to Disk and not RAM now."")
                 filename = os.path.join(temporary_location, f""{name}.pt"")
                 torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)
                 # weights_only = True weirdly fails?
@@ -2129,17 +2129,31 @@ def unsloth_generic_save(
 ):
     if token is None and push_to_hub: token = get_token()
 
-    merge_and_overwrite_lora(
-        get_model_name,
-        create_huggingface_repo,
-        model,
-        save_location        = save_directory,
-        push_to_hub          = push_to_hub,
-        token                = token,
-        upload_location      = save_directory if push_to_hub else None,
-        low_disk_space_usage = True,
-        private              = private,
-    )
+    import unsloth_zoo
+    if Version(unsloth_zoo.__version__) <= Version(""2024.12.1""):
+        merge_and_overwrite_lora(
+            get_model_name,
+            create_huggingface_repo,
+            model,
+            save_location        = save_directory,
+            push_to_hub          = push_to_hub,
+            token                = token,
+            upload_location      = save_directory if push_to_hub else None,
+            low_disk_space_usage = True,
+            private              = private,
+        )
+    else:
+        merge_and_overwrite_lora(
+            get_model_name,
+            model,
+            save_directory       = save_directory,
+            push_to_hub          = push_to_hub,
+            private              = private,
+            token                = token,
+            low_disk_space_usage = False,
+            use_temp_file        = False,
+        )
+    pass
     return
 pass
 
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 302017d..384b4bb 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -585,26 +585,43 @@ def load_correct_tokenizer(
 pass
 
 
+def _find_end_position(template, endfor, endif):
+    where_endfor = template.find(endfor)
+    where_endif = template.find(endif)
+    if where_endfor == where_endif == -1:
+        return None
+    elif where_endfor > where_endif:
+        return endfor
+    else:
+        return endif
+    pass
+pass
+
+
 def _fix_chat_template(chat_template):
-    endfor = ""{% endif %}""
-    where = chat_template.find(endfor)
-    if where == -1:
-        endfor = ""{%- endif %}""
-        where = chat_template.find(endfor)
-    if where == -1:
+    endfor = ""{% endfor %}""
+    endif = ""{% endif %}""
+    chosen_end = _find_end_position(chat_template, endfor, endif)
+    if chosen_end is None:
+        endfor = ""{%- endfor %}""
+        endif = ""{%- endif %}""
+        chosen_end = _find_end_position(chat_template, endfor, endif)
+    if chosen_end is None:
         return chat_template
+    
+    where = chat_template.find(chosen_end)
 
-    after_endfor = chat_template[where + len(endfor):]
+    after_endfor = chat_template[where + len(chosen_end):]
 
-    dash = ""-"" if endfor.startswith(""{%-"") else """"
+    dash = ""-"" if chosen_end.startswith(""{%-"") else """"
 
     if ""{%"" + dash + "" if"" not in after_endfor and ""{%"" + dash + "" set "" not in after_endfor and \
         after_endfor.startswith(""{{"") and after_endfor.endswith(""}}"") and \
         after_endfor.count(""{{"") == 1 and after_endfor.count(""}}"") == 1:
 
-        after_endfor = ""{%"" + dash + "" if add_generation_prompt %}"" + after_endfor + endfor
+        after_endfor = ""{%"" + dash + "" if add_generation_prompt %}"" + after_endfor + endif
 
-        chat_template = chat_template[:where + len(endfor)] + after_endfor
+        chat_template = chat_template[:where + len(chosen_end)] + after_endfor
     pass
     return chat_template
 pass
"
"diff --git a/.github/ISSUE_TEMPLATE/feature_request.md b/.github/ISSUE_TEMPLATE/feature_request.md
index f3615ed..a2956b5 100644
--- a/.github/ISSUE_TEMPLATE/feature_request.md
+++ b/.github/ISSUE_TEMPLATE/feature_request.md
@@ -2,7 +2,7 @@
 name: Feature request
 about: Suggest an idea for this project
 title: ""[FEAT]""
-labels: enhancement
+labels: feature request
 assignees: ''
 
 ---
"
"diff --git a/pyproject.toml b/pyproject.toml
index 3ea50ca..324e011 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -178,6 +178,9 @@ colab-no-deps = [
     ""xformers"",
     ""bitsandbytes"",
 ]
+colab = [
+    ""unsloth[cu121]"",
+]
 colab-ampere = [
     ""unsloth[colab-ampere-torch220]"",
     ""packaging"",
diff --git a/unsloth/save.py b/unsloth/save.py
index 05ff748..5970d74 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -684,6 +684,9 @@ def save_to_gguf(
 ):
     from transformers.models.llama.modeling_llama import logger
 
+    if quantization_method.startswith(""iq2""):
+        raise RuntimeError(""Unsloth: Currently iq2 type quantizations aren't supported yet - sorry!"")
+
     # Careful convert.py is only for Llama / Mistral based archs
     use_fast_convert = False
     if   model_type == ""llama"":   use_fast_convert = True
@@ -743,8 +746,11 @@ def save_to_gguf(
         if   first_conversion == ""f32"" : pass
         elif first_conversion == ""f16"" : pass
         elif first_conversion == ""q8_0"":
-            logger.warning_once(""Unsloth: We must use f16 for quantization first."")
-            first_conversion = ""f16""
+            logger.warning_once(
+                ""Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, ""\
+                ""but saves disk space!""
+            )
+            # first_conversion = ""f16""
         pass
     pass
 
"
"diff --git a/blackwell/README.md b/blackwell/README.md
index 3a51992..cb16dc5 100644
--- a/blackwell/README.md
+++ b/blackwell/README.md
@@ -38,7 +38,7 @@ The installation order is important, since we want the overwrite bundled depende
 2) Install `vllm`
 
     ```bash
-    uv pip install -U vllm --torch-backend=cu128 --extra-index-url https://wheels.vllm.ai/nightly
+    uv pip install -U vllm --torch-backend=cu128
     ```
 
     Note that we have to specify `cu128`, otherwise `vllm` will install `torch==2.7.0` but with `cu126`.
@@ -64,15 +64,7 @@ The installation order is important, since we want the overwrite bundled depende
 
     Note that we have to explicitly set `TORCH_CUDA_ARCH_LIST=12.0`.
 
-5) Update `triton`
-
-    ```bash
-    uv pip install -U triton>=3.3.1
-    ```
-
-    `triton>=3.3.1` is required for `Blackwell` support.
-
-6) `transformers`
+5) `transformers`
     `transformers >= 4.53.0` breaks `unsloth` inference.  Specifically, `transformers` with `gradient_checkpointing` enabled will automatically [switch off caching](https://github.com/huggingface/transformers/blob/67ddc82fbc7e52c6f42a395b4a6d278c55b77a39/src/transformers/modeling_layers.py#L52-L59).
 
     When using `unsloth` `FastLanguageModel` to `generate` directly after training with `use_cache=True`, this will result in mismatch between expected and actual outputs [here](https://github.com/unslothai/unsloth/blob/bfa6a3678e2fb8097c5ece41d095a8051f099db3/unsloth/models/llama.py#L939).
"
"diff --git a/pyproject.toml b/pyproject.toml
index 02bcf4b..7f24aab 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.16"",
+    ""unsloth_zoo>=2025.3.17"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -351,7 +351,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.16"",
+    ""unsloth_zoo>=2025.3.17"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 708eeaf..d401b72 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.16""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.17""):
         print(
             ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
             ""To disable this, set `os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'`""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0044c7e..840c15c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.18""
+__version__ = ""2025.3.19""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -1176,9 +1176,10 @@ def unsloth_compile_transformers(
             ""so turning off some optimizations!""
         )
         return
-    if disable: return
-
     model_types = list(dict().fromkeys(model_types).keys())
+    if disable: return model_types, False
+
+    supports_sdpa = [True]
     for model_type in model_types:
         _unsloth_compile_transformers(
             model_type,
@@ -1206,12 +1207,13 @@ def unsloth_compile_transformers(
             import_from_cache      = import_from_cache,
             disable                = disable,
             return_logits          = return_logits,
+            supports_sdpa          = supports_sdpa,
         )
     pass
     # Redo patches which override compiler
     for temporary_patch in TEMPORARY_PATCHES:
         temporary_patch()
-    return model_types
+    return model_types, supports_sdpa[0]
 pass
 
 # We need an empty logits flag to warn people logits will not be returned anymore unless asked ie
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b3b49a0..722b50d 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2024,6 +2024,14 @@ class FastLlamaModel:
         **kwargs,
     ):
         if os.environ.get(""UNSLOTH_USE_NEW_MODEL"", ""0"") == ""1"":
+            # Check for other PEFT args in kwargs
+            for (peft_arg, flag) in (
+                (""finetune_vision_layers"", False),
+                (""finetune_language_layers"", True),
+                (""finetune_attention_modules"", True),
+                (""finetune_mlp_modules"", True),
+            ):
+                if peft_arg not in kwargs: kwargs[peft_arg] = flag
             return FastBaseModel.get_peft_model(
                 model                      = model,
                 r                          = r,
@@ -2031,10 +2039,6 @@ class FastLlamaModel:
                 lora_alpha                 = lora_alpha,
                 lora_dropout               = lora_dropout,
                 bias                       = bias,
-                finetune_vision_layers     = False,
-                finetune_language_layers   = True,
-                finetune_attention_modules = True,
-                finetune_mlp_modules       = True,
                 layers_to_transform        = layers_to_transform,
                 layers_pattern             = layers_pattern,
                 use_gradient_checkpointing = use_gradient_checkpointing,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 670e082..cac5acd 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -663,7 +663,7 @@ class FastModel(FastBaseModel):
 
         with redirector:
             patch_loss_functions(torch_compile = False)
-            model_types = unsloth_compile_transformers(
+            model_types, supports_sdpa = unsloth_compile_transformers(
                 dtype                   = dtype,
                 model_name              = model_name,
                 model_types             = model_types,
@@ -726,6 +726,7 @@ class FastModel(FastBaseModel):
             tokenizer_name    = tokenizer_name,
             auto_model        = auto_model,
             use_gradient_checkpointing = use_gradient_checkpointing,
+            supports_sdpa     = supports_sdpa,
             *args, **kwargs,
         )
 
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index cf250dd..91ed262 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -728,6 +728,16 @@ __INT_TO_FLOAT_MAPPER = \
         ""mistralai/Mistral-Small-3.1-24B-Base-2503"",
         ""unsloth/Mistral-Small-3.1-24B-Base-2503-bnb-4bit"",
     ),
+    ""unsloth/orpheus-3b-0.1-pretrained-unsloth-bnb-4bit"" : (
+        ""unsloth/orpheus-3b-0.1-pretrained"",
+        ""canopylabs/orpheus-3b-0.1-pretrained"",
+        ""unsloth/orpheus-3b-0.1-pretrained-bnb-4bit"",
+    ),
+    ""unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit"" : (
+        ""unsloth/orpheus-3b-0.1-ft"",
+        ""canopylabs/orpheus-3b-0.1-ft"",
+        ""unsloth/orpheus-3b-0.1-ft-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index a3b2d1d..376d1e9 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -79,7 +79,7 @@ def sft_trainer_prepare_dataset(function_name, function):
         function_name != ""_prepare_dataset"": return function
 
     fast_sft_prepare_dataset = RL_REPLACEMENTS.get(""sft_prepare_dataset"", None)
-    if fast_sft_prepare_dataset is not None and ""pack_examples"" in function:
+    if fast_sft_prepare_dataset is not None:
         params = inspect.signature(fast_sft_prepare_dataset).parameters.keys()
         params = "".*?"".join(params)
         matched = re.match(
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index ef32ab1..f05cc95 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -66,11 +66,6 @@ __all__ = [
     ""FastBaseModel"",
 ]
 
-global FORCE_EAGER_ATTENTION
-FORCE_EAGER_ATTENTION = [
-    ""pixtral"",    # Pixtral SDPA not implemented
-]
-
 global NUM_LOGITS_TO_KEEP
 NUM_LOGITS_TO_KEEP = dict()
 global PROMPT_LOOPKUP
@@ -145,8 +140,11 @@ def unsloth_base_fast_generate(
         kwargs[key] = 1
     global PROMPT_LOOPKUP
     if arch not in PROMPT_LOOPKUP:
-        PROMPT_LOOPKUP[arch] = True
-
+        # Only works for VLMs and not LLMs!
+        if is_vlm:
+            PROMPT_LOOPKUP[arch] = False
+        else:
+            PROMPT_LOOPKUP[arch] = True
     if bsz == 1 and PROMPT_LOOPKUP[arch]:
         kwargs[""prompt_lookup_num_tokens""] = 3
 
@@ -237,8 +235,14 @@ class FastBaseModel:
         tokenizer_name    = None,
         auto_model        = AutoModelForVision2Seq,
         use_gradient_checkpointing = ""unsloth"",
+        supports_sdpa     = True,
         **kwargs,
     ):
+        if model_types is None:
+            raise RuntimeError(
+                ""Unsloth: Please use FastModel or FastVisionModel and not use FastBaseModel directly!""
+            )
+
         os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""1""
         if trust_remote_code:
             print(
@@ -299,16 +303,11 @@ class FastBaseModel:
             bnb_compute_dtype = torch.float16
             do_forced_float32 = True
         pass
-
-        global FORCE_EAGER_ATTENTION
-        attn_implementation = ""sdpa""
-        for disable_name in FORCE_EAGER_ATTENTION:
-            if (disable_name.lower() == model_type_arch.lower() or \
-                disable_name.lower() in model_name.lower()):
-
-                print(f""Unsloth: {model_type_arch} does not support SDPA - switching to eager!"")
-                attn_implementation = ""eager""
-                break
+        # Stop SDPA for some archs like Pixtral / Mistral3
+        kwargs[""attn_implementation""] = ""sdpa""
+        if not supports_sdpa:
+            print(f""Unsloth: {model_type_arch.title()} does not support SDPA - switching to eager!"")
+            del kwargs[""attn_implementation""]
         pass
 
         bnb_config = None
@@ -347,8 +346,6 @@ class FastBaseModel:
             os.environ[""UNSLOTH_ENABLE_FULL_FINETUNING""] = ""0""
         pass
 
-        kwargs.pop(""attn_implementation"", None); # No need since we auto call it
-
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
 
@@ -362,7 +359,7 @@ class FastBaseModel:
             # quantization_config   = bnb_config,
             token                   = token,
             trust_remote_code       = trust_remote_code,
-            attn_implementation     = attn_implementation,
+            # attn_implementation   = attn_implementation,
             **kwargs,
         )
         # Return old flag
@@ -431,11 +428,12 @@ class FastBaseModel:
         m.is_loaded_in_8bit = True if not full_finetuning else False
 
         # Patch generate
-        if model.generate.__name__ != ""unsloth_base_fast_generate"":
-            model._old_generate = model.generate
-            unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
-            model.generate = types.MethodType(unsloth_base_fast_generate, model)
-
+        if os.environ.get(""UNSLOTH_DISABLE_FAST_GENERATION"", ""0"") == ""0"":
+            if model.generate.__name__ != ""unsloth_base_fast_generate"":
+                model._old_generate = model.generate
+                unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
+                model.generate = types.MethodType(unsloth_base_fast_generate, model)
+        pass
         # Post patches
         model = FastBaseModel.post_patch_model(
             model,
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 7bfec43..6bd8a6f 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -143,7 +143,7 @@ pass
 from math import sqrt as math_sqrt
 
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
-@torch.inference_mode
+# @torch.inference_mode
 def GemmaModel_fast_forward_inference(
     self,
     input_ids,
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 8b86feb..574385f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -657,7 +657,7 @@ pass
 
 
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
-@torch.inference_mode
+# @torch.inference_mode
 def LlamaModel_fast_forward_inference(
     self,
     input_ids,
@@ -753,7 +753,8 @@ def CausalLM_fast_forward(fast_forward_inference):
         hidden_states = outputs[0]
         bsz, q_len, hd = hidden_states.shape
         if bsz == 1 and q_len == 1:
-            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
+            lm_head = self.lm_head.weight
+            logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
             logits = logits.unsqueeze(0).unsqueeze(0)
         else:
             logits = self.lm_head(hidden_states)
@@ -893,6 +894,16 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
 pass
 
 
+def _wrap_fast_inference(generate, device_type, dtype):
+    # Wraps inference with bfloat16 / float16
+    @torch.inference_mode
+    def _fast_generate(*args, **kwargs):
+        with torch.autocast(device_type = device_type, dtype = dtype):
+            return generate(*args, **kwargs)
+    return _fast_generate
+pass
+
+
 class FastLlamaModel:
 
     @staticmethod
@@ -1581,6 +1592,15 @@ class FastLlamaModel:
             internal_model.gradient_checkpointing = False
             internal_model.training = False
         pass
+
+        # Also check if lm_head / embeddings are trained
+        lm_head = getattr(model, ""model"", model).lm_head.weight
+        device_type = lm_head.device.type
+        dtype = model.config.torch_dtype
+
+        # Wrap model.generate
+        model._unwrapped_old_generate = model.generate
+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)
     pass
 
 
@@ -1601,5 +1621,14 @@ class FastLlamaModel:
             internal_model.gradient_checkpointing = use_gradient_checkpointing
             internal_model.training = True
         pass
+
+        # Also revert model.generate
+        if hasattr(model, ""_unwrapped_old_generate""):
+            model.generate = model._unwrapped_old_generate
+            del model._unwrapped_old_generate
+        pass
     pass
 pass
+
+
+
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 2db71f0..f650d8f 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -225,7 +225,8 @@ def MistralForCausalLM_fast_forward(
     hidden_states = outputs[0]
     bsz, q_len, hd = hidden_states.shape
     if bsz == 1 and q_len == 1:
-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
+        lm_head = self.lm_head.weight
+        logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
         logits = logits.unsqueeze(0).unsqueeze(0)
     else:
         logits = self.lm_head(hidden_states)
diff --git a/unsloth/save.py b/unsloth/save.py
index 5970d74..c8b4053 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -32,6 +32,8 @@ __all__ = [
     ""patch_saving_functions"",
 ]
 
+# Check Kaggle
+IS_A_KAGGLE_ENVIRONMENT = ""KAGGLE_CONTAINER_NAME"" in os.environ
 
 LLAMA_WEIGHTS = (
     ""self_attn.q_proj"", ""self_attn.k_proj"", ""self_attn.v_proj"", ""self_attn.o_proj"",
@@ -66,9 +68,9 @@ ALLOWED_QUANTS = \
     ""q5_1""    : ""Even higher accuracy, resource usage and slower inference."",
     ""q5_k_s""  : ""Uses Q5_K for all tensors"",
     ""q6_k""    : ""Uses Q8_K for all tensors"",
-    ""iq2_xxs"" : ""2.06 bpw quantization"",
-    ""iq2_xs""  : ""2.31 bpw quantization"",
-    ""iq3_xxs"" : ""3.06 bpw quantization"",
+    # ""iq2_xxs"" : ""2.06 bpw quantization"", # Not supported sadly
+    # ""iq2_xs""  : ""2.31 bpw quantization"",
+    # ""iq3_xxs"" : ""3.06 bpw quantization"",
     ""q3_k_xs"" : ""3-bit extra small quantization"",
 }
 
@@ -79,6 +81,27 @@ def print_quantization_methods():
 pass
 
 
+def _free_cached_model(model):
+    from huggingface_hub import scan_cache_dir
+    cached_repos = list(scan_cache_dir().repos)
+
+    # Go through every cached repo, and delete the one that matches the model we want to save.
+    # Can save 4GB of disk space - useful for Kaggle systems.
+    for cached_repo in cached_repos:
+        if cached_repo.repo_id == model.config._name_or_path:
+            remove_cache_commit = list(cached_repo.revisions)[0].commit_hash
+            delete_strategy = scan_cache_dir().delete_revisions(remove_cache_commit,)
+
+            logger.warning_once(
+                ""Unsloth: Will remove a cached repo with size "" + \
+                delete_strategy.expected_freed_size_str,
+            )
+
+            delete_strategy.execute()
+        pass
+    pass
+pass
+
 
 def _merge_lora(layer, name):
 
@@ -153,6 +176,19 @@ def unsloth_save_model(
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.9,
 ):
+    # First check for a token!
+    if push_to_hub:
+        from huggingface_hub import whoami
+        try: 
+            username = whoami(token = token)[""name""]
+        except:
+            raise RuntimeError(
+                ""Unsloth: Please supply a token!\n""\
+                ""Go to https://huggingface.co/settings/tokens""
+            )
+        pass
+    pass
+
     if commit_message is None: commit_message = """"
     if ""Unsloth"" not in commit_message:
         commit_message += "" (Trained with Unsloth)""
@@ -411,6 +447,16 @@ def unsloth_save_model(
         os.makedirs(temporary_location)
     pass
 
+    # Check if Kaggle, since only 20GB of Disk space allowed.
+    if IS_A_KAGGLE_ENVIRONMENT:
+        # We free up 4GB of space
+        logger.warning_once(
+            ""Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\n""\
+            ""model which will save 4GB of disk space, allowing you to save on Kaggle.""
+        )
+        _free_cached_model(internal_model)
+    pass
+
     # HF also uses a OrderedDict
     from collections import OrderedDict
     state_dict = OrderedDict()
@@ -480,12 +526,35 @@ def unsloth_save_model(
         )
     pass
 
+    # First check if we're pushing to an organization!
+    save_directory = save_pretrained_settings[""save_directory""]
+
+    if save_pretrained_settings[""push_to_hub""]:
+        new_save_directory, new_username = _determine_username(save_directory, username, token)
+
+        if token is not None:
+            from huggingface_hub import whoami
+            actual_username = whoami(token = token)[""name""]
+        else:
+            actual_username = username
+    pass
+
+    # Check if pushing to an organization
+    if save_pretrained_settings[""push_to_hub""] and (username != actual_username):
+        print(f""Unsloth: Saving to organization with address {new_save_directory}"")
+        # We upload everything at the end!
+        tokenizer_save_settings[""push_to_hub""] = False
+        tokenizer_save_settings[""save_directory""] = new_save_directory
+    pass
+
+    # Save tokenizer
     if tokenizer is not None:
         print(""Unsloth: Saving tokenizer..."", end = """")
         tokenizer.save_pretrained(**tokenizer_save_settings)
         print("" Done."")
     else:
         print()
+    pass
 
     print(""Unsloth: Saving model... This might take 5 minutes for Llama-7b..."")
 
@@ -502,7 +571,35 @@ def unsloth_save_model(
     model.config = new_config
 
     # Save!
-    internal_model.save_pretrained(**save_pretrained_settings)
+
+    # Check if pushing to an organization
+    if save_pretrained_settings[""push_to_hub""] and (username != actual_username):
+        print(f""Unsloth: Saving to organization with address {new_save_directory}"")
+        # Pushing to organization!
+        # Sadly .save_pretrained doesn't work :(
+        # We first save it via .save_pretrained, then upload manually!
+        save_pretrained_settings[""save_directory""] = new_save_directory
+        save_pretrained_settings[""push_to_hub""] = False
+        internal_model.save_pretrained(**save_pretrained_settings)
+
+        # Now manually go through each file and upload them manually!
+        filenames = os.listdir(new_save_directory)
+
+        from huggingface_hub import HfApi
+        hf_api = HfApi(token = save_pretrained_settings[""token""])
+
+        print(""Unsloth: Uploading all files... Please wait!"")
+        hf_api.upload_folder(
+            folder_path = new_save_directory,
+            path_in_repo = ""."",
+            repo_id = new_save_directory,
+            repo_type = ""model"",
+            commit_message  = ""(Trained with Unsloth)"",
+            ignore_patterns = ""*.md"",
+        )
+    else:
+        internal_model.save_pretrained(**save_pretrained_settings)
+    pass
 
     # Revert config back
     original_model = model
@@ -616,13 +713,16 @@ def install_llama_cpp_old(version = -10):
 pass
 
 
-def install_llama_cpp_blocking():
+def install_llama_cpp_blocking(use_cuda = True):
+    use_cuda = ""LLAMA_CUBLAS=1"" if use_cuda else """"
+
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
-        f""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}"",
+        f""cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}"",
         ""pip install gguf protobuf"",
     ]
     if os.path.exists(""llama.cpp""): return
+
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
             for line in sp.stdout:
@@ -954,17 +1054,8 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/
 [<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)
 """"""
 
-def upload_to_huggingface(
-    model,
-    save_directory,
-    token,
-    method,
-    extra = """",
-    file_location = None,
-    old_username = None,
-    private = None,
-):
-    # Check for username
+
+def _determine_username(save_directory, old_username, token):
     username = """"
     save_directory = save_directory.lstrip(""./"")
     if ""/"" not in save_directory:
@@ -980,6 +1071,21 @@ def upload_to_huggingface(
     else:
         username = save_directory.split(""/"")[0]
     pass
+    return save_directory, username
+pass
+
+
+def upload_to_huggingface(
+    model,
+    save_directory,
+    token,
+    method,
+    extra = """",
+    file_location = None,
+    old_username = None,
+    private = None,
+):
+    save_directory, username = _determine_username(save_directory, old_username, token)
 
     from huggingface_hub import create_repo
     try:
@@ -1107,18 +1213,15 @@ def unsloth_save_pretrained_gguf(
 
     # Non blocking install GGUF first
     if not os.path.exists(""llama.cpp""):
-        git_clone = install_llama_cpp_clone_non_blocking()
-        python_install = install_python_non_blocking([""gguf"", ""protobuf""])
-        git_clone.wait()
-        makefile  = install_llama_cpp_make_non_blocking()
-        new_save_directory, old_username = unsloth_save_model(**arguments)
-        python_install.wait()
-    else:
-        try:
+
+        if IS_A_KAGGLE_ENVIRONMENT:
+            # Kaggle is weird - no blocking installs, and no CUDA?
+            python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+            python_install.wait()
+            install_llama_cpp_blocking(use_cuda = False)
             new_save_directory, old_username = unsloth_save_model(**arguments)
             makefile = None
-        except:
-            # Retry by recloning llama.cpp
+        else:
             git_clone = install_llama_cpp_clone_non_blocking()
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             git_clone.wait()
@@ -1126,6 +1229,28 @@ def unsloth_save_pretrained_gguf(
             new_save_directory, old_username = unsloth_save_model(**arguments)
             python_install.wait()
         pass
+    else:
+        try:
+            new_save_directory, old_username = unsloth_save_model(**arguments)
+            makefile = None
+        except:
+            # Retry by recloning llama.cpp
+            if IS_A_KAGGLE_ENVIRONMENT:
+                # Kaggle is weird - no blocking installs, and no CUDA?
+                python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+                python_install.wait()
+                install_llama_cpp_blocking(use_cuda = False)
+                new_save_directory, old_username = unsloth_save_model(**arguments)
+                makefile = None
+            else:
+                git_clone = install_llama_cpp_clone_non_blocking()
+                python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+                git_clone.wait()
+                makefile  = install_llama_cpp_make_non_blocking()
+                new_save_directory, old_username = unsloth_save_model(**arguments)
+                python_install.wait()
+            pass
+        pass
     pass
 
     for _ in range(3):
@@ -1208,25 +1333,44 @@ def unsloth_push_to_hub_gguf(
 
     # Non blocking install GGUF first
     if not os.path.exists(""llama.cpp""):
-        git_clone = install_llama_cpp_clone_non_blocking()
-        python_install = install_python_non_blocking([""gguf"", ""protobuf""])
-        git_clone.wait()
-        makefile  = install_llama_cpp_make_non_blocking()
-        new_save_directory, old_username = unsloth_save_model(**arguments)
-        python_install.wait()
-    else:
-        try:
+
+        if IS_A_KAGGLE_ENVIRONMENT:
+            # Kaggle is weird - no blocking installs, and no CUDA?
+            python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+            python_install.wait()
+            install_llama_cpp_blocking(use_cuda = False)
             new_save_directory, old_username = unsloth_save_model(**arguments)
             makefile = None
-        except:
-            # Retry by recloning llama.cpp
+        else:
             git_clone = install_llama_cpp_clone_non_blocking()
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             git_clone.wait()
-            makefile = install_llama_cpp_make_non_blocking()
+            makefile  = install_llama_cpp_make_non_blocking()
             new_save_directory, old_username = unsloth_save_model(**arguments)
             python_install.wait()
         pass
+    else:
+        try:
+            new_save_directory, old_username = unsloth_save_model(**arguments)
+            makefile = None
+        except:
+            # Retry by recloning llama.cpp
+            if IS_A_KAGGLE_ENVIRONMENT:
+                # Kaggle is weird - no blocking installs, and no CUDA?
+                python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+                python_install.wait()
+                install_llama_cpp_blocking(use_cuda = False)
+                new_save_directory, old_username = unsloth_save_model(**arguments)
+                makefile = None
+            else:
+                git_clone = install_llama_cpp_clone_non_blocking()
+                python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+                git_clone.wait()
+                makefile  = install_llama_cpp_make_non_blocking()
+                new_save_directory, old_username = unsloth_save_model(**arguments)
+                python_install.wait()
+            pass
+        pass
     pass
 
     for _ in range(3):
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index e6c9280..445e502 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -570,7 +570,14 @@ def LlamaModel_fast_forward(
     # Embed positions
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
-
+        
+    if self.config.torch_dtype == ""float32"":
+        self.config.torch_dtype = torch.float32
+    elif self.config.torch_dtype == ""bfloat16"":
+        self.config.torch_dtype = torch.bfloat16
+    elif self.config.torch_dtype == ""float16"":
+        self.config.torch_dtype = torch.float16
+        
     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
 
     # Normalized from Gemma
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7f0c2f6..2e88e21 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -214,6 +214,14 @@ try:
 except:
     pass
 
+# MXFP4 quantization requires triton >= 3.4.0
+try:
+    from transformers.quantizers.quantizer_mxfp4 import logger as mxfp4_logger
+    mxfp4_logger.addFilter(HideLoggingMessage(""requires triton""))
+    del mxfp4_logger
+except:
+    pass
+
 # Errors out on
 # Some weights of Gemma3nForConditionalGeneration were not initialized from the model checkpoint
 from transformers.modeling_utils import logger as transformers_logger
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 9529794..623f263 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -644,7 +644,7 @@ class FastBaseModel:
             ""loftq_config""      : loftq_config,
         }
         lora_config = LoraConfig(
-            **{k:v for k,v in lora_config_dict.items() if key in LoraConfig.__doc__},
+            **{k:v for k,v in lora_config_dict.items() if k in LoraConfig.__doc__},
         )
         model = prepare_model_for_kbit_training(
             model,
"
"diff --git a/unsloth/models/qwen3.py b/unsloth/models/qwen3.py
index 9d658cf..3f48449 100644
--- a/unsloth/models/qwen3.py
+++ b/unsloth/models/qwen3.py
@@ -85,13 +85,19 @@ def Qwen3Attention_fast_forward(
     assert(n_kv_heads * n_groups == n_heads)
 
     Q, K, V = self.apply_qkv(self, hidden_states)
-    Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)
-    K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+    Q = Q.view(bsz, q_len, n_heads,    head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    K = K.view(bsz, q_len, n_kv_heads, head_dim)#.transpose(1, 2) # we will transpose after normalisation
     V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
 
     #Qwen3 has QKNorm. This seems to be the only difference from Qwen2.
-    Q = fast_layernorm_compiled(self.q_norm, Q)
-    K = fast_layernorm_compiled(self.k_norm, K)
+    # Note that using fast_layernorm_compiled causes issues as the dimensions don't match up.
+    # I tried to add a compiled version of the new norm but the numbers don't match up with Transformers
+    # TODO: Check on the differences here.
+    Q = fast_rms_layernorm(self.q_norm, Q)
+    K = fast_rms_layernorm(self.k_norm, K)
+
+    Q = Q.transpose(1, 2)
+    K = K.transpose(1, 2)
 
     kv_seq_len = K.shape[-2]
     if past_key_value is not None:
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index f5d00ea..8d0eadb 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.2.8""
+__version__ = ""2025.2.9""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index ec6706e..1eae97f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -708,7 +708,7 @@ def LlamaModel_fast_forward(
     if attention_mask is None:
         padding_mask = None
     elif self.training:
-    # elif attention_mask is not None and self.training:
+    # elif attention_mask is None:
         attention_mask = None
         padding_mask = None
     else:
@@ -724,7 +724,8 @@ def LlamaModel_fast_forward(
             past_key_values_length,
             sliding_window = getattr(self.config, ""sliding_window"", None),
         )
-        attention_mask = attention_mask.to(torch.bool)
+        if attention_mask is not None:
+            attention_mask = attention_mask.to(torch.bool)
     pass
 
     hidden_states = inputs_embeds
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index fc094b0..3d601b0 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -565,8 +565,8 @@ pass
 
 
 def PatchFastRL(algorithm = None, FastLanguageModel = None):
-    return
-    # if FastLanguageModel is not None: PatchRL(FastLanguageModel)
-    # patch_trl_rl_trainers()
-    # if algorithm is not None: PatchRLStatistics(algorithm)
+    if FastLanguageModel is not None: PatchRL(FastLanguageModel)
+    patch_trl_rl_trainers()
+    if type(algorithm) is str and algorithm.islower():
+        PatchRLStatistics(algorithm)
 pass
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 4d7a4db..82fd3f8 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -101,23 +101,20 @@ RL_FUNCTIONS[""sft_trainer""].append(sft_trainer_prepare_dataset)
 
 # Ignore mean_token_accuracy since it needs logits
 # We override it directly with our version
-def _sft_trainer_compute_loss(self, model, inputs, return_outputs = False, num_items_in_batch = None):
-    (loss, outputs) = super().compute_loss(
-        model,
-        inputs,
-        return_outputs = return_outputs,
-        num_items_in_batch = num_items_in_batch,
-    )
-    return (loss, outputs) if return_outputs else loss
-pass
-
 def sft_trainer_compute_loss(function_name, function):
     if  function_name != ""compute_loss"": return function
 
-    function = inspect.getsource(_sft_trainer_compute_loss)
-    function = function.replace(""def _sft_trainer_compute_loss"", ""def compute_loss"")
-    function = function.split(""\n"")
-    function = ""\n"".join("" ""*4+x for x in function)
+    def compute_loss(self, model, inputs, return_outputs = False, num_items_in_batch = None):
+        outputs = super().compute_loss(
+            model,
+            inputs,
+            return_outputs = return_outputs,
+            num_items_in_batch = num_items_in_batch,
+        )
+        return outputs
+    pass
+
+    function = inspect.getsource(compute_loss)
     return function
 pass
 RL_FUNCTIONS[""sft_trainer""].append(sft_trainer_compute_loss)
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index e2bfe1d..34deb8f 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -155,14 +155,14 @@ class FastLanguageModel(FastLlamaModel):
         try:
             model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
             is_model = True
-        except Exception as autoconfig_error:
-            autoconfig_error = str(autoconfig_error)
+        except Exception as error:
+            autoconfig_error = str(error)
             is_model = False
         try:
             peft_config = PeftConfig .from_pretrained(model_name, token = token, revision = revision)
             is_peft = True
-        except Exception as peft_error:
-            peft_error = str(peft_error)
+        except Exception as error:
+            peft_error = str(error)
             is_peft = False
         pass
 
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index aae8608..0c6f6c1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2768,40 +2768,35 @@ class FastLlamaModel:
         if lora_dropout == 0 and bias == ""none"":
             for idx, layer in enumerate(model.model.model.layers):
 
-                # Determine MLP module name (falcon_h1 has feed_forward, llama style has mlp)
-                if hasattr(layer, ""mlp""):
-                    mlp_module_name = ""mlp""
-                elif hasattr(layer, ""feed_forward""):
-                    mlp_module_name = ""feed_forward""
-                else:
-                    logger.warning_once(f""Unsloth: No MLP module found in layer {idx} so skipping peft mlp patching"")
-                    continue
-
-                mlp_module = getattr(layer, mlp_module_name)
-
-                # MLP patching
-                gate_proj = mlp_module.gate_proj
-                up_proj   = mlp_module.  up_proj
-                down_proj = mlp_module.down_proj
-
-                if  hasattr(gate_proj, ""lora_A"") and \
-                    hasattr(  up_proj, ""lora_A"") and \
-                    hasattr(down_proj, ""lora_A"") and \
-                    (getattr(gate_proj, ""base_layer"", gate_proj).bias is None) and \
-                    (getattr(  up_proj, ""base_layer"",   up_proj).bias is None) and \
-                    (getattr(down_proj, ""base_layer"", down_proj).bias is None) and \
-                    (len(getattr(gate_proj, ""lora_magnitude_vector"", []) or []) == 0) and \
-                    (len(getattr(  up_proj, ""lora_magnitude_vector"", []) or []) == 0) and \
-                    (len(getattr(down_proj, ""lora_magnitude_vector"", []) or []) == 0):
-
-                    # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
-                    mlp_module.forward = types.MethodType(_apply_lora_mlp, mlp_module)
-                    n_mlp += 1
-                else:
-                    logger.warning_once(
-                        ""Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n""\
-                        ""are not enabled or a bias term (like in Qwen) is used.""
-                    )
+                if model_type != ""falcon_h1"":
+                    # LoRAMLP.apply doesn't have functionality for gate and down mutlipliers yet.
+                    # Don't patch falcon h1 for the time being.
+
+                    # MLP patching
+                    mlp_module = layer.mlp
+                    gate_proj = mlp_module.gate_proj
+                    up_proj   = mlp_module.  up_proj
+                    down_proj = mlp_module.down_proj
+
+                    if hasattr(gate_proj, ""lora_A"") and \
+                        hasattr(  up_proj, ""lora_A"") and \
+                        hasattr(down_proj, ""lora_A"") and \
+                        (getattr(gate_proj, ""base_layer"", gate_proj).bias is None) and \
+                        (getattr(  up_proj, ""base_layer"",   up_proj).bias is None) and \
+                        (getattr(down_proj, ""base_layer"", down_proj).bias is None) and \
+                        (len(getattr(gate_proj, ""lora_magnitude_vector"", []) or []) == 0) and \
+                        (len(getattr(  up_proj, ""lora_magnitude_vector"", []) or []) == 0) and \
+                        (len(getattr(down_proj, ""lora_magnitude_vector"", []) or []) == 0):
+
+                        # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
+                        mlp_module.forward = types.MethodType(_apply_lora_mlp, mlp_module)
+                        n_mlp += 1
+                    else:
+                        logger.warning_once(
+                            ""Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n""\
+                            ""are not enabled or a bias term (like in Qwen) is used.""
+                        )
+                    pass
                 pass
 
                 # QKV attention patching
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2587c5a..53c956c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -27,6 +27,11 @@ from unsloth_zoo.peft_utils import SKIP_QUANTIZATION_MODULES
 transformers_version = Version(transformers_version)
 # Transformers moved rotary embeddings out of all attention layers
 IS_ATTENTION_REFACTOR = transformers_version > Version(""4.47.1"")
+try:
+    from transformers.modeling_layers import GradientCheckpointingLayer
+except:
+    GradientCheckpointingLayer = type(None)
+
 from transformers.models.llama.modeling_llama import (
     logger,
     BaseModelOutputWithPast,
@@ -877,7 +882,12 @@ def LlamaModel_fast_forward(
                 mask = self. GA_mask if use_static_mask else dynamic_GA_mask
         pass
 
-        if gradient_checkpointing:
+        try:
+            is_gradient_checkpointing_layer = isinstance(decoder_layer, GradientCheckpointingLayer)
+        except:
+            is_gradient_checkpointing_layer = False
+
+        if gradient_checkpointing and not is_gradient_checkpointing_layer:
             def create_custom_forward(module):
                 def custom_forward(*inputs):
                     return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask, position_embeddings = position_embeddings)
"
"diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index fe7f4ac..5ea61cb 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -90,9 +90,11 @@ def sft_trainer_prepare_dataset(function_name, function):
     ""if getattr(tokenizer, 'bos_token', None) is not None else False\n""\
     ""if 'add_special_tokens' not in locals() and has_bos_token_already:\n""\
     ""    from functools import partial\n""\
-    ""    tokenizer = partial(tokenizer, add_special_tokens = False)\n""\
+    ""    tokenizer_call = tokenizer.__call__\n""\
+    ""    tokenizer.__call__ = partial(tokenizer_call, add_special_tokens = False)\n""\
     ""    processing_class = tokenizer\n""\
     ""else:\n""\
+    ""    tokenizer_call = None\n""\
     ""    add_special_tokens = False if has_bos_token_already else locals().get('add_special_tokens', False)\n""
 
     check_text = check_text.split(""\n"")
@@ -109,6 +111,14 @@ def sft_trainer_prepare_dataset(function_name, function):
         replacer = replacer[0]
         function = function.replace(replacer, replacer + check_text)
     pass
+
+    # Return tokenizer's original state
+    return_state = ""if tokenizer_call is not None: tokenizer.__call__ = tokenizer_call\n""
+    function = re.sub(
+        r""\n([ ]{4,})(return .*?[\s]{0,})$"",
+        rf""\1{return_state}\1\2"",
+        function,
+    )
     return function
 pass
 RL_FUNCTIONS[""sft_trainer""].append(sft_trainer_prepare_dataset)
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index de9f73d..91bb020 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -1047,9 +1047,3 @@ pass
 
 # Finally patch TRL tokenizer things -> moved to RL
 # patch_sft_trainer_tokenizer()
-
-# Temporary measure to stop tokenizing data twice
-if hasattr(trl, ""data_utils""):
-    def maybe_apply_chat_template(example, *args, **kwargs): return example
-    trl.data_utils.maybe_apply_chat_template = maybe_apply_chat_template
-pass
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 70dd896..3865857 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -557,7 +557,7 @@ class FastModel(FastBaseModel):
             raise RuntimeError(""Unsloth: Cohere's Command model only works on transformers >= 4.50.0."" + NIGHTLY)
         elif ""csm-1b"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Sesame fails
-            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""all;torch.float32;torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
         elif 'granite-4' in lowered_model_name:
             # granite-4 rms norms are stored as 16 bit, but we upcast
             os.environ[""UNSLOTH_UPCAST_LAYERNORM""] = ""1""
@@ -566,6 +566,7 @@ class FastModel(FastBaseModel):
             raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
         elif ""gemma-3n"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
+            s.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""float16;torch.float16;torch.float16;if name.endswith(('.conv')): module.to(torch.float32)""
             if transformers_version < Version(""4.53.0""):
                 raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
         else:
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 5e8c4ae..be9fcd3 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -352,11 +352,20 @@ class FastBaseModel:
         correct_dtype = None
         if os.environ.get(""UNSLOTH_FORCE_CUSTOM_DTYPE"", """") != """":
             custom_datatype = os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""]
-            assert custom_datatype.count("";"") == 1
-            bnb_compute_dtype, custom_datatype = custom_datatype.split("";"", 1)
-            dtype = torch.float32
-            bnb_compute_dtype = eval(bnb_compute_dtype)
-            correct_dtype = bnb_compute_dtype
+            assert custom_datatype.count("";"") == 3
+            checker, _dtype, _bnb_compute_dtype, _custom_datatype = custom_datatype.split("";"", 3)
+
+            # Allow custom dtypes on all runs
+            allow_all_runs = (checker == ""all"")
+            # Allow only on float16 datatypes
+            allow_float16_runs = (checker == ""float16"" and dtype == torch.float16)
+
+            if allow_all_runs or allow_float16_runs:
+                dtype = eval(_dtype)
+                bnb_compute_dtype = eval(_bnb_compute_dtype)
+                correct_dtype = bnb_compute_dtype
+                custom_datatype = _custom_datatype
+            pass
         pass
 
         # Stop SDPA for some archs like Pixtral / Mistral3
"
"diff --git a/pyproject.toml b/pyproject.toml
index b24abd3..d9df119 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -39,7 +39,7 @@ triton = [
     ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2025.1.2"",
+    ""unsloth_zoo>=2025.1.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -285,7 +285,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.1.2"",
+    ""unsloth_zoo>=2025.1.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 7f37a20..4882eaf 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.1.2""):
+    if Version(unsloth_zoo_version) < Version(""2025.1.4""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 2c16bf6..bfb1786 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.1.5""
+__version__ = ""2025.1.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/README.md b/README.md
index 2a3322d..3e56ec1 100644
--- a/README.md
+++ b/README.md
@@ -1,68 +1,105 @@
-<p align=""center"">
-  <picture>
-    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png"">
-    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png"">
-    <img alt=""unsloth logo"" src=""./images/unsloth%20logo%20black%20text.png"" height=""120"" style=""max-width: 100%;"">
-  </picture>
-</p>
-<p align=""center"">
-  <a href=""https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing""><img src=""./images/Free version button.png"" height=""50""></a>
-  <a href=""https://discord.gg/u54VK8m8tk""><img src=""./images/Discord button.png"" height=""50""></a>
-  <a href=""https://ko-fi.com/unsloth""><img src=""./images/Kofi button.png"" height=""50""></a>
-</p>
-
-<h2 align=""center"">
-    Finetune Mistral, Llama 2-5x faster with 50% less memory!
-</h2>
-<br>
-
-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |
-|-----------------------------|-----------------------------|-------------------------|------------------------|
-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |
-| [Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing"") | [Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)
-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |
-
-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. **Free!** DPO Zephyr, Mistral example! <a href=""https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing""><img src=""./images/Colab.png"" height=""20"">  [More info](#DPO) on DPO
-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! **Free!** example <a href=""https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing""><img src=""./images/Colab.png"" height=""20"">
-* **NEW!** We're in  Huggingface's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).
-* All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language. **Manual backprop engine**.
-* **0% loss in accuracy** - no approximation methods - all exact.
-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
-* Works on **Linux** and **Windows** via WSL.
-* **NEW!** Download 4 bit models 4x faster from  Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`
-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!
-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!
-
-| 1 A100 40GB  |  Hugging Face | Flash Attention |  Unsloth Open Source | [ Unsloth Pro](https://unsloth.ai/pricing) |
+<div align=""center"">
+
+  <a href=""https://unsloth.ai""><picture>
+    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png"">
+    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"">
+    <img alt=""unsloth logo"" src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"" height=""110"" style=""max-width: 100%;"">
+  </picture></a>
+  
+<a href=""https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png"" height=""48""></a>
+<a href=""https://discord.gg/u54VK8m8tk""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
+<a href=""https://ko-fi.com/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png"" height=""48""></a>
+
+### Finetune Mistral, Llama 2-5x faster with 70% less memory!
+
+![](https://i.ibb.co/sJ7RhGG/image-41.png)
+
+</div>
+
+##  Finetune for Free
+
+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.
+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
+| **Mistral 7b**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
+| **Llama-2 7b**      | [ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |
+| **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |
+| **TinyLlama**  | [ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |
+| **CodeLlama 34b** A100   | [ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |
+| **Mistral 7b** 2xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |
+
+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.
+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.
+
+##  Unsloth.ai News
+-  [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.
+-  [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.
+-  We did a [blog](https://huggingface.co/blog/unsloth-trl) with Hugging Face! We're in Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).
+-  Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!
+-  **Download models 4x faster** from Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!
+
+##  Links and Resources
+| Type                            | Links                               |
+| ------------------------------- | --------------------------------------- |
+|  **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |
+|  **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|
+| <img height=""14"" src=""https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg"" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
+|  **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)
+|  **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|
+|  **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|
+
+##  Key Features
+- All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language. **Manual backprop engine**.
+- **0% loss in accuracy** - no approximation methods - all exact.
+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
+- Works on **Linux** and **Windows** via WSL.
+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!
+- If you trained a model with Unsloth, you can use this cool sticker! &nbsp; <img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png"" height=""50"" align=""center"" />
+
+
+##  Performance Benchmarking
+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)
+
+| 1 A100 40GB  | Hugging Face | Flash Attention | Unsloth Open Source | [Unsloth Pro](https://unsloth.ai/pricing) |
 |--------------|--------------|-----------------|---------------------|-----------------|
 | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |
 | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |
 | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |
 | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |
 
-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!
+- Benchmarking table below was conducted by [Hugging Face](https://huggingface.co/blog/unsloth-trl).
 
-<img src=""./images/unsloth made with love.png"" width=""200"" />
-If you trained a model with  Unsloth, we made a cool sticker if you want to use it!
+| Free Colab T4 | Dataset | Hugging Face | Pytorch 2.1.1 | Unsloth |  VRAM reduction |
+| --- | --- | --- | --- | --- | --- |
+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |
+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |
+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |
+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |
 
-# Installation Instructions - Conda
-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.
+![](https://i.ibb.co/sJ7RhGG/image-41.png)
+
+##  Installation Instructions
+### Conda Installation
+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.
 ```bash
-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \
-  -c pytorch -c nvidia -c xformers -c conda-forge -y
+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia
+
+conda install xformers -c xformers -y
+
+pip install bitsandbytes
+
 pip install ""unsloth[conda] @ git+https://github.com/unslothai/unsloth.git""
 ```
 
-# Installation Instructions - Pip
+### Pip Installation
 Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.
 
 1. Find your CUDA version via
 ```python
 import torch; torch.version.cuda
 ```
-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `""ampere""` path. For Pytorch 2.1.1: got to step 3.
+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `""ampere""` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.
 ```bash
 pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \
   --index-url https://download.pytorch.org/whl/cu121
@@ -84,16 +121,25 @@ pip install ""unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.
 pip install ""unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git""
 ```
-4. We're working on Pytorch 2.1.2 support.
+4. For Pytorch 2.2.0: Use the `""ampere""` path for newer RTX 30xx GPUs or higher.
+```bash
+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \
+  --index-url https://download.pytorch.org/whl/cu121
+```
+```bash
+pip install ""unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git""
+```
 5. If you get errors, try the below first, then go back to step 1:
 ```bash
 pip install --upgrade pip
 ```
 
-# Documentation
-We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
-
-We're in  Huggingface's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
+##  Documentation
+- We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
+- We're in Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
 
 ```python
 from unsloth import FastLanguageModel
@@ -159,10 +205,10 @@ trainer.train()
 ```
 
 <a name=""DPO""></a>
-# DPO (Direct Preference Optimization) Support
-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).
+## DPO Support
+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).
 
-We're in  Huggingface's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
+We're in Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
 
 ```python
 from unsloth import FastLanguageModel, PatchDPOTrainer
@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(
 dpo_trainer.train()
 ```
 
-# Support us!
-We're currently 2 brothers trying to make LLMs for everyone! It'll be super cool if you can support our work!!
-<a href=""https://ko-fi.com/unsloth""><img src=""./images/Kofi button.png"" height=""50""></a>
-
-# Future Milestones and limitations
-1. Support Mixtral.
-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)
-3. Dropout, bias in LoRA matrices are supported, just not optimized.
-
-# Performance comparisons on 1 Tesla T4 GPU:
-**Time taken for 1 epoch**
-
-One Tesla T4 on Google Colab
-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = ""adamw_8bit"", schedule = ""linear"", schedule_steps = 10`
-
-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
-| --- | --- | --- | --- | --- | --- |
-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |
-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |
-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |
-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |
-
-**Peak Memory Usage**
-
-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
-| --- | --- | --- | --- | --- | --- |
-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |
-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |
-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |
-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |
-
-# Performance comparisons on 2 Tesla T4 GPUs via DDP:
-**Time taken for 1 epoch**
-
-Two Tesla T4s on Kaggle
-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = ""adamw_8bit"", schedule = ""linear"", schedule_steps = 10`
-
-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |
-| --- | --- | --- | --- | --- | --- |
-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |
-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |
-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |
-
-**Peak Memory Usage on a Multi GPU System (2 GPUs)**
-
-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |
-| --- | --- | --- | --- | --- | --- |
-| Huggingface | 2 T4 | 8.4GB \| 6GB | 7.2GB \| 5.3GB | 14.3GB \| 6.6GB | 10.9GB \| 5.9GB * |
-| Unsloth Pro | 2 T4 | 7.7GB \| 4.9GB | 7.5GB \| 4.9GB | 8.5GB \| 4.9GB | 6.2GB \| 4.7GB * |
-| Unsloth Max | 2 T4 | 10.5GB \| 5GB | 10.6GB \| 5GB | 10.6GB \| 5GB | 10.5GB \| 5GB * |
-
-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.
+##  Detailed Benchmarking Tables
+- Click ""Code"" for fully reproducible examples
+- ""Unsloth Equal"" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.
+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)
+  
+| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |
+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |
+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |
+| memory MB| 18235 | 15365 | 9631 | 8525 | | |
+| % saved| | 15.74 | 47.18 | 53.25 | | | |
 
-# Llama-Factory 3rd party benchmarking
+### Llama-Factory 3rd party benchmarking
+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.
 
 | Method | Bits | TGS | GRAM | Speed |
 | --- | --- | --- | --- | --- |
@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle
 | HF | 4 | 2415 | 9GB | 101% |
 | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |
 
-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.
-
-# How did we make it faster?
-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!
-
-# Troubleshooting
-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:
-```bash
-!ldconfig /usr/lib64-nvidia
-```
-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.
-
-3. If it doesn't install - maybe try updating `pip`.
-
-
-# Full benchmarking tables
-Click  ""Code"" for a fully reproducible example.
-""Unsloth Equal"" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.
-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |
-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |
-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |
-| memory MB| 18235 | 15365 | 9631 | 8525 | | |
-| % saved| | 15.74 | 47.18 | 53.25 | | | |
-
-
-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |
-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |
-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |
-| memory MB| 7763  | 8047  | 7763 | 6441 | | |
-| % saved| | -3.66 | 0.00  | 17.03 | | | |
-
-
-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |
-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |
-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |
-| memory MB| 26431 | 16565 | 12267| 11223| | |
-| % saved| | 37.33 | 53.59 | 57.54 | | |
-
-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |
-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |
-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |
-| memory MB| 24557 | 15681 | 10595| 9007 | | |
-| % saved| | 36.14 | 56.86 | 63.32 | | |
-
+### Performance comparisons between popular models
+<details>
+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>
+  
 ### Mistral 7b
 | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
 |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
@@ -345,7 +304,7 @@ Click  ""Code"" for a fully reproducible example.
 | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
 |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
 | Code Llama 34B   | OOM          | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |
-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |
+| code | [ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |
 | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |
 | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |
 | % saved|    | 16.96| 31.47 | 44.60 |       | | |
@@ -355,87 +314,74 @@ Click  ""Code"" for a fully reproducible example.
 | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |
 |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|
 | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |
-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |
+| code | [ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |
 | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |
 | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |
 | % saved        |         | 1.94        | 10.28           | 24.39        |               | |
 
-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |
-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|
-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |
-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |
-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | 
-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |
-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |
-
-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |
-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|
-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |
-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |
-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |
-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |
-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |
-
-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |
-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|
-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |
-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |
-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |
-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |
-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |
-
 ### 2 Tesla T4s via DDP
 
  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
 |--------------|----------|-------------|-----------------|--------------|---------------|-------------|
 | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |
-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |
+| code | [ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |
 | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |
 | memory MB| 9176 | 9128 | 6904 | 6782 |  | |
 | % saved |     | 0.52 | 24.76 | 26.09 |  | | |
+</details>
 
- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|
-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |
-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |
-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |
-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |
-| % saved |     | 0.00 | 21.65 | 18.89 |  | |
+### Performance comparisons on 1 Tesla T4 GPU:
+<details>
+  <summary>Click for Time taken for 1 epoch</summary>
 
- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|
-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |
-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |
-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |
-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |
-| % saved |     | 2.25 | 44.38 | 40.27 |  | |
+One Tesla T4 on Google Colab
+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = ""adamw_8bit"", schedule = ""linear"", schedule_steps = 10`
 
- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|
-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |
-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |
-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |
-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |
-| % saved |     | -0.29| 41.04 | 32.70 |  | | |
+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
+| --- | --- | --- | --- | --- | --- |
+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |
+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |
+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |
+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |
 
- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|
-| OASST (bsz=2)        | OOM       | OOM        |            |          |          |  |
-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |
-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |
-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |
-| % saved | OOM  | OOM  |       |       |  | |
+**Peak Memory Usage**
 
- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|
-| Slim Orca (bsz=2)    | OOM        | OOM        |            |         |          | |
-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |
-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |
-| memory MB| OOM  | OOM  | 7594 | 8881 | | |
-| % saved | OOM  | OOM  |       |       |  | |
+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
+| --- | --- | --- | --- | --- | --- |
+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |
+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |
+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |
+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |
+</details>
+
+<details>
+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>
+**Time taken for 1 epoch**
+
+Two Tesla T4s on Kaggle
+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = ""adamw_8bit"", schedule = ""linear"", schedule_steps = 10`
+
+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |
+| --- | --- | --- | --- | --- | --- |
+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |
+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |
+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |
+
+**Peak Memory Usage on a Multi GPU System (2 GPUs)**
+
+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |
+| --- | --- | --- | --- | --- | --- |
+| Huggingface | 2 T4 | 8.4GB \| 6GB | 7.2GB \| 5.3GB | 14.3GB \| 6.6GB | 10.9GB \| 5.9GB * |
+| Unsloth Pro | 2 T4 | 7.7GB \| 4.9GB | 7.5GB \| 4.9GB | 8.5GB \| 4.9GB | 6.2GB \| 4.7GB * |
+| Unsloth Max | 2 T4 | 10.5GB \| 5GB | 10.6GB \| 5GB | 10.6GB \| 5GB | 10.5GB \| 5GB * |
+
+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.
+</details>
+
+![](https://i.ibb.co/sJ7RhGG/image-41.png)
+<br>
 
-# Credits
+### Credits
 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
 2. [152334H](https://github.com/152334H) for experimental DPO support
 3. [atgctg](https://github.com/atgctg) for syntax highlighting
-<img src=""./images/unsloth loading page render.png"" width=""300"" />
diff --git a/images/buy me a coffee button.png b/images/buy me a coffee button.png
new file mode 100644
index 0000000..5eccb8e
Binary files /dev/null and b/images/buy me a coffee button.png differ
diff --git a/images/made with unsloth.png b/images/made with unsloth.png
new file mode 100644
index 0000000..6c5e90d
Binary files /dev/null and b/images/made with unsloth.png differ
diff --git a/images/start free finetune button.png b/images/start free finetune button.png
new file mode 100644
index 0000000..0f6c879
Binary files /dev/null and b/images/start free finetune button.png differ
diff --git a/images/unsloth end.png b/images/unsloth end.png
new file mode 100644
index 0000000..f8a774a
Binary files /dev/null and b/images/unsloth end.png differ
"
"diff --git a/pyproject.toml b/pyproject.toml
index 2bceca5..70b0322 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ huggingface = [
     ""datasets"",
     ""sentencepiece"",
     ""accelerate"",
-    ""trl"",
+    ""trl==0.7.7"",
     ""peft"",
     ""packaging"",
     ""ninja"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 879b092..19902a3 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -83,3 +83,4 @@ except:
 pass
 
 from .models import *
+from .save import *
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 711169e..5de19c8 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -17,6 +17,7 @@ from .rms_layernorm import fast_rms_layernorm
 from .rope_embedding import fast_rope_embedding, inplace_rope_embedding
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
 from .fast_lora import (
+	get_lora_parameters,
 	apply_lora_mlp,
 	apply_lora_qkv,
 	apply_lora_o,
diff --git a/unsloth/save.py b/unsloth/save.py
new file mode 100644
index 0000000..f94a1b8
--- /dev/null
+++ b/unsloth/save.py
@@ -0,0 +1,207 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from peft import PeftModelForCausalLM
+from collections import OrderedDict
+import bitsandbytes as bnb
+import peft
+import gc
+import os
+from tqdm import tqdm as ProgressBar
+import shutil
+from typing import Optional, Callable, Union
+import torch
+from transformers.models.llama.modeling_llama import logger
+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters
+
+__all__ = [
+    ""unsloth_save_model"",
+    #""colab_quantize_to_gguf"",
+]
+
+LLAMA_WEIGHTS = (
+    ""self_attn.q_proj"", ""self_attn.k_proj"", ""self_attn.v_proj"", ""self_attn.o_proj"",
+    ""mlp.gate_proj"", ""mlp.up_proj"", ""mlp.down_proj"",
+)
+LLAMA_LAYERNORMS = (
+    ""input_layernorm"", ""post_attention_layernorm"",
+)
+
+# From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html
+ALLOWED_QUANTS = \
+{
+    ""q2_k""   : ""Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors."",
+    ""q3_k_l"" : ""Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+    ""q3_k_m"" : ""Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+    ""q3_k_s"" : ""Uses Q3_K for all tensors"",
+    ""q4_0""   : ""Original quant method, 4-bit."",
+    ""q4_1""   : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
+    ""q4_k_m"" : ""Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K"",
+    ""q4_k_s"" : ""Uses Q4_K for all tensors"",
+    ""q5_0""   : ""Higher accuracy, higher resource usage and slower inference."",
+    ""q5_1""   : ""Even higher accuracy, resource usage and slower inference."",
+    ""q5_k_m"" : ""Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K"",
+    ""q5_k_s"" : ""Uses Q5_K for all tensors"",
+    ""q6_k""   : ""Uses Q8_K for all tensors"",
+    ""q8_0""   : ""Almost indistinguishable from float16. High resource use and slow. Not recommended for most users."",
+}
+
+
+def _merge_lora(layer, name):
+    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):
+        # Is LoRA so we need to merge!
+        W, quant_state, A, B, s = get_lora_parameters(layer)
+        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]
+        W = fast_dequantize(W, quant_state).to(torch.float32).t()
+        sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))
+        W += sAB
+        if not torch.isfinite(W).all():
+            raise ValueError(f""Unsloth: Merge failed.\n{name} has some elements = infinity."")
+        W = W.t().to(dtype)
+    else:
+        W = layer.weight
+    return W
+pass
+
+
+@torch.inference_mode
+def unsloth_save_model(
+    model,
+    tokenizer,
+    save_directory: Union[str, os.PathLike],
+    is_main_process: bool = True,
+    state_dict: Optional[dict] = None,
+    save_function: Callable = torch.save,
+    push_to_hub: bool = False,
+    max_shard_size: Union[int, str] = ""7GB"",
+    safe_serialization: bool = True,
+    variant: Optional[str] = None,
+    token: Optional[Union[str, bool]] = None,
+    save_peft_format: bool = True,
+    temporary_location = ""_unsloth_temporary_saved_buffers"",
+    **kwargs,
+):
+    logger.warning_once(
+        ""Unsloth: `unsloth_save_model` is still in development mode.\n""\
+        ""If anything errors or breaks, please file a ticket on Github.\n""\
+        ""Also, if you used this successfully, please tell us on Discord!""
+    )
+
+    if not os.path.exists(temporary_location):
+        os.makedirs(temporary_location)
+    pass
+
+    assert(hasattr(model, ""model""))
+    assert(hasattr(model.model, ""model""))
+    assert(hasattr(model.model.model, ""layers""))
+
+    # HF also uses a OrderedDict
+    state_dict = OrderedDict()
+    state_dict[""model.embed_tokens.weight""] = model.model.model.embed_tokens.weight
+
+    print(""Unsloth: Merging 4bit and LoRA weights to 16bit..."")
+    for j, layer in enumerate(ProgressBar(model.model.model.layers)):
+        for item in LLAMA_WEIGHTS:
+            proj = eval(f""layer.{item}"")
+            name = f""model.layers.{j}.{item}.weight""
+            W = _merge_lora(proj, name)
+            filename = os.path.join(temporary_location, f""{name}.pt"")
+            torch.save(W, filename)
+            state_dict[name] = torch.load(filename, map_location = ""cpu"", mmap = True)
+        pass
+        for item in LLAMA_LAYERNORMS:
+            state_dict[f""model.layers.{j}.{item}.weight""] = eval(f""layer.{item}.weight"")
+        pass
+    pass
+
+    state_dict[""model.norm.weight""] = model.model.model.norm.weight
+    state_dict[""lm_head.weight""]    = model.model.lm_head.weight
+
+    print(""Unsloth: Saving tokenizer..."")
+    tokenizer.save_pretrained(
+        save_directory = save_directory,
+        is_main_process = is_main_process,
+        state_dict = state_dict,
+        save_function = save_function,
+        push_to_hub = push_to_hub,
+        max_shard_size = max_shard_size,
+        safe_serialization = safe_serialization,
+        variant = variant,
+        token = token,
+        save_peft_format = save_peft_format,
+    )
+
+    print(""Unsloth: Saving model. This will take 5 minutes for Llama-7b..."")
+    model.model.save_pretrained(
+        save_directory = save_directory,
+        is_main_process = is_main_process,
+        state_dict = state_dict,
+        save_function = save_function,
+        push_to_hub = push_to_hub,
+        max_shard_size = max_shard_size,
+        safe_serialization = safe_serialization,
+        variant = variant,
+        token = token,
+        save_peft_format = save_peft_format,
+    )
+
+    # Remove temporary location
+    shutil.rmtree(temporary_location)
+pass
+
+
+""""""
+def _colab_quantize_to_gguf(save_directory, quantization_method = ""q4_k_m""):
+
+    logger.warning_once(
+        ""Unsloth: `colab_quantize_to_gguf` is still in development mode.\n""\
+        ""If anything errors or breaks, please file a ticket on Github.\n""\
+        ""Also, if you used this successfully, please tell us on Discord!""
+    )
+
+    if quantization_method not in ALLOWED_QUANTS.keys():
+        error = f""Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\n""
+        for key, value in ALLOWED_QUANTS.items():
+            error += f""[{key}] => {value}\n""
+        raise RuntimeError(error)
+    pass
+
+    print_info = \
+        f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
+        f""   \\\   /|    [0] Installing llama.cpp will take 3 minutes.\n""\
+        f""O^O/ \_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n""\
+        f""\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n""\
+        f' ""-____-""     In total, you will have to wait around 26 minutes.\n'
+    print(print_info)
+
+    if not os.path.exists(""llama.cpp""):
+        print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
+        !git clone https://github.com/ggerganov/llama.cpp
+        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j
+        !pip install gguf protobuf
+    pass
+
+    print(""Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes..."")
+    !python llama.cpp/convert.py {save_directory} \
+        --outfile {save_directory}-unsloth.gguf \
+        --outtype f16
+
+    print(""Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes..."")
+    final_location = f""./{save_directory}-{quantization_method}-unsloth.gguf""
+    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \
+        {final_location} {quantization_method}
+
+    print(f""Unsloth: Output location: {final_location}"")
+pass
+""""""
"
"diff --git a/.github/ISSUE_TEMPLATE/custom.md b/.github/ISSUE_TEMPLATE/custom.md
new file mode 100644
index 0000000..48d5f81
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/custom.md
@@ -0,0 +1,10 @@
+---
+name: Custom issue template
+about: Describe this issue template's purpose here.
+title: ''
+labels: ''
+assignees: ''
+
+---
+
+
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index 471897c..744ec48 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -80,7 +80,8 @@ def _merge_lora(layer, name):
         if quant_state is not None:
             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]
             W = fast_dequantize(W, quant_state)
-        pass
+        else:
+            dtype = W.dtype
         W = W.to(torch.float32).t()
 
         if A is not None:
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 6f0237c..5ad34d6 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -221,6 +221,7 @@ CHAT_TEMPLATES[""alpaca""] = (alpaca_template, alpaca_eos_token,)
 # Notice we must use |trim for lstrip and rstrip. <start_of_turn> maps to 106.
 # <end_of_turn> maps to 107. user and model are normal 1 word tokens.
 gemma_template = \
+    ""{{ bos_token }}""\
     ""{% for message in messages %}""\
         ""{% if message['role'] == 'user' %}""\
             ""{{'<start_of_turn>user\n' + message['content'] | trim + '<end_of_turn>\n'}}""\
@@ -238,7 +239,7 @@ CHAT_TEMPLATES[""gemma""] = (gemma_template, gemma_eos_token,)
 
 
 # Gemma with ChatML instead
-gemma_chatml_template = chatml_template
+gemma_chatml_template = ""{{ bos_token }}"" + chatml_template
 gemma_chatml_eos_token = (
     {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""},
     ""<|im_end|>"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c4de719..e92be5e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -240,61 +240,30 @@ pass
 
 # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??
 # For mixed precision, we need it to be in float32 not float16.
-def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,
-    use_rslora = False):
-    # This code works for linear layers, override for other layer types
-    if r <= 0:
-        raise ValueError(f""`r` should be a positive integer value but the value passed is {r}"")
-
-    self.r[adapter_name] = r
-    self.lora_alpha[adapter_name] = lora_alpha
-    if lora_dropout > 0.0:
-        lora_dropout_layer = torch.nn.Dropout(p=lora_dropout)
-    else:
-        lora_dropout_layer = torch.nn.Identity()
-
-    self.lora_dropout.update(torch.nn.ModuleDict({adapter_name: lora_dropout_layer}))
-    # Actual trainable parameters
-    self.lora_A[adapter_name] = torch.nn.Linear(self.in_features, r, bias=False)
-    self.lora_B[adapter_name] = torch.nn.Linear(r, self.out_features, bias=False)
-    if use_rslora:
-        self.scaling[adapter_name] = lora_alpha / math.sqrt(r)
-    else:
-        self.scaling[adapter_name] = lora_alpha / r
-
-    if init_lora_weights == ""loftq"":
-        # We manually check for PEFT
-        if not hasattr(self, ""loftq_init""):
-            import peft
-            raise RuntimeError(
-                f""Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\n""\
-                ""Please install PEFT 0.7.2 or higher.\n""\
-                ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
-            )
-        pass
-        self.loftq_init(adapter_name)
-
-    elif init_lora_weights:
-        self.reset_lora_parameters(adapter_name, init_lora_weights)
+from peft.tuners.lora.layer import LoraLayer
+import inspect, re
+try:
+    source = inspect.getsource(LoraLayer.update_layer)
+    text = ""if weight is not None:\n""
+    start = source.find(text) + len(text)
+    end = source.find(""self.to(weight.device)"", start)
+    spaces = re.findall(r""^([ ]{1,})break"", source, flags = re.MULTILINE)[0]
+    source = source.replace(source[start : end], spaces)
+    spaces = len(re.match(r""[\s]{1,}"", source).group(0))
+    lines = source.split(""\n"")
+    source = ""\n"".join(x[spaces:] for x in lines)
+    source = re.sub(""([^\.])nn\."", r""\1torch.nn."", source)
+    source = source.replace(""def update_layer"", ""def LoraLayer_update_layer"")
+    exec(source, globals())
 
-    # check weight and qweight (for GPTQ)
-    for weight_name in (""weight"", ""qweight""):
-        weight = getattr(self.get_base_layer(), weight_name, None)
-        if weight is not None:
-            # [INCORRECT code]
-            # 
-            # the layer is already completely initialized, this is an update
-            # if weight.dtype.is_floating_point or weight.dtype.is_complex:
-            #     self.to(weight.device, dtype=weight.dtype)
-            # else:
-            #     self.to(weight.device)
-            self.to(weight.device, non_blocking = True)
-            break
-    self.set_adapter(self.active_adapters)
+    # Fix up incorrect downcasting of LoRA weights
+    from peft.tuners.lora.layer import LoraLayer
+    LoraLayer.update_layer = LoraLayer_update_layer
+    from peft.tuners.lora import LoraLayer
+    LoraLayer.update_layer = LoraLayer_update_layer
+except:
+    logger.warning_once(
+        ""Unsloth unsuccessfully patched LoraLayer.update_layer. Please file a bug report.\n""\
+        ""Luckily, your training run will still work in the meantime!""
+    )
 pass
-
-# Fix up incorrect downcasting of LoRA weights
-from peft.tuners.lora.layer import LoraLayer
-LoraLayer.update_layer = LoraLayer_update_layer
-from peft.tuners.lora import LoraLayer
-LoraLayer.update_layer = LoraLayer_update_layer
"
"diff --git a/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
deleted file mode 100644
index 0bf548b..0000000
--- a/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
+++ /dev/null
@@ -1,255 +0,0 @@
-# -*- coding: utf-8 -*-
-
-from unsloth import FastVisionModel
-
-import torch
-from qwen_vl_utils import process_vision_info
-import os
-from datasets import load_dataset
-from trl import SFTTrainer, SFTConfig
-
-import sys
-from pathlib import Path
-
-
-REPO_ROOT = Path(__file__).parents[3]
-sys.path.insert(0, str(REPO_ROOT))
-
-from tests.utils.cleanup_utils import safe_remove_directory
-from tests.utils.ocr_eval import OCRModelEvaluator
-
-
-## Dataset Preparation
-from datasets import load_dataset
-
-dataset = load_dataset(""lbourdois/OCR-liboaccn-OPUS-MIT-5M-clean"", 'en', split=""train"")
-# To select the first 2000 examples
-train_dataset = dataset.select(range(2000))
-
-# To select the next 200 examples for evaluation
-eval_dataset = dataset.select(range(2000, 2200))
-
-# Convert dataset to OAI messages
-def format_data(sample):
-    return {""messages"": [
-                {
-                    ""role"": ""system"",
-                    ""content"": [{""type"": ""text"", ""text"": system_message}],
-                },
-                {
-                    ""role"": ""user"",
-                    ""content"": [
-                        {
-                            ""type"": ""text"",
-                            ""text"": sample[""question""],
-                        },{
-                            ""type"": ""image"",
-                            ""image"": sample[""image""],
-                        }
-                    ],
-                },
-                {
-                    ""role"": ""assistant"",
-                    ""content"": [{""type"": ""text"", ""text"": sample[""answer""]}],
-                },
-            ],
-        }
-
-system_message = ""You are an expert french ocr system.""
-# Convert dataset to OAI messages
-# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes
-train_dataset = [format_data(sample) for sample in train_dataset]
-eval_dataset = [format_data(sample) for sample in eval_dataset]
-
-## Setup OCR main evaluation function and helpers
-import os
-import torch
-from tqdm import tqdm
-import pandas as pd
-from jiwer import wer, cer
-from qwen_vl_utils import process_vision_info
-
-#
-ocr_evaluator = OCRModelEvaluator()
-model_comparison_results = {}
-
-## Finetuning Setup and Run
-# Load Base Model
-
-model, tokenizer = FastVisionModel.from_pretrained(
-    model_name = ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
-    max_seq_length = 2048, # Choose any for long context!
-    load_in_4bit = True,  # 4 bit quantization to reduce memory
-    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
-    full_finetuning = False, # [NEW!] We have full finetuning now!
-)
-
-# benchmark base model performance
-model_name = ""Unsloth Base model""
-FastVisionModel.for_inference(model)
-avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_base_model_results"")
-ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-
-## Lora Finetuning
-model = FastVisionModel.get_peft_model(
-    model,
-    finetune_vision_layers     = True, # Turn off for just text!
-    finetune_language_layers   = True,  # Should leave on!
-    finetune_attention_modules = True,  # Attention good for GRPO
-    finetune_mlp_modules       = True,  # SHould leave on always!
-
-    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
-    #target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
-                      #""gate_proj"", ""up_proj"", ""down_proj"",],
-    lora_alpha = 32,
-    lora_dropout = 0, # Supports any, but = 0 is optimized
-    bias = ""none"",    # Supports any, but = ""none"" is optimized
-    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
-    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
-    random_state = 3407,
-    use_rslora = False,  # We support rank stabilized LoRA
-    loftq_config = None, # And LoftQ
-)
-
-from unsloth import is_bf16_supported
-from unsloth.trainer import UnslothVisionDataCollator
-FastVisionModel.for_training(model) # Enable for training!
-model.config.use_cache = False
-
-
-trainer = SFTTrainer(
-    model = model,
-    tokenizer = tokenizer,
-    data_collator = UnslothVisionDataCollator(model, tokenizer),
-    train_dataset = train_dataset,
-    args = SFTConfig(
-        #per_device_train_batch_size = 4,
-        #gradient_accumulation_steps = 8,
-        per_device_train_batch_size = 2,
-        gradient_accumulation_steps = 4,
-        gradient_checkpointing=True,
-        gradient_checkpointing_kwargs = {""use_reentrant"": False}, # use reentrant checkpointing
-        max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper
-        warmup_ratio=0.03,
-        #num_train_epochs = 2, # Set this instead of max_steps for full training runs
-        max_steps=60,
-        learning_rate = 2e-4,
-        fp16 = not is_bf16_supported(),
-        bf16 = is_bf16_supported(),
-        logging_steps = 5,
-        save_strategy=""epoch"",
-        optim = ""adamw_torch_fused"",
-        weight_decay = 0.01,
-        lr_scheduler_type = ""linear"",
-        seed = 3407,
-        output_dir = ""unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"",
-        report_to = ""none"",     # For Weights and Biases
-
-        # You MUST put the below items for vision finetuning:
-        remove_unused_columns = False,
-        dataset_text_field = """",
-        dataset_kwargs = {""skip_prepare_dataset"": True},
-        dataset_num_proc = 4,
-        max_seq_length = 2048,
-    ),
-)
-
-# run training
-trainer_stats = trainer.train()
-
-model.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"", tokenizer)
-tokenizer.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
-
-## Measure Adapter Performance
-
-# benchmark lora model performance
-model_name = ""Unsloth lora adapter model""
-FastVisionModel.for_inference(model)
-avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_lora_model_results"")
-ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-
-## Merge Model
-
-def find_lora_base_model(model_to_inspect):
-    current = model_to_inspect
-    if hasattr(current, ""base_model""):
-        current = current.base_model
-    if hasattr(current, ""model""):
-        current = current.model
-    return current
-pass
-
-base = find_lora_base_model(model)
-
-print((base.__class__.__name__))
-
-# merge default 16 bits
-model.save_pretrained_merged(save_directory=""qwen2.5-ocr-merged-finetune-merge-16bit"", tokenizer=tokenizer)
-
-
-## Benchmark merged model performance
-
-### 16 bits merged model
-
-model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=False)
-
-# benchmark 4bit loaded, 16bits merged model performance
-model_name = ""Unsloth 16bits-merged model load-16bits""
-model.config.use_cache = True
-
-avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_16bits_results"")
-ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-
-# load 16bits-merged model in 4 bits
-model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=True, load_in_8bit=False)
-
-# benchmark 4bit loaded, 16bits merged model performance
-model_name = ""Unsloth 16bits-merged model load-4bits""
-model.config.use_cache = True
-
-avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_4bits_results"")
-ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-
-# load model in 8 bits
-model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=True)
-
-# benchmark 4bit loaded, 16bits merged model performance
-model_name = ""Unsloth 16bits-merged model load-8bits""
-avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_8bits_results"")
-ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-
-# """"""### 4 bits merged model""""""
-#
-# # load 4bits-merged model in 4 bits
-# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=True, load_in_8bit=False)
-#
-# # benchmark 4bit loaded, 4bits merged model performance
-# model_name = ""Unsloth 4bits-merged model load-4bits""
-#
-# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_4bits_results"")
-# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-#
-# # load model in 8 bits
-# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=False, load_in_8bit=True)
-#
-# # benchmark 8bit loaded, 4bits merged model performance
-# model_name = ""Unsloth 4bits-merged model load-8bits""
-#
-# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_8bits_results"")
-# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
-
-# Model comparison report
-#print model comparison
-ocr_evaluator.print_model_comparison()
-
-
-
-# Final cleanup
-print(""\n Cleaning up temporary files..."")
-safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
-safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"")
-safe_remove_directory(""./unsloth_compiled_cache"")
-safe_remove_directory(""./qwen2.5-ocr-merged-finetune-merge-16bit"")
-
-print(""\n Pipeline completed successfully!"")
-print(""="" * 80)
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 28fa163..f559c6c 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -618,11 +618,6 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen2.5-VL-7B-Instruct"",
         ""unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit"" : (
-        ""unsloth/Qwen2.5-VL-32B-Instruct"",
-        ""Qwen/Qwen2.5-VL-32B-Instruct"",
-        ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
-    ),
     ""unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-VL-72B-Instruct"",
         ""Qwen/Qwen2.5-VL-72B-Instruct"",
"
"diff --git a/README.md b/README.md
index 5c4de1f..6cd1be1 100644
--- a/README.md
+++ b/README.md
@@ -32,12 +32,13 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 | **DPO Zephyr**     | [ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
 
 - **Kaggle Notebooks** for [Llama 3.1 (8B)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook), [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
-- Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral v0.3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
+- Run [Llama 3.1 conversational notebook](https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_?usp=sharing) and [Mistral v0.3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
 - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text
 - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language
 - Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.
 
 ##  Unsloth.ai News
+-  NEW! [Llama 3.1 Conversational notebook](https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_?usp=sharing) includes training only on completions / outputs (increase accuracy), ShareGPT standardization and more!
 -  NEW! [Phi-3.5 (mini)](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing) now supported
 -  NEW! `pip install unsloth` now works! Head over to [pypi](https://pypi.org/project/unsloth/) to check it out! This allows non git pull installs. Use `pip install unsloth[colab-new]` for non dependency installs.
 -  NEW! [Gemma-2-2b](https://colab.research.google.com/drive/1weTpKOjBZxZJ5PQ-Ql8i6ptAY2x-FWVA?usp=sharing) now supported! Try out [Chat interface](https://colab.research.google.com/drive/1i-8ESvtLRGNkkUQQr_-z_rcSAIo9c3lM?usp=sharing)!
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 376b4b4..f62f0f1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -61,6 +61,7 @@ from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
 from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 from ..save import patch_saving_functions
 import re, os, inspect, math, sys
+from huggingface_hub.utils._token import get_token
 
 
 def original_apply_qkv(self, X):
@@ -1263,7 +1264,7 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
         # in FP32. They are applied (multiplied) in FP32 as well.
         self.current_rope_size = seq_len
         
-        t = torch.arange(self.current_rope_size, device=self.inv_freq.device, dtype=torch.int64).float()
+        t = torch.arange(self.current_rope_size, device=self.long_inv_freq.device, dtype=torch.int64).float()
         # Long sequences
         freqs = torch.outer(t, self.long_inv_freq)
         emb = torch.cat((freqs, freqs), dim=-1)
@@ -1417,13 +1418,7 @@ class FastLlamaModel:
                 ""Are you certain you want to do remote code execution?""
             )
         pass
-
-        if token is None and ""HF_TOKEN"" in os.environ:
-            token = os.environ[""HF_TOKEN""]
-
-        if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
-            token = os.environ[""HUGGINGFACE_TOKEN""]
-
+        if token is None: token = get_token()
         if model_patcher is None: model_patcher = FastLlamaModel
         SUPPORTS_BFLOAT16 = is_bfloat16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 02ed00f..e1f17ac 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -21,6 +21,7 @@ from transformers import __version__ as transformers_version
 from peft import PeftConfig, PeftModel
 from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER, MAP_TO_UNSLOTH_16bit
 import os
+from huggingface_hub.utils._token import get_token
 
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
 from packaging.version import Version
@@ -152,12 +153,8 @@ class FastLanguageModel(FastLlamaModel):
         revision                   = None,
         *args, **kwargs,
     ):
-        if token is None and ""HF_TOKEN"" in os.environ:
-            token = os.environ[""HF_TOKEN""]
-
-        if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
-            token = os.environ[""HUGGINGFACE_TOKEN""]
-
+        if token is None: token = get_token()
+        
         old_model_name = model_name
         model_name = get_model_name(model_name, load_in_4bit)
 
diff --git a/unsloth/save.py b/unsloth/save.py
index f45d806..66e2ec6 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -29,6 +29,7 @@ import re
 from transformers.models.llama.modeling_llama import logger
 from .tokenizer_utils import fix_sentencepiece_gguf
 from huggingface_hub import HfApi
+from huggingface_hub.utils._token import get_token
 
 __all__ = [
     ""print_quantization_methods"",
@@ -207,12 +208,7 @@ def unsloth_save_model(
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.9,
 ):
-    if token is None and ""HF_TOKEN"" in os.environ:
-        token = os.environ[""HF_TOKEN""]
-    elif token is None and ""hf_token"" in os.environ:
-        token = os.environ[""hf_token""]
-    elif token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
-        token = os.environ[""HUGGINGFACE_TOKEN""]
+    if token is None: token = get_token()
 
     if commit_message is None: commit_message = """"
     if ""Unsloth"" not in commit_message:
@@ -1321,12 +1317,8 @@ def create_huggingface_repo(
     token = None,
     private = False,
 ):
-    if token is None and ""HF_TOKEN"" in os.environ:
-        token = os.environ[""HF_TOKEN""]
-    elif token is None and ""hf_token"" in os.environ:
-        token = os.environ[""hf_token""]
-    elif token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
-        token = os.environ[""HUGGINGFACE_TOKEN""]
+    if token is None :
+        token = get_token()
     pass
     save_directory, username = _determine_username(save_directory, """", token)
 
"
"diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 2941fb3..91a2dd7 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -375,6 +375,9 @@ class FastMistralModel(FastLlamaModel):
         
         # Log Unsloth version for future fastpaths for inference
         model.config.update({""unsloth_version"" : __version__})
+
+        # Add save modules
+        patch_saving_functions(model)
         
         return model, tokenizer
     pass
diff --git a/unsloth/save.py b/unsloth/save.py
index baa8f3f..4cda8b9 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -278,7 +278,6 @@ def unsloth_save_model(
         not hasattr(internal_model.model, ""layers"")
     ):
         # Do general saving
-        print(type(model))
         # Edit save_pretrained_settings
         # [TODO] _create_repo has errors due to **kwargs getting accepted
         for deletion in \
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index 1d9e2f9..4c6ab77 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -1,5 +1,5 @@
 ---
-name: Bug report
+name:  Bug report
 about: Create a report to help us improve
 title: ""[Bug]""
 labels: bug
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index d3ba192..0f75ecf 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -254,7 +254,7 @@ def unsloth_save_model(
     # First check for a token!
     if push_to_hub:
         from huggingface_hub import whoami
-        try: 
+        try:
             username = whoami(token = token)[""name""]
         except:
             raise RuntimeError(
@@ -385,7 +385,7 @@ def unsloth_save_model(
     else:
         internal_model = model
     pass
-        
+
     # Cannot be converted properly!
     if (save_method == ""merged_4bit"") or (save_method == ""lora"") or (
         not hasattr(model, ""model"") or \
@@ -481,7 +481,7 @@ def unsloth_save_model(
         gb_found = re.match(""([0-9]{1,})[\s]{0,}GB"", max_shard_size, flags = re.IGNORECASE)
         mb_found = re.match(""([0-9]{1,})[\s]{0,}MB"", max_shard_size, flags = re.IGNORECASE)
         if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024
-        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 
+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024
     elif type(max_shard_size) is int:
         sharded_ram_usage = sharded_ram_usage
     pass
@@ -612,7 +612,7 @@ def unsloth_save_model(
     # Edit save_pretrained_settings
     # [TODO] _create_repo has errors due to **kwargs getting accepted
     save_pretrained_settings[""state_dict""] = state_dict
-    
+
     # commit_description does not seem to work?
     what_to_delete = (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",) \
         if not push_to_hub else \
@@ -665,7 +665,7 @@ def unsloth_save_model(
 
         # Revert back padding side
         tokenizer.padding_side = old_padding_side
-            
+
         print("" Done."")
     else:
         print()
@@ -877,10 +877,15 @@ def install_llama_cpp_old(version = -10):
     pass
 
     # Check if successful
-    if not os.path.exists(""llama.cpp/quantize"") and not os.path.exists(""llama.cpp/llama-quantize""):
+    if not (
+        os.path.exists(""llama.cpp/llama-quantize.exe"") or
+        os.path.exists(""llama.cpp/llama-quantize"") or
+        os.path.exists(""llama.cpp/quantize.exe"") or
+        os.path.exists(""llama.cpp/quantize"")
+    ):
         raise RuntimeError(
             ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
-            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
         )
     pass
 pass
@@ -957,7 +962,7 @@ def save_to_gguf(
     else:
         raise TypeError(""Unsloth: quantization_method can only be a string or a list of strings"")
     pass
-    
+
     # Check if bfloat16 is supported
     if model_dtype == ""bf16"" and not torch.cuda.is_bf16_supported():
         logger.warning(
@@ -973,7 +978,7 @@ def save_to_gguf(
     pass
 
     # Check I quants
-    for quant_method in quantization_method: 
+    for quant_method in quantization_method:
         if quant_method.startswith(""iq2""):
             raise RuntimeError(""Unsloth: Currently iq2 type quantizations aren't supported yet - sorry!"")
     pass
@@ -1026,9 +1031,9 @@ def save_to_gguf(
     pass
 
     # Determine whether the system already has llama.cpp installed and the scripts are executable
-    quantize_location = get_executable([""llama-quantize"", ""quantize""])
+    quantize_location = get_executable([""llama-quantize"", ""quantize"", ""llama-quantize.exe"", ""quantize.exe""])
     convert_location  = get_executable([""convert-hf-to-gguf.py"", ""convert_hf_to_gguf.py""])
-    
+
     error = 0
     if quantize_location is not None and convert_location is not None:
         print(""Unsloth: llama.cpp found in the system. We shall skip installation."")
@@ -1062,14 +1067,18 @@ def save_to_gguf(
         # and llama.cpp/main changed to llama.cpp/llama-cli
         # See https://github.com/ggerganov/llama.cpp/pull/7809
         quantize_location = None
-        if os.path.exists(""llama.cpp/quantize""):
+        if os.path.exists(""llama.cpp/quantize.exe""):
+            quantize_location = ""llama.cpp/quantize.exe""
+        elif os.path.exists(""llama.cpp/quantize""):
             quantize_location = ""llama.cpp/quantize""
+        elif os.path.exists(""llama.cpp/llama-quantize.exe""):
+            quantize_location = ""llama.cpp/llama-quantize.exe""
         elif os.path.exists(""llama.cpp/llama-quantize""):
             quantize_location = ""llama.cpp/llama-quantize""
         else:
             raise RuntimeError(
-                ""Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\n""\
-                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+                ""Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\n""\
+                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
             )
         pass
 
@@ -1150,7 +1159,7 @@ def save_to_gguf(
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
 
     final_location = str((Path(model_directory) / f""unsloth.{first_conversion.upper()}.gguf"").absolute())
-    
+
     print(f""Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\n""\
           f""The output location will be {final_location}\n""\
           ""This might take 3 minutes..."")
@@ -1217,7 +1226,7 @@ def save_to_gguf(
 
             command = f""./{quantize_location} {full_precision_location} ""\
                 f""{final_location} {quant_method} {n_cpus}""
-            
+
             try_execute([command,], force_complete = True)
 
             # Check if quantization succeeded!
@@ -1378,7 +1387,7 @@ def _determine_username(save_directory, old_username, token):
     save_directory = save_directory.lstrip(""./"")
     if ""/"" not in save_directory:
         from huggingface_hub import whoami
-        try: 
+        try:
             username = whoami(token = token)[""name""]
             if type(old_username) is str and username != old_username:
                 username = old_username
@@ -1412,7 +1421,7 @@ def create_huggingface_repo(
             repo_type = ""model"",
             exist_ok  = False,
             private   = private,
-        ) 
+        )
 
         # Create model card
         from huggingface_hub import ModelCard
@@ -1453,7 +1462,7 @@ def upload_to_huggingface(
             repo_type = ""model"",
             exist_ok  = False,
             private   = private,
-        ) 
+        )
 
         # Create model card
         from huggingface_hub import ModelCard
@@ -1527,7 +1536,7 @@ def fix_tokenizer_bos_token(tokenizer):
     # Check if BOS added already, then warn
     fix_bos_token = False
     chat_template = getattr(tokenizer, ""chat_template"", None)
-    
+
     if (tokenizer(""A"").input_ids[0] == getattr(tokenizer, ""bos_token_id"", None)):
         if chat_template is not None and \
             (
@@ -1546,7 +1555,7 @@ def fix_tokenizer_bos_token(tokenizer):
             new_chat_template = re.sub(r""\{[\s]{0,}\{[\s]{0,}bos\_token[\s]{0,}\}[\s]{0,}\}"", """", chat_template)
             # Remove {{bos_token +
             new_chat_template = re.sub(r""\{[\s]{0,}\{[\s]{0,}bos\_token[\s]{0,}\+[\s]{0,}"", """", new_chat_template)
-            
+
             tokenizer.chat_template = new_chat_template
 
         pass
@@ -1580,7 +1589,7 @@ def create_ollama_modelfile(tokenizer, gguf_location):
     modelfile = modelfile\
         .replace(FILE_LOCATION_REPLACER, ""{__FILE_LOCATION__}"")\
         .replace(EOS_TOKEN_REPLACER,     ""{__EOS_TOKEN__}"")
-    
+
     if ""__EOS_TOKEN__"" in modelfile:
         modelfile = modelfile.format(
             __FILE_LOCATION__  = gguf_location,
@@ -1591,7 +1600,7 @@ def create_ollama_modelfile(tokenizer, gguf_location):
             __FILE_LOCATION__  = gguf_location,
         )
     pass
-    
+
     modelfile = modelfile\
         .replace(""@#"", ""{"")\
         .replace(""@#"", ""}"")\
@@ -1733,7 +1742,7 @@ def unsloth_save_pretrained_gguf(
 
     # Save to GGUF
     all_file_locations, want_full_precision = save_to_gguf(
-        model_type, model_dtype, is_sentencepiece_model, 
+        model_type, model_dtype, is_sentencepiece_model,
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
@@ -1911,7 +1920,7 @@ def unsloth_push_to_hub_gguf(
 
     # Save to GGUF
     all_file_locations, want_full_precision = save_to_gguf(
-        model_type, model_dtype, is_sentencepiece_model, 
+        model_type, model_dtype, is_sentencepiece_model,
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
@@ -1928,7 +1937,7 @@ def unsloth_push_to_hub_gguf(
 
     # If not needing full precision, skip the first
     if not want_full_precision: all_file_locations = all_file_locations[1:]
-    
+
     for file_location in all_file_locations:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
         username = upload_to_huggingface(
@@ -2044,8 +2053,8 @@ def unsloth_convert_lora_to_ggml_and_push_to_hub(
 
 def unsloth_convert_lora_to_ggml_and_save_locally(
     self,
-    save_directory: str, # Added parameter for the folder name 
-    tokenizer, 
+    save_directory: str, # Added parameter for the folder name
+    tokenizer,
     temporary_location: str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage: float = 0.85,
 ):
@@ -2162,7 +2171,7 @@ def unsloth_generic_save_pretrained_merged(
     tags                 : List[str] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.75,
-):   
+):
     """"""
         Same as .push_to_hub(...) except 4bit weights are auto
         converted to float16 with as few overhead as possible.
"
"diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index dcc8877..adec68d 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -17,10 +17,16 @@ from .utils import fast_dequantize, QUANT_STATE
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
 
 def get_lora_parameters(proj):
-    active_adapter = proj.active_adapters[0] if \
-        hasattr(proj, ""active_adapters"") else proj.active_adapter
+    # For DPO or disabled adapters
     base_layer = (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
     W = base_layer.weight
+
+    if proj.disable_adapters or proj.merged:
+        return W, QUANT_STATE(W), None, None, None
+    pass
+
+    active_adapter = proj.active_adapters[0] if \
+        hasattr(proj, ""active_adapters"") else proj.active_adapter
     A = proj.lora_A [active_adapter].weight
     B = proj.lora_B [active_adapter].weight
     s = proj.scaling[active_adapter]
@@ -31,7 +37,6 @@ pass
 def matmul_lora(X, W, W_quant, A, B, s, out = None):
     dtype = X.dtype
     W = fast_dequantize(W.t(), W_quant)
-    A, B = A.t(), B.t()
 
     if X.dim() == 3:
         batch, seq_len, d = X.shape
@@ -43,7 +48,13 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):
 
     out = torch.matmul(X, W, out = out)
     if W_quant is not None: del W
-    out += (X @ A.to(dtype)) @ (s * B.to(dtype))
+
+    if A is not None:
+        # LoRA is enabled
+        A, B = A.t(), B.t()
+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))
+    pass
+    
     return out.view(batch, seq_len, -1) if reshape else out
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f6cc078..22179d4 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -369,8 +369,21 @@ def LlamaModel_fast_forward(
         raise ValueError(""Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds"")
 
     seq_length_with_past = seq_length
+
+    # Fix out of bounds tokenization
     if hasattr(self, ""max_seq_length""):
-        assert(seq_length <= self.max_seq_length)
+        if seq_length > self.max_seq_length:
+            logger.warning_once(
+                f""Unsloth: Input IDs of length {seq_length} > the model's max sequence length of {self.max_seq_length}.\n""\
+                ""We shall truncate it ourselves. It's imperative if you correct this issue first.""
+            )
+        if input_ids is not None:
+            input_ids = input_ids[:,:self.max_seq_length]
+        elif inputs_embeds is not None:
+            inputs_embeds = inputs_embeds[:,:self.max_seq_length,:]
+        pass
+    pass
+    
     past_key_values_length = 0
 
     if past_key_values is not None:
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index e4bac44..bb6a68f 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -97,21 +97,36 @@ def MistralAttention_fast_forward(
         Q = Q.transpose(1, 2)
         K = K.transpose(1, 2)
         V = V.transpose(1, 2)
+        M = bsz * q_len
+
+        has_sliding_window = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)
 
         # Group query attention
-        if n_groups != 1:
-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)
-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)
-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
-            if hidden_states.requires_grad:
-                # Xformers does not support backward, so we have to convert
-                # GQA to MQA by cloning K and V
-                K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made
-                V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made
-            else:
-                # Xformers does support the forward pass though
-                Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        # if n_groups != 1:
+        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)
+        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)
+        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        if hidden_states.requires_grad:
+            # Xformers does not support backward, so we have to convert
+            # GQA to MQA by cloning K and V
+            K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made
+            V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made
+
+            if has_sliding_window:
+                Q = Q.view(1, M, n_heads, head_dim)
+                K = K.view(1, M, n_heads, head_dim)
+                V = V.view(1, M, n_heads, head_dim)
+            pass
+        else:
+            # Xformers does support the forward pass though
+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
+
+            if has_sliding_window:
+                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)
+                K = K.view(1, M, n_kv_heads, n_groups, head_dim)
+                V = V.view(1, M, n_kv_heads, n_groups, head_dim)
+            pass
         pass
 
         A = xformers_attention(Q, K, V, attn_bias = causal_mask)
@@ -131,12 +146,12 @@ def MistralAttention_fast_forward(
         A = flash_attn_func(Q, K, V, causal = True, window_size = window)
     else:
         # Grouped query attention
-        if n_groups != 1:
-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-            K = K.reshape(bsz, n_heads, q_len, head_dim)
-            V = V.reshape(bsz, n_heads, q_len, head_dim)
-        pass
+        # if n_groups != 1:
+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+        K = K.reshape(bsz, n_heads, q_len, head_dim)
+        V = V.reshape(bsz, n_heads, q_len, head_dim)
+        # pass
         # Needs (batch_size, n_heads, seq_len, head_dim)
         # is_casual and attention_mask must not be both set!
         A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 09e035e..a53de42 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -15,11 +15,12 @@
 import torch
 from typing import Union, Optional, List, Any, Callable
 import warnings
-warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
-warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""torch"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""huggingface_hub"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
-warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""transformers"")
-warnings.filterwarnings(action = ""ignore"", category = FutureWarning, module = ""accelerate"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""transformers"")
+warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""accelerate"")
+warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""huggingface_hub"")
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
@@ -388,3 +389,35 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     pass
 pass
 
+
+""""""
+    Remove warnings about missing kwargs
+""""""
+try:
+    from transformers.utils.quantization_config import BitsAndBytesConfig, QuantizationMethod
+    from inspect import getsource
+    import re
+    BitsAndBytesConfig__init__ = getsource(BitsAndBytesConfig.__init__)
+    BitsAndBytesConfig__init__ = re.sub(
+        r""if[\s]{1,}kwargs\:[\s]{1,}.+?\n"",
+        """",
+        BitsAndBytesConfig__init__,
+        flags = re.MULTILINE,
+    )
+    BitsAndBytesConfig__init__ = BitsAndBytesConfig__init__.split(""\n"")
+    length_spaces = len(re.match(r""[\s]{1,}"", BitsAndBytesConfig__init__[0]).group(0))
+    BitsAndBytesConfig__init__ = ""\n"".join(x[length_spaces:] for x in BitsAndBytesConfig__init__)
+    BitsAndBytesConfig__init__ = BitsAndBytesConfig__init__.replace(
+        ""__init__"",
+        ""_BitsAndBytesConfig__init__"",
+    )
+    exec(BitsAndBytesConfig__init__, globals())
+    
+    import transformers.utils.quantization_config
+    transformers.utils.quantization_config.BitsAndBytesConfig.__init__ = _BitsAndBytesConfig__init__
+except:
+    logger.warning_once(
+        ""Unsloth unsuccessfully patched bitsandbytes. Please file a bug report.\n""\
+        ""Luckily, your training run will still work in the meantime!""
+    )
+pass
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index c0cce75..5dd2a5a 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -71,7 +71,7 @@ def GemmaDecoderLayer_fast_forward(
     padding_mask:         Optional[torch.LongTensor] = None,
     *args, **kwargs,
 ):
-    if use_cache: #past_key_value is not None:
+    if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda"")
 
         # Self Attention
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f08c403..f7fd5f1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -407,7 +407,7 @@ def LlamaDecoderLayer_fast_forward(
             (see `past_key_values`).
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
-    if use_cache:
+    if use_cache and hasattr(self, ""_flag_for_generation""):
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
@@ -789,7 +789,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         return_dict: Optional[bool] = None,
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
-
+        
         if past_key_values is not None:
             outputs = fast_forward_inference(
                 self,
@@ -968,12 +968,34 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
 pass
 
 
-def _wrap_fast_inference(generate, device_type, dtype):
+def _wrap_fast_inference(generate, device_type, dtype, model):
     # Wraps inference with bfloat16 / float16
     @torch.inference_mode
     def _fast_generate(*args, **kwargs):
+
+        # Set a flag for generation!
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            internal_model._flag_for_generation = True
+            internal_model = internal_model.model
+        pass
+        internal_model._flag_for_generation = True
+
+        # Autocasted
         with torch.autocast(device_type = device_type, dtype = dtype):
-            return generate(*args, **kwargs)
+            output = generate(*args, **kwargs)
+        pass
+
+        # Unset a flag for generation!
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_flag_for_generation""): del internal_model._flag_for_generation
+            internal_model = internal_model.model
+        pass
+        if hasattr(internal_model, ""_flag_for_generation""): del internal_model._flag_for_generation
+
+        return output
+    pass
     return _fast_generate
 pass
 
@@ -1787,7 +1809,7 @@ class FastLlamaModel:
 
         # Wrap model.generate
         model._unwrapped_old_generate = model.generate
-        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)
+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)
 
         # Patch tokenizer to pad to the left
         internal_model = model
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 7a1d2dd..1cbe49b 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -369,10 +369,11 @@ def load_correct_tokenizer(
             cache_dir         = cache_dir,
         )
     except:
-        print(
-            f""Unsloth: {tokenizer_name} has no tokenizer.model file.\n""\
-            ""Just informing you about this - this is not a critical error.""
-        )
+        pass
+        # print(
+        #     f""Unsloth: {tokenizer_name} has no tokenizer.model file.\n""\
+        #     ""Just informing you about this - this is not a critical error.""
+        # )
     pass
 
     fast_tokenizer = AutoTokenizer.from_pretrained(
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index d83d9b7..8b86feb 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -505,11 +505,10 @@ def LlamaModel_fast_forward(
             position_ids = position_ids.repeat((batch_size, 1))
     pass
 
-    # embed positions
+    # Embed positions
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
-    # Downcast to the correct dtype ie float32 to float16
     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
 
     # Normalized from Gemma
@@ -759,6 +758,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         else:
             logits = self.lm_head(hidden_states)
         pass
+        logits = logits.to(self.config.torch_dtype)
 
         loss = None
         if labels is not None:
@@ -929,6 +929,7 @@ class FastLlamaModel:
         fix_tokenizer  = True,
         model_patcher  = None,
         tokenizer_name = None,
+        trust_remote_code = False,
         **kwargs,
     ):
         if model_patcher is None: model_patcher = FastLlamaModel
@@ -989,6 +990,7 @@ class FastLlamaModel:
             token                   = token,
             rope_scaling            = rope_scaling,
             max_position_embeddings = max_position_embeddings,
+            trust_remote_code       = trust_remote_code,
             **kwargs,
         )
 
@@ -996,9 +998,10 @@ class FastLlamaModel:
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         tokenizer = AutoTokenizer.from_pretrained(
             tokenizer_name,
-            model_max_length = max_position_embeddings,
-            padding_side     = ""right"",
-            token            = token,
+            model_max_length  = max_position_embeddings,
+            padding_side      = ""right"",
+            token             = token,
+            trust_remote_code = trust_remote_code,
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
@@ -1338,7 +1341,6 @@ class FastLlamaModel:
                     ""We shall do it for you!""
                 )
                 train_lm_head = True
-                model.model.embed_tokens.to(torch.float32, non_blocking = True)
 
             elif module == ""embed_tokens"":
                 logger.warning_once(
@@ -1346,7 +1348,6 @@ class FastLlamaModel:
                     ""We shall do it for you!""
                 )
                 train_embed_tokens = True
-                model.lm_head.to(torch.float32, non_blocking = True)
 
             else:
                 assert(module in accepted_modules)
@@ -1388,9 +1389,17 @@ class FastLlamaModel:
 
         # Now patch lm_head and embed_tokens
         if train_embed_tokens:
-            model.model.model.embed_tokens.requires_grad_(True)
+            print(""Unsloth: Casting embed_tokens to float32"")
+            assert(hasattr(model.model.model.embed_tokens, ""modules_to_save""))
+            model.model.model.embed_tokens.modules_to_save.default.to(torch.float32)
+            model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
+        pass
+
         if train_lm_head:
-            model.model.lm_head.requires_grad_(True)
+            print(""Unsloth: Casting lm_head to float32"")
+            assert(hasattr(model.model.lm_head, ""modules_to_save""))
+            model.model.lm_head.modules_to_save.default.to(torch.float32)
+            model.model.lm_head.modules_to_save.default.requires_grad_(True)
         pass
 
         return model
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 47b568a..29d25f3 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -74,6 +74,7 @@ class FastLanguageModel(FastLlamaModel):
         device_map     = ""sequential"",
         rope_scaling   = None,
         fix_tokenizer  = True,
+        trust_remote_code = False,
         use_gradient_checkpointing = True,
         *args, **kwargs,
     ):
@@ -139,6 +140,7 @@ class FastLanguageModel(FastLlamaModel):
             fix_tokenizer  = fix_tokenizer,
             model_patcher  = dispatch_model,
             tokenizer_name = tokenizer_name,
+            trust_remote_code = trust_remote_code,
             *args, **kwargs,
         )
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index c1e39e4..2db71f0 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -230,6 +230,7 @@ def MistralForCausalLM_fast_forward(
     else:
         logits = self.lm_head(hidden_states)
     pass
+    logits = logits.to(self.config.torch_dtype)
 
     loss = None
     if labels is not None:
@@ -295,6 +296,7 @@ class FastMistralModel(FastLlamaModel):
         fix_tokenizer  = True,
         model_patcher  = None,
         tokenizer_name = None,
+        trust_remote_code = False,
         **kwargs,
     ):
         if model_patcher is None: model_patcher = FastMistralModel
@@ -353,6 +355,7 @@ class FastMistralModel(FastLlamaModel):
             quantization_config = bnb_config,
             token               = token,
             # rope_scaling      = rope_scaling,
+            trust_remote_code   = trust_remote_code,
             **kwargs,
         )
 
@@ -360,9 +363,10 @@ class FastMistralModel(FastLlamaModel):
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         tokenizer = AutoTokenizer.from_pretrained(
             tokenizer_name,
-            model_max_length = max_position_embeddings,
-            padding_side     = ""right"",
-            token            = token,
+            model_max_length  = max_position_embeddings,
+            padding_side      = ""right"",
+            token             = token,
+            trust_remote_code = trust_remote_code,
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
"
"diff --git a/pyproject.toml b/pyproject.toml
index c4c3ebe..83b75b0 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.8.7"",
+    ""unsloth_zoo>=2025.8.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1"",
@@ -453,7 +453,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.8.7"",
+    ""unsloth_zoo>=2025.8.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index a43dc4f..a6ea8f4 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -17,6 +17,10 @@ from packaging.version import Version
 import os, re, subprocess, inspect
 import numpy as np
 
+# Fix some issues before importing other packages
+from .import_fixes import fix_message_factory_issue
+fix_message_factory_issue(); del fix_message_factory_issue;
+
 # Check if modules that need patching are already imported
 critical_modules = ['trl', 'transformers', 'peft']
 already_imported = [mod for mod in critical_modules if mod in sys.modules]
@@ -53,6 +57,7 @@ os.environ[""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""] = ""python""
 # Log Unsloth is being used
 os.environ[""UNSLOTH_IS_PRESENT""] = ""1""
 
+# Try importing PyTorch and check version
 try:
     import torch
 except ModuleNotFoundError:
@@ -93,7 +98,7 @@ if DEVICE_TYPE == ""cuda"" and os.environ.get(""UNSLOTH_VLLM_STANDBY"", ""0"")==""0"":
 
 # We support Pytorch 2
 # Fixes https://github.com/unslothai/unsloth/issues/38
-torch_version = str(torch.__version__).split(""."")
+torch_version = str(re.match(r""[0-9\.]{3,}"", str(torch.__version__)).group(0)).split(""."")
 major_torch, minor_torch = torch_version[0], torch_version[1]
 major_torch, minor_torch = int(major_torch), int(minor_torch)
 if (major_torch < 2):
@@ -104,35 +109,21 @@ elif (major_torch == 2) and (minor_torch < 2):
     del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
 pass
 
-# Fix Xformers performance issues since 0.0.25
+# CCE fails on Torch 2.8 and above
+# OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages`
+if (major_torch >= 2 and minor_torch >= 8) or (major_torch > 2):
+    os.environ[""UNSLOTH_ENABLE_CCE""] = ""0""
+pass
+
+# Fix other issues
 import importlib.util
 from pathlib import Path
 from importlib.metadata import version as importlib_version
 from packaging.version import Version
-try:
-    xformers_version = importlib_version(""xformers"")
-    if Version(xformers_version) < Version(""0.0.29""):
-        xformers_location = importlib.util.find_spec(""xformers"").origin
-        xformers_location = os.path.split(xformers_location)[0]
-        cutlass = Path(xformers_location) / ""ops"" / ""fmha"" / ""cutlass.py""
-
-        if cutlass.exists():
-            with open(cutlass, ""r+"", encoding = ""utf-8"") as f:
-                text = f.read()
-                # See https://github.com/facebookresearch/xformers/issues/1176#issuecomment-2545829591
-                if ""num_splits_key=-1,"" in text:
-                    text = text.replace(""num_splits_key=-1,"", ""num_splits_key=None,"")
-                    f.seek(0)
-                    f.write(text)
-                    f.truncate()
-                    print(""Unsloth: Patching Xformers to fix some performance issues."")
-                pass
-            pass
-        pass
-    pass
-except:
-    pass
-pass
+from .import_fixes import fix_xformers_performance_issue
+fix_xformers_performance_issue(); del fix_xformers_performance_issue;
+from .import_fixes import fix_vllm_aimv2_issue
+fix_vllm_aimv2_issue(); del fix_vllm_aimv2_issue;
 
 # Torch 2.4 has including_emulation
 if DEVICE_TYPE == ""cuda"":
@@ -154,7 +145,6 @@ elif DEVICE_TYPE == ""xpu"":
     SUPPORTS_BFLOAT16 = torch.xpu.is_bf16_supported()
 pass
 
-
 # For Gradio HF Spaces?
 # if ""SPACE_AUTHOR_NAME"" not in os.environ and ""SPACE_REPO_NAME"" not in os.environ:
 import triton
@@ -222,7 +212,7 @@ elif DEVICE_TYPE == ""xpu"":
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.8.1""):
+    if Version(unsloth_zoo_version) < Version(""2025.8.8""):
         print(
             ""Unsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\n""\
             ""Do this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`""
@@ -240,31 +230,6 @@ except:
     raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`"")
 pass
 
-try:
-    # Fix up AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
-    # MUST do this at the start primarily due to tensorflow causing issues
-    import google.protobuf.message_factory
-    class MessageFactory:
-        def CreatePrototype(self, *args, **kwargs): return
-        def GetMessages(self, *args, **kwargs): return
-        def GetPrototype(self, *args, **kwargs): return
-    if not hasattr(google.protobuf.message_factory, ""MessageFactory""):
-        google.protobuf.message_factory.MessageFactory = MessageFactory
-    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
-        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
-        not hasattr(google.protobuf.message_factory, ""GetMessageClass""):
-        google.protobuf.message_factory.MessageFactory = MessageFactory
-    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
-        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
-        hasattr(google.protobuf.message_factory, ""GetMessageClass""):
-        GetMessageClass = google.protobuf.message_factory.GetMessageClass
-        def GetPrototype(self, descriptor):
-            return GetMessageClass(descriptor)
-        google.protobuf.message_factory.MessageFactory.GetPrototype = GetPrototype
-    pass
-except:
-    pass
-
 from .models import *
 from .models import __version__
 from .save import *
diff --git a/unsloth/import_fixes.py b/unsloth/import_fixes.py
new file mode 100644
index 0000000..a07f997
--- /dev/null
+++ b/unsloth/import_fixes.py
@@ -0,0 +1,119 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import importlib.util
+from pathlib import Path
+from importlib.metadata import version as importlib_version
+from packaging.version import Version
+UNSLOTH_ENABLE_LOGGING = os.environ.get(""UNSLOTH_ENABLE_LOGGING"", ""0"") == ""1""
+
+# Fix up AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
+# MUST do this at the start primarily due to tensorflow causing issues
+def fix_message_factory_issue():
+    try:
+        import google.protobuf.message_factory
+        class MessageFactory:
+            def CreatePrototype(self, *args, **kwargs): return
+            def GetMessages(self, *args, **kwargs): return
+            def GetPrototype(self, *args, **kwargs): return
+        if not hasattr(google.protobuf.message_factory, ""MessageFactory""):
+            if UNSLOTH_ENABLE_LOGGING:
+                print(""Unsloth: Patching protobuf.MessageFactory as it doesn't exist"")
+            google.protobuf.message_factory.MessageFactory = MessageFactory
+        elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
+            not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
+            not hasattr(google.protobuf.message_factory, ""GetMessageClass""):
+            google.protobuf.message_factory.MessageFactory = MessageFactory
+            if UNSLOTH_ENABLE_LOGGING:
+                print(""Unsloth: Patching protobuf.MessageFactory as it doesn't exist"")
+        elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
+            not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
+            hasattr(google.protobuf.message_factory, ""GetMessageClass""):
+            GetMessageClass = google.protobuf.message_factory.GetMessageClass
+            def GetPrototype(self, descriptor):
+                return GetMessageClass(descriptor)
+            google.protobuf.message_factory.MessageFactory.GetPrototype = GetPrototype
+            if UNSLOTH_ENABLE_LOGGING:
+                print(""Unsloth: Patching protobuf.MessageFactory.GetPrototype"")
+        pass
+    except:
+        pass
+pass
+
+# Fix Xformers performance issues since 0.0.25
+def fix_xformers_performance_issue():
+    if importlib.util.find_spec(""xformers"") is None: return
+    xformers_version = importlib_version(""xformers"")
+    if Version(xformers_version) < Version(""0.0.29""):
+        xformers_location = importlib.util.find_spec(""xformers"").origin
+        xformers_location = os.path.split(xformers_location)[0]
+        cutlass = Path(xformers_location) / ""ops"" / ""fmha"" / ""cutlass.py""
+        try:
+            if cutlass.exists():
+                with open(cutlass, ""r+"", encoding = ""utf-8"") as f:
+                    text = f.read()
+                    # See https://github.com/facebookresearch/xformers/issues/1176#issuecomment-2545829591
+                    if ""num_splits_key=-1,"" in text:
+                        text = text.replace(
+                            ""num_splits_key=-1,"",
+                            ""num_splits_key=None,"",
+                        )
+                        f.seek(0)
+                        f.write(text)
+                        f.truncate()
+                        if UNSLOTH_ENABLE_LOGGING:
+                            print(""Unsloth: Patching Xformers to fix some performance issues."")
+        except Exception as e:
+            if UNSLOTH_ENABLE_LOGGING:
+                print(f""Unsloth: Failed patching Xformers with error = {str(e)}"")
+pass
+
+# ValueError: 'aimv2' is already used by a Transformers config, pick another name.
+def fix_vllm_aimv2_issue():
+    if importlib.util.find_spec(""vllm"") is None: return
+    vllm_version = importlib_version(""vllm"")
+    if Version(vllm_version) < Version(""0.10.1""):
+        vllm_version = importlib.util.find_spec(""vllm"").origin
+        vllm_version = os.path.split(vllm_version)[0]
+        ovis_config = Path(vllm_version) / ""transformers_utils"" / ""configs"" / ""ovis.py""
+        try:
+            if ovis_config.exists():
+                with open(ovis_config, ""r+"", encoding = ""utf-8"") as f:
+                    text = f.read()
+                    # See https://github.com/vllm-project/vllm-ascend/issues/2046
+                    if 'AutoConfig.register(""aimv2"", AIMv2Config)' in text:
+                        text = text.replace(
+                            'AutoConfig.register(""aimv2"", AIMv2Config)',
+                            '',
+                        )
+                        text = text.replace(
+                            '''backbone_config.pop('model_type')
+                backbone_config = AutoConfig.for_model(model_type,
+                                                       **backbone_config)''',
+                            '''if model_type != ""aimv2"":
+                    backbone_config.pop('model_type')
+                    backbone_config = AutoConfig.for_model(model_type, **backbone_config)
+                else:
+                    backbone_config = AIMv2Config(**backbone_config)'''
+                        )
+                        f.seek(0)
+                        f.write(text)
+                        f.truncate()
+                        if UNSLOTH_ENABLE_LOGGING:
+                            print(""Unsloth: Patching vLLM to fix `'aimv2' is already used by a Transformers config, pick another name.`"")
+        except Exception as e:
+            if UNSLOTH_ENABLE_LOGGING:
+                print(f""Unsloth: Failed patching vLLM with error = {str(e)}"")
+pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 85f1a9a..fde776a 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.8.8""
+__version__ = ""2025.8.9""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index fae6ae0..3aed865 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -618,9 +618,6 @@ class FastModel(FastBaseModel):
                 ""os.environ['TRITON_F32_DEFAULT'] = 'ieee'""
         elif ""gpt-oss"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
-            # CCE fails on Tesla T4
-            # OutOfResources: out of resource: shared memory, Required: 98304, Hardware limit: 65536. Reducing block sizes or `num_stages`
-            os.environ[""UNSLOTH_ENABLE_CCE""] = ""0""
             if not load_in_4bit:
                 # Only upcast MoE biases for MXFP4, not BnB
                 # Set norms to float32 since anyways they get upcasted to float32
@@ -639,11 +636,13 @@ class FastModel(FastBaseModel):
                 # Set down projection compute dtype to be float32 for float16 machines
                 # Set norms to float32 since anyways they get upcasted to float32
                 os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
-                    ""all;None;None;""\
-                    ""if 'down_projs' in name and hasattr(module, 'weight') and ""\
-                    ""torch.amax(dequantize_module_weight(module)) >= 1024:""\
+                    ""torch.float16;torch.bfloat16;torch.float16;""\
+                    ""if ('down_projs' in name) and hasattr(module, 'weight') and ""\
+                    ""torch.amax(dequantize_module_weight(module)) >= 0:""\
                     ""module._pre_set_compute_dtype = torch.float32\n""\
                     """"\
+                    ""if ('mlp.router' in name) and hasattr(module, 'weight'):""\
+                    ""module._pre_set_compute_dtype = torch.float32\n""\
                     "";""
             # Set norms to float32 since anyways they get upcasted to float32
             os.environ[""UNSLOTH_HIGH_PRECISION_LAYERNORM""] = ""1""
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 298ed13..feb550b 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -39,10 +39,7 @@ if ""CUDA_VISIBLE_DEVICES"" in os.environ:
         first_id = devices.split("","")[0]
         warnings.warn(
             f""Unsloth: 'CUDA_VISIBLE_DEVICES' is currently {devices} \n""\
-            ""Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so ""\
-            ""enabling it will require much more work, so we have to prioritize. Please understand!""\
-            ""We do have a beta version, which you can contact us about!\n""\
-            ""Thank you for your understanding and we appreciate it immensely!\n\n""\
+            ""Unsloth currently does not support multi GPU setups - but we are working on it!\n""\
             ""Multiple CUDA devices detected but we require a single device.\n""\
             f""We will override CUDA_VISIBLE_DEVICES to first device: {first_id}.""
         )
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index fd1b87c..e0df218 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -21,6 +21,7 @@ __all__ = [
     ""xformers_version"",
     ""__version__"",
     ""HAS_FLASH_ATTENTION"",
+    ""PRE_CHECK"",
     ""platform_system"",
     ""patch_tokenizer"",
     ""get_statistics"",
@@ -32,30 +33,27 @@ __all__ = [
     ""unsloth_offloaded_gradient_checkpoint"",
     ""torch_compile_options"",
     ""patch_linear_scaling"",
+    ""check_nvidia"",
     ""create_boolean_mask"",
 ]
 
 import torch
 from typing import Union, Optional, List, Any, Callable, Tuple
-import warnings
 from platform import system as platform_system
 platform_system = platform_system()
-import math
 import numpy as np
-import os
-import psutil
-import inspect
-import re
+import warnings, subprocess, re, inspect, psutil, os, math
 
 # =============================================
 # Disable some warnings which can get annoying
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""torch"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""huggingface_hub"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""transformers"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""accelerate"")
-warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""huggingface_hub"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""multiprocessing"")
+warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""multiprocess"")
 
 # Stop ""Special tokens have been added in the vocabulary, ...""
 import logging
@@ -74,7 +72,10 @@ for model_name in model_architectures:
     config_filename = f""{model_name.title()}Config""
     exec(f""from {config_filepath} import {config_filename}"", globals())
 
-    config = inspect.getsource(eval(config_filename))
+    try:
+        config = inspect.getsource(eval(config_filename))
+    except:
+        continue
     if ""rope_scaling"" in config: continue
     config = re.sub(
         r""(\*\*kwargs)[\s]{0,}\,[\s]{0,}\)[\s]{0,}\:"",
@@ -345,7 +346,6 @@ def get_statistics():
     # We simply download a README.md file from HF - all data is made public.
     # This is simply so we can check if some envs are broken or not.
     try:
-        from huggingface_hub import hf_hub_download
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         import psutil
         n_cpus = psutil.cpu_count(logical = False)
@@ -367,7 +367,13 @@ def get_statistics():
                 disable_progress_bars()
                 disabled = True
             pass
-            hf_hub_download(f""unslothai/statistics-{statistics}"", ""README.md"", force_download = True)
+
+            from transformers import AutoModelForCausalLM
+            stats_model = AutoModelForCausalLM.from_pretrained(
+                f""unslothai/statistics-{statistics}"",
+                force_download = True,
+            )
+            del stats_model
             if disabled:
                 enable_progress_bars()
             pass
@@ -659,6 +665,19 @@ def patch_linear_scaling(
 pass
 
 
+def check_nvidia():
+    # Unsloth doesn't work yet on AMD devices - we're working on it!
+    try:
+        output = subprocess.check_output(""nvidia-smi --query-gpu=memory.used --format=csv"", shell = True)
+    except:
+        raise RuntimeError(""Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!"")
+    output = re.findall(rb'([\d]{1,})[\s]{1,}M', output)
+    output = np.array([int(x.decode('utf-8'))/1024 for x in output])
+    return output
+pass
+PRE_CHECK = check_nvidia()
+
+
 def create_boolean_mask(n = 4096, sliding_window = 2048):
     # Creates a boolean mask for attention
     mask = torch.ones(n, n, dtype = torch.bool)
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index c7ae67e..32f2c6b 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -83,7 +83,8 @@ def _fast_prepare_inputs_for_generation(self, input_ids, **kwargs,):
     if ""past_key_values"" in kwargs:
         input_ids = input_ids[:,[-1]]
         kwargs[""attention_mask""] = kwargs[""attention_mask""][:,[-1]]
-    kwargs[""position_ids""] = kwargs[""cache_position""]
+    if ""cache_position"" in kwargs:
+        kwargs[""position_ids""] = kwargs[""cache_position""]
     return { ""input_ids"" : input_ids, **kwargs, }
 pass
 
@@ -1128,7 +1129,7 @@ class FastLlamaModel:
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
         model_patcher.pre_patch()
-        # get_statistics()
+        get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
@@ -1180,6 +1181,8 @@ class FastLlamaModel:
             # Add to kwargs
             kwargs[""rope_scaling""] = rope_scaling
         pass
+        # We currently only support NVIDIA GPUs - AMD / Intel is a work in progress!
+        pre_check = check_nvidia()
 
         bnb_config = None
         if load_in_4bit:
@@ -1206,6 +1209,8 @@ class FastLlamaModel:
             attn_implementation     = ""eager"",
             **kwargs,
         )
+        # We currently only support NVIDIA GPUs - AMD / Intel is a work in progress!
+        post_check = check_nvidia()
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
@@ -1235,14 +1240,12 @@ class FastLlamaModel:
             else:
                 inner_training_loop = Trainer._original_training_loop
         except:
-            raise RuntimeError(
-                'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\
-                'enabling it will require much more work, so we have to prioritize. Please understand!\n'\
-                'We do have a separate beta version, which you can contact us about!\n'\
-                'Thank you for your understanding and we appreciate it immensely!'
-            )
+            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
         pass
 
+        if ((post_check - post_check) >= 1).sum() > 1:
+            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
+
         import transformers.trainer
         items_in_trainer = dir(transformers.trainer)
         good_items = []
@@ -1266,16 +1269,15 @@ class FastLlamaModel:
         f""\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
         f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
         logger.warning(debug_info)
-        import subprocess, re, gc
-        output = subprocess.check_output(
-            'nvidia-smi --query-gpu=memory.used --format=csv', shell = True)
-        output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)
-        output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)
-        if output > 1: print(
-            '********************\\nUnsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\
-            'enabling it will require much more work, so we have to prioritize. Please understand!\\n'\\
-            '********************\\nWe do have a separate beta version, which you can contact us about!\\n'\\
-            '********************\\nThank you for your understanding and we appreciate it immensely!')
+        import subprocess, re, gc, numpy as np
+        try:
+            a = subprocess.check_output('nvidia-smi --query-gpu=memory.used --format=csv', shell = True)
+        except:
+            raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')
+        a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)
+        a = np.array([int(x.decode('utf-8'))/1024 for x in a])
+        if ((a - PRE_CHECK) >= 1).sum() > 1:
+            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()""""""
@@ -1287,12 +1289,7 @@ class FastLlamaModel:
         debug_info = """"""n_total_devices = total_train_batch_size // \\
             args.gradient_accumulation_steps // self._train_batch_size
         if n_total_devices > 1:
-            logger.warning_once(
-                '* Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so ' \\
-                '* enabling it will require much more work, so we have to prioritize. Please understand!\\n' \\
-                '* We do have a separate beta version, which you can contact us about!\\n'\\
-                '* Thank you for your understanding and we appreciate it immensely!'
-            )
+            logger.warning_once('Unsloth currently does not support multi GPU setups - but we are working on it!')
         debug_info =""""""
         debug_info = debug_info.split('\n')
         debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
@@ -1317,12 +1314,7 @@ class FastLlamaModel:
         total_batches = bsz * ga * args.world_size
         n_total_devices = total_batches // ga // bsz
         if n_total_devices > 1:
-            logger.warning_once(
-                '* Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so ' \\
-                '* enabling it will require much more work, so we have to prioritize. Please understand!\\n' \\
-                '* We do have a separate beta version, which you can contact us about!\\n'\\
-                '* Thank you for your understanding and we appreciate it immensely!'
-            )
+            logger.warning_once('Unsloth currently does not support multi GPU setups - but we are working on it!')
             divisor = n_total_devices / 1
             bsz = self._train_batch_size = max(int(bsz / divisor), 1)
             if total_batches // ga // bsz > 1:
@@ -1346,12 +1338,7 @@ class FastLlamaModel:
             ""False"",
         )
         if ""n_total_devices >"" not in inner_training_loop:
-            raise RuntimeError(
-                'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\
-                'enabling it will require much more work, so we have to prioritize. Please understand!\n'\
-                'We do have a separate beta version, which you can contact us about!\n'\
-                'Thank you for your understanding and we appreciate it immensely!'
-            )
+            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
         pass
         inner_training_loop = inner_training_loop.replace(
             ""is_sagemaker_mp_enabled()"",
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index b0bc514..440a53c 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -24,6 +24,7 @@ import itertools
 import collections
 import numpy as np
 import gc
+import subprocess
 
 __all__ = [
     ""load_correct_tokenizer"",
@@ -907,6 +908,19 @@ def add_new_tokens(
 pass
 
 
+def check_nvidia():
+    # Unsloth doesn't work yet on AMD devices - we're working on it!
+    try:
+        output = subprocess.check_output(""nvidia-smi --query-gpu=memory.used --format=csv"", shell = True)
+    except:
+        raise RuntimeError(""Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!"")
+    output = re.findall(rb'([\d]{1,})[\s]{1,}M', output)
+    output = np.array([int(x.decode('utf-8'))/1024 for x in output])
+    return output
+pass
+PRE_CHECK = check_nvidia()
+
+
 from inspect import getsource
 import trl.trainer.sft_trainer
 from trl.trainer.sft_trainer import *
@@ -957,17 +971,15 @@ def patch_sft_trainer_tokenizer():
     ""       'Please do not edit specific areas of the Unsloth codebase or you will get CUDA segfaults.'\n""\
     ""    )\n""\
     ""pass\n""\
-    ""n_devices = torch.cuda.device_count()\n""\
-    ""import subprocess, re\n""\
-    ""output = subprocess.check_output(\n""\
-    ""    'nvidia-smi --query-gpu=memory.used --format=csv', shell = True)\n""\
-    ""output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)\n""\
-    ""output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)\n""\
-    ""if output > 1: print(\n""\
-    ""    '********************\\nUnsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\\n""\
-    ""    'enabling it will require much more work, so we have to prioritize. Please understand!\\n'\\\n""\
-    ""    '********************\\nWe do have a separate beta version, which you can contact us about!\\n'\\\n""\
-    ""    '********************\\nThank you for your understanding and we appreciate it immensely!')\n""\
+    ""import subprocess, re, gc, numpy as np\n""\
+    ""try:\n""\
+    ""    a = subprocess.check_output('nvidia-smi --query-gpu=memory.used --format=csv', shell = True)\n""\
+    ""except:\n""\
+    ""    raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')\n""\
+    ""a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)\n""\
+    ""a = np.array([int(x.decode('utf-8'))/1024 for x in a])\n""\
+    ""if ((a - PRE_CHECK) >= 1).sum() > 1:\n""\
+    ""    raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')\n""\
     ""for _ in range(3):\n""\
     ""    gc.collect()\n""\
     ""    torch.cuda.empty_cache()\n""\
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 3752d46..9d75fda 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -15,6 +15,10 @@
 __version__ = ""2025.1.1""
 
 __all__ = [
+    ""SUPPORTS_BFLOAT16"",
+    ""is_bfloat16_supported"",
+    ""USE_BFLOAT16"",
+
     ""prepare_model_for_kbit_training"",
     ""xformers"",
     ""xformers_attention"",
@@ -30,7 +34,6 @@ __all__ = [
     ""offload_to_disk"",
     ""offload_input_embeddings"",
     ""offload_output_embeddings"",
-    ""is_bfloat16_supported"",
     ""unsloth_offloaded_gradient_checkpoint"",
     ""torch_compile_options"",
     ""patch_linear_scaling"",
@@ -773,9 +776,13 @@ def offload_output_embeddings(model, temporary_location : str = ""_unsloth_tempor
 pass
 
 
+# Log dtype used - sometimes people use float16 on bfloat16 platforms
+global USE_BFLOAT16
+USE_BFLOAT16 = SUPPORTS_BFLOAT16
 # Fixes a weird Torch 2.3 bug which says T4s have bfloat16
 def is_bfloat16_supported():
-    return SUPPORTS_BFLOAT16
+    global USE_BFLOAT16
+    return SUPPORTS_BFLOAT16 and USE_BFLOAT16
 pass
 
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 0765e42..4ffb18f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -68,6 +68,8 @@ pass
 from triton import __version__ as triton_version
 BlockDiagonalCausalMask = xformers.attn_bias.BlockDiagonalCausalMask if xformers is not None else None
 
+from ._utils import SUPPORTS_BFLOAT16, USE_BFLOAT16
+
 
 def original_apply_qkv(self, X):
     Q = self.q_proj(X)
@@ -1387,7 +1389,8 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
         # self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
 
         # Short sequences
-        dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16
+        global USE_BFLOAT16
+        dtype = torch.bfloat16 if USE_BFLOAT16 else torch.float16
         t = torch.arange(original_max_position_embeddings, device=self.short_inv_freq.device, dtype=torch.int64).float()
         freqs = torch.outer(t, self.short_inv_freq)
         emb = torch.cat((freqs, freqs), dim=-1)
@@ -1580,7 +1583,6 @@ class FastLlamaModel:
         pass
         if token is None: token = get_token()
         if model_patcher is None: model_patcher = FastLlamaModel
-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
@@ -1612,6 +1614,10 @@ class FastLlamaModel:
 
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
+        # Log global device type used
+        global USE_BFLOAT16
+        USE_BFLOAT16 = True if dtype == torch.bfloat16 else False
+
         # RoPE Scaling
         model_config = AutoConfig.from_pretrained(model_name, token = token)
         model_max_seq_length = model_config.max_position_embeddings
"
"diff --git a/pyproject.toml b/pyproject.toml
index 7dfca63..5b9dc8b 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.7"",
+    ""unsloth_zoo>=2025.3.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.7"",
+    ""unsloth_zoo>=2025.3.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 38453f3..5bbb85d 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.7""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.8""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index a187ee5..317525c 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -13,7 +13,7 @@
 # limitations under the License.
 
 from .llama   import FastLlamaModel
-from .loader  import FastLanguageModel, FastVisionModel
+from .loader  import FastLanguageModel, FastVisionModel, FastTextModel, FastModel
 from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
 from .granite import FastGraniteModel
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 37c69ef..03eb21f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.8""
+__version__ = ""2025.3.9""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a490fb8..3504037 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -91,7 +91,7 @@ def original_apply_o(self, X):
 pass
 
 from math import sqrt as math_sqrt
-KV_CACHE_INCREMENT = 256 # KV Cache update size
+KV_CACHE_INCREMENT = 512 # KV Cache update size
 torch_nn_functional_softmax = torch.nn.functional.softmax
 # SDPA has GQA internally
 SDPA_HAS_GQA = ""enable_gqa"" in scaled_dot_product_attention.__doc__
@@ -1656,6 +1656,13 @@ class FastLlamaModel:
                 ""Are you certain you want to do remote code execution?""
             )
         pass
+        if fast_inference:
+            import platform
+            if platform.system().lower() == 'windows':
+                print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
+                fast_inference = False
+        pass
+
         if token is None: token = get_token()
         if model_patcher is None: model_patcher = FastLlamaModel
         SUPPORTS_BFLOAT16 = is_bfloat16_supported()
@@ -1966,12 +1973,17 @@ class FastLlamaModel:
             for layer in model.model.layers:
                 layer.self_attn.rotary_emb = rotary_emb
         pass
-        
+
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
+        model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
+
         # Patch generate
         if model.generate.__name__ != ""unsloth_fast_generate"":
             model._old_generate = model.generate
             unsloth_fast_generate.__doc__ = model._old_generate.__doc__
             model.generate = types.MethodType(unsloth_fast_generate, model)
+        pass
         return model, tokenizer
     pass
 
@@ -2404,7 +2416,7 @@ class FastLlamaModel:
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
         model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
-        
+
         # Patch generate
         if model.generate.__name__ != ""unsloth_fast_generate"":
             model._old_generate = model.generate
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 30128cd..800c016 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -383,10 +383,13 @@ from ..kernels import (
     patch_loss_functions,
     post_patch_loss_function,
 )
-from .vision import FastBaseVisionModel
-
+from .vision import FastBaseModel
+from transformers import (
+    AutoModelForVision2Seq,
+    AutoModelForCausalLM,
+)
 
-class FastVisionModel(FastBaseVisionModel):
+class FastModel(FastBaseModel):
     @staticmethod
     def from_pretrained(
         model_name                 = ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
@@ -413,7 +416,7 @@ class FastVisionModel(FastBaseVisionModel):
         patch_compiling_bitsandbytes()
         if use_gradient_checkpointing == ""unsloth"":
             patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
-        
+
         old_model_name = model_name
         if not use_exact_model_name:
             model_name = get_model_name(model_name, load_in_4bit)
@@ -427,7 +430,7 @@ class FastVisionModel(FastBaseVisionModel):
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         was_disabled = are_progress_bars_disabled()
         disable_progress_bars()
-        
+
         autoconfig_error = None
         peft_error = None
         try:
@@ -458,7 +461,7 @@ class FastVisionModel(FastBaseVisionModel):
 
         # Old transformers versions check
         both_exist = (is_model and is_peft) and not SUPPORTS_LLAMA32
-        
+
         # New transformers need to check manually.
         if SUPPORTS_LLAMA32:
             # Check if folder exists locally
@@ -515,9 +518,12 @@ class FastVisionModel(FastBaseVisionModel):
         if not was_disabled: enable_progress_bars()
 
         do_logging = os.environ.get(""UNSLOTH_ENABLE_LOGGING"", ""0"") == ""1""
-        redirector = sys.stdout if do_logging else open(os.devnull, ""w"")
+        if do_logging:
+            redirector = contextlib.nullcontext()
+        else:
+            redirector = contextlib.redirect_stdout(open(os.devnull, ""w""))
 
-        with contextlib.redirect_stdout(redirector):
+        with redirector:
             patch_loss_functions(torch_compile = False)
             model_types = unsloth_compile_transformers(
                 model_name              = model_name,
@@ -547,7 +553,6 @@ class FastVisionModel(FastBaseVisionModel):
                 return_logits           = return_logits,
             )
         pass
-        if do_logging: redirector.close()
 
         # Check if this is local model since the tokenizer gets overwritten
         if  os.path.exists(os.path.join(old_model_name, ""tokenizer_config.json"")) and \
@@ -559,7 +564,12 @@ class FastVisionModel(FastBaseVisionModel):
             tokenizer_name = None
         pass
 
-        model, tokenizer = FastBaseVisionModel.from_pretrained(
+        # Check if VLM
+        is_vlm = (x.endswith(""ForConditionalGeneration"") for x in model_config.architectures)
+        is_vlm = is_vlm or hasattr(model_config, ""vision_config"")
+        auto_model = AutoModelForVision2Seq if is_vlm else AutoModelForCausalLM
+
+        model, tokenizer = FastBaseModel.from_pretrained(
             model_name        = model_name,
             max_seq_length    = max_seq_length,
             dtype             = _get_dtype(dtype),
@@ -570,6 +580,7 @@ class FastVisionModel(FastBaseVisionModel):
             revision          = revision if not is_peft else None,
             model_types       = model_types,
             tokenizer_name    = tokenizer_name,
+            auto_model        = auto_model,
             *args, **kwargs,
         )
         
@@ -617,8 +628,14 @@ class FastVisionModel(FastBaseVisionModel):
                 trust_remote_code = trust_remote_code,
             )
             # Patch it as well!
-            model = FastBaseVisionModel.patch_peft_model(model, use_gradient_checkpointing)
+            model = FastBaseModel.patch_peft_model(model, use_gradient_checkpointing)
         pass
         return model, tokenizer
     pass
 pass
+
+class FastVisionModel(FastModel):
+    pass
+
+class FastTextModel(FastModel):
+    pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index da7f449..a2e609f 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -611,6 +611,21 @@ __INT_TO_FLOAT_MAPPER = \
         ""open-thoughts/OpenThinker-7B"",
         ""unsloth/OpenThinker-7B-bnb-4bit"",
     ),
+    ""unsloth/granite-3.2-2b-instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/granite-3.2-2b-instruct"",
+        ""ibm-granite/granite-3.2-2b-instruct"",
+        ""unsloth/granite-3.2-2b-instruct-bnb-4bit"",
+    ),
+    ""unsloth/granite-3.2-8b-instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/granite-3.2-8b-instruct"",
+        ""ibm-granite/granite-3.2-8b-instruct"",
+        ""unsloth/granite-3.2-8b-instruct-bnb-4bit"",
+    ),
+    ""unsloth/QwQ-32B-unsloth-bnb-4bit"" : (
+        ""unsloth/QwQ-32B"",
+        ""Qwen/QwQ-32B"",
+        ""unsloth/QwQ-32B-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index d13d394..ff07ef6 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -17,6 +17,8 @@ from transformers import (
     BitsAndBytesConfig,
     AutoModelForVision2Seq,
     AutoProcessor,
+    AutoTokenizer,
+    AutoModelForCausalLM,
 )
 from .llama import *
 from ..kernels import (
@@ -31,48 +33,60 @@ from unsloth_zoo.peft_utils import (
     requires_grad_for_gradient_checkpointing,
 )
 from triton import __version__ as triton_version
+from unsloth_zoo.utils import _get_dtype
+from unsloth_zoo.patching_utils import patch_model_and_tokenizer
+import types
+import functools
 
 __all__ = [
-    ""FastBaseVisionModel"",
+    ""FastBaseModel"",
 ]
 
-def _wrap_fast_inference(generate, device_type, dtype, model):
-    # Wraps inference with bfloat16 / float16
-    @torch.inference_mode
-    def _fast_generate(*args, **kwargs):
-        # For num_logits_to_keep
-        # kwargs[""num_logits_to_keep""] = 1
 
-        # Remove token_type_ids
-        kwargs.pop(""token_type_ids"", None)
+def unsloth_base_fast_generate(
+    self,
+    *args,
+    **kwargs,
+):
+    FastBaseModel.for_inference(self)
+    dtype = _get_dtype(self.config.torch_dtype)
 
-        # Check pad_token
-        model_eos_token_id = getattr(model.config, ""eos_token_id"", None)
-        if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
-            model_eos_token_id = model_eos_token_id[0]
+    # Check if VLM
+    is_vlm = (x.endswith(""ForConditionalGeneration"") for x in self.config.architectures)
+    is_vlm = is_vlm or hasattr(self.config, ""vision_config"")
 
-        kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
+    # Remove token_type_ids
+    kwargs.pop(""token_type_ids"", None)
 
-        try:
-            kwargs[""pixel_values""] = kwargs[""pixel_values""].to(model.dtype)
-        except:
-            pass
+    # VLMs do not allow logits_to_keep
+    if not is_vlm: kwargs[""logits_to_keep""] = 1
 
-        # Autocasted
-        with torch.autocast(device_type = device_type, dtype = dtype):
-            output = generate(*args, **kwargs)
-        pass
-        return output
+    # Check pad_token
+    model_eos_token_id = getattr(self.config, ""eos_token_id"", None)
+    if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
+        model_eos_token_id = model_eos_token_id[0]
+
+    kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
+
+    # Get pixel values for VLMs
+    try: kwargs[""pixel_values""] = kwargs[""pixel_values""].to(dtype)
+    except: pass
+
+    # Mixed precision autocast
+    with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
+        output = self._old_generate(*args, **kwargs)
     pass
-    return _fast_generate
+
+    FastBaseModel.for_training(self)
+    return output
 pass
 
 
-class FastBaseVisionModel:
+class FastBaseModel:
 
     @staticmethod
     def from_pretrained(
-        model_name        = ""unsloth/llama-3-8b-bnb-4bit"",
+        model_name        = ""unsloth/Llama-3.2-1B-Instruct"",
         max_seq_length    = None,
         dtype             = None,
         load_in_4bit      = True,
@@ -81,6 +95,7 @@ class FastBaseVisionModel:
         trust_remote_code = False,
         model_types       = None,
         tokenizer_name    = None,
+        auto_model        = AutoModelForVision2Seq,
         **kwargs,
     ):
         if trust_remote_code:
@@ -94,12 +109,16 @@ class FastBaseVisionModel:
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
+        from importlib.metadata import version as importlib_version
+        try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
+        except: vllm_version = """"
+
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} vision patching. Transformers: {transformers_version}.\n""\
-           f""   {chr(92)}{chr(92)}   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
+           f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
            f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
-           f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
+           f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
         print(statistics)
 
         # Warn about fast transfers
@@ -136,8 +155,8 @@ class FastBaseVisionModel:
 
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
-        
-        model = AutoModelForVision2Seq.from_pretrained(
+
+        model = auto_model.from_pretrained(
             model_name,
             device_map              = device_map,
             torch_dtype             = dtype,
@@ -152,26 +171,25 @@ class FastBaseVisionModel:
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
-        tokenizer = AutoProcessor.from_pretrained(
+        auto_processor = AutoProcessor if auto_model is AutoModelForVision2Seq else AutoTokenizer
+        tokenizer = auto_processor.from_pretrained(
             tokenizer_name,
             padding_side = ""right"",
             token        = token,
         )
         # Add padding side as well
-        tokenizer.tokenizer.padding_side = ""right""
+        if hasattr(tokenizer, ""tokenizer""):
+            tokenizer.tokenizer.padding_side = ""right""
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = post_patch_loss_function(model)
-
-        # Fix up config for transformers uploading PEFT
-        # Not necessary anymore since we require transformers>=4.37!
-        if False:
-            name = model.config._name_or_path
-            if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-                name = name[:len(name) - len(""-bnb-4bit"")]
-                model.config.update({""_name_or_path"" : name})
-            pass
-        pass
+        # Fix other stuff like BnB compute data types
+        model, tokenizer = patch_model_and_tokenizer(
+            model,
+            tokenizer,
+            downcast_rope = False,
+            fix_embeddings = False,
+        )
 
         # Log Unsloth version for future fastpaths for inference
         if hasattr(model, ""config""):
@@ -187,13 +205,22 @@ class FastBaseVisionModel:
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
         tokenizer.tokenizer.padding_side = ""left"" # Force inference
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model._saved_temp_tokenizer = tokenizer
-            internal_model = internal_model.model
+        m = model
+        while hasattr(m, ""model""):
+            m._saved_temp_tokenizer = tokenizer
+            # Also set is_loaded_in_8bit to disable incorrect DDP
+            m.is_loaded_in_8bit = True
+            m = m.model
         pass
-        internal_model._saved_temp_tokenizer = tokenizer
-        
+        m._saved_temp_tokenizer = tokenizer
+        # Also set is_loaded_in_8bit to disable incorrect DDP
+        m.is_loaded_in_8bit = True
+
+        # Patch generate
+        if model.generate.__name__ != ""unsloth_base_fast_generate"":
+            model._old_generate = model.generate
+            unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
+            model.generate = types.MethodType(unsloth_base_fast_generate, model)
         return model, tokenizer
     pass
 
@@ -272,7 +299,7 @@ class FastBaseVisionModel:
         # Enable gradients on modules which are trainable
         requires_grad_for_gradient_checkpointing(model)
 
-        model = FastBaseVisionModel.patch_peft_model(model, use_gradient_checkpointing)
+        model = FastBaseModel.patch_peft_model(model, use_gradient_checkpointing)
 
         # Clear deleted GPU items
         for _ in range(3):
@@ -281,6 +308,9 @@ class FastBaseVisionModel:
         pass
         patch_saving_functions(model, vision = True)
 
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastBaseModel.for_training,  model)
+        model.for_inference = functools.partial(FastBaseModel.for_inference, model)
         return model
     pass
 
@@ -314,62 +344,57 @@ class FastBaseVisionModel:
         patch_saving_functions(model, vision = True)
 
         # Patch tokenizer to pad to the right
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
+        m = model
+        while hasattr(m, ""model""):
+            if hasattr(m, ""_saved_temp_tokenizer""):
+                m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
-            internal_model = internal_model.model
+            # Also set is_loaded_in_8bit to disable incorrect DDP
+            m.is_loaded_in_8bit = True
+            m = m.model
         pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
+        if hasattr(m, ""_saved_temp_tokenizer""):
+            m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
+        # Also set is_loaded_in_8bit to disable incorrect DDP
+        m.is_loaded_in_8bit = True
 
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()
         pass
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastBaseModel.for_training,  model)
+        model.for_inference = functools.partial(FastBaseModel.for_inference, model)
+
+        # Patch generate
+        if model.generate.__name__ != ""unsloth_base_fast_generate"":
+            model._old_generate = model.generate
+            unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
+            model.generate = types.MethodType(unsloth_base_fast_generate, model)
         return model
     pass
 
 
     @staticmethod
     def for_inference(model):
-        model.gradient_checkpointing = False
-        model.training = False
-
-        for name, module in model.named_modules():
-            if hasattr(module, ""gradient_checkpointing""):
-                module.gradient_checkpointing = False
-            if hasattr(module, ""training""):
-                module.training = False
-        pass
-
-        dtype = model.config.torch_dtype
-        if type(dtype) is str:
-            if   dtype ==  ""float16"": dtype = torch.float16
-            elif dtype == ""bfloat16"": dtype = torch.bfloat16
-        pass
-        device_type = model.device.type
-
-        # Wrap model.generate
-        if model.generate.__name__ != ""_fast_generate"":
-            model._unwrapped_old_generate = model.generate
-            model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)
-        pass
-        
-        # Patch tokenizer to pad to the left
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
+        if not hasattr(model, ""parameters""):
+            raise TypeError(""Unsloth: I think you're passing a tokenizer, not the model to for_inference!"")
+
+        def _for_inference(m):
+            if hasattr(m, ""gradient_checkpointing""): m.gradient_checkpointing = False
+            if hasattr(m, ""training""): m.training = False
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""): m._saved_temp_tokenizer.padding_side = ""left""
+            # Set a flag for generation!
+            m._flag_for_generation = True
         pass
+        m = model
+        while hasattr(m, ""model""):
+            _for_inference(m)
+            m = m.model
+        _for_inference(m)
 
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -380,40 +405,34 @@ class FastBaseVisionModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = False
         pass
-
         return model
     pass
 
 
     @staticmethod
     def for_training(model, use_gradient_checkpointing = True):
-        model.gradient_checkpointing = use_gradient_checkpointing
-        model.training = True
-
-        for name, module in model.named_modules():
-            if hasattr(module, ""gradient_checkpointing""):
-                module.gradient_checkpointing = use_gradient_checkpointing
-            if hasattr(module, ""training""):
-                module.training = True
-        pass
+        if not hasattr(model, ""parameters""):
+            raise TypeError(""Unsloth: I think you're passing a tokenizer, not the model to for_training!"")
 
-        # Also revert model.generate
-        if hasattr(model, ""_unwrapped_old_generate""):
-            model.generate = model._unwrapped_old_generate
-            del model._unwrapped_old_generate
+        # Delete all fast inference loras
+        for param in model.parameters():
+            if hasattr(param, ""_fast_lora""):
+                del param._fast_lora
         pass
 
-        # Patch tokenizer to pad to the right
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
+        def _for_training(m):
+            if hasattr(m, ""gradient_checkpointing""): m.gradient_checkpointing = use_gradient_checkpointing
+            if hasattr(m, ""training""): m.training = True
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""): m._saved_temp_tokenizer.padding_side = ""right""
+            # Set a flag for generation!
+            if hasattr(m, ""_flag_for_generation""): del m._flag_for_generation
         pass
+        m = model
+        while hasattr(m, ""model""):
+            _for_training(m)
+            m = m.model
+        _for_training(m)
 
         # Also re-enable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -424,7 +443,6 @@ class FastBaseVisionModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = True
         pass
-
         return model
     pass
 pass
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index cb0ce2f..c98feec 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -193,6 +193,10 @@ def LlamaAttention_fast_forward_inference(
 
     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+
+    # Need to do it prior 2 steps before hitting full on short KV cache
+    # or else error
+    self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
     cos, sin = self.rotary_emb.get_cached(kv_seq_len)
     cos = cos[position_ids].unsqueeze(1)
     sin = sin[position_ids].unsqueeze(1)
@@ -1122,7 +1126,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1248,7 +1252,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1363,7 +1367,7 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
"
"diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 06ae821..f88e362 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -164,7 +164,7 @@ RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__prepare_inputs)
 # Remove _move_model_to_vllm
 def grpo_trainer__move_model_to_vllm(function_name, function):
     if  function_name != ""_move_model_to_vllm"": return function
-    
+
     def _move_model_to_vllm(self, *args, **kwargs): return None
 
     function = inspect.getsource(_move_model_to_vllm)
@@ -246,14 +246,20 @@ def grpo_trainer_compute_loss(function_name, function):
                 self, _input_ids, logits_to_keep, completion_mask, advantages,
                 n_chunks = self.args.unsloth_num_chunks,
             )
-        
+
         # Log the metrics
         # completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()
-        self._metrics[""completion_length""].append(completion_length.item())
 
         # mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         # self._metrics[""kl""].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())
-        self._metrics[""kl""].append(mean_kl.item())
+
+        if ""train"" in self._metrics:
+            mode = ""eval"" if self.control.should_evaluate else ""train""
+            self._metrics[mode][""completion_length""].append(completion_length.item())
+            self._metrics[mode][""kl""].append(mean_kl.item())
+        else:
+            self._metrics[""completion_length""].append(completion_length.item())
+            self._metrics[""kl""].append(mean_kl.item())
         return loss
     pass
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 34a61e2..9bd0fc1 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.6.6""
+__version__ = ""2025.6.7""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 8a49026..184be83 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -48,14 +48,15 @@ import importlib.util
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
 from unsloth_zoo.utils import Version, _get_dtype
 transformers_version = Version(transformers_version)
-SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
-SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
-SUPPORTS_GEMMA2  = transformers_version >= Version(""4.42"")
-SUPPORTS_LLAMA31 = transformers_version >= Version(""4.43.2"")
-SUPPORTS_LLAMA32 = transformers_version  > Version(""4.45.0"")
-SUPPORTS_GRANITE = transformers_version >= Version(""4.46.0"")
-SUPPORTS_QWEN3   = transformers_version >= Version(""4.50.3"")
+SUPPORTS_FOURBIT   = transformers_version >= Version(""4.37"")
+SUPPORTS_GEMMA     = transformers_version >= Version(""4.38"")
+SUPPORTS_GEMMA2    = transformers_version >= Version(""4.42"")
+SUPPORTS_LLAMA31   = transformers_version >= Version(""4.43.2"")
+SUPPORTS_LLAMA32   = transformers_version  > Version(""4.45.0"")
+SUPPORTS_GRANITE   = transformers_version >= Version(""4.46.0"")
+SUPPORTS_QWEN3     = transformers_version >= Version(""4.50.3"")
 SUPPORTS_QWEN3_MOE = transformers_version >= Version(""4.50.3"")
+SUPPORTS_GEMMA3N   = transformers_version >= Version(""4.53.0"")
 if SUPPORTS_GEMMA:
     from .gemma  import FastGemmaModel
 if SUPPORTS_GEMMA2:
@@ -543,6 +544,8 @@ class FastModel(FastBaseModel):
             os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
         elif ""olmo-2"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""gemma-3n"" in lowered_model_name and transformers_version < Version(""4.53.0""):
+            raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
         else:
             for check_model_name in DISABLE_COMPILE_MODEL_NAMES:
                 if check_model_name in lowered_model_name:
@@ -727,6 +730,10 @@ class FastModel(FastBaseModel):
                 unsloth_force_compile   = unsloth_force_compile,
             )
         pass
+        # Fix SDPA
+        if ""gemma-3n"" in lowered_model_name:
+            supports_sdpa = False
+        pass
 
         # Check if this is local model since the tokenizer gets overwritten
         if  os.path.exists(os.path.join(old_model_name, ""tokenizer_config.json"")) and \
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index fdf9569..6225984 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -879,6 +879,26 @@ __INT_TO_FLOAT_MAPPER = \
         ""mistralai/Mistral-Small-3.2-24B-Instruct-2506"",
         ""unsloth/Mistral-Small-3.2-24B-Instruct-2506-bnb-4bit"",
     ),
+    ""unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit"" : (
+        ""unsloth/gemma-3n-E4B-it"",
+        ""google/gemma-3n-E4B-it"",
+        ""unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit"",
+    ),
+    ""unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit"" : (
+        ""unsloth/gemma-3n-E2B-it"",
+        ""google/gemma-3n-E2B-it"",
+        ""unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit"",
+    ),
+    ""unsloth/gemma-3n-E4B-unsloth-bnb-4bit"" : (
+        ""unsloth/gemma-3n-E4B"",
+        ""google/gemma-3n-E4B"",
+        ""unsloth/gemma-3n-E4B-unsloth-bnb-4bit"",
+    ),
+    ""unsloth/gemma-3n-E2B-unsloth-bnb-4bit"" : (
+        ""unsloth/gemma-3n-E2B"",
+        ""google/gemma-3n-E2B"",
+        ""unsloth/gemma-3n-E2B-unsloth-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 32d20ae..0ccbe91 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -19,6 +19,14 @@ import warnings
 import gc
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
 import bitsandbytes as bnb
+from transformers.models.llama.modeling_llama import logger
+
+__version__ = ""2023.12""
+__all__ = [
+    ""prepare_model_for_kbit_training"",
+    ""patch_tokenizer"",
+    ""print_unsloth_message"",
+]
 
 
 def prepare_model_for_kbit_training(
@@ -59,3 +67,38 @@ def prepare_model_for_kbit_training(
 
     return model
 pass
+
+
+def patch_tokenizer(model, tokenizer):
+    if not hasattr(tokenizer, ""pad_token"") or tokenizer.pad_token is None:
+        # Fixes https://github.com/unslothai/unsloth/issues/5
+        if hasattr(tokenizer, ""unk_token""):
+            tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token})
+            tokenizer.pad_token = tokenizer.unk_token
+        else:
+            logger.warning_one(
+                f""{model.config._name_or_path} does not have a padding or unknown token!\n""\
+                f""Will use the EOS token of id {tokenizer.eos_token_id} as padding.""
+            )
+            assert(hasattr(tokenizer, ""eos_token""))
+            tokenizer.add_special_tokens({""pad_token"" : tokenizer.eos_token})
+            tokenizer.pad_token = tokenizer.eos_token
+        config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
+    pass
+    return model, tokenizer
+pass
+
+
+def print_unsloth_message(name):
+    SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
+    gpu_stats = torch.cuda.get_device_properties(0)
+    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
+
+    statistics = \
+       f""==((====))==  Unsloth: Fast {name} patching release {__version__}\n""\
+       f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
+       f""O^O/ \_/ \\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\n""\
+       f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
+       f' ""-____-""     bfloat16 support = {str(SUPPORTS_BFLOAT16).upper()}\n'
+    print(statistics)
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9e9c3cc..a3a91ab 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -22,9 +22,7 @@ from transformers.models.llama.modeling_llama import (
     CausalLMOutputWithPast,
 )
 from ..kernels import *
-from ._utils import (
-    prepare_model_for_kbit_training,
-)
+from ._utils import *
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
@@ -615,18 +613,8 @@ class FastLlamaModel:
         device_map = ""sequential"",
         rope_scaling = None,
     ):
-        gpu_stats = torch.cuda.get_device_properties(0)
-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
-
-        statistics = \
-            ""==((====))==  Unsloth: Fast Llama patching release 2023.12\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
-           f""O^O/ \_/ \\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\n""\
-           f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
-           f' ""-____-""     bfloat16 support = {str(SUPPORTS_BFLOAT16).upper()}\n'
-        print(statistics)
-
+        print_unsloth_message(""Mistral"")
         FastLlamaModel.pre_patch()
 
         if dtype is None:
@@ -676,22 +664,7 @@ class FastLlamaModel:
             token = token,
         )
 
-        if not hasattr(tokenizer, ""pad_token""):
-            # Fixes https://github.com/unslothai/unsloth/issues/5
-            if hasattr(tokenizer, ""unk_token""):
-                tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token})
-                tokenizer.pad_token = tokenizer.unk_token
-            else:
-                logger.warning_one(
-                    f""{model_name} does not have a padding or unknown token!\n""\
-                    f""Will use the EOS token of id {tokenizer.eos_token_id} as padding.""
-                )
-                assert(hasattr(tokenizer, ""eos_token""))
-                tokenizer.add_special_tokens({""pad_token"" : tokenizer.eos_token})
-                tokenizer.pad_token = tokenizer.eos_token
-            config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
-        pass
-
+        model, tokenizer = patch_tokenizer(model, tokenizer)
         model = FastLlamaModel.post_patch(model)
 
         # Patch up QKV / O and MLP
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 0c13f99..8b1fd0f 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -231,18 +231,8 @@ class FastMistralModel(FastLlamaModel):
         device_map = ""sequential"",
         # rope_scaling = None, Mistral does not support RoPE scaling
     ):
-        gpu_stats = torch.cuda.get_device_properties(0)
-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
-
-        statistics = \
-            ""==((====))==  Unsloth: Fast Mistral patching release 2023.12\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
-           f""O^O/ \_/ \\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\n""\
-           f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
-           f' ""-____-""     bfloat16 support = {str(SUPPORTS_BFLOAT16).upper()}\n'
-        print(statistics)
-
+        print_unsloth_message(""Mistral"")
         FastMistralModel.pre_patch()
 
         if dtype is None:
@@ -277,22 +267,7 @@ class FastMistralModel(FastLlamaModel):
             token = token,
         )
 
-        if not hasattr(tokenizer, ""pad_token""):
-            # Fixes https://github.com/unslothai/unsloth/issues/5
-            if hasattr(tokenizer, ""unk_token""):
-                tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token})
-                tokenizer.pad_token = tokenizer.unk_token
-            else:
-                logger.warning_one(
-                    f""{model_name} does not have a padding or unknown token!\n""\
-                    f""Will use the EOS token of id {tokenizer.eos_token_id} as padding.""
-                )
-                assert(hasattr(tokenizer, ""eos_token""))
-                tokenizer.add_special_tokens({""pad_token"" : tokenizer.eos_token})
-                tokenizer.pad_token = tokenizer.eos_token
-            config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
-        pass
-
+        model, tokenizer = patch_tokenizer(model, tokenizer)
         model = FastMistralModel.post_patch(model)
 
         # Patch up QKV / O and MLP
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 3dccf63..e3f1e61 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -208,8 +208,8 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
     def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None,
         config = None, # [TODO] Hack to pass in config - need to remove later
     ):
-        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         super().__init__()
+        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
@@ -270,9 +270,8 @@ class GemmaFixedLinearScalingRotaryEmbedding(GemmaFixedRotaryEmbedding):
     def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0,
         config = None, # [TODO] Hack to pass in config - need to remove later
     ):
-        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         self.scaling_factor = scaling_factor
-        super().__init__(dim, max_position_embeddings, base, device)
+        super().__init__(dim = dim, max_position_embeddings = max_position_embeddings, base = base, device = device, config = config)
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index d043f03..a4a6527 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -980,8 +980,9 @@ class LlamaRotaryEmbedding(torch.nn.Module):
     def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None,
         config = None, # [TODO] Hack to pass in config - need to remove later
     ):
-        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         super().__init__()
+        if config is not None: return # [TODO] Hack to pass in config - need to remove later
+        
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
@@ -1036,9 +1037,8 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
     def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0,
         config = None, # [TODO] Hack to pass in config - need to remove later
     ):
-        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         self.scaling_factor = scaling_factor
-        super().__init__(dim, max_position_embeddings, base, device)
+        super().__init__(dim = dim, max_position_embeddings = max_position_embeddings, base = base, device = device, config = config)
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -1064,8 +1064,9 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
     def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None,
         config = None, # [TODO] Hack to pass in config - need to remove later
     ):
-        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         super().__init__()
+        if config is not None: return # [TODO] Hack to pass in config - need to remove later
+        
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b5244ed..e6c9280 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -14,6 +14,7 @@
 
 import torch
 import gc
+import math
 from typing import Optional, Tuple, List, Union
 from ._utils import *
 from ._utils import __version__
@@ -1036,7 +1037,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1109,7 +1110,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
         # in FP32. They are applied (multiplied) in FP32 as well.
         self.current_rope_size = seq_len
         
-        t = torch.arange(self.current_rope_size, device=""cpu"", dtype=torch.int64).float()
+        t = torch.arange(self.current_rope_size, device=self.inv_freq.device, dtype=torch.int64).float()
 
         freqs = torch.outer(t, self.inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
@@ -1158,7 +1159,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
"
"diff --git a/README.md b/README.md
index ab06abe..1d33510 100644
--- a/README.md
+++ b/README.md
@@ -10,7 +10,7 @@
 <a href=""https://discord.gg/u54VK8m8tk""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
 <a href=""https://ko-fi.com/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png"" height=""48""></a>
 
-### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!
+### Finetune Llama 3, Mistral, Phi-3 & Gemma 2-5x faster with 80% less memory!
 
 ![](https://i.ibb.co/sJ7RhGG/image-41.png)
 
@@ -24,24 +24,24 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 |-----------|---------|--------|----------|
 | **Llama 3 (8B)**      | [ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
 | **Mistral v3 (7B)**    | [ Start for free](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing)               | 2.2x faster | 73% less |
-| **Mistral v1 (7B)**    | [ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |
+| **Phi-3 (medium)** | [ Start for free](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)               | 2x faster | 50% less |
+| **Phi-3 (mini)** | [ Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |
 | **Gemma (7B)**      | [ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |
 | **ORPO**     | [ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |
 | **DPO Zephyr**     | [ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
-| **Phi-3 (3.8B)** | [ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |
 | **TinyLlama**  | [ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |
 
-- Benchmarking compared to FA2 + Hugging Face combined.
-- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
-- Also [Llama-3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing). [Mistral 7b v1 ChatML](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing). [Mistral 7b v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing).
-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.
+- **Kaggle Notebooks** for [Llama 3 8B](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7B](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7B](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
+- Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text
+
 
 ##  Unsloth.ai News
--  NEW! Mistral v3 Base and Instruct now supported! 2x faster, 70% less VRAM notebooks for the [base model](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [instruct with ShareGPT](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
+-  NEW! [Phi-3 medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) and [Phi-3 mini](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing) support is here!
+-  NEW! [Mistral v3 Base](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [Mistral v3 Instruct](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing) support is here!
 -  NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly's PR [#428](https://github.com/unslothai/unsloth/pull/428)
 -  NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).
 -  NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!
--  NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!
 -  NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you're using our notebooks. To enable, simply change 1 line:
 ```python
 model = FastLanguageModel.get_peft_model(
@@ -195,15 +195,15 @@ dataset = load_dataset(""json"", data_files = {""train"" : url}, split = ""train"")
 
 # 4bit pre quantized models we support for 4x faster downloading + no OOMs.
 fourbit_models = [
+    ""unsloth/mistral-7b-v0.3-bnb-4bit"",      # New Mistral v3 2x faster!
+    ""unsloth/mistral-7b-instruct-v0.3-bnb-4bit"",
+    ""unsloth/llama-3-8b-bnb-4bit"",           # Llama-3 15 trillion tokens model 2x faster!
+    ""unsloth/llama-3-8b-Instruct-bnb-4bit"",
+    ""unsloth/llama-3-70b-bnb-4bit"",
+    ""unsloth/Phi-3-mini-4k-instruct"",        # Phi-3 2x faster!
+    ""unsloth/Phi-3-medium-4k-instruct"",
     ""unsloth/mistral-7b-bnb-4bit"",
-    ""unsloth/mistral-7b-instruct-v0.2-bnb-4bit"",
-    ""unsloth/llama-2-7b-bnb-4bit"",
-    ""unsloth/gemma-7b-bnb-4bit"",
-    ""unsloth/gemma-7b-it-bnb-4bit"", # Instruct version of Gemma 7b
-    ""unsloth/gemma-2b-bnb-4bit"",
-    ""unsloth/gemma-2b-it-bnb-4bit"", # Instruct version of Gemma 2b
-    ""unsloth/llama-3-8b-bnb-4bit"", # [NEW] 15 Trillion token Llama-3
-    ""unsloth/Phi-3-mini-4k-instruct-bnb-4bit"",
+    ""unsloth/gemma-7b-bnb-4bit"",             # Gemma 2.2x faster!
 ] # More models at https://huggingface.co/unsloth
 
 model, tokenizer = FastLanguageModel.from_pretrained(
"
"diff --git a/pyproject.toml b/pyproject.toml
index 91654e6..3015563 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -43,7 +43,7 @@ huggingface = [
     ""numpy"",
     ""accelerate>=0.26.1"",
     ""trl>=0.7.9"",
-    ""peft>=0.7.1,<0.11.0"",
+    ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
 ]
 cu118only = [
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index cb04377..b1fdba8 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -31,3 +31,9 @@ from .fast_lora import (
 	apply_lora_o,
 )
 from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora
+
+try:
+	print("" Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+except:
+	print(""Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ee9647c..a773b13 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init
         self.scaling[adapter_name] = lora_alpha / r
 
     if init_lora_weights == ""loftq"":
+        # We manually check for PEFT
+        if not hasattr(self, ""loftq_init""):
+            import peft
+            raise RuntimeError(
+                f""Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\n""\
+                ""Please install PEFT 0.7.2 or higher.\n""\
+                ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
+            )
+        pass
         self.loftq_init(adapter_name)
+        
     elif init_lora_weights:
         self.reset_lora_parameters(adapter_name, init_lora_weights)
 
diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index 519914d..92fde81 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -28,6 +28,7 @@ DPOTrainer_metrics = [
     ""logits/rejected"",
     ""logits/chosen"",
 ]
+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)
 
 
 def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):
@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa
     if args.evaluation_strategy == IntervalStrategy.NO and ""loss"" in logs:
         values = {""Training Loss"": logs[""loss""]}
         for metric in DPOTrainer_metrics:
-            if metric in logs:
-                values[metric.replace(""/"", "" / "")] = logs[metric]
-            else:
-                # Maybe not a DPO Trainer anymore? Redo the tracker
-                column_names = [self.first_column] + [""Training Loss""]
-                if args.evaluation_strategy != IntervalStrategy.NO:
-                    column_names.append(""Validation Loss"")
-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)
-                break
-            pass
+            values[metric.replace(""/"", "" / "")] = logs[metric]
         pass
         # First column is necessarily Step since we're not in epoch eval strategy
         values[""Step""] = state.global_step
@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):
         self.inner_table = [list(values.keys()), list(values.values())]
     else:
         columns = self.inner_table[0]
-        print(columns)
-        for key in values.keys():
-            if key not in columns:
-                columns.append(key)
+        new_values = {}
+        for key, value in values.items():
+            lowered = key.lower()
+            if lowered in set_DPOTrainer_metrics:
+                new_values[lowered.replace(""/"", "" / "")] = value
+            else:
+                new_values[key] = value
+        pass
+        values = new_values
+
         self.inner_table[0] = columns
         if len(self.inner_table) > 1:
             last_values = self.inner_table[-1]
@@ -104,7 +102,7 @@ pass
 
 def PatchDPOTrainer():
     # Patch DPO notebook printing
-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 06b78d7..1df1a3e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -912,14 +912,13 @@ class FastLlamaModel:
         assert(type(use_rslora) is bool)
         if use_rslora:
             if not SUPPORTS_RSLORA:
-                # We do it ourselves!
-                new_alpha = lora_alpha / (r**0.5)
+                # We manually check for PEFT
                 import peft
-                logger.warning_once(
-                    f""Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\n""\
-                    f""But, we do it ourselves by setting `alpha = {new_alpha}.`""
+                raise RuntimeError(
+                    f""Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\n""\
+                    ""Please install PEFT 0.7.2 or higher.\n""\
+                    ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
                 )
-                lora_alpha = new_alpha
             pass
         pass
 
diff --git a/unsloth/save.py b/unsloth/save.py
index 3f279df..543ffd8 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(
     self,
     repo_id              : str,
     tokenizer            = None,
-    save_method         : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
     use_temp_dir         : Optional[bool] = None,
     commit_message       : Optional[str] = None,
     private              : Optional[bool] = None,
@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(
     self,
     save_directory       : Union[str, os.PathLike],
     tokenizer            = None,
-    quantization_method         : str = ""fast_quantized"",
+    quantization_method  : str = ""fast_quantized"",
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
     is_main_process      : bool = True,
@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(
     arguments[""push_to_hub""]  = False # We save ourselves
     arguments[""save_method""] = ""merged_16bit"" # Must be 16bit
     del arguments[""self""]
-    del arguments[""quantization""]
+    del arguments[""quantization_method""]
 
     # Non blocking install GGUF first
     git_clone = install_llama_cpp_clone_non_blocking()
@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(
     self,
     repo_id              : str,
     tokenizer            = None,
-    quantization_method         : str = ""fast_quantized"",
+    quantization_method  : str = ""fast_quantized"",
     use_temp_dir         : Optional[bool] = None,
     commit_message       : Optional[str] = None,
     private              : Optional[bool] = None,
@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(
     arguments[""save_method""]   = ""merged_16bit"" # Must be 16bit
     del arguments[""self""]
     del arguments[""repo_id""]
-    del arguments[""quantization""]
+    del arguments[""quantization_method""]
 
     # Non blocking install GGUF first
     git_clone = install_llama_cpp_clone_non_blocking()
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index a1a39fd..4da08da 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -22,7 +22,7 @@ critical_modules = ['trl', 'transformers', 'peft']
 already_imported = [mod for mod in critical_modules if mod in sys.modules]
 
 # This check is critical because Unsloth optimizes these libraries by modifying
-# their code at import time. If they're imported first, the original (slower, 
+# their code at import time. If they're imported first, the original (slower,
 # more memory-intensive) implementations will be used instead of Unsloth's
 # optimized versions, potentially causing OOM errors or slower training.
 
@@ -73,6 +73,17 @@ def get_device_type():
 pass
 DEVICE_TYPE : str = get_device_type()
 
+def get_device_count():
+    if DEVICE_TYPE == ""cuda"":
+        return torch.cuda.device_count()
+    elif DEVICE_TYPE == ""xpu"":
+        return torch.xpu.device_count()
+    else:
+        return 0
+pass
+
+DEVICE_COUNT : int = get_device_count()
+
 # Reduce VRAM usage by reducing fragmentation
 # And optimize pinning of memory
 if DEVICE_TYPE == ""cuda"" and os.environ.get(""UNSLOTH_VLLM_STANDBY"", ""0"")==""0"":
@@ -237,4 +248,4 @@ from .tokenizer_utils import *
 from .trainer import *
 
 # Patch TRL trainers for backwards compatibility
-_patch_trl_trainer()
\ No newline at end of file
+_patch_trl_trainer()
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 1c65246..645319d 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -18,7 +18,7 @@ MAX_FUSED_SIZE : int = 65536
 next_power_of_2 = triton.next_power_of_2
 import functools
 from typing import Optional
-from unsloth import DEVICE_TYPE
+from unsloth import DEVICE_TYPE, DEVICE_COUNT
 
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
 import torch
@@ -89,10 +89,11 @@ else:
     get_ptr = bnb.functional.get_ptr
 
 
-if DEVICE_TYPE == ""cuda"" and torch.cuda.device_count() > 1:
-    torch_gpu_device = torch.cuda.device
-elif DEVICE_TYPE == ""xpu"" and torch.xpu.device_count() > 1:
-    torch_gpu_device = torch.xpu.device
+if DEVICE_COUNT > 1:
+    if DEVICE_TYPE == ""cuda"":
+        torch_gpu_device = torch.cuda.device
+    elif DEVICE_TYPE == ""xpu"":
+        torch_gpu_device = torch.xpu.device
 else:
     from contextlib import nullcontext
     def torch_gpu_device(device): return nullcontext()
@@ -100,7 +101,7 @@ else:
 
 # INTEL GPU Specific Logic
 if DEVICE_TYPE == ""xpu"":
-    _gpu_getCurrentRawStream = torch._C._xpu_getCurrentRawStream 
+    _gpu_getCurrentRawStream = torch._C._xpu_getCurrentRawStream
 # NVIDIA GPU Default Logic
 else:
     _gpu_getCurrentRawStream = torch._C._cuda_getCurrentRawStream
@@ -121,12 +122,12 @@ global ABSMAX_BUFFERS
 if DEVICE_TYPE == ""xpu"":
     _XPU_STREAMS = {
         (index := torch.xpu.device(i).idx) : ctypes.c_void_p(torch._C._xpu_getCurrentRawStream(index))
-        for i in range(torch.xpu.device_count())
+        for i in range(DEVICE_COUNT)
     }
     XPU_STREAMS   = [None] * (max(_XPU_STREAMS.keys()) + 1)
     WEIGHT_BUFFERS = [None] * (max(_XPU_STREAMS.keys()) + 1)
     ABSMAX_BUFFERS = [None] * (max(_XPU_STREAMS.keys()) + 1)
-    for k, v in _XPU_STREAMS.items(): 
+    for k, v in _XPU_STREAMS.items():
         XPU_STREAMS[k] = v
     XPU_STREAMS = tuple(XPU_STREAMS)
     del _XPU_STREAMS
@@ -134,7 +135,7 @@ else:
     # NVIDIA GPU Default Logic
     _CUDA_STREAMS = {
         (index := torch.cuda.device(i).idx) : ctypes.c_void_p(torch._C._cuda_getCurrentRawStream(index))
-        for i in range(torch.cuda.device_count())
+        for i in range(DEVICE_COUNT)
     }
     CUDA_STREAMS   = [None] * (max(_CUDA_STREAMS.keys()) + 1)
     WEIGHT_BUFFERS = [None] * (max(_CUDA_STREAMS.keys()) + 1)
@@ -152,16 +153,16 @@ if DEVICE_TYPE == ""xpu"":
     # TODO: After adding XPU BNB support, this function should be implemented
     def cdequantize_blockwise_fp32(*args, **kwargs):
         raise RuntimeError(""XPU BNB support is not implemented yet. cdequantize_blockwise_fp32 should not be called now."")
-    
+
     def cdequantize_blockwise_fp16_nf4(*args, **kwargs):
         raise RuntimeError(""XPU BNB support is not implemented yet. cdequantize_blockwise_fp16_nf4 should not be called now."")
-    
+
     def cdequantize_blockwise_bf16_nf4(*args, **kwargs):
         raise RuntimeError(""XPU BNB support is not implemented yet. cdequantize_blockwise_bf16_nf4 should not be called now."")
-    
+
     def cgemm_4bit_inference_naive_fp16(*args, **kwargs):
         raise RuntimeError(""XPU BNB support is not implemented yet. cgemm_4bit_inference_naive_fp16 should not be called now."")
-    
+
     def cgemm_4bit_inference_naive_bf16(*args, **kwargs):
         raise RuntimeError(""XPU BNB support is not implemented yet. cgemm_4bit_inference_naive_bf16 should not be called now."")
 else:
@@ -193,7 +194,7 @@ def get_lora_parameters(proj):
     adapter = getattr(proj, ""active_adapters"", None)
     if adapter is None: adapter = getattr(proj, ""active_adapter"", (""default""))
     adapter = adapter[0]
-    
+
     return (
         W,
         getattr(W, ""quant_state"", None),
@@ -232,7 +233,7 @@ pass
 if DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
     @torch.inference_mode
     def fast_dequantize(W, quant_state = None, out = None, use_global_buffer = False):
-        # TODO: After adding XPU BNB support, check this function 
+        # TODO: After adding XPU BNB support, check this function
         if quant_state is None: return W
         if type(quant_state) is not list:
             # New quant_state as a class
@@ -535,7 +536,7 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
         device = W.device
         device_index = device.index
         CUDA_STREAM = CUDA_STREAMS[device_index]
-        
+
         # assert(dtype == X.dtype)
         bout = shape[0]
 
@@ -669,7 +670,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
             lora_A._fast_lora = lora_A.to(dtype)
             lora_B._fast_lora = lora_B.to(dtype)
         pass
-        
+
         if bsz == 1:
             out = out.view(out_dim)
             temp_lora = torch_mv(lora_A._fast_lora, X.ravel(), out = temp_lora)
@@ -709,6 +710,6 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):
         out.addmm_(XA, B.to(dtype), alpha = s)
         # out += (X @ A.to(dtype)) @ (s * B.to(dtype))
     pass
-    
+
     return out.view(batch, seq_len, -1) if reshape else out
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index f576d17..c6ff7b6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -77,7 +77,7 @@ import contextlib
 import re
 import warnings, subprocess, re, inspect, psutil, os, math
 from unsloth_zoo.utils import Version
-from unsloth_zoo import DEVICE_TYPE
+from unsloth import DEVICE_TYPE, DEVICE_COUNT
 
 from unsloth_zoo.tokenizer_utils import (
     patch_tokenizer as _patch_tokenizer,
@@ -142,12 +142,6 @@ warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""
 import logging
 logging.getLogger(""transformers.tokenization_utils_base"").setLevel(logging.CRITICAL+1)
 
-def get_device_num():
-    if DEVICE_TYPE == ""xpu"":
-        return torch.xpu.device_count()
-    else:
-        return torch.cuda.device_count()
-
 # Ignore logging messages
 class HideLoggingMessage(logging.Filter):
     __slots__ = ""text"",
@@ -746,8 +740,7 @@ def get_statistics():
         pass
     pass
     try:
-        devices = get_device_num()
-        _get_statistics(f""{devices if devices <= 8 else 9}"")
+        _get_statistics(f""{DEVICE_COUNT if DEVICE_COUNT <= 8 else 9}"")
     except:
         pass
     if disabled: enable_progress_bars()
@@ -773,7 +766,7 @@ BitsAndBytesConfig__init__ = BitsAndBytesConfig__init__.replace(
 )
 exec(BitsAndBytesConfig__init__, globals())
 
-if get_device_num() == 1:
+if DEVICE_COUNT == 1:
     from accelerate.utils.dataclasses import DistributedType
     def _prepare_backend(self, *args, **kwargs): return None, DistributedType.NO
     import accelerate.state
@@ -781,6 +774,36 @@ if get_device_num() == 1:
     accelerate.accelerator.Accelerator.distributed_type = lambda *args, **kwargs: DistributedType.NO
 pass
 
+# to move multiple tensors to the same device
+def move_to_device(target_device, *tensors):
+    """"""
+    Move multiple tensors to target device if they're not already there.
+
+    Args:
+        target_device: The target device to move tensors to
+        *tensors: Variable number of tensors to potentially move
+
+    Returns:
+        tuple: The tensors on the target device (same objects if already on device, new if moved)
+    """"""
+    if isinstance(target_device, int):
+        target_device = torch.device(target_device)
+    elif isinstance(target_device, str):
+        # if string we expect it to be a device name like ""cuda:0""
+        target_device = torch.device(target_device)
+    elif isinstance(target_device, torch.device):
+        pass
+    else:
+        raise ValueError(f""Invalid target device: {target_device}"")
+    pass
+    moved_tensors = []
+    for tensor in tensors:
+        if tensor.device != target_device:
+            moved_tensors.append(tensor.to(target_device))
+        else:
+            moved_tensors.append(tensor)
+    return tuple(moved_tensors) if len(moved_tensors) > 1 else moved_tensors[0]
+
 import transformers.utils.quantization_config
 transformers.utils.quantization_config.BitsAndBytesConfig.__init__ = _BitsAndBytesConfig__init__
 # =============================================
diff --git a/unsloth/models/cohere.py b/unsloth/models/cohere.py
index bfa833b..d4691fb 100644
--- a/unsloth/models/cohere.py
+++ b/unsloth/models/cohere.py
@@ -78,7 +78,7 @@ def CohereAttention_fast_forward(
     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -254,6 +254,7 @@ def CohereAttention_fast_forward_inference(
     do_prefill = False,
     attention_mask = None,
 ):
+
     Xn = hidden_states
     bsz, _, hd = hidden_states.size()
     K1, V1 = past_key_value
@@ -281,14 +282,14 @@ def CohereAttention_fast_forward_inference(
         self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
-        
+
         # Mistral Nemo 12b has weird dimensions
         if attention_size != hidden_size:
             self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         else:
             self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
-        
+
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
@@ -320,7 +321,7 @@ def CohereAttention_fast_forward_inference(
 
     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
-    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len, Qn.device.index)
     cos = cos[position_ids].unsqueeze(1)
     sin = sin[position_ids].unsqueeze(1)
     h = self.half_head_dim
@@ -338,7 +339,7 @@ def CohereAttention_fast_forward_inference(
     torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
-    
+
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
     # Vn = torch.cat([V1, Vn], dim = 2)
@@ -397,7 +398,7 @@ def CohereModel_fast_forward_inference(
     position_ids,
     attention_mask = None,
 ):
-    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda:0"")
+    out_weights = tuple(torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = torch.device(x)) for x in range(DEVICE_COUNT))
     input_ids = input_ids[:,:self.max_seq_length]
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
@@ -417,8 +418,12 @@ def CohereModel_fast_forward_inference(
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.model.layers):
+        device_index = getattr(decoder_layer, ""_per_layer_device_index"", 0)
+        hidden_states, position_ids = move_to_device(
+            device_index, hidden_states, position_ids
+        )
         residual = hidden_states
-        hidden_states = fast_layernorm_inference(decoder_layer.input_layernorm, hidden_states, out_weight)
+        hidden_states = fast_layernorm_inference(decoder_layer.input_layernorm, hidden_states, out_weights[device_index])
         hidden_states_attention, present_key_value = CohereAttention_fast_forward_inference(
             decoder_layer.self_attn,
             hidden_states = hidden_states,
@@ -435,7 +440,7 @@ def CohereModel_fast_forward_inference(
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_layernorm_inference(self.model.norm, hidden_states, out_weight)
+    hidden_states = fast_layernorm_inference(self.model.norm, hidden_states, out_weights[device_index])
 
     return BaseModelOutputWithPast(
         last_hidden_state = hidden_states,
@@ -468,7 +473,7 @@ class FastCohereModel(FastLlamaModel):
         CohereForCausalLM    .forward = CausalLM_fast_forward(CohereModel_fast_forward_inference)
         PeftModelForCausalLM .forward = PeftModel_fast_forward
         fix_prepare_inputs_for_generation(CohereForCausalLM)
-        
+
         import transformers.models.cohere.modeling_cohere
         transformers.models.cohere.modeling_cohere.CohereRotaryEmbedding = LlamaRotaryEmbedding
         return
diff --git a/unsloth/models/falcon_h1.py b/unsloth/models/falcon_h1.py
index 8978e4d..2cbb78f 100644
--- a/unsloth/models/falcon_h1.py
+++ b/unsloth/models/falcon_h1.py
@@ -69,7 +69,7 @@ def FalconH1Attention_fast_forward(
     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -110,12 +110,13 @@ def FalconH1Attention_fast_forward(
         # Extend RoPE dynamically to fit in VRA
         rotary_emb = self.rotary_emb
         rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
+        device_index = Q.device.index
 
         if position_ids is None:
             # Useful for LongRoPE
-            cos, sin = rotary_emb.get_cached(kv_seq_len)
+            cos, sin = rotary_emb.get_cached(kv_seq_len, device_index)
         else:
-            cos, sin = rotary_emb(V, seq_len = kv_seq_len)
+            cos, sin = rotary_emb.get_cached(kv_seq_len, device_index)
     Q, K = fast_rope_embedding(Q, K, cos, sin)
 
     if past_key_value is not None:
@@ -245,14 +246,14 @@ def FalconH1Attention_fast_forward_inference(
         self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
-        
+
         # Mistral Nemo 12b has weird dimensions
         if attention_size != hidden_size:
             self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = device)
         else:
             self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
-        
+
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
@@ -280,7 +281,7 @@ def FalconH1Attention_fast_forward_inference(
     # Need to do it prior 2 steps before hitting full on short KV cache
     # or else error
     self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
-    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len, Qn.device.index)
     cos = cos[position_ids].unsqueeze(1)
     sin = sin[position_ids].unsqueeze(1)
     h = self.half_head_dim
@@ -298,7 +299,7 @@ def FalconH1Attention_fast_forward_inference(
     RH_K[:,:,:,:h].neg_() #torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
-    
+
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
     # Vn = torch.cat([V1, Vn], dim = 2)
@@ -580,7 +581,7 @@ def _fast_prepare_inputs_for_generation(
     **kwargs,):
     # Overwitten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`
     empty_past_kv = past_key_values is None
-    
+
     # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens
     # Exception 1: when passing input_embeds, input_ids may be missing entries
     # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 8ad1c7e..e43b205 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -149,7 +149,7 @@ def GemmaModel_fast_forward_inference(
     position_ids,
     attention_mask = None,
 ):
-    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda:0"")
+    out_weights = tuple(torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = torch.device(x)) for x in range(DEVICE_COUNT))
     input_ids = input_ids[:,:self.max_seq_length]
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
@@ -170,8 +170,13 @@ def GemmaModel_fast_forward_inference(
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.model.layers):
+        device_index = getattr(decoder_layer, ""_per_layer_device_index"", 0)
+        hidden_states, position_ids = move_to_device(
+            device_index, hidden_states, position_ids
+        )
+
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weights[device_index])
         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
             decoder_layer.self_attn,
             hidden_states = hidden_states,
@@ -183,13 +188,13 @@ def GemmaModel_fast_forward_inference(
         hidden_states += residual
 
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weights[device_index])
         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)
         hidden_states += residual
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)
+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weights[device_index])
 
     return BaseModelOutputWithPast(
         last_hidden_state = hidden_states,
@@ -224,9 +229,16 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
         self.base = base
         # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
         self.current_rope_size = min(4 * 8192, self.max_position_embeddings)
+        self.multi_gpu_cos_cached = [None]*DEVICE_COUNT
+        self.multi_gpu_sin_cached = [None]*DEVICE_COUNT
 
         # Build here to make `torch.jit.trace` work.
-        self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
+        for device in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(seq_len=self.current_rope_size, device=torch.device(device), dtype=torch.get_default_dtype())
+
+        # dummy so that patch_utils doesn't fail for now
+        self.cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -245,32 +257,38 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
 
         emb = torch.cat((radians_new, radians_new), dim = -1)
         # We must do RoPE in float32!
-        cos = emb.cos().to(device = ""cuda"", non_blocking = True)#, dtype = dtype)
-        sin = emb.sin().to(device = ""cuda"", non_blocking = True)#, dtype = dtype)
-        self.register_buffer(""cos_cached"", cos, persistent = False)
-        self.register_buffer(""sin_cached"", sin, persistent = False)
+        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)
+        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)
+        self.multi_gpu_cos_cached[device.index] = cos
+        self.multi_gpu_sin_cached[device.index] = sin
+        return cos, sin
     pass
 
     def forward(self, x, position_ids=None, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
-        if seq_len > self.current_rope_size:
+        if seq_len is not None and seq_len > self.current_rope_size:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
+        device_index = x.device.index
+
         return (
-            self.cos_cached[:seq_len].to(dtype=x.dtype),
-            self.sin_cached[:seq_len].to(dtype=x.dtype),
+            self.multi_gpu_cos_cached[device_index][:seq_len],
+            self.multi_gpu_sin_cached[device_index][:seq_len],
         )
     pass
 
-    def get_cached(self, seq_len = None):
-        return self.cos_cached, self.sin_cached
+    def get_cached(self, seq_len = None, device_index = None):
+        if device_index is None:
+            device_index = torch.cuda.current_device()
+        return self.multi_gpu_cos_cached[device_index], self.multi_gpu_sin_cached[device_index]
     pass
 
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
         self.current_rope_size = math.ceil(seq_len / 8192) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = ""cuda"", dtype = x.dtype)
+        for device in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(self.current_rope_size, device = torch.device(device), dtype = x.dtype)
     pass
 pass
 
@@ -288,7 +306,7 @@ class GemmaFixedLinearScalingRotaryEmbedding(GemmaFixedRotaryEmbedding):
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-# Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
         # in FP32. They are applied (multiplied) in FP32 as well.
         self.current_rope_size = seq_len
 
@@ -304,10 +322,11 @@ class GemmaFixedLinearScalingRotaryEmbedding(GemmaFixedRotaryEmbedding):
 
         emb = torch.cat((radians_new, radians_new), dim = -1)
         # We must do RoPE in float32!
-        cos = emb.cos().to(device = ""cuda"", non_blocking = True)#, dtype = dtype)
-        sin = emb.sin().to(device = ""cuda"", non_blocking = True)#, dtype = dtype)
-        self.register_buffer(""cos_cached"", cos, persistent = False)
-        self.register_buffer(""sin_cached"", sin, persistent = False)
+        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)
+        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)
+        self.multi_gpu_cos_cached[device.index] = cos
+        self.multi_gpu_sin_cached[device.index] = sin
+        return cos, sin
     pass
 pass
 
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 23b91ff..5597995 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -84,7 +84,7 @@ def Gemma2Attention_fast_forward(
     padding_mask:         Optional[torch.LongTensor] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -113,12 +113,13 @@ def Gemma2Attention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
+    device_index = Q.device.index
     if position_ids is None:
-        cos = self.rotary_emb.cos_cached
-        sin = self.rotary_emb.sin_cached
+        cos = self.rotary_emb.multi_gpu_cos_cached[device_index]
+        sin = self.rotary_emb.multi_gpu_sin_cached[device_index]
         Q, K = fast_rope_embedding(Q, K, cos, sin)
     else:
-        cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
+        cos, sin = self.rotary_emb.get_cached(kv_seq_len, device_index)
         Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
     pass
 
@@ -281,7 +282,7 @@ def Gemma2Attention_fast_forward_inference(
         # Only for Gemma2
         self.temp_O  = torch.empty((1, bsz, hidden_size), dtype = dtype, device = device)
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
-        
+
         # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
         # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
         # We default to using the config file itself
@@ -307,8 +308,9 @@ def Gemma2Attention_fast_forward_inference(
 
     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
-    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)
-    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len, Qn.device.index)
+    cos = cos[position_ids].unsqueeze(1)
+    sin = sin[position_ids].unsqueeze(1)
     h = self.half_head_dim
 
     RH_Q = self.RH_Q
@@ -324,7 +326,7 @@ def Gemma2Attention_fast_forward_inference(
     torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
-    
+
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
     # Vn = torch.cat([V1, Vn], dim = 2)
@@ -386,7 +388,7 @@ def Gemma2Model_fast_forward_inference(
     position_ids,
     attention_mask = None,
 ):
-    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda:0"")
+    out_weights = tuple(torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = torch.device(x)) for x in range(DEVICE_COUNT))
     input_ids = input_ids[:,:self.max_seq_length]
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
@@ -422,10 +424,17 @@ def Gemma2Model_fast_forward_inference(
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.model.layers):
 
+        # For pipeline parallelism, we need to move all tensors to the same device
+        # note that this movement is once per GPU in PP
+        device_index = getattr(decoder_layer, ""_per_layer_device_index"", 0)
+        hidden_states, position_ids = move_to_device(
+            device_index, hidden_states, position_ids
+        )
+
         use_sliding_window = idx % 2 == 0
 
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weights[device_index])
         hidden_states, present_key_value = Gemma2Attention_fast_forward_inference(
             decoder_layer.self_attn,
             hidden_states = hidden_states,
@@ -435,18 +444,18 @@ def Gemma2Model_fast_forward_inference(
             do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
             use_sliding_window = use_sliding_window,
         )
-        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weights[device_index])
         hidden_states += residual
 
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer. pre_feedforward_layernorm, hidden_states, out_weight)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer. pre_feedforward_layernorm, hidden_states, out_weights[device_index])
         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)
-        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_feedforward_layernorm, hidden_states, out_weight)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_feedforward_layernorm, hidden_states, out_weights[device_index])
         hidden_states += residual
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)
+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weights[device_index])
 
     return BaseModelOutputWithPast(
         last_hidden_state = hidden_states,
@@ -479,7 +488,7 @@ class FastGemma2Model(FastLlamaModel):
         Gemma2ForCausalLM    .forward = CausalLM_fast_forward(Gemma2Model_fast_forward_inference)
         PeftModelForCausalLM .forward = PeftModel_fast_forward
         fix_prepare_inputs_for_generation(Gemma2ForCausalLM)
-        
+
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
         # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
index 243922f..a3d79c8 100644
--- a/unsloth/models/granite.py
+++ b/unsloth/models/granite.py
@@ -71,7 +71,7 @@ def GraniteAttention_fast_forward(
     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -162,7 +162,7 @@ def GraniteAttention_fast_forward(
         # Go back to (batch_size, seq_len, n_heads, head_dim)
         A = A.transpose(1, 2).contiguous()
     pass
-    
+
     attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
     attn_output = self.apply_o(self, attn_output)
     attn_weights = None
@@ -256,7 +256,7 @@ def GraniteAttention_fast_forward_inference(
     use_sliding_window = False,
     position_embeddings : Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
 ):
-    
+
     assert position_embeddings is not None, f""Granite model requires position embeddings to be specified""
 
     Xn = hidden_states
@@ -326,7 +326,7 @@ def GraniteAttention_fast_forward_inference(
     torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
-    
+
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
     # Vn = torch.cat([V1, Vn], dim = 2)
@@ -349,7 +349,7 @@ def GraniteAttention_fast_forward_inference(
 
     Qn *= self.scaling
     A = torch_matmul(Qn, Kn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
-    
+
     # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
 
     A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
@@ -395,10 +395,14 @@ def GraniteModel_fast_forward_inference(
         attention_mask = None
     pass
 
-    position_embeddings = self.model.rotary_emb(hidden_states, position_ids, self.max_seq_length)
+    position_embeddings = self.model.rotary_emb.get_cached(self.max_seq_length, hidden_states.device.index)
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.model.layers):
+        device_index = getattr(decoder_layer, ""_per_layer_device_index"", 0)
+        hidden_states, position_ids = move_to_device(
+            device_index, hidden_states, position_ids
+        )
 
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)
@@ -532,7 +536,7 @@ class FastGraniteModel(FastLlamaModel):
 
                 elif hasattr(module, ""short_cos_cached"") and \
                     (module.short_cos_cached.dtype != correct_dtype):
-                    
+
                     module.short_cos_cached = module.short_cos_cached.to(correct_dtype)
                     module.short_sin_cached = module.short_sin_cached.to(correct_dtype)
                 pass
@@ -547,4 +551,3 @@ class FastGraniteModel(FastLlamaModel):
         return model, tokenizer
     pass
 pass
-
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 1253e64..db0e884 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -20,11 +20,12 @@ from typing import Optional, Tuple, List, Union
 from ._utils import *
 from ._utils import patch_unsloth_smart_gradient_checkpointing
 from ._utils import __version__
+from ._utils import move_to_device
 from torch.nn.functional import scaled_dot_product_attention
 from transformers import __version__ as transformers_version
 from unsloth_zoo.utils import Version, _get_dtype
 from unsloth_zoo.peft_utils import SKIP_QUANTIZATION_MODULES
-from unsloth import DEVICE_TYPE
+from unsloth import DEVICE_TYPE, DEVICE_COUNT
 
 transformers_version = Version(transformers_version)
 # Transformers moved rotary embeddings out of all attention layers
@@ -121,12 +122,12 @@ def _fast_prepare_inputs_for_generation(self, input_ids, attention_mask=None, **
         else:
             bs, cache_length = input_ids.shape
             input_ids = input_ids[:,[-1]]
-            
+
             # Get to the base model
             base_model = self
             if hasattr(base_model, 'base_model_prefix'):
                 base_model = getattr(base_model, base_model.base_model_prefix)
-                
+
             if hasattr(base_model, ""_prepare_4d_causal_attention_mask_with_cache_position""):
                 def needs_device_kw(fn) -> bool:
                     try:
@@ -243,14 +244,14 @@ def LlamaAttention_fast_forward_inference(
         self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
-        
+
         # Mistral Nemo 12b has weird dimensions
         if attention_size != hidden_size:
             self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = device)
         else:
             self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
-        
+
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
@@ -274,7 +275,7 @@ def LlamaAttention_fast_forward_inference(
     # Need to do it prior 2 steps before hitting full on short KV cache
     # or else error
     self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
-    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len, Qn.device.index)
     cos = cos[position_ids].unsqueeze(1)
     sin = sin[position_ids].unsqueeze(1)
     h = self.half_head_dim
@@ -292,7 +293,7 @@ def LlamaAttention_fast_forward_inference(
     RH_K[:,:,:,:h].neg_() #torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
-    
+
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
     # Vn = torch.cat([V1, Vn], dim = 2)
@@ -361,10 +362,10 @@ def fast_swiglu_inference(self, X, temp_gate = None, temp_up = None, gate_multip
     # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
 
     gate = fast_linear_forward(self.gate_proj, X, out = temp_gate)
-    
+
     if gate_multiplier is not None:
         gate *= gate_multiplier
-    
+
     up   = fast_linear_forward(self.  up_proj, X, out = temp_up)
 
     gate = torch_nn_functional_silu(gate, inplace = True)
@@ -447,7 +448,7 @@ def LlamaAttention_fast_forward(
     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -483,11 +484,12 @@ def LlamaAttention_fast_forward(
         rotary_emb = self.rotary_emb
         rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
 
-        if position_ids is None:
-            # Useful for LongRoPE
-            cos, sin = rotary_emb.get_cached(kv_seq_len)
-        else:
-            cos, sin = rotary_emb(V, seq_len = kv_seq_len)
+        # if position_ids is None:
+        #     # Useful for LongRoPE
+        #     cos, sin = rotary_emb.get_cached(kv_seq_len, device = Q.device)
+        # else:
+        #     cos, sin = rotary_emb.get_cached(seq_len = kv_seq_len, device = Q.device)
+        cos, sin = rotary_emb.get_cached(kv_seq_len, Q.device.index)
 
     # Q, K = (
     #     fast_rope_embedding(Q, K, cos, sin)
@@ -672,7 +674,7 @@ def LlamaModel_fast_forward(
     return_dict:          Optional[bool] = None,
     *args, **kwargs,
 ) -> Union[Tuple, BaseModelOutputWithPast]:
-    
+
     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
     assert(output_attentions is False)
     output_hidden_states = (
@@ -707,7 +709,7 @@ def LlamaModel_fast_forward(
             inputs_embeds = inputs_embeds[:,:self.max_seq_length,:]
         pass
     pass
-    
+
     past_key_values_length = 0
 
     if past_key_values is not None:
@@ -794,7 +796,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif self.training and os.environ.get(""UNSLOTH_KEEP_PADDING"", ""0"") != '1':    
+    elif self.training and os.environ.get(""UNSLOTH_KEEP_PADDING"", ""0"") != '1':
         attention_mask = None
         padding_mask = None
     else:
@@ -911,7 +913,7 @@ def LlamaModel_fast_forward(
         # Also, transformers 4.45.0 supports granite but with the attention refactor (it always had the refactor)
         # unsloth's check for granite too has ""version >= 4.45.0 (rightly so)"".
         # so let granite always use the attention refactor implementation.
-        position_embeddings = self.rotary_emb(hidden_states, position_ids, self.config.max_position_embeddings)
+        position_embeddings = self.rotary_emb.get_cached(self.config.max_position_embeddings, hidden_states.device.index)
     else:
         position_embeddings = None
 
@@ -1021,7 +1023,7 @@ def _LlamaModel_fast_forward_inference(attention_fast_forward_inference=LlamaAtt
         XX, XX2 = _XX[0], _XX[1]
         variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = f""{DEVICE_TYPE}:0"")
         temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = f""{DEVICE_TYPE}:0"")
-        temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
+        temp_gates, temp_ups = tuple(temp_mlp[0].to(torch.device(x)) for x in range(DEVICE_COUNT)), tuple(temp_mlp[1].to(torch.device(x)) for x in range(DEVICE_COUNT))
 
         seq_len = past_key_values[0][0].shape[-2]
         if bsz != 1:
@@ -1039,6 +1041,10 @@ def _LlamaModel_fast_forward_inference(attention_fast_forward_inference=LlamaAtt
         next_decoder_cache = []
 
         for idx, decoder_layer in enumerate(self.model.layers):
+            device_index = getattr(decoder_layer, ""_per_layer_device_index"", 0)
+            X, residual, position_ids = move_to_device(
+                device_index, X, residual, position_ids
+            )
             residual.copy_(X) # residual = X
             X = fast_rms_layernorm_inference(
                 decoder_layer.input_layernorm,
@@ -1068,8 +1074,8 @@ def _LlamaModel_fast_forward_inference(attention_fast_forward_inference=LlamaAtt
             X = mlp_fast_forward_inference(
                 decoder_layer.mlp,
                 X,
-                temp_gate = temp_gate,
-                temp_up = temp_up,
+                temp_gate = temp_gates[device_index],
+                temp_up = temp_ups[device_index],
             )
             X += residual
 
@@ -1154,7 +1160,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         logit_scaling     = getattr(self.config, ""logit_scale"", 0)
         dtype = lm_head.dtype
         num_logits_to_keep = max(num_logits_to_keep, logits_to_keep)
-        
+
         # Move items to same device as lm_head
         hidden_states = hidden_states.to(lm_head_device)
         if labels is not None: labels = labels.to(lm_head_device)
@@ -1181,7 +1187,7 @@ def CausalLM_fast_forward(fast_forward_inference):
             RETURN_LOGITS = os.environ.get(""UNSLOTH_RETURN_LOGITS"", ""0"") == ""1""
             # < 1024 Normal Unsloth uses less VRAM!
             if bsz*q_len <= 1024: RETURN_LOGITS = True
-            
+
             if not RETURN_LOGITS and HAS_CUT_CROSS_ENTROPY and labels is not None:
 
                 n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
@@ -1295,15 +1301,15 @@ def PeftModel_fast_forward(
     **kwargs,
 ):
     is_classification = ""Classification"" in str(type(self.base_model.model))
-    if is_classification: 
+    if is_classification:
         return self.base_model(
             input_ids = input_ids,
-            attention_mask = attention_mask, 
-            inputs_embeds = inputs_embeds, 
-            labels = labels, 
+            attention_mask = attention_mask,
+            inputs_embeds = inputs_embeds,
+            labels = labels,
             output_attentions = output_attentions,
-            output_hidden_states = output_hidden_states, 
-            return_dict = return_dict, 
+            output_hidden_states = output_hidden_states,
+            return_dict = return_dict,
             **kwargs,
         )
     else:
@@ -1351,9 +1357,16 @@ class LlamaRotaryEmbedding(torch.nn.Module):
         self.base = base
         # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
         self.current_rope_size = min(4 * 8192, self.max_position_embeddings)
+        self.multi_gpu_cos_cached = [None]*DEVICE_COUNT
+        self.multi_gpu_sin_cached = [None]*DEVICE_COUNT
 
         # Build here to make `torch.jit.trace` work.
-        self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
+        for device_idx in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(seq_len=self.current_rope_size, device=torch.device(device_idx), dtype=torch.get_default_dtype())
+
+        # dummy so that patch_utils doesn't fail for now
+        self.cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -1368,30 +1381,37 @@ class LlamaRotaryEmbedding(torch.nn.Module):
         freqs = torch.outer(t, inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
         emb = torch.cat((freqs, freqs), dim=-1)
-        self.register_buffer(""cos_cached"", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
-        self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+        cos = emb.cos().to(dtype=dtype, device=device, non_blocking=True)
+        sin = emb.sin().to(dtype=dtype, device=device, non_blocking=True)
+        self.multi_gpu_cos_cached[device.index] = cos
+        self.multi_gpu_sin_cached[device.index] = sin
+        return cos, sin
     pass
 
     def forward(self, x, position_ids=None, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
-        if seq_len > self.current_rope_size:
+        if seq_len is not None and seq_len > self.current_rope_size:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
+        device_index = x.device.index
         return (
-            self.cos_cached[:seq_len].to(dtype = x.dtype),
-            self.sin_cached[:seq_len].to(dtype = x.dtype),
+            self.multi_gpu_cos_cached[device_index][:seq_len],
+            self.multi_gpu_sin_cached[device_index][:seq_len],
         )
     pass
 
-    def get_cached(self, seq_len = None):
-        return self.cos_cached, self.sin_cached
+    def get_cached(self, seq_len = None, device_index = None):
+        if device_index is None:
+            device_index = torch.cuda.current_device()
+        return self.multi_gpu_cos_cached[device_index], self.multi_gpu_sin_cached[device_index]
     pass
 
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
         self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = DEVICE_TYPE, dtype = x.dtype)
+        for device_idx in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(self.current_rope_size, device = torch.device(device_idx), dtype = x.dtype)
     pass
 pass
 
@@ -1419,8 +1439,11 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
         freqs = torch.outer(t, inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
         emb = torch.cat((freqs, freqs), dim=-1)
-        self.register_buffer(""cos_cached"", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
-        self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+        cos = emb.cos().to(dtype=dtype, device=device, non_blocking=True)
+        sin = emb.sin().to(dtype=dtype, device=device, non_blocking=True)
+        self.multi_gpu_cos_cached[device.index] = cos
+        self.multi_gpu_sin_cached[device.index] = sin
+        return cos, sin
     pass
 pass
 
@@ -1446,6 +1469,8 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
         self.base = base
         # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
         self.current_rope_size = min(4 * 8192, self.max_position_embeddings)
+        self.multi_gpu_cos_cached = [None]*DEVICE_COUNT
+        self.multi_gpu_sin_cached = [None]*DEVICE_COUNT
 
         # Normal Llama-3 RoPE
         inv_freq = 1.0 / (
@@ -1455,21 +1480,54 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
         self.register_buffer(""inv_freq"", inv_freq, persistent = False)
 
         # Build here to make `torch.jit.trace` work.
-        self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
+        for device_idx in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(seq_len=self.current_rope_size, device=torch.device(device_idx), dtype=torch.get_default_dtype())
+
+        # dummy so that patch_utils doesn't fail for now
+        self.cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
         # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
         # in FP32. They are applied (multiplied) in FP32 as well.
         self.current_rope_size = seq_len
-        
+
         t = torch.arange(self.current_rope_size, device=self.inv_freq.device, dtype=torch.int64).float()
 
         freqs = torch.outer(t, self.inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
         emb = torch.cat((freqs, freqs), dim=-1)
-        self.register_buffer(""cos_cached"", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
-        self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+        cos = emb.cos().to(dtype=dtype, device=device, non_blocking=True)
+        sin = emb.sin().to(dtype=dtype, device=device, non_blocking=True)
+        self.multi_gpu_cos_cached[device.index] = cos
+        self.multi_gpu_sin_cached[device.index] = sin
+        return cos, sin
+    pass
+
+    def forward(self, x, position_ids=None, seq_len=None):
+        # x: [bs, num_attention_heads, seq_len, head_size]
+        if seq_len is not None and seq_len > self.current_rope_size:
+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
+        device_index = x.device.index
+        return (
+            self.multi_gpu_cos_cached[device_index][:seq_len],
+            self.multi_gpu_sin_cached[device_index][:seq_len],
+        )
+    pass
+
+    def get_cached(self, seq_len = None, device_index = None):
+        if device_index is None:
+            device_index = torch.cuda.current_device()
+        return self.multi_gpu_cos_cached[device_index], self.multi_gpu_sin_cached[device_index]
+    pass
+
+    def extend_rope_embedding(self, x, seq_len):
+        if seq_len <= self.current_rope_size: return
+        # Iteratively grow by increments of 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
+        for device_idx in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(self.current_rope_size, device = torch.device(device_idx), dtype = x.dtype)
     pass
 
     # From https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/api/model.py#L41
@@ -1497,28 +1555,6 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
                 new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)
         return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)
     pass
-
-    def forward(self, x, position_ids=None, seq_len=None):
-        # x: [bs, num_attention_heads, seq_len, head_size]
-        if seq_len > self.current_rope_size:
-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
-
-        return (
-            self.cos_cached[:seq_len].to(dtype = x.dtype),
-            self.sin_cached[:seq_len].to(dtype = x.dtype),
-        )
-    pass
-
-    def get_cached(self, seq_len = None):
-        return self.cos_cached, self.sin_cached
-    pass
-
-    def extend_rope_embedding(self, x, seq_len):
-        if seq_len <= self.current_rope_size: return
-        # Iteratively grow by increments of 8192
-        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = DEVICE_TYPE, dtype = x.dtype)
-    pass
 pass
 
 
@@ -1554,6 +1590,10 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
         self.base = base
         # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
         self.current_rope_size = min(original_max_position_embeddings, self.max_position_embeddings)
+        self.multi_gpu_short_cos_cached = [None]*DEVICE_COUNT
+        self.multi_gpu_short_sin_cached = [None]*DEVICE_COUNT
+        self.multi_gpu_long_cos_cached = [None]*DEVICE_COUNT
+        self.multi_gpu_long_sin_cached = [None]*DEVICE_COUNT
 
         # Long RoPE similar to RoPE except short sequences have 1 cos / sin
         # and long sequences have another cos / sin
@@ -1575,64 +1615,78 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
         # Short and long inv_freq
         self.register_buffer(""short_inv_freq"", short_inv_freq, persistent = False)
         self.register_buffer(""long_inv_freq"",  long_inv_freq,  persistent = False)
-        # Build here to make `torch.jit.trace` work.
-        # self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
 
-        # Short sequences
+        # Build here to make `torch.jit.trace` work.
+        # Initialize short sequences cache for all devices
         dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16
         t = torch.arange(original_max_position_embeddings, device=self.short_inv_freq.device, dtype=torch.int64).float()
         freqs = torch.outer(t, self.short_inv_freq)
         emb = torch.cat((freqs, freqs), dim=-1)
-        cos_cached = (emb.cos() * self.scaling_factor).to(dtype=dtype, device=device, non_blocking=True)
-        sin_cached = (emb.sin() * self.scaling_factor).to(dtype=dtype, device=device, non_blocking=True)
-        self.register_buffer(""short_cos_cached"", cos_cached, persistent=False)
-        self.register_buffer(""short_sin_cached"", sin_cached, persistent=False)
+
+        for device_idx in range(DEVICE_COUNT):
+            device_obj = torch.device(device_idx)
+            cos_cached = (emb.cos() * self.scaling_factor).to(dtype=dtype, device=device_obj, non_blocking=True)
+            sin_cached = (emb.sin() * self.scaling_factor).to(dtype=dtype, device=device_obj, non_blocking=True)
+            self.multi_gpu_short_cos_cached[device_idx] = cos_cached
+            self.multi_gpu_short_sin_cached[device_idx] = sin_cached
+
+        # dummy so that patch_utils doesn't fail for now
+        self.short_cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.short_sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.long_cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.long_sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
         # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
         # in FP32. They are applied (multiplied) in FP32 as well.
         self.current_rope_size = seq_len
-        
+
         t = torch.arange(self.current_rope_size, device=self.long_inv_freq.device, dtype=torch.int64).float()
         # Long sequences
         freqs = torch.outer(t, self.long_inv_freq)
         emb = torch.cat((freqs, freqs), dim=-1)
         cos_cached = (emb.cos() * self.scaling_factor).to(dtype=dtype, device=device, non_blocking=True)
         sin_cached = (emb.sin() * self.scaling_factor).to(dtype=dtype, device=device, non_blocking=True)
-        self.register_buffer(""long_cos_cached"", cos_cached, persistent=False)
-        self.register_buffer(""long_sin_cached"", sin_cached, persistent=False)
+        self.multi_gpu_long_cos_cached[device.index] = cos_cached
+        self.multi_gpu_long_sin_cached[device.index] = sin_cached
+        return cos_cached, sin_cached
     pass
 
     def forward(self, x, position_ids=None, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
-        if seq_len > self.current_rope_size:
+        if seq_len is not None and seq_len > self.current_rope_size:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
-        if seq_len < self.original_max_position_embeddings:
+        device_index = x.device.index
+
+        if seq_len is not None and seq_len < self.original_max_position_embeddings:
             return (
-                self.short_cos_cached[:seq_len].to(dtype = x.dtype),
-                self.short_sin_cached[:seq_len].to(dtype = x.dtype),
+                self.multi_gpu_short_cos_cached[device_index][:seq_len],
+                self.multi_gpu_short_sin_cached[device_index][:seq_len],
             )
         else:
             return (
-                self.long_cos_cached[:seq_len].to(dtype = x.dtype),
-                self.long_sin_cached[:seq_len].to(dtype = x.dtype),
+                self.multi_gpu_long_cos_cached[device_index][:seq_len],
+                self.multi_gpu_long_sin_cached[device_index][:seq_len],
             )
         pass
     pass
 
-    def get_cached(self, seq_len = None):
-        if seq_len < self.original_max_position_embeddings:
-            return self.short_cos_cached, self.short_sin_cached
-        return self.long_cos_cached, self.long_sin_cached
+    def get_cached(self, seq_len = None, device_index = None):
+        if device_index is None:
+            device_index = torch.cuda.current_device()
+        if seq_len is not None and seq_len < self.original_max_position_embeddings:
+            return self.multi_gpu_short_cos_cached[device_index], self.multi_gpu_short_sin_cached[device_index]
+        return self.multi_gpu_long_cos_cached[device_index], self.multi_gpu_long_sin_cached[device_index]
     pass
 
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
         self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = DEVICE_TYPE, dtype = x.dtype)
+        for device_idx in range(DEVICE_COUNT):
+            self._set_cos_sin_cache(self.current_rope_size, device = torch.device(device_idx), dtype = x.dtype)
     pass
 pass
 
@@ -1754,7 +1808,7 @@ class FastLlamaModel:
         max_lora_rank     = 16,
         disable_log_stats = False,
         unsloth_vllm_standby = False,
-        num_labels =  None, 
+        num_labels =  None,
         **kwargs,
     ):
         os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""0""
@@ -1787,7 +1841,6 @@ class FastLlamaModel:
             gpu_stats = torch.cuda.get_device_properties(0)
             gpu_version = torch.version.cuda
             gpu_stats_snippet = f""CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {gpu_version}.""
-            num_gpus = torch.cuda.device_count()
 
             from importlib.metadata import version as importlib_version
             try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
@@ -1795,7 +1848,6 @@ class FastLlamaModel:
         elif DEVICE_TYPE == ""xpu"":
             gpu_stats = torch.xpu.get_device_properties(0)
             gpu_version = torch.version.xpu
-            num_gpus = torch.xpu.device_count()
             gpu_stats_snippet = f""Intel Toolkit: {gpu_version}.""
 
             try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
@@ -1807,7 +1859,7 @@ class FastLlamaModel:
 
         statistics = \
         f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.{vllm_version}\n""\
-        f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {num_gpus}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+        f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {DEVICE_COUNT}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
         f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. {gpu_stats_snippet} Triton: {triton_version}\n""\
         f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
         f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
@@ -1827,7 +1879,7 @@ class FastLlamaModel:
         if old_hf_transfer != ""0"": os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
         model_patcher.pre_patch()
-        get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
+        get_statistics() # For debugging - we use a download counter to see if environments are not breaking
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
@@ -1914,7 +1966,7 @@ class FastLlamaModel:
 
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
-        
+
         if num_labels is not None:
             model = AutoModelForSequenceClassification.from_pretrained(
                 model_name,
@@ -2012,7 +2064,7 @@ class FastLlamaModel:
         except:
             raise RuntimeError('Unsloth: Unsuccessfully patched inner_training_loop')
         pass
-        
+
         import transformers.trainer
         items_in_trainer = dir(transformers.trainer)
         good_items = []
@@ -2362,7 +2414,7 @@ class FastLlamaModel:
                 )
                 loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)
             pass
-            
+
             if hasattr(model.config, ""quantization_config""):
                 raise ValueError(
                     ""Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\n""\
@@ -2492,7 +2544,7 @@ class FastLlamaModel:
 
         is_classification =  ""Classification"" in str(type(model))
         # Get LoRA
-        # 
+        #
 
         arguments = dict(
             r                   = r,
@@ -2518,7 +2570,7 @@ class FastLlamaModel:
         input_embeddings_device  = model.get_input_embeddings().weight.device
         if is_classification:
              output_embeddings_device = model.score.weight.device
-        else: 
+        else:
             output_embeddings_device = model.get_output_embeddings().weight.device
 
         if use_gradient_checkpointing == ""unsloth"":
@@ -2678,7 +2730,7 @@ class FastLlamaModel:
             # model.peft_config[active_adapter].revision = f""unsloth""
         pass
 
-        from transformers.trainer import Trainer 
+        from transformers.trainer import Trainer
         if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
             raise RuntimeError(""Unsloth: Unsuccessfully patched Trainer! Please file a bug report!"")
         pass
@@ -2812,7 +2864,7 @@ class FastLlamaModel:
             internal_model.max_seq_length = max_seq_length
             internal_model = internal_model.model
         pass
-        internal_model.max_seq_length = max_seq_length        
+        internal_model.max_seq_length = max_seq_length
 
         # Patch tokenizer to pad to the right
         internal_model = model
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index a3e07e3..68d4ba4 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -51,7 +51,7 @@ def MistralAttention_fast_forward(
     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -83,12 +83,10 @@ def MistralAttention_fast_forward(
     # Extend RoPE dynamically to fit in VRAM
     self.rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
 
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len, Q.device.index)
     if position_ids is None:
-        cos = self.rotary_emb.cos_cached
-        sin = self.rotary_emb.sin_cached
         Q, K = fast_rope_embedding(Q, K, cos, sin)
     else:
-        cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
         Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
     pass
 
@@ -162,7 +160,7 @@ def MistralAttention_fast_forward(
         # Go back to (batch_size, seq_len, n_heads, head_dim)
         A = A.transpose(1, 2).contiguous()
     pass
-    
+
     attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
     attn_output = self.apply_o(self, attn_output)
     attn_weights = None
@@ -201,7 +199,7 @@ def MistralForCausalLM_fast_forward(
                 causal_mask = xformers.attn_bias.BlockDiagonalCausalMask\
                     .from_seqlens([q_len]*bsz)\
                     .make_local_attention(window_size = sliding_window)
-        
+
         elif not HAS_XFORMERS and attention_mask is None:
             if sliding_window is None or sliding_window == ""null"" or sliding_window <= 0 or q_len <= sliding_window:
                 # Fully causal mask
@@ -212,10 +210,10 @@ def MistralForCausalLM_fast_forward(
                 # Sliding window attention
                 q_indices = torch.arange(q_len, device=input_ids.device).view(-1, 1)
                 k_indices = torch.arange(q_len, device=input_ids.device).view(1, -1)
-                
+
                 causal_bool_mask = k_indices <= q_indices
                 window_bool_mask = (q_indices - k_indices) < sliding_window
-                
+
                 mask = torch.where(causal_bool_mask & window_bool_mask, 0.0, -torch.inf)
                 attention_mask = mask[None, None, :, :].expand(bsz, 1, q_len, q_len)
 
@@ -258,7 +256,7 @@ def MistralForCausalLM_fast_forward(
     bsz, q_len, hd = hidden_states.shape
     lm_head = self.lm_head.weight
     lm_head_device = lm_head.device
-    
+
     # Move items to same device as lm_head
     hidden_states = hidden_states.to(lm_head_device)
     if labels is not None: labels = labels.to(lm_head_device)
@@ -301,7 +299,7 @@ def MistralForCausalLM_fast_forward(
             if not return_dict:
                 output = (logits,) + outputs[1:]
                 return (loss,) + output if loss is not None else output
-            
+
             output = CausalLMOutputWithPast(
                 loss = loss,
                 logits = EMPTY_LOGITS,
@@ -390,7 +388,7 @@ class FastMistralModel(FastLlamaModel):
         MistralForCausalLM    .forward = MistralForCausalLM_fast_forward
         PeftModelForCausalLM  .forward = PeftModel_fast_forward
         fix_prepare_inputs_for_generation(MistralForCausalLM)
-        
+
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
         # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
diff --git a/unsloth/models/qwen3.py b/unsloth/models/qwen3.py
index 80bd6ee..b20f22d 100644
--- a/unsloth/models/qwen3.py
+++ b/unsloth/models/qwen3.py
@@ -66,7 +66,7 @@ def Qwen3Attention_fast_forward(
     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-    
+
     # Clear inference
     if hasattr(self, ""paged_attention""):
         del self.paged_attention_K
@@ -111,12 +111,13 @@ def Qwen3Attention_fast_forward(
         # Extend RoPE dynamically to fit in VRA
         rotary_emb = self.rotary_emb
         rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
+        device_index = Q.device.index
 
         if position_ids is None:
             # Useful for LongRoPE
-            cos, sin = rotary_emb.get_cached(kv_seq_len)
+            cos, sin = rotary_emb.get_cached(kv_seq_len, device_index)
         else:
-            cos, sin = rotary_emb(V, seq_len = kv_seq_len)
+            cos, sin = rotary_emb.get_cached(kv_seq_len, device_index)
     Q, K = fast_rope_embedding(Q, K, cos, sin)
 
     if past_key_value is not None:
@@ -260,14 +261,14 @@ def Qwen3Attention_fast_forward_inference(
         self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
-        
+
         # Mistral Nemo 12b has weird dimensions
         if attention_size != hidden_size:
             self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = device)
         else:
             self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
-        
+
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
@@ -297,7 +298,7 @@ def Qwen3Attention_fast_forward_inference(
     # Need to do it prior 2 steps before hitting full on short KV cache
     # or else error
     self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
-    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len, Qn.device.index)
     cos = cos[position_ids].unsqueeze(1)
     sin = sin[position_ids].unsqueeze(1)
     h = self.half_head_dim
@@ -315,7 +316,7 @@ def Qwen3Attention_fast_forward_inference(
     RH_K[:,:,:,:h].neg_() #torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
-    
+
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
     # Vn = torch.cat([V1, Vn], dim = 2)
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 53c5497..a358594 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -61,7 +61,7 @@ except:
     # Old HF Hub versions <= 0.0.25
     from huggingface_hub.utils._token import get_token
 pass
-from unsloth import DEVICE_TYPE
+from unsloth import DEVICE_TYPE, DEVICE_COUNT
 
 __all__ = [
     ""FastBaseModel"",
@@ -281,7 +281,6 @@ class FastBaseModel:
             gpu_stats = torch.cuda.get_device_properties(0)
             gpu_version = torch.version.cuda
             gpu_stats_snippet = f""CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {gpu_version}.""
-            num_gpus = torch.cuda.device_count()
 
             from importlib.metadata import version as importlib_version
             try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
@@ -289,7 +288,6 @@ class FastBaseModel:
         elif DEVICE_TYPE == ""xpu"":
             gpu_stats = torch.xpu.get_device_properties(0)
             gpu_version = torch.version.xpu
-            num_gpus = torch.xpu.device_count()
             gpu_stats_snippet = f""Intel Toolkit: {gpu_version}.""
 
             # TODO: After adding vLLM support for XPU, changed this
@@ -306,11 +304,11 @@ class FastBaseModel:
 
         statistics = \
         f""==((====))==  Unsloth {__version__}: Fast {model_type_arch.title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
-        f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {num_gpus}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+        f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {DEVICE_COUNT}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
         f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. {gpu_stats_snippet} Triton: {triton_version}\n""\
         f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
         f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
-        
+
         print(statistics)
 
         # Warn about fast transfers
@@ -325,7 +323,7 @@ class FastBaseModel:
         pass
         if old_hf_transfer != ""0"": os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
-        get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
+        get_statistics() # For debugging - we use a download counter to see if environments are not breaking
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
@@ -604,7 +602,7 @@ class FastBaseModel:
         else:
             assert(type(target_modules) in (list, tuple,))
         pass
-        
+
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
@@ -678,7 +676,7 @@ class FastBaseModel:
             float32_mixed_precision    = float32_mixed_precision,
         )
 
-        from transformers.trainer import Trainer 
+        from transformers.trainer import Trainer
         if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"" and trust_remote_code == False:
             raise RuntimeError('Unsloth: Unsuccessfully patched inner_training_loop')
         pass
"
"diff --git a/.github/ISSUE_TEMPLATE/bug---issue.md b/.github/ISSUE_TEMPLATE/bug---issue.md
index 8868313..397d725 100644
--- a/.github/ISSUE_TEMPLATE/bug---issue.md
+++ b/.github/ISSUE_TEMPLATE/bug---issue.md
@@ -1,7 +1,7 @@
 ---
 name: Bug / Issue
 about: Bug / Issue
-title: ""[Bug]""
+title: ""[Bug] Please fill in your issue title here.""
 labels: bug
 assignees: ''
 
@@ -14,7 +14,8 @@ assignees: ''
 5. Which Unsloth version, TRL version, transformers version, PyTorch version?
 6. Which trainer? `SFTTrainer`, `GRPOTrainer` etc
 
- You can also ask via our Reddit page: https://www.reddit.com/r/unsloth/
 ```python
 Put Minimal code to reproduce error here ###Remove Hugging Face token###
 ```
+
+ You can also ask via our Reddit page: https://www.reddit.com/r/unsloth/
"
"diff --git a/README.md b/README.md
index eabd938..b477419 100644
--- a/README.md
+++ b/README.md
@@ -469,6 +469,18 @@ Two Tesla T4s on Kaggle
 ![](https://i.ibb.co/sJ7RhGG/image-41.png)
 <br>
 
+### Citing
+
+You can cite the Unsloth repo as follows:
+```bibtex
+@software{unsloth,
+  author = {Daniel Han, Michael Han and Unsloth team},
+  title = {Unsloth},
+  url = {http://github.com/unslothai/unsloth},
+  year = {2023}
+}
+```
+
 ### Thank You to
 - [HuyNguyen-hust](https://github.com/HuyNguyen-hust) for making [RoPE Embeddings 28% faster](https://github.com/unslothai/unsloth/pull/238)
 - [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index 9c032b2..c52d14f 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+
+from .granite import FastGraniteModel
 from .loader  import FastLanguageModel, FastVisionModel
 from .llama   import FastLlamaModel
 from .mistral import FastMistralModel
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 11423f9..8e9f09d 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.2""
+__version__ = ""2024.12.3""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -188,7 +188,7 @@ pass
 
 from transformers import __version__ as transformers_version
 from transformers import PretrainedConfig
-model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"",]
+model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"", ""granite""]
 
 for model_name in model_architectures:
     config_filepath = f""transformers.models.{model_name}.configuration_{model_name}""
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 62ecb96..e47a743 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -193,7 +193,7 @@ def Gemma2DecoderLayer_fast_forward(
             output_attentions=output_attentions,
             use_cache=use_cache,
             padding_mask=padding_mask,
-            _flag_for_generation=True,
+            _flag_for_generation=self._flag_for_generation,
         )
         hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)
         hidden_states += residual
diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
new file mode 100644
index 0000000..2229636
--- /dev/null
+++ b/unsloth/models/granite.py
@@ -0,0 +1,523 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .llama import *
+import os
+from ._utils import __version__
+from .llama import (
+    LlamaRotaryEmbedding,
+    LlamaLinearScalingRotaryEmbedding,
+)
+from .mistral import *
+
+try:
+    from transformers.models.granite.modeling_granite import (
+        GraniteAttention,
+        GraniteDecoderLayer,
+        GraniteModel,
+        GraniteForCausalLM,
+    )
+except:
+    from packaging.version import Version
+
+    transformers_version = Version(transformers_version)
+    if not transformers_version >= Version(""4.45.0""):
+        raise ImportError(
+            f""Unsloth: Your transformers version of {transformers_version} does not support Gemma2.\n""\
+            f""The minimum required version is 4.42.3.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.42.3""`\n'\
+            f""to obtain the latest transformers build, then restart this session.""\
+        )
+    pass
+pass
+
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
+
+# For Pytorch 2.1.1
+try:
+    from transformers.models.granite.modeling_granite import (
+        GraniteSdpaAttention,
+        GraniteFlashAttention2,
+    )
+except:
+    GraniteSdpaAttention   = GraniteAttention
+    GraniteFlashAttention2 = GraniteAttention
+pass
+
+def GraniteAttention_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    bool = False,
+    use_cache:            bool = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+    *args, **kwargs,
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+    
+    # Clear inference
+    if hasattr(self, ""paged_attention""):
+        del self.paged_attention_K
+        del self.paged_attention_V
+        del self.paged_attention
+        del self.temp_QA
+        del self.temp_KV
+        del self.RH_Q
+        del self.attention
+    pass
+
+    bsz, q_len, _ = hidden_states.size()
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    assert(n_kv_heads * n_groups == n_heads)
+
+    Q, K, V = self.apply_qkv(self, hidden_states)
+    Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)
+    K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+    V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+
+    kv_seq_len = K.shape[-2]
+    if past_key_value is not None:
+        kv_seq_len += past_key_value[0].shape[-2]
+
+    assert position_embeddings is not None
+    cos, sin = position_embeddings
+    if position_ids is None:
+        Q, K = fast_rope_embedding(Q, K, cos, sin)
+    else:
+        Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
+
+    if past_key_value is not None:
+        K = torch.cat([past_key_value[0], K], dim = 2)
+        V = torch.cat([past_key_value[1], V], dim = 2)
+    pass
+    past_key_value = (K, V) if use_cache else None
+
+    # Attention module
+    if (not HAS_FLASH_ATTENTION and attention_mask is None):
+        # Xformers memory efficient attention
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+        K_M = V_M = bsz * kv_seq_len
+        Q_M = bsz * q_len
+
+        # Group query attention
+        K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+        V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+        K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+        V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+        if hidden_states.requires_grad:
+            K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)
+            V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)
+        else:
+            # Xformers does support the forward pass though
+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        pass
+
+        A = xformers_attention(Q, K, V, attn_bias = causal_mask, scale=self.scaling)
+        A = A.view(bsz, q_len, n_heads, head_dim)
+
+    elif HAS_FLASH_ATTENTION and attention_mask is None:
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+        window = (kv_seq_len, kv_seq_len)
+        A = flash_attn_func(Q, K, V, causal = True, window_size = window, softmax_scale=self.scaling)
+    else:
+        # Grouped query attention
+        # if n_groups != 1:
+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+        K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
+        V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
+        # pass
+        # Must be contiguous or else results are False!
+        # https://github.com/pytorch/pytorch/issues/112577
+        Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
+        # Needs (batch_size, n_heads, seq_len, head_dim)
+        # is_casual and attention_mask must not be both set!
+        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, scale = self.scaling, is_causal = False)
+        # Go back to (batch_size, seq_len, n_heads, head_dim)
+        A = A.transpose(1, 2).contiguous()
+    pass
+    
+    attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
+    attn_output = self.apply_o(self, attn_output)
+    attn_weights = None
+    return attn_output, attn_weights, past_key_value
+pass
+
+
+def GraniteDecoderLayer_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    Optional[bool] = False,
+    use_cache:            Optional[bool] = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    position_embeddings:  Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+    *args, **kwargs,
+):
+    if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+            position_embeddings = position_embeddings,
+            _flag_for_generation=self._flag_for_generation,
+        )
+        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_swiglu_inference(self.mlp, hidden_states)
+        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+    else:
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+            position_embeddings = position_embeddings,
+        )
+        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+    pass
+
+    outputs = (hidden_states,)
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
+    return outputs
+pass
+
+
+from math import sqrt as math_sqrt
+KV_CACHE_INCREMENT = 256 # KV Cache update size
+torch_nn_functional_softmax = torch.nn.functional.softmax
+torch_matmul = torch.matmul
+torch_tanh   = torch.tanh
+
+def GraniteAttention_fast_forward_inference(
+    self,
+    hidden_states:  torch.Tensor,
+    past_key_value: Optional[Tuple[torch.Tensor]],
+    position_ids,
+    do_prefill = False,
+    attention_mask = None,
+    use_sliding_window = False,
+    position_embeddings : Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+):
+    
+    assert position_embeddings is not None, f""Granite model requires position embeddings to be specified""
+
+    Xn = hidden_states
+    bsz, _, hd = hidden_states.size()
+    K1, V1 = past_key_value
+    dtype = Xn.dtype
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    attention_size = n_heads*head_dim
+    # assert(n_kv_heads * n_groups == n_heads)
+    seq_len = K1.shape[-2]
+    kv_seq_len = seq_len + 1
+
+    # Prefill phase
+    # if not hasattr(self, ""paged_attention""):
+    if do_prefill:
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = ""cuda:0"")
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
+        self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+        # Only for Gemma2
+        self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
+
+
+        self.half_head_dim = head_dim // 2
+    elif kv_seq_len >= self.paged_attention.shape[0]:
+        self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.attention.resize_((bsz, n_heads, 1, self.attention.shape[-1]+KV_CACHE_INCREMENT))
+    pass
+
+    Qn = fast_linear_forward(self.q_proj, Xn, out = self.temp_QA[0])
+    Kn = fast_linear_forward(self.k_proj, Xn, out = self.temp_KV[0])
+    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])
+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)
+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+
+    # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
+    # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+    cos, sin = position_embeddings
+    cos, sin = cos[position_ids], sin[position_ids]
+    h = self.half_head_dim
+
+    RH_Q = self.RH_Q
+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    Qn *= cos
+    Qn.addcmul_(RH_Q, sin)
+
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]
+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]
+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    Kn *= cos
+    Kn.addcmul_(RH_K, sin)
+    
+    # New KV cache
+    # Kn = torch.cat([K1, Kn], dim = 2)
+    # Vn = torch.cat([V1, Vn], dim = 2)
+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+
+    # Grouped query attention
+    _, _, cached_len, _ = Kn.shape
+    if n_groups != 1:
+        Kn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Kn = Kn.reshape(bsz, n_heads, cached_len, head_dim)
+        Vn = Vn.reshape(bsz, n_heads, cached_len, head_dim)
+    pass
+    # else:
+    #     Kn, Vn = Kn, Vn
+    # pass
+
+    Qn *= self.scaling
+    A = torch_matmul(Qn, Kn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+    
+    # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
+
+    A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
+    A = torch_matmul(A, Vn, out = Qn)
+    # else:
+    #     A = scaled_dot_product_attention(Qn, Kn, Vn, attn_mask = attention_mask, is_causal = False)
+    # pass
+    A = A.transpose(1, 2)
+    A = A.reshape(bsz, 1, attention_size)
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_O)
+    return A, (Kn, Vn)
+pass
+
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
+# @torch.inference_mode
+def GraniteModel_fast_forward_inference(
+    self,
+    input_ids,
+    past_key_values,
+    position_ids,
+    attention_mask = None,
+):
+    input_ids = input_ids[:,:self.max_seq_length]
+    hidden_states = self.model.embed_tokens(input_ids)
+    hidden_states = hidden_states.to(self.config.torch_dtype)
+    hidden_states *= self.model.embedding_multiplier
+
+    bsz, q_len, hd = hidden_states.shape
+    seq_len = past_key_values[0][0].shape[-2]
+    if bsz != 1:
+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+        )
+    else:
+        attention_mask = None
+    pass
+
+    position_embeddings = self.model.rotary_emb(hidden_states, position_ids, self.max_seq_length)
+
+    next_decoder_cache = []
+    for idx, decoder_layer in enumerate(self.model.layers):
+
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)
+        hidden_states, present_key_value = GraniteAttention_fast_forward_inference(
+            decoder_layer.self_attn,
+            hidden_states = hidden_states,
+            past_key_value = past_key_values[idx],
+            position_ids = position_ids,
+            attention_mask = attention_mask,
+            do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
+            position_embeddings = position_embeddings,
+        )
+
+        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
+        hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)
+        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+
+        next_decoder_cache.append(present_key_value)
+    pass
+    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)
+
+    return BaseModelOutputWithPast(
+        last_hidden_state = hidden_states,
+        past_key_values = next_decoder_cache,
+        hidden_states = [],
+        attentions = [],
+    )
+pass
+
+class GraniteRotaryEmbedding(LlamaRotaryEmbedding):
+    def __init__(self, config):
+        super().__init__(config = config)
+
+class FastGraniteModel(FastLlamaModel):
+
+    @staticmethod
+    def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""granite"",
+            rope_module        = GraniteRotaryEmbedding,
+            scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
+            attention_module   = GraniteAttention,
+        )
+        if init_name is not None:
+            exec(function, globals())
+            GraniteAttention.__init__  = eval(init_name)
+        pass
+        GraniteAttention      .forward = GraniteAttention_fast_forward
+        GraniteSdpaAttention  .forward = GraniteAttention_fast_forward
+        GraniteFlashAttention2.forward = GraniteAttention_fast_forward
+        GraniteDecoderLayer   .forward = GraniteDecoderLayer_fast_forward
+        GraniteModel          .forward = LlamaModel_fast_forward
+        GraniteForCausalLM    .forward = CausalLM_fast_forward(GraniteModel_fast_forward_inference)
+        PeftModelForCausalLM .forward = PeftModelForCausalLM_fast_forward
+        fix_prepare_inputs_for_generation(GraniteForCausalLM)
+
+        import transformers.models.granite.modeling_granite
+        transformers.models.granite.modeling_granite.GraniteRotaryEmbedding = GraniteRotaryEmbedding
+
+        return
+    pass
+
+
+    @staticmethod
+    def post_patch(model):
+
+        # Torch.compile fails on embedding matrix??
+        # Workaround randomnly fixes it for torch versions < 2.2
+        model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)
+        model.config.update({""unsloth_version"" : __version__})
+
+        # We also do this for the lm_head
+        lm_head = torch.nn.Linear(1, 1, bias = None)
+        del lm_head.weight
+        lm_head.weight = model.lm_head.weight
+        lm_head.in_features  = lm_head.weight.shape[1]
+        lm_head.out_features = lm_head.weight.shape[0]
+        model.lm_head = lm_head
+
+        # Granite has tied weights! This means lm_head == embed_tokens
+        if model.model.embed_tokens.weight.data_ptr() != model.lm_head.weight.data_ptr():
+            lm_head = torch.nn.Linear(1, 1, bias = None)
+            del lm_head.weight
+            lm_head.weight = model.model.embed_tokens.weight
+            lm_head.in_features  = lm_head.weight.shape[1]
+            lm_head.out_features = lm_head.weight.shape[0]
+            model.lm_head = lm_head
+        pass
+
+        # Also patch all dtypes - BnB seems to not allocate the correct type?
+        # BnB default dtype seems to be float16!
+        correct_dtype = lm_head.weight.dtype
+
+        for name, module in model.named_modules():
+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
+                weight = module.weight
+                quant_state = weight.quant_state
+
+                if type(quant_state) is list:
+                    # BnB seems to have float16 as default!
+                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
+                else:
+                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+                    quant_state.dtype = correct_dtype
+                pass
+            pass
+            # Downcast RoPE embedding to correct data type
+            if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")):
+
+                if hasattr(module, ""cos_cached"") and \
+                    (module.cos_cached.dtype != correct_dtype):
+
+                    module.cos_cached = module.cos_cached.to(correct_dtype)
+                    module.sin_cached = module.sin_cached.to(correct_dtype)
+
+                elif hasattr(module, ""short_cos_cached"") and \
+                    (module.short_cos_cached.dtype != correct_dtype):
+                    
+                    module.short_cos_cached = module.short_cos_cached.to(correct_dtype)
+                    module.short_sin_cached = module.short_sin_cached.to(correct_dtype)
+                pass
+            pass
+        pass
+
+        # Clear deleted GPU items
+        import gc
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        return model
+    pass
+pass
+
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 1bffb0c..cc41a8b 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -616,9 +616,10 @@ def LlamaModel_fast_forward(
     pass
 
     # Normalized from Gemma
-    IS_GEMMA  = self.config.model_type.startswith(""gemma"")
-    IS_GEMMA2 = self.config.model_type.startswith(""gemma2"")
-    IS_COHERE = self.config.model_type.startswith(""cohere"")
+    IS_GEMMA   = self.config.model_type.startswith(""gemma"")
+    IS_GEMMA2  = self.config.model_type.startswith(""gemma2"")
+    IS_COHERE  = self.config.model_type.startswith(""cohere"")
+    IS_GRANITE = self.config.model_type.startswith(""granite"")
     train_embed_tokens = self.embed_tokens.weight.requires_grad
 
     if IS_GEMMA:
@@ -684,6 +685,8 @@ def LlamaModel_fast_forward(
     pass
 
     hidden_states = inputs_embeds
+    if IS_GRANITE: #granite has embedding multiplier
+        hidden_states = self.embedding_multiplier * hidden_states
 
     if past_key_values is None and self.training:
         use_cache = False
@@ -773,6 +776,12 @@ def LlamaModel_fast_forward(
         pass
     pass
 
+    
+    if IS_GRANITE:
+        position_embeddings = self.rotary_emb(hidden_states, position_ids, self.max_position_embeddings)
+    else:
+        position_embeddings = None
+
     # Go through every layer!
     for idx, decoder_layer in enumerate(self.layers):
 
@@ -797,12 +806,14 @@ def LlamaModel_fast_forward(
                 past_key_values,
                 output_attentions,
                 use_cache,
+                None,
+                position_embeddings,
             )[0]
 
         elif gradient_checkpointing:
             def create_custom_forward(module):
                 def custom_forward(*inputs):
-                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)
+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask, position_embeddings = position_embeddings)
                 return custom_forward
             pass
 
@@ -827,6 +838,7 @@ def LlamaModel_fast_forward(
                 output_attentions=output_attentions,
                 use_cache=use_cache,
                 padding_mask=padding_mask,
+                position_embeddings = position_embeddings
             )
             hidden_states = layer_outputs[0]
         pass
@@ -1014,6 +1026,15 @@ def CausalLM_fast_forward(fast_forward_inference):
         pass
 
         loss = None
+        logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
+        logit_scaling     = getattr(self.config, ""logit_scale"", 0)
+        if self.config.model_type == ""granite"":
+            # granite uses logit_scaling as key and they divide by the scale unlike cohere
+            # notice that for granite, logits_scale is 16 and for cohere it is 0.125 (aka 1/8) in their respective configs
+            # granite: https://github.com/huggingface/transformers/blob/4d1d0f29a493098e6bc6b904b82e29cb331827f5/src/transformers/models/granite/modeling_granite.py#L1103
+            # cohere: https://github.com/huggingface/transformers/blob/4d1d0f29a493098e6bc6b904b82e29cb331827f5/src/transformers/models/cohere/modeling_cohere.py#L1176
+            logit_scaling = 1 / getattr(self.config, ""logits_scaling"", 1)
+
         if labels is not None:
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
@@ -2245,6 +2266,7 @@ class FastLlamaModel:
         elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu_approx
         elif model_type == ""gemma2"":  apply_lora_mlp = apply_lora_mlp_geglu_approx
         elif model_type == ""cohere"":  apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""granite"": apply_lora_mlp = apply_lora_mlp_swiglu
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
         pass
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index f8ed3a8..3b2c8ff 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 from ._utils import is_bfloat16_supported, HAS_FLASH_ATTENTION, HAS_FLASH_ATTENTION_SOFTCAPPING
+from .granite import FastGraniteModel
 from .llama   import FastLlamaModel, logger
 from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
@@ -38,6 +39,7 @@ SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
 SUPPORTS_GEMMA2  = transformers_version >= Version(""4.42"")
 SUPPORTS_LLAMA31 = transformers_version >= Version(""4.43.2"")
 SUPPORTS_LLAMA32 = transformers_version  > Version(""4.45.0"")
+SUPPORTS_GRANITE = transformers_version >= Version(""4.46.0"")
 if SUPPORTS_GEMMA:
     from .gemma  import FastGemmaModel
 if SUPPORTS_GEMMA2:
@@ -175,7 +177,7 @@ class FastLanguageModel(FastLlamaModel):
 
         model_type = model_config.model_type
 
-        if   model_type == ""llama"":
+        if model_type == ""llama"":
             scaling_type = None
             if getattr(model_config, ""rope_scaling"", None) is not None:
                 scaling_type1 = model_config.rope_scaling.get(""type"", None)
@@ -231,6 +233,8 @@ class FastLanguageModel(FastLlamaModel):
             dispatch_model = FastQwen2Model
         elif model_type == ""cohere"":
             dispatch_model = FastCohereModel
+        elif model_type == ""granite"":
+            dispatch_model = FastGraniteModel
         else:
             raise NotImplementedError(
                 f""Unsloth: {model_name} not supported yet!\n""\
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 0ba03ce..41f7444 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -516,6 +516,10 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/QwQ-32B-Preview"",
         ""Qwen/QwQ-32B-Preview"",
     ),
+    ""unsloth/Llama-3.3-70B-Instruct-bnb-4bit"" : (
+        ""unsloth/Llama-3.3-70B-Instruct"",
+        ""meta-llama/Llama-3.3-70B-Instruct"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 0f682c6..a8ef9c0 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -164,6 +164,8 @@ class FastBaseVisionModel:
             padding_side = ""right"",
             token        = token,
         )
+        # Add padding side as well
+        tokenizer.tokenizer.padding_side = ""right""
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = post_patch_loss_function(model)
@@ -187,6 +189,7 @@ class FastBaseVisionModel:
 
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
+        tokenizer.tokenizer.padding_side = ""left"" # Force inference
         internal_model = model
         while hasattr(internal_model, ""model""):
             internal_model._saved_temp_tokenizer = tokenizer
@@ -315,12 +318,12 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""right""
+                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""right""
+            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
 
         # Clear deleted GPU items
@@ -361,12 +364,12 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""left""
+                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
             pass
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""left""
+            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
         pass
 
         # Also disable training for embeddings for NEFTune
@@ -405,12 +408,12 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""right""
+                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""right""
+            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
 
         # Also re-enable training for embeddings for NEFTune
"
"diff --git a/README.md b/README.md
index f8c7bd0..1f85647 100644
--- a/README.md
+++ b/README.md
@@ -232,10 +232,8 @@ print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://git
 
 ```python
 from unsloth import FastLanguageModel 
-from unsloth import is_bfloat16_supported
 import torch
-from trl import SFTTrainer
-from transformers import TrainingArguments
+from trl import SFTTrainer, SFTConfig
 from datasets import load_dataset
 max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
 # Get LAION dataset
@@ -244,21 +242,28 @@ dataset = load_dataset(""json"", data_files = {""train"" : url}, split = ""train"")
 
 # 4bit pre quantized models we support for 4x faster downloading + no OOMs.
 fourbit_models = [
-    ""unsloth/mistral-7b-v0.3-bnb-4bit"",      # New Mistral v3 2x faster!
+    ""unsloth/Meta-Llama-3.1-8B-bnb-4bit"",      # Llama-3.1 2x faster
+    ""unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit"",
+    ""unsloth/Meta-Llama-3.1-70B-bnb-4bit"",
+    ""unsloth/Meta-Llama-3.1-405B-bnb-4bit"",    # 4bit for 405b!
+    ""unsloth/Mistral-Small-Instruct-2409"",     # Mistral 22b 2x faster!
     ""unsloth/mistral-7b-instruct-v0.3-bnb-4bit"",
-    ""unsloth/llama-3-8b-bnb-4bit"",           # Llama-3 15 trillion tokens model 2x faster!
-    ""unsloth/llama-3-8b-Instruct-bnb-4bit"",
-    ""unsloth/llama-3-70b-bnb-4bit"",
-    ""unsloth/Phi-3-mini-4k-instruct"",        # Phi-3 2x faster!
+    ""unsloth/Phi-3.5-mini-instruct"",           # Phi-3.5 2x faster!
     ""unsloth/Phi-3-medium-4k-instruct"",
-    ""unsloth/mistral-7b-bnb-4bit"",
-    ""unsloth/gemma-7b-bnb-4bit"",             # Gemma 2.2x faster!
+    ""unsloth/gemma-2-9b-bnb-4bit"",
+    ""unsloth/gemma-2-27b-bnb-4bit"",            # Gemma 2x faster!
+
+    ""unsloth/Llama-3.2-1B-bnb-4bit"",           # NEW! Llama 3.2 models
+    ""unsloth/Llama-3.2-1B-Instruct-bnb-4bit"",
+    ""unsloth/Llama-3.2-3B-bnb-4bit"",
+    ""unsloth/Llama-3.2-3B-Instruct-bnb-4bit"",
+
+    ""unsloth/Llama-3.3-70B-Instruct-bnb-4bit"" # NEW! Llama 3.3 70B!
 ] # More models at https://huggingface.co/unsloth
 
 model, tokenizer = FastLanguageModel.from_pretrained(
-    model_name = ""unsloth/llama-3-8b-bnb-4bit"",
+    model_name = ""unsloth/Llama-3.2-1B"",
     max_seq_length = max_seq_length,
-    dtype = None,
     load_in_4bit = True,
 )
 
@@ -282,16 +287,14 @@ model = FastLanguageModel.get_peft_model(
 trainer = SFTTrainer(
     model = model,
     train_dataset = dataset,
-    dataset_text_field = ""text"",
-    max_seq_length = max_seq_length,
     tokenizer = tokenizer,
-    args = TrainingArguments(
+    args = SFTConfig(
+        dataset_text_field = ""text"",
+        max_seq_length = max_seq_length,
         per_device_train_batch_size = 2,
         gradient_accumulation_steps = 4,
         warmup_steps = 10,
         max_steps = 60,
-        fp16 = not is_bfloat16_supported(),
-        bf16 = is_bfloat16_supported(),
         logging_steps = 1,
         output_dir = ""outputs"",
         optim = ""adamw_8bit"",
@@ -323,17 +326,14 @@ RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth.
 import os
 os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"" # Optional set GPU device ID
 
-from unsloth import FastLanguageModel, PatchDPOTrainer
-from unsloth import is_bfloat16_supported
-PatchDPOTrainer()
+from unsloth import FastLanguageModel
 import torch
-from transformers import TrainingArguments
-from trl import DPOTrainer
+from trl import DPOTrainer, DPOConfig
+max_seq_length = 2048
 
 model, tokenizer = FastLanguageModel.from_pretrained(
     model_name = ""unsloth/zephyr-sft-bnb-4bit"",
     max_seq_length = max_seq_length,
-    dtype = None,
     load_in_4bit = True,
 )
 
@@ -355,24 +355,22 @@ model = FastLanguageModel.get_peft_model(
 dpo_trainer = DPOTrainer(
     model = model,
     ref_model = None,
-    args = TrainingArguments(
+    train_dataset = YOUR_DATASET_HERE,
+    # eval_dataset = YOUR_DATASET_HERE,
+    tokenizer = tokenizer,
+    args = DPOConfig(
         per_device_train_batch_size = 4,
         gradient_accumulation_steps = 8,
         warmup_ratio = 0.1,
         num_train_epochs = 3,
-        fp16 = not is_bfloat16_supported(),
-        bf16 = is_bfloat16_supported(),
         logging_steps = 1,
         optim = ""adamw_8bit"",
         seed = 42,
         output_dir = ""outputs"",
+        max_length = 1024,
+        max_prompt_length = 512,
+        beta = 0.1,
     ),
-    beta = 0.1,
-    train_dataset = YOUR_DATASET_HERE,
-    # eval_dataset = YOUR_DATASET_HERE,
-    tokenizer = tokenizer,
-    max_length = 1024,
-    max_prompt_length = 512,
 )
 dpo_trainer.train()
 ```
diff --git a/pyproject.toml b/pyproject.toml
index de1583e..73e69dc 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 windows=[
-    ""unsloth_zoo>=2025.2.7"",
+    ""unsloth_zoo>=2025.3.1"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -61,7 +61,7 @@ windows=[
     ""xformers>=0.0.22.post7 ; platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2025.2.7"",
+    ""unsloth_zoo>=2025.3.1"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index e33d165..c8f2926 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.2.6""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.1""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
@@ -212,6 +212,7 @@ except:
 pass
 
 from .models import *
+from .models import __version__
 from .save import *
 from .chat_templates import *
 from .tokenizer_utils import *
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index fcba2eb..006dfff 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -15,7 +15,13 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh, triton_cast
+from .utils import (
+    calculate_settings,
+    MAX_FUSED_SIZE,
+    triton_tanh,
+    triton_cast,
+    torch_cuda_device,
+)
 from transformers.models.llama.modeling_llama import logger
 from packaging.version import Version
 
@@ -279,10 +285,11 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         n_rows : int
         vocab_size : int
         n_rows, vocab_size = logits.shape
+        device = logits.device
 
         div, mod = divmod(vocab_size, MAX_FUSED_SIZE)
         n_chunks : int = div + (mod != 0)
-        losses = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
+        losses = torch.empty(n_rows, dtype = torch.float32, device = device)
 
         DO_SOFTCAPPING   : bool = bool(logit_softcapping != 0)
         DO_LOGIT_SCALING : bool = bool(logit_scaling != 0)
@@ -292,39 +299,41 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         if n_chunks == 1:
             # For small vocabs <= 65336 like Llama, Mistral
             BLOCK_SIZE, num_warps = calculate_settings(vocab_size)
-            logsumexp = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
-
-            _cross_entropy_forward[(n_rows,)](
-                logits, logits.stride(0),
-                losses,
-                logsumexp,
-                labels,
-                VOCAB_SIZE       = vocab_size,
-                BLOCK_SIZE       = BLOCK_SIZE,
-                DO_SOFTCAPPING   = DO_SOFTCAPPING,
-                SOFTCAP          = logit_softcapping,
-                DO_LOGIT_SCALING = DO_LOGIT_SCALING,
-                LOGIT_SCALE      = logit_scaling,
-                num_warps        = num_warps,
-            )
+            logsumexp = torch.empty(n_rows, dtype = torch.float32, device = device)
+
+            with torch_cuda_device(device):
+                _cross_entropy_forward[(n_rows,)](
+                    logits, logits.stride(0),
+                    losses,
+                    logsumexp,
+                    labels,
+                    VOCAB_SIZE       = vocab_size,
+                    BLOCK_SIZE       = BLOCK_SIZE,
+                    DO_SOFTCAPPING   = DO_SOFTCAPPING,
+                    SOFTCAP          = logit_softcapping,
+                    DO_LOGIT_SCALING = DO_LOGIT_SCALING,
+                    LOGIT_SCALE      = logit_scaling,
+                    num_warps        = num_warps,
+                )
         else:
             # For large vocabs > 65336 like Gemma 256K
-            logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = ""cuda:0"")
-
-            _chunked_cross_entropy_forward[(n_rows, n_chunks,)](
-                logits, logits.stride(0),
-                losses,
-                logsumexp,
-                labels,
-                VOCAB_SIZE       = vocab_size,
-                N_CHUNKS         = n_chunks,
-                BLOCK_SIZE       = MAX_FUSED_SIZE,
-                DO_SOFTCAPPING   = DO_SOFTCAPPING,
-                SOFTCAP          = logit_softcapping,
-                DO_LOGIT_SCALING = DO_LOGIT_SCALING,
-                LOGIT_SCALE      = logit_scaling,
-                num_warps        = 32,
-            )
+            logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = device)
+
+            with torch_cuda_device(device):
+                _chunked_cross_entropy_forward[(n_rows, n_chunks,)](
+                    logits, logits.stride(0),
+                    losses,
+                    logsumexp,
+                    labels,
+                    VOCAB_SIZE       = vocab_size,
+                    N_CHUNKS         = n_chunks,
+                    BLOCK_SIZE       = MAX_FUSED_SIZE,
+                    DO_SOFTCAPPING   = DO_SOFTCAPPING,
+                    SOFTCAP          = logit_softcapping,
+                    DO_LOGIT_SCALING = DO_LOGIT_SCALING,
+                    LOGIT_SCALE      = logit_scaling,
+                    num_warps        = 32,
+                )
             # logsumexp(chunked_logsumexp) - x
             # Do the -x separately
             logsumexp = torch.logsumexp(logsumexp, dim = 1) # Row sum
@@ -354,19 +363,20 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         div, mod = divmod(vocab_size, BLOCK_SIZE)
         n_blocks : int = div + (mod != 0)
 
-        _cross_entropy_backward[(n_rows, n_blocks,)](
-            logits,   logits.stride(0),
-            dlosses, dlosses.stride(0),
-            logsumexp,
-            labels,
-            VOCAB_SIZE       = vocab_size,
-            BLOCK_SIZE       = BLOCK_SIZE,
-            DO_SOFTCAPPING   = ctx.DO_SOFTCAPPING,
-            SOFTCAP          = ctx.logit_softcapping,
-            DO_LOGIT_SCALING = ctx.DO_LOGIT_SCALING,
-            LOGIT_SCALE      = ctx.logit_scaling,
-            num_warps        = 8,
-        )
+        with torch_cuda_device(dlosses.device):
+            _cross_entropy_backward[(n_rows, n_blocks,)](
+                logits,   logits.stride(0),
+                dlosses, dlosses.stride(0),
+                logsumexp,
+                labels,
+                VOCAB_SIZE       = vocab_size,
+                BLOCK_SIZE       = BLOCK_SIZE,
+                DO_SOFTCAPPING   = ctx.DO_SOFTCAPPING,
+                SOFTCAP          = ctx.logit_softcapping,
+                DO_LOGIT_SCALING = ctx.DO_LOGIT_SCALING,
+                LOGIT_SCALE      = ctx.logit_scaling,
+                num_warps        = 8,
+            )
         return logits, None, None, None,
     pass
 pass
diff --git a/unsloth/kernels/geglu.py b/unsloth/kernels/geglu.py
index 9fedae7..1ece87c 100644
--- a/unsloth/kernels/geglu.py
+++ b/unsloth/kernels/geglu.py
@@ -15,7 +15,11 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, triton_tanh
+from .utils import (
+    calculate_settings,
+    triton_tanh,
+    torch_cuda_device,
+)
 
 
 @triton.jit
@@ -41,9 +45,11 @@ pass
 def geglu_exact_forward_kernel(gate, up):
     batch, seq_len, hd = gate.shape
     n_elements = gate.numel()
-    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda:0"")
+    device = gate.device
+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = device)
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
+    with torch_cuda_device(device):
+        _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
 pass
 
@@ -99,7 +105,8 @@ def geglu_exact_backward_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
+    with torch_cuda_device(e.device):
+        _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
 
@@ -133,9 +140,11 @@ pass
 def geglu_approx_forward_kernel(gate, up):
     batch, seq_len, hd = gate.shape
     n_elements = gate.numel()
-    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda:0"")
+    device = gate.device
+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = device)
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
+    with torch_cuda_device(device):
+        _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
 pass
 
@@ -198,6 +207,7 @@ def geglu_approx_backward_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
+    with torch_cuda_device(e.device):
+        _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
diff --git a/unsloth/kernels/layernorm.py b/unsloth/kernels/layernorm.py
index ffcc5cc..26a77f0 100644
--- a/unsloth/kernels/layernorm.py
+++ b/unsloth/kernels/layernorm.py
@@ -16,7 +16,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings
+from .utils import calculate_settings, torch_cuda_device
 from unsloth_zoo.patching_utils import (
     patch_layernorm,
 )
@@ -111,17 +111,18 @@ class Fast_Layernorm(torch.autograd.Function):
         r  = torch.empty(n_rows, dtype = torch.float32, device = device)
         mu = torch.empty(n_rows, dtype = torch.float32, device = device)
 
-        layernorm_forward[(n_rows,)](
-            Y, Y.stride(0),
-            X, X.stride(0),
-            W,
-            b,
-            r,
-            mu,
-            n_cols, eps,
-            BLOCK_SIZE = BLOCK_SIZE,
-            num_warps  = num_warps,
-        )
+        with torch_cuda_device(device):
+            layernorm_forward[(n_rows,)](
+                Y, Y.stride(0),
+                X, X.stride(0),
+                W,
+                b,
+                r,
+                mu,
+                n_cols, eps,
+                BLOCK_SIZE = BLOCK_SIZE,
+                num_warps  = num_warps,
+            )
         ctx.eps = eps
         ctx.BLOCK_SIZE = BLOCK_SIZE
         ctx.num_warps  = num_warps
@@ -137,17 +138,18 @@ class Fast_Layernorm(torch.autograd.Function):
         X, W, b, r, mu = ctx.saved_tensors
         n_rows, n_cols = dY.shape
 
-        layernorm_backward[(n_rows,)](
-            dY, dY.stride(0),
-            X,  X .stride(0),
-            W,
-            b,
-            r,
-            mu,
-            n_cols, ctx.eps,
-            BLOCK_SIZE = ctx.BLOCK_SIZE,
-            num_warps  = ctx.num_warps,
-        )
+        with torch_cuda_device(dY.device):
+            layernorm_backward[(n_rows,)](
+                dY, dY.stride(0),
+                X,  X .stride(0),
+                W,
+                b,
+                r,
+                mu,
+                n_cols, ctx.eps,
+                BLOCK_SIZE = ctx.BLOCK_SIZE,
+                num_warps  = ctx.num_warps,
+            )
         dX = dY.view(*shape)
         return dX, None, None, None, None
     pass
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 7487c10..1cde638 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -15,8 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings
-
+from .utils import calculate_settings, torch_cuda_device
 
 @triton.jit
 def _rms_layernorm_forward(
@@ -154,15 +153,16 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         r = torch.empty(n_rows, dtype = torch.float32, device = device)
 
         fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward
-        fx[(n_rows,)](
-            Y, Y.stride(0),
-            X, X.stride(0),
-            W, W.stride(0),
-            r, r.stride(0),
-            n_cols, eps,
-            BLOCK_SIZE = BLOCK_SIZE,
-            num_warps  = num_warps,
-        )
+        with torch_cuda_device(device):
+            fx[(n_rows,)](
+                Y, Y.stride(0),
+                X, X.stride(0),
+                W, W.stride(0),
+                r, r.stride(0),
+                n_cols, eps,
+                BLOCK_SIZE = BLOCK_SIZE,
+                num_warps  = num_warps,
+            )
         ctx.eps = eps
         ctx.BLOCK_SIZE = BLOCK_SIZE
         ctx.num_warps  = num_warps
@@ -183,18 +183,19 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         # dW = X
         dX = torch.empty_like(dY) if ctx.GEMMA else dY
 
-        _rms_layernorm_backward[(n_rows,)](
-            dY, dY.stride(0),
-            dX, dX.stride(0),
-            X,  X .stride(0),
-            W,  W .stride(0),
-            r,  r .stride(0),
-            # dW, dW.stride(0),
-            n_cols, ctx.eps,
-            GEMMA      = ctx.GEMMA,
-            BLOCK_SIZE = ctx.BLOCK_SIZE,
-            num_warps  = ctx.num_warps,
-        )
+        with torch_cuda_device(dY.device):
+            _rms_layernorm_backward[(n_rows,)](
+                dY, dY.stride(0),
+                dX, dX.stride(0),
+                X,  X .stride(0),
+                W,  W .stride(0),
+                r,  r .stride(0),
+                # dW, dW.stride(0),
+                n_cols, ctx.eps,
+                GEMMA      = ctx.GEMMA,
+                BLOCK_SIZE = ctx.BLOCK_SIZE,
+                num_warps  = ctx.num_warps,
+            )
         dX = dX.view(*shape)
         return dX, None, None, None
     pass
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index 88b9cca..a14a485 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings
+from .utils import calculate_settings, torch_cuda_device
 ROPE_GROUP_SIZE : int = 4
 
 def _rope_embedding(
@@ -100,16 +100,17 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         div, mod = divmod(n_heads, ROPE_GROUP_SIZE)
         n_groups : int = div + (mod != 0)
 
-        _rope_embedding[(n_rows, n_groups, )](
-              Q,   Q.stride(0),
-            cos, cos.stride(0),
-            sin, sin.stride(0),
-            seq_len,
-            head_dim, n_heads,
-            BACKWARD_PASS = False,
-            BLOCK_SIZE = BLOCK_SIZE,
-            num_warps  = num_warps,
-        )
+        with torch_cuda_device(Q.device):
+            _rope_embedding[(n_rows, n_groups, )](
+                  Q,   Q.stride(0),
+                cos, cos.stride(0),
+                sin, sin.stride(0),
+                seq_len,
+                head_dim, n_heads,
+                BACKWARD_PASS = False,
+                BLOCK_SIZE = BLOCK_SIZE,
+                num_warps  = num_warps,
+            )
         ctx.BLOCK_SIZE = BLOCK_SIZE
         ctx.num_warps  = num_warps
         ctx.n_groups = n_groups
@@ -134,15 +135,16 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         cos = ctx.cos
         sin = ctx.sin
 
-        _rope_embedding[(n_rows, ctx.n_groups, )](
-            dY,  dY .stride(0),
-            cos, cos.stride(0),
-            sin, sin.stride(0),
-            seq_len, head_dim, n_heads,
-            BACKWARD_PASS = True,
-            BLOCK_SIZE = ctx.BLOCK_SIZE,
-            num_warps  = ctx.num_warps,
-        )
+        with torch_cuda_device(dY.device):
+            _rope_embedding[(n_rows, ctx.n_groups, )](
+                dY,  dY .stride(0),
+                cos, cos.stride(0),
+                sin, sin.stride(0),
+                seq_len, head_dim, n_heads,
+                BACKWARD_PASS = True,
+                BLOCK_SIZE = ctx.BLOCK_SIZE,
+                num_warps  = ctx.num_warps,
+            )
         dY = dY.view(batch, seq_len, n_heads, head_dim)
         return dY, None, None,
     pass
diff --git a/unsloth/kernels/swiglu.py b/unsloth/kernels/swiglu.py
index 688e9f9..12f1f5e 100644
--- a/unsloth/kernels/swiglu.py
+++ b/unsloth/kernels/swiglu.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings
+from .utils import calculate_settings, torch_cuda_device
 
 
 @triton.jit
@@ -43,7 +43,8 @@ def swiglu_fg_kernel(e, g):
     n_elements = e.numel()
     h = torch.empty((batch, seq_len, hd), dtype = e.dtype, device = e.device)
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _fg_kernel[grid](e, g, h, n_elements, BLOCK_SIZE = 1024,)
+    with torch_cuda_device(e.device):
+        _fg_kernel[grid](e, g, h, n_elements, BLOCK_SIZE = 1024,)
     return h
 pass
 
@@ -94,6 +95,7 @@ def swiglu_DWf_DW_dfg_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _DWf_DW_dfg_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
+    with torch_cuda_device(e.device):
+        _DWf_DW_dfg_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 985adaa..5eb9b8f 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -19,6 +19,7 @@ import functools
 
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
 import torch
+torch_Tensor = torch.Tensor
 from packaging.version import Version
 if Version(torch.__version__) < Version(""2.4.0""):
     torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
@@ -67,6 +68,18 @@ import ctypes
 HAS_CUDA_STREAM = Version(bnb.__version__) > Version(""0.43.3"")
 get_ptr = bnb.functional.get_ptr
 
+if torch.cuda.device_count() > 1:
+    torch_cuda_device = torch.cuda.device
+else:
+    from contextlib import nullcontext
+    def torch_cuda_device(device): return nullcontext()
+pass
+_cuda_getCurrentRawStream = torch._C._cuda_getCurrentRawStream
+c_void_p = ctypes.c_void_p
+def _get_tensor_stream(tensor: torch_Tensor) -> c_void_p:
+    return c_void_p(_cuda_getCurrentRawStream(tensor.device.index))
+pass
+
 # Get array of CUDA streams and other buffers
 global CUDA_STREAMS
 global WEIGHT_BUFFERS
@@ -92,27 +105,29 @@ cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_
 cgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16
 cgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16
 
-
-def QUANT_STATE(W):
-    return getattr(W, ""quant_state"", None)
-pass
-
+def QUANT_STATE(W): return getattr(W, ""quant_state"", None)
 
 def get_lora_parameters(proj):
     # For DPO or disabled adapters
-    base_layer = (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
+    base_layer = getattr(proj, ""base_layer"", proj) # (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
     W = base_layer.weight
 
-    if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
-        return W, QUANT_STATE(W), None, None, None
+    # if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
+    if getattr(proj, ""disable_adapters"", True) or proj.merged:
+        return W, getattr(W, ""quant_state"", None), None, None, None
     pass
 
-    active_adapter = proj.active_adapters[0] if \
-        hasattr(proj, ""active_adapters"") else proj.active_adapter
-    A = proj.lora_A [active_adapter].weight
-    B = proj.lora_B [active_adapter].weight
-    s = proj.scaling[active_adapter]
-    return W, QUANT_STATE(W), A, B, s
+    adapter = getattr(proj, ""active_adapters"", None)
+    if adapter is None: adapter = getattr(proj, ""active_adapter"", (""default""))
+    adapter = adapter[0]
+    
+    return (
+        W,
+        getattr(W, ""quant_state"", None),
+        proj.lora_A [adapter].weight,
+        proj.lora_B [adapter].weight,
+        proj.scaling[adapter],
+    )
 pass
 
 
@@ -120,19 +135,24 @@ def get_lora_parameters_bias(proj):
     # For DPO or disabled adapters
     base_layer = getattr(proj, ""base_layer"", proj) # (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
     W = base_layer.weight
-    bias = base_layer.bias
 
     # if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
     if getattr(proj, ""disable_adapters"", True) or proj.merged:
-        return W, QUANT_STATE(W), None, None, None, bias
+        return W, getattr(W, ""quant_state"", None), None, None, None, bias
     pass
 
-    active_adapter = proj.active_adapters[0] if \
-        getattr(proj, ""active_adapters"", ) else proj.active_adapter
-    A = proj.lora_A [active_adapter].weight
-    B = proj.lora_B [active_adapter].weight
-    s = proj.scaling[active_adapter]
-    return W, QUANT_STATE(W), A, B, s, bias
+    adapter = getattr(proj, ""active_adapters"", None)
+    if adapter is None: adapter = getattr(proj, ""active_adapter"", (""default""))
+    adapter = adapter[0]
+
+    return (
+        W,
+        getattr(W, ""quant_state"", None),
+        proj.lora_A [adapter].weight,
+        proj.lora_B [adapter].weight,
+        proj.scaling[adapter],
+        base_layer.bias,
+    )
 pass
 
 if HAS_CUDA_STREAM:
@@ -193,18 +213,19 @@ if HAS_CUDA_STREAM:
 
         # NF4 dequantization of statistics
         ptr_out_absmax = get_ptr(out_absmax)
-        cdequantize_blockwise_fp32(
-            get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
-            ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax), CUDA_STREAM,
-        )
-        out_absmax += offset
-
-        # Dequantize W
-        fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
-             cdequantize_blockwise_bf16_nf4
-        fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
-           ctypes_c_int(blocksize), ctypes_c_int(out.numel()), CUDA_STREAM,)
-
+        with torch_cuda_device(device):
+            cdequantize_blockwise_fp32(
+                get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
+                ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax), CUDA_STREAM
+            )
+            out_absmax += offset
+
+            # Dequantize W
+            fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+                 cdequantize_blockwise_bf16_nf4
+            fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
+               ctypes_c_int(blocksize), ctypes_c_int(out.numel()), CUDA_STREAM,)
+        pass
         # Careful returning transposed data
         is_transposed = (True if W.shape[0] == 1 else False)
         return out.t() if is_transposed else out
@@ -316,19 +337,21 @@ if HAS_CUDA_STREAM:
         ldc = ctypes_c_int32(ldc)
 
         df = torch.empty(absmax.shape, dtype = torch.float32, device = device)
-        cdequantize_blockwise_fp32(
-            get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
-            ctypes_c_int(blocksize2), ctypes_c_int(df.numel()), CUDA_STREAM,
-        )
-        df += offset
-        absmax = df
-
-        fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
-            cgemm_4bit_inference_naive_bf16
-
-        blocksize = ctypes_c_int32(blocksize)
-        fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
-           lda, ldb, ldc, blocksize, CUDA_STREAM,)
+        with torch_cuda_device(device):
+            cdequantize_blockwise_fp32(
+                get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
+                ctypes_c_int(blocksize2), ctypes_c_int(df.numel()), CUDA_STREAM,
+            )
+            df += offset
+            absmax = df
+
+            fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
+                cgemm_4bit_inference_naive_bf16
+
+            blocksize = ctypes_c_int32(blocksize)
+            fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
+               lda, ldb, ldc, blocksize, CUDA_STREAM,)
+        pass
 
         return out
     pass
@@ -458,7 +481,6 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):
     else:
         reshape = False
     pass
-
     out = torch_matmul(X, W, out = out)
     if W_quant is not None: del W
 
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index 29ad78d..e11cd54 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -19,5 +19,5 @@ from .llama   import FastLlamaModel
 from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
 from .dpo     import PatchDPOTrainer, PatchKTOTrainer
-from ._utils  import is_bfloat16_supported
+from ._utils  import is_bfloat16_supported, __version__
 from .rl      import PatchFastRL, vLLMSamplingParams
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index cca77bb..0f0d4c1 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -755,7 +755,8 @@ def offload_to_disk(W, model, name, temporary_location : str = ""_unsloth_tempora
     filename = os.path.join(file_location, f""{name}.pt"")
     W = W.weight if hasattr(W, ""weight"") else W
     torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)
-    offloaded_W = torch.load(filename, map_location = ""cpu"", mmap = True)
+    # We must use weights_only = False due to pickling
+    offloaded_W = torch.load(filename, map_location = ""cpu"", mmap = True, weights_only = False)
     offloaded_W._offloaded_file_location = filename
     return offloaded_W
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index fe0627f..bcabbd5 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -18,6 +18,7 @@ import math
 from functools import partial
 from typing import Optional, Tuple, List, Union
 from ._utils import *
+from ._utils import patch_unsloth_smart_gradient_checkpointing
 from ._utils import __version__
 from torch.nn.functional import scaled_dot_product_attention
 from transformers import __version__ as transformers_version
@@ -758,14 +759,9 @@ def LlamaModel_fast_forward(
 
     # Check checkpointing method
     gradient_checkpointing = False
-    offloaded_gradient_checkpointing = False
 
     if (self.gradient_checkpointing and self.training and not use_cache):
-
         gradient_checkpointing = True
-
-        if output_attentions is False and hasattr(self, ""_offloaded_gradient_checkpointing""):
-            offloaded_gradient_checkpointing = True
     pass
 
     # Gemma2 has alternating SWA and global attn
@@ -850,27 +846,12 @@ def LlamaModel_fast_forward(
                 mask = self. GA_mask if use_static_mask else dynamic_GA_mask
         pass
 
-        if offloaded_gradient_checkpointing:
-            hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(
-                decoder_layer,
-                hidden_states,
-                mask,
-                attention_mask,
-                position_ids,
-                past_key_values,
-                output_attentions,
-                use_cache,
-                None,
-                position_embeddings,
-            )[0]
-
-        elif gradient_checkpointing:
+        if gradient_checkpointing:
             def create_custom_forward(module):
                 def custom_forward(*inputs):
                     return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask, position_embeddings = position_embeddings)
                 return custom_forward
             pass
-
             layer_outputs = torch.utils.checkpoint.checkpoint(
                 create_custom_forward(decoder_layer),
                 hidden_states,
@@ -1703,10 +1684,10 @@ class FastLlamaModel:
 
         statistics = \
            f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.\n""\
-           f""   {chr(92)}{chr(92)}   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
            f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
-           f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
+           f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
         print(statistics)
 
         # Warn about fast transfers
@@ -1898,11 +1879,11 @@ class FastLlamaModel:
         # Cannot use \\ since it will cause a SyntaxWarning in Python 3.12
         # Instead use chr(92) == \\
         debug_info = """"""debug_info = \\
-        f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\n""\\
-        f""   {chr(92)}{chr(92)}   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
-        f""O^O/ {chr(92)}_/ {chr(92)}    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
-        f""{chr(92)}        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
-        f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
+        f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = {len(set(p.device for p in model.parameters()))}\\n""\\
+        f""   {chr(92)}{chr(92)}   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,} | Total steps = {max_steps:,}\\n""\\
+        f""O^O/ {chr(92)}_/ {chr(92)}    Batch size per device = {self._train_batch_size:,} | Gradient accumulation steps = {args.gradient_accumulation_steps}\\n""\\
+        f""{chr(92)}        /    Data Parallel GPUs = {args.world_size} | Total batch size ({self._train_batch_size} x {args.gradient_accumulation_steps} x {args.world_size}) = {total_train_batch_size:,}\\n""\\
+        f' ""-____-""     Trainable parameters = {get_model_param_count(model, trainable_only=True):,}/{get_model_param_count(model):,} ({get_model_param_count(model, trainable_only=True)/get_model_param_count(model)*100:.2f}% trained)'
         logger.warning(debug_info)
         import subprocess, re, gc
         for _ in range(3):
@@ -1989,9 +1970,14 @@ class FastLlamaModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             internal_model._saved_temp_tokenizer = tokenizer
+            # Also set is_loaded_in_8bit to disable incorrect DDP
+            internal_model.is_loaded_in_8bit = True
+
             internal_model = internal_model.model
         pass
         internal_model._saved_temp_tokenizer = tokenizer
+        # Also set is_loaded_in_8bit to disable incorrect DDP
+        internal_model.is_loaded_in_8bit = True
 
         # For transformers > 4.47.1, we need to add rotary_emb to all attention layers
         if IS_ATTENTION_REFACTOR or hasattr(model.model, ""rotary_emb""):
@@ -2034,6 +2020,9 @@ class FastLlamaModel:
     ):
         transformers_set_seed(random_state)
 
+        if use_gradient_checkpointing == ""unsloth"":
+            patch_unsloth_smart_gradient_checkpointing(dtype = model.get_input_embeddings().weight.dtype)
+
         if type(r) is not int:
             raise TypeError(f""Unsloth: Rank of {str(r)} must be an integer."")
         if r <= 0:
@@ -2398,11 +2387,15 @@ class FastLlamaModel:
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
                 internal_model._saved_temp_tokenizer.padding_side = ""right""
             pass
+            # Also set is_loaded_in_8bit to disable incorrect DDP
+            internal_model.is_loaded_in_8bit = True
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
             internal_model._saved_temp_tokenizer.padding_side = ""right""
         pass
+        # Also set is_loaded_in_8bit to disable incorrect DDP
+        internal_model.is_loaded_in_8bit = True
 
         # Clear deleted GPU items
         for _ in range(3):
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 186545c..30128cd 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -59,7 +59,15 @@ if SUPPORTS_GEMMA2:
     from .gemma2 import FastGemma2Model
 pass
 import torch
-
+from ._utils import (
+    patch_compiling_bitsandbytes,
+    patch_model_and_tokenizer,
+    prepare_model_for_kbit_training,
+    patch_unsloth_smart_gradient_checkpointing,
+    patch_compiled_autograd,
+    process_vision_info,
+    unsloth_compile_transformers,
+)
 
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
@@ -87,6 +95,10 @@ class FastLanguageModel(FastLlamaModel):
         *args, **kwargs,
     ):
         if token is None: token = get_token()
+        assert (dtype is None or dtype == torch.float16 or dtype == torch.bfloat16)
+
+        if use_gradient_checkpointing == ""unsloth"":
+            patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
 
         if fast_inference:
             if importlib.util.find_spec(""vllm"") is None:
@@ -367,15 +379,6 @@ class FastLanguageModel(FastLlamaModel):
 pass
 
 
-from ._utils import (
-    patch_compiling_bitsandbytes,
-    patch_model_and_tokenizer,
-    prepare_model_for_kbit_training,
-    patch_unsloth_smart_gradient_checkpointing,
-    patch_compiled_autograd,
-    process_vision_info,
-    unsloth_compile_transformers,
-)
 from ..kernels import (
     patch_loss_functions,
     post_patch_loss_function,
@@ -404,6 +407,7 @@ class FastVisionModel(FastBaseVisionModel):
         *args, **kwargs,
     ):
         if token is None: token = get_token()
+        assert (dtype is None or dtype == torch.float16 or dtype == torch.bfloat16)
 
         patch_compiled_autograd()
         patch_compiling_bitsandbytes()
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 8f34607..3a9d651 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -495,7 +495,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         RLTrainer_source,
         f""trl.trainer.{trainer_file}"",
         imports,
-        overwrite = True,
+        overwrite = False,
     )
     
     # Patch Trainer
diff --git a/unsloth/save.py b/unsloth/save.py
index af95de0..d03f47e 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -17,6 +17,8 @@ from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
 from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 from peft.tuners.lora import Linear as Peft_Linear
 from typing import Optional, Callable, Union, List
+import sys
+import requests
 import torch
 import os
 import shutil
@@ -1613,6 +1615,112 @@ def create_ollama_modelfile(tokenizer, gguf_location):
     return modelfile
 pass
 
+def create_ollama_model(
+    username: str, 
+    model_name: str, 
+    tag: str, 
+    modelfile_path: str
+):
+    try:
+        init_check = subprocess.run(
+            ['curl', 'http://localhost:11434'], capture_output=True, text=True,  timeout=3
+        )
+        if init_check.returncode == 0:
+            print(init_check.stdout.strip())
+        else:
+            print(""Ollama Server is not Running"")
+    except subprocess.TimeoutExpired:
+        return ""Ollama Request Timeout""
+
+    process = subprocess.Popen(
+            ['ollama', 'create', f'{username}/{model_name}:{tag}', '-f', f'{modelfile_path}'],
+        stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        text=True,
+        bufsize=1,
+        universal_newlines=True
+    )
+
+    for line in iter(process.stdout.readline, ''):
+        print(line, end='')
+        sys.stdout.flush()
+
+    return_code = process.wait()
+
+    if return_code != 0:
+        print(f""\nMODEL CREATED FAILED WITH RETURN CODE {return_code}"")
+    else:
+        print(""\nMODEL CREATED SUCCESSFULLY"")
+pass
+
+
+def push_to_ollama_hub(username: str, model_name: str, tag: str):
+    try:
+        init_check = subprocess.run(
+            ['curl', 'http://localhost:11434'], capture_output=True, text=True,  timeout=3
+        )
+        if init_check.returncode == 0:
+            print(init_check.stdout.strip())
+        else:
+            print(""Ollama Server is not Running"")
+    except subprocess.TimeoutExpired:
+        return ""Ollama Request Timeout""
+
+    process = subprocess.Popen(
+            ['ollama', 'push', f'{username}/{model_name}:{tag}'],
+        stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        text=True,
+        bufsize=1,
+        universal_newlines=True
+    )
+
+    for line in iter(process.stdout.readline, ''):
+        print(line, end='')
+        sys.stdout.flush()
+
+    return_code = process.wait()
+
+    if return_code != 0:
+        print(f""\nMODEL PUBLISHED FAILED WITH RETURN CODE {return_code}"")
+    else:
+        print(""\nMODEL PUBLISHED SUCCESSFULLY"")
+
+
+def push_to_ollama(
+    tokenizer,
+    gguf_location,
+    username: str,
+    model_name: str,
+    tag: str
+):
+    model_file = create_ollama_modelfile(
+        tokenizer=tokenizer,
+        gguf_location=gguf_location
+    )
+
+    with open(f""Modelfile_{model_name}"", ""w"") as f:
+        f.write(model_file)
+        f.close()
+    
+    create_ollama_model(
+        username=username,
+        model_name=model_name,
+        tag=tag,
+        modelfile_path=f""Modelfile_{model_name}""
+    )
+
+    push_to_ollama_hub(
+        username=username,
+        model_name=model_name,
+        tag=tag
+    )
+
+    print(""Succesfully pushed to ollama"")
+
+
+
+
 
 def unsloth_save_pretrained_gguf(
     self,
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 242d234..6dd17e7 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -285,6 +285,8 @@ pass
 
 # =============================================
 # Fix new Xformers versions TypeError: Multiple dispatch failed for 'torch._ops.aten.to.dtype_layout'
+accelerate_old_send_to_device = None
+accelerate_new_send_to_device = None
 if Version(xformers_version) >= Version(""0.0.27""):
     import accelerate.utils.operations
     if hasattr(accelerate.utils.operations, ""send_to_device"") and \
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3fcb8a7..3999812 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1371,8 +1371,10 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
         internal_model._flag_for_generation = True
 
         # Must patch accelerate for Xformers
-        import accelerate.utils.operations
-        accelerate.utils.operations.send_to_device = accelerate_new_send_to_device
+        if accelerate_new_send_to_device is not None:
+            import accelerate.utils.operations
+            accelerate.utils.operations.send_to_device = accelerate_new_send_to_device
+        pass
 
         # For newer HF
         kwargs[""cache_implementation""] = ""dynamic""
@@ -1411,7 +1413,9 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
         if hasattr(internal_model, ""_flag_for_generation""): del internal_model._flag_for_generation
 
         # Return accelerate back
-        accelerate.utils.operations.send_to_device = accelerate_old_send_to_device
+        if accelerate_new_send_to_device is not None:
+            accelerate.utils.operations.send_to_device = accelerate_old_send_to_device
+        pass
 
         return output
     pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 2e88f76..5da6ea6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -206,33 +206,18 @@ except:
 # Patch get_model_param_count to record correct 4bit / 8bit
 from transformers.trainer_pt_utils import is_deepspeed_zero3_enabled
 
-def extract_approx_params_from_config(config):
+def extract_quant_model_param_count(model):
     """"""
-    Extract approximate parameter count from model config's name_or_path
-    Returns int (param count) or None if not found.
+    Calculate quant model param count based on difference in param class. Returns int for param count.
     """"""
-    lowercase_b_families = [""gemma""] # gemma uses small 'b' : google/gemma-3-1b-it
-    model_name = getattr(config, ""name_or_path"", """")
-    import re
-    cleaned = re.sub(r""[-_]?bnb[-_]?4bit|[-_]?4bit|[-_]?8bit|[-_]?bnb"", """", model_name, flags=re.IGNORECASE) # replace bnb and xbit
-    match_B = re.search(r""([0-9]+(?:\.[0-9]+)?)\s*B"", cleaned) # first prefer searching 'B'
-    if match_B:
-        # most model names would come in this flow
-        billions = float(match_B.group(1))
-        return int(1_000_000_000 * billions)
-    else:
-        if any(fam in cleaned.lower() for fam in lowercase_b_families):
-            match_b = re.search(r""([0-9]+(?:\.[0-9]+)?)\s*b"", cleaned)
-            if match_b:
-                billions = float(match_b.group(1))
-                return int(1_000_000_000 * billions)
+    count: int = 0
+    for name, p in model.named_parameters():
+        if p.__class__.__name__ == ""Params4bit"":
+            count += 2 * p.numel()
         else:
-            match_any = re.search(r""([0-9]+(?:\.[0-9]+)?)\s*[bB]"", cleaned)
-            if match_any:
-                billions = float(match_any.group(1))
-                return int(1_000_000_000 * billions)
-    return None
-
+            count += p.numel()
+    return count
+pass
 
 def get_model_param_count(model, trainable_only = False):
     """"""
@@ -248,7 +233,7 @@ def get_model_param_count(model, trainable_only = False):
     if (not trainable_only) and \
         hasattr(model, ""config"") and \
         hasattr(model.config, ""quantization_config""):
-        approx = extract_approx_params_from_config(model.config)
+        approx = extract_quant_model_param_count(model)
         if approx is not None:
             s = approx
     return s
@@ -370,7 +355,7 @@ if is_openai_available():
         def _is_openai_available(): return False
         transformers.utils.is_openai_available = _is_openai_available
     pass
-pass 
+pass
 
 # =============================================
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
@@ -1085,7 +1070,7 @@ pass
 
 
 def patch_gradient_accumulation_fix(Trainer):
-    # Fixes gradient accumulation 
+    # Fixes gradient accumulation
     import inspect
     if hasattr(Trainer, ""get_batch_samples""):
         if Trainer.get_batch_samples.__name__ == ""_unsloth_get_batch_samples"": return
@@ -1159,10 +1144,10 @@ def patch_gradient_accumulation_fix(Trainer):
         ""\2if num_items_in_batch is None:\n""\
         ""\3loss = loss / self.args.gradient_accumulation_steps\n""\
         ""\1self.accelerator.backward(loss, **kwargs)"",
-        
+
         function,
     )
-    
+
     exec(function, globals())
     Trainer.training_step = _unsloth_training_step
 pass
@@ -1356,7 +1341,7 @@ def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, m
             )
             loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)
         pass
-        
+
         if hasattr(model.config, ""quantization_config""):
             raise ValueError(
                 ""Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\n""\
@@ -1365,4 +1350,4 @@ def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, m
         pass
     pass
 
-    return loftq_config
\ No newline at end of file
+    return loftq_config
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 6074a51..b8473e6 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -303,6 +303,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
 pass
 
 
+@torch._disable_dynamo
 def fast_cross_entropy_loss(logits, labels, logit_softcapping = 0):
     """"""
     Arguments:
diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index b7c7305..e707435 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+__all__ = [
+    ""PatchDPOTrainer"",
+]
+
 try:
     from transformers.utils.notebook import (
         IntervalStrategy,
@@ -22,6 +26,12 @@ try:
 except:
     HAS_NOTEBOOK = False
 pass
+import torch
+from ._utils import torch_compile_options
+import inspect
+import torch.nn as nn
+from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union
+
 
 DPOTrainer_metrics = [
     ""rewards/chosen"",
@@ -37,11 +47,11 @@ set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)
 
 
 def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):
-    self.first_column = ""Epoch"" if args.evaluation_strategy == IntervalStrategy.EPOCH else ""Step""
+    self.first_column = ""Epoch"" if args.eval_strategy == IntervalStrategy.EPOCH else ""Step""
     self.training_loss = 0
     self.last_log = 0
     column_names = [self.first_column] + [""Training Loss""]
-    if args.evaluation_strategy != IntervalStrategy.NO:
+    if args.eval_strategy != IntervalStrategy.NO:
         column_names.append(""Validation Loss"")
     column_names += [x.replace(""/"", "" / "") for x in DPOTrainer_metrics]
     self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)
@@ -50,7 +60,7 @@ pass
 
 def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwargs):
     # Only for when there is no evaluation
-    if args.evaluation_strategy == IntervalStrategy.NO and ""loss"" in logs:
+    if args.eval_strategy == IntervalStrategy.NO and ""loss"" in logs:
         values = {""Training Loss"": logs[""loss""]}
         for metric in DPOTrainer_metrics:
             values[metric.replace(""/"", "" / "")] = logs[metric]
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2a07da6..6f1bb62 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -961,6 +961,7 @@ def CausalLM_fast_forward(fast_forward_inference):
 pass
 
 
+@torch._disable_dynamo
 def PeftModelForCausalLM_fast_forward(
     self,
     input_ids=None,
"
"diff --git a/pyproject.toml b/pyproject.toml
index b61437e..455f847 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,6 +33,7 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
+    ""unsloth_zoo"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.44.2"",
@@ -210,6 +211,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
+    ""unsloth_zoo"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.44.2"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index e7db41c..abee9c9 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -27,6 +27,13 @@ import numpy as np
 #     pass
 # pass
 
+# Check for unsloth_zoo
+try:
+    import unsloth_zoo
+except:
+    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth-zoo`"")
+pass
+
 # Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
@@ -124,7 +131,7 @@ if ""SPACE_AUTHOR_NAME"" not in os.environ and ""SPACE_REPO_NAME"" not in os.environ
 
             # Try linking cuda folder, or everything in local
             if len(possible_cudas) == 0:
-                os.system(f""ldconfig /usr/local/"")
+                os.system(""ldconfig /usr/local/"")
             else:
                 find_number = re.compile(r""([\d\.]{2,})"")
                 latest_cuda = np.argsort([float(find_number.search(x).group(1)) for x in possible_cudas])[::-1][0]
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 9b5f9ff..cab6130 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -35,7 +35,9 @@ import shutil
 from .tokenizer_utils import *
 from .models._utils import patch_tokenizer
 import re
-
+from unsloth_zoo.dataset_utils import (
+    train_on_responses_only,
+)
 CHAT_TEMPLATES = {}
 
 # =========================================== Unsloth
@@ -910,7 +912,7 @@ def get_chat_template(
         # Check fast tokenizer
         if not is_fast_tokenizer:
             print(
-                f""Unsloth: Not a fast tokenizer, so can't process it as of yet :(\n""\
+                ""Unsloth: Not a fast tokenizer, so can't process it as of yet :(\n""\
                 ""Please log a Github issue if you want this as a new feature!\n""\
                 ""Your chat template will still work, but it won't add or edit tokens.""
             )
@@ -1236,7 +1238,7 @@ def to_sharegpt(
     n_extensions = max(conversation_extension-1, 0)
     if n_extensions == 0: return dataset
 
-    dataset = dataset.rename_columns({""conversations"" : f""conversations0""})
+    dataset = dataset.rename_columns({""conversations"" : ""conversations0""})
     all_shuffled = [dataset]
     for j in range(1, n_extensions+1):
         shuffled = dataset.shuffle(seed = random_state+j).rename_columns({""conversations0"" : f""conversations{j}""})
@@ -1254,7 +1256,7 @@ def to_sharegpt(
                 f""in zip({', '.join(f'conversations{j}__' for j in range(n_extensions))}):\n""
     function += f""{' '*8}convos.append(""\
                 f""{'+'.join(f'conversations{j}' for j in range(n_extensions))})\n""
-    function += f""{' '*4}return "" + ""{ "" + f""'conversations' : convos"" + "" }""
+    function += f""{' '*4}return "" + ""{ "" + ""'conversations' : convos"" + "" }""
 
     # Map function
     exec(function, globals())
@@ -1812,194 +1814,6 @@ extra_eos_tokens = None,
 pass
 
 
-# From https://www.geeksforgeeks.org/longest-common-substring-array-strings/
-# Longest Common Substring in an Array of Strings
-def _longest_common_substring(arr):
-    n = len(arr)
-    s = arr[0]
-    l = len(s)
-    res = """"
-    for i in range(l):
-        for j in range(i + 1, l + 1):
-            stem = s[i:j]
-            k = 1
-            for k in range(1, n):
-                if stem not in arr[k]:
-                    break
-            if (k + 1 == n and len(res) < len(stem)):
-                res = stem
-    return res
-pass
-
-
-def _find_common_token_ids(component, tokenizer):
-    """"""
-    \n### User:\n\n
-    \n\n### User:\n\n
-    etc
-    we need to find the middle most repeatted part.
-    Tokenizers can tokenize newlines or spaces as 1 token!
-    """"""
-    right_text = """"
-    if   component.endswith ("" ""): right_text = "" ""
-    elif component.endswith(""\n""): right_text = ""\n""
-    left_text = """"
-    if   component.startswith ("" ""): left_text = "" ""
-    elif component.startswith(""\n""): left_text = ""\n""
-    stripped = component.strip()
-
-    # Add current pieces and also newlines
-    all_input_ids = []
-    for left in range(3):
-        for right in range(3):
-            x = left*left_text + stripped + right*right_text
-            x = tokenizer(x, add_special_tokens = False).input_ids
-            all_input_ids.append(x)
-
-            x = left*""\n"" + stripped + right*""\n""
-            x = tokenizer(x, add_special_tokens = False).input_ids
-            all_input_ids.append(x)
-        pass
-    pass
-    substring = _longest_common_substring([str(x + [0]) for x in all_input_ids])
-    substring = substring.split("", "")[:-1]
-    substring = [int(x) for x in substring]
-
-    # Also get rest of tokenized string
-    original = tokenizer(component, add_special_tokens = False).input_ids
-    # Get optional left and right
-    for j in range(len(original)):
-        if original[j : j + len(substring)] == substring: break
-    optional_left  = original[:j]
-    optional_right = original[j+len(substring):]
-    return substring, optional_left, optional_right
-pass
-
-
-def train_on_responses_only(
-    trainer,
-    instruction_part = None,
-    response_part    = None,
-):
-    """"""
-    Trains only on responses and not on the instruction by masking out
-    the labels with -100 for the instruction part.
-    """"""
-    tokenizer = trainer.tokenizer
-    
-    if  not hasattr(tokenizer, ""_unsloth_input_part"") or \
-        not hasattr(tokenizer, ""_unsloth_output_part""):
-        
-        if instruction_part is None or response_part is None:
-            raise ValueError(""Unsloth: instruction_part and response_part must be given!"")
-        pass
-    elif (instruction_part is not None or response_part is not None) and \
-        (hasattr(tokenizer, ""_unsloth_input_part"") or hasattr(tokenizer, ""_unsloth_output_part"")):
-
-        raise ValueError(""Unsloth: Your tokenizer already has instruction and response parts set - do not give custom ones!"")
-    else:
-        instruction_part = tokenizer._unsloth_input_part
-        response_part    = tokenizer._unsloth_output_part
-    pass
-
-    # Get most common tokens since tokenizers can tokenize stuff differently!
-    Q_must, Q_left, Q_right = _find_common_token_ids(instruction_part, tokenizer)
-    A_must, A_left, A_right = _find_common_token_ids(response_part,    tokenizer)
-
-    # Store some temporary stuff
-    A_first = A_must[0]
-    len_A_must = len(A_must)
-    A_left_reversed = A_left[::-1]
-    A_right_forward = A_right
-
-    Q_first = Q_must[0]
-    len_Q_must = len(Q_must)
-    Q_left_reversed = Q_left[::-1]
-    Q_right_forward = Q_right
-
-    def _train_on_responses_only(examples):
-        input_ids_ = examples[""input_ids""]
-        all_labels = []
-
-        for input_ids in input_ids_:
-            n = len(input_ids)
-            labels = [-100] * n
-            n_minus_1 = n - 1
-            j = 0
-            while j < n:
-                # Find <assistant>
-                if (input_ids[j] == A_first) and \
-                    (input_ids[j : (k := j + len_A_must)] == A_must):
-
-                    # Now backtrack to get previous optional tokens
-                    for optional_left in A_left_reversed:
-                        if j < 1: break
-                        if optional_left == input_ids[j-1]: j -= 1
-                        else: break
-                    pass
-                    # And forwards look as well
-                    for optional_right in A_right_forward:
-                        if k >= n_minus_1: break
-                        if optional_right == input_ids[k+1]: k += 1
-                        else: break
-                    pass
-                    # assistant_j = j
-                    assistant_k = k
-
-                    j = assistant_k
-                    # Given <assistant>, now find next user
-                    while j < n:
-                        # Find <user>
-                        # Also accept last final item if assistant is the last turn
-                        if (j == n_minus_1) or \
-                            ((input_ids[j] == Q_first) and \
-                             (input_ids[j : (k := j + len_Q_must)] == Q_must)):
-
-                            # Now backtrack to get previous optional tokens
-                            for optional_left in Q_left_reversed:
-                                if j < 1: break
-                                if optional_left == input_ids[j-1]: j -= 1
-                                else: break
-                            pass
-                            # And forwards look as well
-                            for optional_right in Q_right_forward:
-                                if k >= n_minus_1: break
-                                if optional_right == input_ids[k+1]: k += 1
-                                else: break
-                            pass
-                            user_j = j
-                            # Account for last item
-                            if user_j != n_minus_1:
-                                # user_k = k
-                                # j = user_k
-                                j = k
-                            else:
-                                user_j = n
-                                k = n
-                            pass
-                            # Now copy input_ids to labels
-                            labels[assistant_k : user_j] = input_ids[assistant_k : user_j]
-                            # print(assistant_j, assistant_k, user_j, user_k)
-                            break
-                        pass
-                        j += 1
-                    pass
-                pass
-                j += 1
-            pass
-            all_labels.append(labels)
-        pass
-        return { ""labels"" : all_labels }
-    pass
-
-    if hasattr(trainer, ""train_dataset"") and trainer.train_dataset is not None:
-        trainer.train_dataset = trainer.train_dataset.map(_train_on_responses_only, batched = True)
-    if hasattr(trainer, ""eval_dataset"") and trainer.eval_dataset is not None:
-        trainer.eval_dataset = trainer.eval_dataset.map(_train_on_responses_only, batched = True)
-    return trainer
-pass
-
-
 def create_stopping_criteria(tokenizer, stop_word = ""eos_token""):
     class StoppingCriteriaSub(StoppingCriteria):
         __slots__ = ""stop_token"", ""single_match"", ""length"",
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 1fec5d7..5abed6a 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -355,6 +355,7 @@ def fast_cross_entropy_loss(
     labels,
     logit_softcapping = 0,
     logit_scaling = 0,
+    n_items = None,
 ):
     """"""
     Arguments:
@@ -372,7 +373,8 @@ def fast_cross_entropy_loss(
         logit_softcapping,
         logit_scaling,
     )
-    n_items = torch.count_nonzero(labels != -100)
+    if n_items is None:
+        n_items = torch.count_nonzero(labels != -100)
     return loss.sum() / n_items
 pass
 
@@ -409,6 +411,7 @@ replacement = """"""    loss = None
             labels = shift_labels,
             logit_softcapping = logit_softcapping,
             logit_scaling     = logit_scaling,
+            n_items           = kwargs.get(""n_items"", None),
         )
     else:
         if logit_scaling != 0:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index b14bb39..aa7a69c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.9.post4""
+__version__ = ""2024.10.0""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a245330..4cd512a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -975,13 +975,14 @@ def CausalLM_fast_forward(fast_forward_inference):
                 # Fixes https://github.com/unslothai/unsloth/issues/10
                 self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
             pass
-            
+
             shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
             loss = fast_cross_entropy_loss(
                 logits = shift_logits,
                 labels = shift_labels,
                 logit_softcapping = logit_softcapping,
                 logit_scaling     = logit_scaling,
+                n_items           = kwargs.get(""n_items"", None),
             )
         else:
             if logit_scaling != 0:
@@ -2019,8 +2020,8 @@ class FastLlamaModel:
             if loftq_config == {}:
                 from peft import LoftQConfig
                 logger.warning_once(
-                    f""Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\n""\
-                    f""We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.""
+                    ""Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\n""\
+                    ""We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.""
                 )
                 loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)
             pass
diff --git a/unsloth/save.py b/unsloth/save.py
index dce30c4..3760e23 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -555,7 +555,7 @@ def unsloth_save_model(
             #     max_ram = max(max_ram - W.nbytes, 0)
             else:
                 # Save to Disk
-                logger.warning_once(f""We will save to Disk and not RAM now."")
+                logger.warning_once(""We will save to Disk and not RAM now."")
                 filename = os.path.join(temporary_location, f""{name}.pt"")
                 torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)
                 # weights_only = True weirdly fails?
@@ -1460,7 +1460,7 @@ def fix_tokenizer_bos_token(tokenizer):
 
             fix_bos_token = True
             logger.warning(
-                f""Unsloth: ##### The current model auto adds a BOS token.\n""\
+                ""Unsloth: ##### The current model auto adds a BOS token.\n""\
                 ""Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.""
             )
 
@@ -1671,7 +1671,7 @@ def unsloth_save_pretrained_gguf(
 
     if fix_bos_token:
         logger.warning(
-            f""Unsloth: ##### The current model auto adds a BOS token.\n""\
+            ""Unsloth: ##### The current model auto adds a BOS token.\n""\
             ""Unsloth: ##### We removed it in GGUF's chat template for you.""
         )
     pass
@@ -1867,7 +1867,7 @@ def unsloth_push_to_hub_gguf(
 
     if fix_bos_token:
         logger.warning(
-            f""Unsloth: ##### The current model auto adds a BOS token.\n""\
+            ""Unsloth: ##### The current model auto adds a BOS token.\n""\
             ""Unsloth: ##### We removed it in GGUF's chat template for you.""
         )
     pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 196e496..63d07c9 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -26,6 +26,15 @@ import numpy as np
 import gc
 import subprocess
 
+from unsloth_zoo.tokenizer_utils import (
+    mean_of_trained_tokens,
+    add_new_tokens,
+    fix_untrained_tokens,
+)
+from unsloth_zoo.training_utils import (
+    fix_zero_training_loss,
+)
+
 __all__ = [
     ""load_correct_tokenizer"",
     ""fix_sentencepiece_tokenizer"",
@@ -807,347 +816,6 @@ def check_tokenizer(
 pass
 
 
-@torch.inference_mode
-def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):
-    """"""
-    Llama-3 for eg has untrained vectors in the base model.
-    These include <|eot_id|>, <|start_header_id|>, <|end_header_id|>
-    We reset them to the mean of the rest of the tokens
-    """"""
-    embedding_matrix = model.get_input_embeddings ().weight
-    lm_head_matrix   = model.get_output_embeddings().weight
-
-    # Ignore some model checks for now
-    if model.config._name_or_path in  IGNORED_TOKENIZER_NAMES:
-        return
-    pass
-
-    # Get untrained tokens
-    indicator_untrained1 = torch.amax(embedding_matrix, axis = 1) <= eps
-    # Check lm_head as well
-
-    # Does NOT work for Llama 3.1!!
-    indicator_untrained2 = torch.amax(lm_head_matrix,   axis = 1) <= eps
-
-    # We instead check for repeated vectors
-    lm_head_where = torch.where(indicator_untrained1)[0]
-    lm_head_bad = lm_head_matrix[lm_head_where]
-    lm_head_bad = lm_head_bad.cpu().float().numpy().round(3)
-    from collections import Counter
-    counter = Counter()
-    for row in lm_head_bad: counter[hash(row.data.tobytes())] += 1
-    counter = Counter({k: c for k, c in counter.items() if c >= 2})
-
-    lm_head_where = lm_head_where.cpu().numpy()
-    final_bad_lm_head = []
-    for j, row in enumerate(lm_head_bad):
-        if hash(row.data.tobytes()) in counter:
-            final_bad_lm_head.append(lm_head_where[j])
-    indicator_untrained2 = indicator_untrained2 | torch.zeros_like(indicator_untrained2)
-    indicator_untrained2[final_bad_lm_head] = True
-
-    # Combine both checks
-    indicator_untrained = indicator_untrained1 & indicator_untrained2
-    
-    where_untrained = torch.where(indicator_untrained)[0]
-    n_untrained = where_untrained.shape[0]
-    n_trained = embedding_matrix.shape[0] - n_untrained
-
-    # Get set and actual tokens
-    where_untrained = where_untrained.tolist()
-    if len(where_untrained) == 0: return
-
-    # Remove untrained indices where it's longer
-    
-    where_untrained_set = frozenset(where_untrained)
-    actual_bad_tokens = tokenizer.convert_ids_to_tokens(where_untrained)
-    # Remove None items in actual_bad_tokens
-    actual_bad_tokens = [x for x in actual_bad_tokens if x is not None]
-
-    # Check if tokenizer and training datasets have bad tokens
-    if_bad_first  = False
-    if_bad_second = False
-    # Check tokenizer's chat template for any untrained tokens
-    chat_template = getattr(tokenizer, ""chat_template"", None)
-    if chat_template is not None:
-        if_bad_first = any(x in chat_template for x in actual_bad_tokens)
-    pass
-
-    # Check the first 250, last 250 input_ids
-    size_dataset = len(train_dataset)
-    size = min(size_dataset, 250)
-    for j in range(size):
-        input_ids = train_dataset[j]
-        if ""input_ids"" in input_ids:
-            input_ids = input_ids[""input_ids""]
-            if_bad = any(item in where_untrained_set for item in input_ids)
-            if if_bad:
-                if_bad_second = True
-                break
-            pass
-        pass
-    pass
-
-    # Check last 250
-    if not if_bad_second:
-        left = max(size_dataset-250, 0)
-        for j in range(left, size_dataset):
-            input_ids = train_dataset[j]
-            if ""input_ids"" in input_ids:
-                input_ids = input_ids[""input_ids""]
-                if_bad = any(item in where_untrained_set for item in input_ids)
-                if if_bad:
-                    if_bad_second = True
-                    break
-                pass
-            pass
-        pass
-    pass
-
-    # Check if bad tokens exists!
-    if not if_bad_first and not if_bad_second: return
-
-    # Check if lm_head / embed_token are trainable!
-    bad_not_trainable = False
-    if not embedding_matrix.requires_grad: bad_not_trainable = True
-    if not lm_head_matrix  .requires_grad: bad_not_trainable = True
-
-    if bad_not_trainable:
-
-        final_bad_items = []
-
-        # Re-check the first 250, last 250 input_ids
-        size_dataset = len(train_dataset)
-        size = min(size_dataset, 250)
-        for j in range(size):
-            input_ids = train_dataset[j]
-            if ""input_ids"" in input_ids:
-                input_ids = input_ids[""input_ids""]
-                for item in input_ids:
-                    if item in where_untrained_set: final_bad_items.append(item)
-            pass
-        pass
-
-        # Re-check last 250
-        left = max(size_dataset-250, 0)
-        for j in range(left, size_dataset):
-            input_ids = train_dataset[j]
-            if ""input_ids"" in input_ids:
-                input_ids = input_ids[""input_ids""]
-                for item in input_ids:
-                    if item in where_untrained_set: final_bad_items.append(item)
-            pass
-        pass
-
-        raise ValueError(
-            f'Unsloth: Untrained tokens of [{list(set(final_bad_items))}] found, but embed_tokens & lm_head not trainable, causing NaNs. '\
-            'Restart then add `embed_tokens` & `lm_head` to '\
-            '`FastLanguageModel.get_peft_model(target_modules = [..., ""embed_tokens"", ""lm_head"",]). `'\
-            'Are you using the `base` model? Instead, use the `instruct` version to silence this warning.',
-        )
-    pass
-
-    # Count all the possible bad tokens
-    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)
-    def mapping(examples):
-        input_ids = examples[""input_ids""]
-        counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)
-        np.add.at(final_counts, counter, 1)
-    pass
-    train_dataset.map(mapping, batched = True, desc = ""Counting untrained tokens"")
-
-    # Get sum of all items
-    sum_embedding = torch.sum(embedding_matrix, dtype = torch.float32, axis = 0)
-    sum_lm_head   = torch.sum(lm_head_matrix,   dtype = torch.float32, axis = 0)
-
-    # Remove bad tokens
-    sum_embedding -= torch.sum(embedding_matrix[where_untrained], dtype = torch.float32, axis = 0)
-    sum_lm_head   -= torch.sum(lm_head_matrix  [where_untrained], dtype = torch.float32, axis = 0)
-
-    # Find correct average by dividing by sum of trained tokens
-    mean_embedding = (sum_embedding / n_trained)
-    mean_lm_head   = (sum_lm_head   / n_trained)
-
-    # Scale each to be equal to 1/max_frequency. Also set some to 0 if none seen
-    scaling = final_counts[where_untrained] / max(final_counts.max(), 1)
-    scaling = torch.tensor(scaling, device = mean_embedding.device).unsqueeze(1)
-    mean_embedding = mean_embedding.repeat((n_untrained, 1,)) * scaling
-    mean_lm_head   = mean_lm_head  .repeat((n_untrained, 1,)) * scaling
-    where_null = scaling.ravel() == 0
-    mean_embedding[where_null] = 0
-    mean_lm_head  [where_null] = 0
-
-    # Set them to the mean
-    logger.warning(
-        ""Unsloth: Setting embed_tokens & lm_head untrained tokens to ""\
-        ""mean(trained) to counteract NaNs during training.""
-    )
-    embedding_matrix[where_untrained] = mean_embedding.to(embedding_matrix.dtype)
-    lm_head_matrix  [where_untrained] = mean_lm_head  .to(lm_head_matrix  .dtype)
-
-    # Clean up
-    for _ in range(3):
-        gc.collect()
-        torch.cuda.empty_cache()
-    pass
-    return
-pass
-
-
-@torch.inference_mode
-def mean_of_trained_tokens(model, eps = 1e-16):
-    """"""
-    Llama-3 for eg has untrained vectors in the base model.
-    These include <|eot_id|>, <|start_header_id|>, <|end_header_id|>
-    We reset them to the mean of the rest of the tokens
-    """"""
-    embedding_matrix = model.get_input_embeddings ().weight.clone()
-    lm_head_matrix   = model.get_output_embeddings().weight.clone()
-
-    # Get untrained tokens
-    indicator_untrained = torch.amax(embedding_matrix, axis = 1) <= eps
-    where_untrained = torch.where(indicator_untrained)[0]
-    n_untrained = where_untrained.shape[0]
-    n_trained = embedding_matrix.shape[0] - n_untrained
-    # if n_untrained != 0:
-    #     print(
-    #         f""Unsloth: Not an error, but your model has {n_untrained} untrained tokens.\n""\
-    #         ""We shall set them to the mean of the other trained tokens.""
-    #     )
-    # pass
-
-    # Get sum of all items
-    sum_embedding = torch.sum(embedding_matrix, dtype = torch.float32, axis = 0)
-    sum_lm_head   = torch.sum(lm_head_matrix,   dtype = torch.float32, axis = 0)
-
-    # Remove bad tokens
-    sum_embedding -= torch.sum(embedding_matrix[where_untrained], dtype = torch.float32, axis = 0)
-    sum_lm_head   -= torch.sum(lm_head_matrix  [where_untrained], dtype = torch.float32, axis = 0)
-
-    # Find correct average by dividing by sum of trained tokens
-    mean_embedding = (sum_embedding / n_trained)
-    mean_lm_head   = (sum_lm_head   / n_trained)
-
-    return mean_embedding, mean_lm_head
-pass
-
-
-@torch.inference_mode
-def add_new_tokens(
-    model,
-    tokenizer,
-    new_tokens = [],
-    method = ""mean"",
-    interpolation = 0.5,
-):
-    """"""
-    Smartly resizes the tokenizer and adds new tokens to the model.
-    We also disregard untrained tokens by removing them from the mean calculation.
-    """"""
-    assert(isinstance(new_tokens, (list, tuple)))
-    assert(len(new_tokens) > 0)
-    assert(method == ""mean"" or method == ""interpolation"")
-    assert(interpolation >= 0 and interpolation <= 1)
-
-    # Check if tokens already exist
-    overlapping_tokens = set(new_tokens) & set(tokenizer.vocab.keys())
-    if len(overlapping_tokens) != 0:
-        print(
-            f""Unsloth: You're adding new_tokens = {new_tokens}\n""\
-            f""There are tokens which are overlapping = {list(overlapping_tokens)}\n""\
-            f""We shall safely ignore these overlapping tokens.""
-        )
-        new_tokens = [x for x in new_tokens if x not in overlapping_tokens]
-    pass
-
-    # Get mean of trained tokens
-    # mean_embedding, mean_lm_head = fix_untrained_tokens(model)
-
-    # Weirdly be careful reserved tokens can pop out
-    mean_embedding, mean_lm_head = mean_of_trained_tokens(model)
-    mean_embedding = mean_embedding.to(torch.float32)
-    mean_lm_head   = mean_lm_head  .to(torch.float32)
-
-    # Add tokens!
-    old_length = len(tokenizer)
-    tokenizer.add_tokens(new_tokens)
-    model.resize_token_embeddings(len(tokenizer))
-
-    # If we use interpolation, we interpolate between the mean embeddings and
-    # the Word2Vec sum of the other vectors
-    embedding_matrix = model.get_input_embeddings ().weight
-    lm_head_matrix   = model.get_output_embeddings().weight
-
-    if method == ""interpolation"":
-        print(
-            ""Unsloth: You are using interpolation to add new tokens.\n""\
-            f""We shall set new tokens = mean(embeddings)*{1-interpolation} + mean(new_tokens)*{interpolation}""
-        )
-        for j, token in enumerate(new_tokens):
-            input_ids = tokenizer(token, add_special_tokens = False).input_ids
-            mean_embedding_token = embedding_matrix[input_ids].mean(axis = 0, dtype = torch.float32)
-            mean_lm_head_token   = lm_head_matrix  [input_ids].mean(axis = 0, dtype = torch.float32)
-
-            # Interpolate
-            mean_embedding_token = mean_embedding*(1-interpolation) + mean_embedding_token*interpolation
-            mean_lm_head_token   = mean_lm_head  *(1-interpolation) + mean_lm_head_token  *interpolation
-
-            # Set the new vector
-            embedding_matrix[old_length+j] = mean_embedding_token
-            lm_head_matrix  [old_length+j] = mean_lm_head_token
-        pass
-    else:
-        # Now set the new tokens to the mean!
-        embedding_matrix[old_length:] = mean_embedding
-        lm_head_matrix  [old_length:] = mean_lm_head
-    pass
-
-    # We set a flag to say we need to train embeddings
-    internal_model = model
-    while hasattr(internal_model, ""model""):
-        internal_model._need_to_train_embeddings = True
-        internal_model = internal_model.model
-    pass
-    internal_model._need_to_train_embeddings = True
-    
-    return
-pass
-
-
-@torch.inference_mode
-def fix_zero_training_loss(model, tokenizer, train_dataset):
-    """"""
-    Sometimes the labels get masked by all -100s, causing the loss
-    to be 0. We check for this!
-    """"""
-    if len(train_dataset) == 0: return
-
-    row = train_dataset[0]
-    if type(row) is dict and ""labels"" in row:
-
-        # Check the first 100 rows
-        seen_bad  = 0
-        seen_good = 0
-        for i, row in enumerate(train_dataset):
-            try:    check_tokens = list(set(row[""labels""]))
-            except: continue
-            if len(check_tokens) == 1 and check_tokens[0] == -100: seen_bad += 1
-            else: seen_good += 1
-            if i >= 100: break
-        pass
-
-        # Check ratio
-        if seen_bad / (seen_bad + seen_good) >= 0.9:
-            logger.warning(
-                ""Unsloth: Most labels in your dataset are -100. Training losses will be 0.\n""\
-                ""For example, are you sure you used `train_on_responses_only` correctly?\n""\
-                ""Or did you mask our tokens incorrectly? Maybe this is intended?""
-            )
-        pass
-    pass
-pass
-
-
 def check_nvidia():
     # Unsloth doesn't work yet on AMD devices - we're working on it!
     output = np.array([0,])
@@ -1260,7 +928,7 @@ def patch_sft_trainer_tokenizer():
         ""    torch.cuda.empty_cache()\n""\
         ""pass\n""\
         ""\n""\
-        ""fix_untrained_tokens(self.model, self.tokenizer, self.train_dataset, eps = 1e-16)\n\n""\
+        ""fix_untrained_tokens(self.model, self.tokenizer, self.train_dataset, IGNORED_TOKENIZER_NAMES, eps = 1e-16)\n\n""\
         ""fix_zero_training_loss(self.model, self.tokenizer, self.train_dataset)\n\n""
 
         # Add NEFTune since it doesn't seem to work?? We need to manually inject it
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index 45616ca..c9c0ca2 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -22,10 +22,12 @@ except:
     from transformers import TrainingArguments
 pass
 from . import is_bfloat16_supported
+from unsloth_zoo.training_utils import unsloth_train
 
 __all__ = [
     ""UnslothTrainingArguments"",
     ""UnslothTrainer"",
+    ""unsloth_train"",
 ]
 
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0658f3f..05c05ae 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -556,10 +556,10 @@ from unsloth import DEVICE_TYPE
 def is_big_gpu(index) -> bool:
 
     if DEVICE_TYPE == ""xpu"":
-        prop = torch.xpu.get_device_properties(index)
+        prop = DeviceProperties.create(torch.device(""xpu"", index) if type(index) is int else index)
         min_sms = 16
     else:
-        prop = torch.cuda.get_device_properties(index)
+        prop = DeviceProperties.create(torch.device(""cuda"", index) if type(index) is int else index)
         min_sms = 80
 
     avail_sms = prop.multi_processor_count
"
"diff --git a/pyproject.toml b/pyproject.toml
index 2cbe68f..b61908a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -43,111 +43,154 @@ huggingface = [
     ""wheel>=0.42.0"",
     ""numpy"",
     ""accelerate>=0.26.1"",
-    ""trl>=0.7.9,<0.9.0"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
     ""hf_transfer"",
 ]
 cu118only = [
-    ""xformers==0.0.22.post7"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu121only = [
-    ""xformers==0.0.22.post7"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu118onlytorch211 = [
-    ""xformers==0.0.23"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu121onlytorch211 = [
-    ""xformers==0.0.23"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu118onlytorch212 = [
-    ""xformers==0.0.23.post1"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu121onlytorch212 = [
-    ""xformers==0.0.23.post1"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu118onlytorch220 = [
-    ""xformers==0.0.24"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu121onlytorch220 = [
-    ""xformers==0.0.24"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
 cu118onlytorch230 = [
-    ""xformers==0.0.26.post1"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
 ]
 cu121onlytorch230 = [
-    ""xformers==0.0.26.post1"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
+]
+cu118onlytorch240 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
+]
+cu121onlytorch240 = [
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27.post2-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27.post2-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
 ]
-
 cu118 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118only]"",
 ]
 cu121 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121only]"",
 ]
 cu118-torch211 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch211]"",
 ]
 cu121-torch211 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch211]"",
 ]
 cu118-torch212 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch212]"",
 ]
 cu121-torch212 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch212]"",
 ]
 cu118-torch220 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch220]"",
 ]
 cu121-torch220 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch220]"",
 ]
 cu118-torch230 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch230]"",
 ]
 cu121-torch230 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch230]"",
 ]
+cu118-torch240 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu118onlytorch240]"",
+]
+cu121-torch240 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu121onlytorch240]"",
+]
 kaggle = [
     ""unsloth[huggingface]"",
 ]
 kaggle-new = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
 ]
 conda = [
     ""unsloth[huggingface]"",
 ]
 colab-torch211 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch211]"",
 ]
 colab-ampere-torch211 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch211]"",
     ""packaging"",
     ""ninja"",
@@ -155,12 +198,12 @@ colab-ampere-torch211 = [
 ]
 colab-torch220 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch220]"",
 ]
 colab-ampere-torch220 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch220]"",
     ""packaging"",
     ""ninja"",
@@ -182,10 +225,10 @@ colab-new = [
 ]
 colab-no-deps = [
     ""accelerate>=0.26.1"",
-    ""trl>=0.7.9"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
     ""peft>=0.7.1"",
     ""xformers<0.0.27"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""protobuf<4.0.0"",
 ]
 colab = [
@@ -199,7 +242,7 @@ colab-ampere = [
 ]
 cu118-ampere = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118only]"",
     ""packaging"",
     ""ninja"",
@@ -207,7 +250,7 @@ cu118-ampere = [
 ]
 cu121-ampere = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121only]"",
     ""packaging"",
     ""ninja"",
@@ -215,7 +258,7 @@ cu121-ampere = [
 ]
 cu118-ampere-torch211 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch211]"",
     ""packaging"",
     ""ninja"",
@@ -223,7 +266,7 @@ cu118-ampere-torch211 = [
 ]
 cu121-ampere-torch211 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch211]"",
     ""packaging"",
     ""ninja"",
@@ -231,7 +274,7 @@ cu121-ampere-torch211 = [
 ]
 cu118-ampere-torch220 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch220]"",
     ""packaging"",
     ""ninja"",
@@ -239,7 +282,7 @@ cu118-ampere-torch220 = [
 ]
 cu121-ampere-torch220 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch220]"",
     ""packaging"",
     ""ninja"",
@@ -247,7 +290,7 @@ cu121-ampere-torch220 = [
 ]
 cu118-ampere-torch230 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch230]"",
     ""packaging"",
     ""ninja"",
@@ -255,12 +298,28 @@ cu118-ampere-torch230 = [
 ]
 cu121-ampere-torch230 = [
     ""unsloth[huggingface]"",
-    ""bitsandbytes"",
+    ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch230]"",
     ""packaging"",
     ""ninja"",
     ""flash-attn>=2.6.3"",
 ]
+cu118-ampere-torch240 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu118onlytorch240]"",
+    ""packaging"",
+    ""ninja"",
+    ""flash-attn>=2.6.3"",
+]
+cu121-ampere-torch240 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu121onlytorch240]"",
+    ""packaging"",
+    ""ninja"",
+    ""flash-attn>=2.6.3"",
+]
 
 [project.urls]
 homepage = ""http://www.unsloth.ai""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 195fd5b..0c00574 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -53,7 +53,9 @@ from packaging.version import Version
 # Disable some warnings which can get annoying
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""torch"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""trl"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""xformers"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""transformers"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""accelerate"")
@@ -133,6 +135,28 @@ else:
 pass
 # =============================================
 
+# =============================================
+# Fix KeyError: 'Cache only has 0 layers, attempted to access layer with index 0'
+import transformers.cache_utils
+if hasattr(transformers.cache_utils, ""DynamicCache"") and \
+    transformers.cache_utils.DynamicCache.__getitem__.__name__ != ""__cache_utils_getitem__"":
+
+    source = inspect.getsource(transformers.cache_utils.DynamicCache.__getitem__)
+    start = source.find(""def"")
+    spaces = start*"" ""
+    source = source.split(""\n"")
+    source = ""\n"".join(x[start:] for x in source)
+    where = source.find(""raise KeyError"")
+    source = source[:where] + \
+        f""if len(self) == 0:\n{spaces}{spaces}""\
+        ""    raise RuntimeError('Unsloth: You must call `FastLanguageModel.for_inference(model)` before doing inference for Unsloth models.')\n"" + \
+        f""{spaces}{spaces}else:\n{spaces}{spaces}{spaces}"" + source[where:]
+    source = source.replace(""__getitem__"", ""__cache_utils_getitem__"", 1)
+    exec(source)
+    transformers.cache_utils.DynamicCache.__getitem__ = __cache_utils_getitem__
+pass
+# =============================================
+
 # =============================================
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 import bitsandbytes as bnb
@@ -192,7 +216,7 @@ from transformers.models.llama.modeling_llama import logger
 # Get Xformers
 from xformers import __version__ as xformers_version
 # Temporarily disable 0.0.27 and higher - inference issues
-if Version(xformers_version) >= Version(""0.0.27""):
+if False: #Version(xformers_version) >= Version(""0.0.27""):
     raise ImportError(
         ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
         ""then press Disconnect Runtime and then Restart it.\n""\
@@ -200,10 +224,10 @@ if Version(xformers_version) >= Version(""0.0.27""):
         ""%%capture\n""
         ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
         '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
-        '!pip install --no-deps ""xformers<0.0.27"" ""trl<0.9.0"" peft accelerate bitsandbytes\n'\
+        '!pip install --no-deps ""xformers<=0.0.27"" trl peft accelerate bitsandbytes\n'\
         '\n'\
         f""Otherwise in local machines, your xformers version of {xformers_version} is too new.\n""\
-        'Please downgrade xformers via `pip install --force-reinstall ""xformers<0.0.27""'
+        'Please downgrade xformers via `pip install --force-reinstall ""xformers<=0.0.27""'
     )
 pass
 
@@ -217,10 +241,10 @@ elif Version(torch_version) < Version(""2.3.0"") and Version(xformers_version) >=
         f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
         f""Please install xformers < 0.0.26 for torch = {torch_version}.""
     )
-elif Version(torch_version) < Version(""2.4.0"") and Version(xformers_version) >= Version(""0.0.27""):
+elif Version(torch_version) < Version(""2.4.0"") and Version(xformers_version) > Version(""0.0.27""):
     raise ImportError(
         f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
-        f""Please install xformers < 0.0.27 for torch = {torch_version}.""
+        f""Please install xformers <= 0.0.27 for torch = {torch_version}.""
     )
 pass
 
@@ -241,7 +265,8 @@ xformers_attention = xformers.memory_efficient_attention
 
 # Check TRL version
 from trl import __version__ as trl_version
-if Version(trl_version) >= Version(""0.9.0""):
+# Unsloth now supports all TRL versions!
+if False:#Version(trl_version) >= Version(""0.9.0""):
     raise ImportError(
         ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
         ""then press Disconnect Runtime and then Restart it.\n""\
@@ -249,13 +274,32 @@ if Version(trl_version) >= Version(""0.9.0""):
         ""%%capture\n""
         ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
         '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
-        '!pip install --no-deps ""xformers<0.0.27"" ""trl<0.9.0"" peft accelerate bitsandbytes\n'\
+        '!pip install --no-deps ""xformers<=0.0.27"" trl peft accelerate bitsandbytes\n'\
         '\n'\
         f""Otherwise in local machines, your TRL version of {trl_version} is too new.\n""\
-        'Please downgrade TRL via `pip install --force-reinstall ""trl<0.9.0""'
+        'Please downgrade TRL via `pip install --force-reinstall trl'
     )
 pass
 
+# =============================================
+# Fix new Xformers versions TypeError: Multiple dispatch failed for 'torch._ops.aten.to.dtype_layout'
+if Version(xformers_version) >= Version(""0.0.27""):
+    import accelerate.utils.operations
+    if hasattr(accelerate.utils.operations, ""send_to_device"") and \
+        accelerate.utils.operations.send_to_device.__name__ != ""_fixed_send_to_device"":
+        from accelerate.utils.operations import *
+        send_to_device = inspect.getsource(accelerate.utils.operations.send_to_device)
+        send_to_device = re.sub(
+            r""([ ]{4,})return tensor\.to\(device\)"",
+            r""\1try: return tensor.to(device)\n\1except: return tensor"",
+            send_to_device,
+        ).replace(""def send_to_device"", ""def _fixed_send_to_device"")
+        exec(send_to_device)
+        accelerate.utils.operations.send_to_device = _fixed_send_to_device
+    pass
+pass
+# =============================================
+
 # =============================================
 # Torch compile settings
 
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 1cbaf5b..ea9f53e 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -156,6 +156,7 @@ def Gemma2Attention_fast_forward(
         )
         A = A.reshape(bsz, q_len, n_heads*head_dim)
     else:
+        mask = causal_mask if attention_mask is None else attention_mask
         A = slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, kv_seq_len)
     pass
     A = self.apply_o(self, A)
@@ -413,7 +414,6 @@ def Gemma2Model_fast_forward_inference(
         SWA = attention_mask
         GA  = attention_mask
     pass
-
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.model.layers):
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index e300e07..2a07da6 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -419,7 +419,7 @@ pass
 def LlamaDecoderLayer_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask           = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
@@ -505,7 +505,7 @@ def LlamaModel_fast_forward(
     return_dict:          Optional[bool] = None,
     *args, **kwargs,
 ) -> Union[Tuple, BaseModelOutputWithPast]:
-
+    
     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
     assert(output_attentions is False)
     output_hidden_states = (
@@ -682,12 +682,27 @@ def LlamaModel_fast_forward(
 
 
     # Gemma2 has alternating SWA and global attn
-    if IS_GEMMA2 and not hasattr(self, ""SWA_mask""):
-        if HAS_FLASH_ATTENTION_SOFTCAPPING:
+    if IS_GEMMA2:
+        if HAS_FLASH_ATTENTION_SOFTCAPPING and attention_mask is None:
             self.SWA_mask = True
             self.GA_mask  = False
-        else:
-            n = self.config.max_position_embeddings
+        elif attention_mask is not None:
+            self.SWA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask,
+                (batch_size, seq_length),
+                inputs_embeds,
+                past_key_values_length,
+                sliding_window = self.config.sliding_window,
+            )
+            self.GA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask,
+                (batch_size, seq_length),
+                inputs_embeds,
+                past_key_values_length,
+                sliding_window = None,
+            )
+        elif not hasattr(self, ""SWA_mask""):
+            n = self.max_seq_length # self.config.max_position_embeddings
             # masked_fill is making stuff slower!
             # self. GA_mask = create_boolean_mask(n = n, sliding_window = 0)
             # self.SWA_mask = create_boolean_mask(n = n, sliding_window = self.config.sliding_window)
@@ -870,7 +885,7 @@ def CausalLM_fast_forward(fast_forward_inference):
             )
         else:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
-    
+
             output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
             output_hidden_states = (
                 output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
@@ -879,7 +894,6 @@ def CausalLM_fast_forward(fast_forward_inference):
 
             # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
             self.model._has_no_labels = labels is None
-
             outputs = self.model(
                 input_ids=input_ids,
                 causal_mask=causal_mask,
@@ -893,7 +907,6 @@ def CausalLM_fast_forward(fast_forward_inference):
                 return_dict=return_dict,
             )
         pass
-
         hidden_states = outputs[0]
         bsz, q_len, hd = hidden_states.shape
         lm_head = self.lm_head.weight
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 47152d6..cce22ae 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -93,7 +93,7 @@ def _get_new_mapper():
 pass
 
 
-def _get_model_name(model_name, load_in_4bit = True):
+def get_model_name(model_name, load_in_4bit = True):
     new_model_name = __get_model_name(
         model_name = model_name,
         load_in_4bit = load_in_4bit,
@@ -145,7 +145,7 @@ class FastLanguageModel(FastLlamaModel):
             token = os.environ[""HUGGINGFACE_TOKEN""]
 
         old_model_name = model_name
-        model_name = _get_model_name(model_name, load_in_4bit)
+        model_name = get_model_name(model_name, load_in_4bit)
 
         # First check if it's a normal model via AutoConfig
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
@@ -192,7 +192,7 @@ class FastLanguageModel(FastLlamaModel):
         # Get base model for PEFT:
         if is_peft:
             # Check base model again for PEFT
-            model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
+            model_name = get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
             model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
         pass
 
diff --git a/unsloth/save.py b/unsloth/save.py
index a5904ef..f45d806 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -28,12 +28,14 @@ import psutil
 import re
 from transformers.models.llama.modeling_llama import logger
 from .tokenizer_utils import fix_sentencepiece_gguf
+from huggingface_hub import HfApi
 
 __all__ = [
     ""print_quantization_methods"",
     ""unsloth_save_model"",
     ""save_to_gguf"",
     ""patch_saving_functions"",
+    ""create_huggingface_repo"",
 ]
 
 # Check environments
@@ -207,8 +209,9 @@ def unsloth_save_model(
 ):
     if token is None and ""HF_TOKEN"" in os.environ:
         token = os.environ[""HF_TOKEN""]
-
-    if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
+    elif token is None and ""hf_token"" in os.environ:
+        token = os.environ[""hf_token""]
+    elif token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
         token = os.environ[""HUGGINGFACE_TOKEN""]
 
     if commit_message is None: commit_message = """"
@@ -555,7 +558,8 @@ def unsloth_save_model(
                 logger.warning_once(f""We will save to Disk and not RAM now."")
                 filename = os.path.join(temporary_location, f""{name}.pt"")
                 torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)
-                state_dict[name] = torch.load(filename, map_location = ""cpu"", mmap = True)
+                # weights_only = True weirdly fails?
+                state_dict[name] = torch.load(filename, map_location = ""cpu"", mmap = True, weights_only = False)
         pass
         for item in LLAMA_LAYERNORMS:
             try:
@@ -675,7 +679,6 @@ def unsloth_save_model(
         # Now manually go through each file and upload them manually!
         filenames = os.listdir(new_save_directory)
 
-        from huggingface_hub import HfApi
         hf_api = HfApi(token = save_pretrained_settings[""token""])
 
         print(""Unsloth: Uploading all files... Please wait..."")
@@ -1312,6 +1315,49 @@ def _determine_username(save_directory, old_username, token):
 pass
 
 
+def create_huggingface_repo(
+    model,
+    save_directory,
+    token = None,
+    private = False,
+):
+    if token is None and ""HF_TOKEN"" in os.environ:
+        token = os.environ[""HF_TOKEN""]
+    elif token is None and ""hf_token"" in os.environ:
+        token = os.environ[""hf_token""]
+    elif token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
+        token = os.environ[""HUGGINGFACE_TOKEN""]
+    pass
+    save_directory, username = _determine_username(save_directory, """", token)
+
+    from huggingface_hub import create_repo
+    try:
+        create_repo(
+            repo_id   = save_directory,
+            token     = token,
+            repo_type = ""model"",
+            exist_ok  = False,
+            private   = private,
+        ) 
+
+        # Create model card
+        from huggingface_hub import ModelCard
+        content = MODEL_CARD.format(
+            username   = username,
+            base_model = model.config._name_or_path,
+            model_type = model.config.model_type,
+            method     = """",
+            extra      = ""unsloth"",
+        )
+        card = ModelCard(content)
+        card.push_to_hub(save_directory, token = token)
+    except:
+        pass
+    hf_api = HfApi(token = token)
+    return save_directory, hf_api
+pass
+
+
 def upload_to_huggingface(
     model,
     save_directory,
@@ -1321,6 +1367,7 @@ def upload_to_huggingface(
     file_location = None,
     old_username = None,
     private = None,
+    create_config = True,
 ):
     save_directory, username = _determine_username(save_directory, old_username, token)
 
@@ -1350,7 +1397,6 @@ def upload_to_huggingface(
 
     if file_location is not None:
         # Now upload file
-        from huggingface_hub import HfApi
         hf_api = HfApi(token = token)
 
         if ""/"" in file_location:
@@ -1372,6 +1418,8 @@ def upload_to_huggingface(
                     repo_type       = ""model"",
                     commit_message  = ""(Trained with Unsloth)"",
                 )
+            pass
+        pass
 
         hf_api.upload_file(
             path_or_fileobj = file_location,
@@ -1382,18 +1430,20 @@ def upload_to_huggingface(
         )
 
         # We also upload a config.json file
-        import json
-        with open(""_temporary_unsloth_config.json"", ""w"") as file:
-            json.dump({""model_type"" : model.config.model_type}, file, indent = 4)
+        if create_config:
+            import json
+            with open(""_temporary_unsloth_config.json"", ""w"") as file:
+                json.dump({""model_type"" : model.config.model_type}, file, indent = 4)
+            pass
+            hf_api.upload_file(
+                path_or_fileobj = ""_temporary_unsloth_config.json"",
+                path_in_repo    = ""config.json"",
+                repo_id         = save_directory,
+                repo_type       = ""model"",
+                commit_message  = ""(Trained with Unsloth)"",
+            )
+            os.remove(""_temporary_unsloth_config.json"")
         pass
-        hf_api.upload_file(
-            path_or_fileobj = ""_temporary_unsloth_config.json"",
-            path_in_repo    = ""config.json"",
-            repo_id         = save_directory,
-            repo_type       = ""model"",
-            commit_message  = ""(Trained with Unsloth)"",
-        )
-        os.remove(""_temporary_unsloth_config.json"")
     pass
     return username
 pass
"
"diff --git a/.github/ISSUE_TEMPLATE/bug---issue.md b/.github/ISSUE_TEMPLATE/bug---issue.md
index ff508cf..2849538 100644
--- a/.github/ISSUE_TEMPLATE/bug---issue.md
+++ b/.github/ISSUE_TEMPLATE/bug---issue.md
@@ -15,5 +15,4 @@ assignees: ''
 6. Which trainer? `SFTTrainer`, `GRPOTrainer` etc
 7. **Minimal code to reproduce error Remove Hugging Face token!**
 
-You can also join our Discord: https://discord.com/invite/unsloth
-Have you tried visiting our Docs? https://docs.unsloth.ai/basics/errors-troubleshooting
+ You can also ask via our Reddit page: https://www.reddit.com/r/unsloth/
"
"diff --git a/pyproject.toml b/pyproject.toml
index 83efab1..506e9a7 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -32,18 +32,8 @@ include-package-data = false
 exclude = [""images*""]
 
 [project.optional-dependencies]
-huggingfacedev = [
-    ""transformers @ git+https://github.com/huggingface/transformers"",
-    ""datasets"",
-    ""sentencepiece"",
-    ""accelerate"",
-    ""trl>=0.7.9"",
-    ""peft"",
-    ""tqdm"",
-    ""psutil"",
-]
 huggingface = [
-    ""transformers"",
+    ""transformers>=4.37.0"",
     ""datasets"",
     ""sentencepiece"",
     ""accelerate"",
@@ -107,15 +97,15 @@ colab_ampere = [
     ""ninja"",
     ""flash-attn"",
 ]
-colab_dev = [
-    ""unsloth[huggingfacedev]"",
+colab_torch211 = [
+    ""unsloth[huggingface]"",
     ""bitsandbytes"",
-    ""unsloth[cu121only]"",
+    ""unsloth[cu121onlytorch211]"",
 ]
-colab_ampere_dev = [
-    ""unsloth[huggingfacedev]"",
+colab_ampere_torch211 = [
+    ""unsloth[huggingface]"",
     ""bitsandbytes"",
-    ""unsloth[cu121only]"",
+    ""unsloth[cu121onlytorch211]"",
     ""packaging"",
     ""ninja"",
     ""flash-attn"",
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index b70e6e4..b487ff9 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -141,7 +141,7 @@ class LoRA_MLP(torch.autograd.Function):
 
         # Gate projection LoRA weights
         d_gateA = X.t() @ (DW_dfg @ gateB.t())
-        d_gateB = (gateA.t() @ X.t() @ DW_dfg)
+        d_gateB = (gateA.t() @ X.t()) @ DW_dfg
         d_gateA *= gateS
         d_gateB *= gateS
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 8ddf4d6..5f35851 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -30,7 +30,19 @@ major_version, minor_version = torch.cuda.get_device_capability()
 if major_version >= 8:
     try:
         from flash_attn import flash_attn_func
-        HAS_FLASH_ATTENTION = True
+        # Check for CUDA linking errors ""undefined symbol: _ZNK3c106SymIntltEl""
+        try:
+            from flash_attn.flash_attn_interface import flash_attn_cuda
+            HAS_FLASH_ATTENTION = True
+        except:
+            logger.warning_once(
+                ""Unsloth: Your Flash Attention 2 installation seems to be broken?\n""\
+                ""A possible explanation is you have a new CUDA version which isn't\n""\
+                ""yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n""\
+                ""We shall now use Xformers instead, which gets a 0.01% performance hit.\n""\
+                ""We found this negligible impact by benchmarking on 1x A100.""
+            )
+            HAS_FLASH_ATTENTION = False
     except:
         HAS_FLASH_ATTENTION = False
 else:
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 40074ad..da99ea4 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(
     bsz, q_len, _ = hidden_states.size()
 
     # Check for inference
-    if past_key_value is not None and q_len == 1:
+    if past_key_value is not None and q_len == 1 and bsz == 1:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -271,6 +271,7 @@ def LlamaAttention_fast_forward(
     if past_key_value is not None:
         K = torch.cat([past_key_value[0], K], dim = 2)
         V = torch.cat([past_key_value[1], V], dim = 2)
+    pass
     past_key_value = (K, V) if use_cache else None
 
     # Attention module
@@ -283,13 +284,13 @@ def LlamaAttention_fast_forward(
 
         # Group query attention
         if n_groups != 1:
-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)
-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)
-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
+            K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+            V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+            K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+            V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
             if hidden_states.requires_grad:
-                K = K.reshape(bsz, q_len, n_heads, head_dim)
-                V = V.reshape(bsz, q_len, n_heads, head_dim)
+                K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)
+                V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)
             else:
                 Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
         pass
@@ -304,10 +305,10 @@ def LlamaAttention_fast_forward(
     else:
         # Grouped query attention
         if n_groups != 1:
-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-            K = K.reshape(bsz, n_heads, q_len, head_dim)
-            V = V.reshape(bsz, n_heads, q_len, head_dim)
+            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+            K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
+            V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
         pass
         # Needs (batch_size, n_heads, seq_len, head_dim)
         # is_casual and attention_mask must not be both set!
@@ -349,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
     bsz, q_len, hd = hidden_states.size()
-    if (past_key_value is not None and q_len == 1):
+    if (past_key_value is not None and q_len == 1 and bsz == 1):
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
@@ -722,9 +723,9 @@ class FastLlamaModel:
         statistics = \
            f""==((====))==  Unsloth: Fast Llama patching release {__version__}\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
-           f""O^O/ \_/ \\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
+           f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
-           f' ""-____-""     Apache 2 free license: http://github.com/unslothai/unsloth'
+           f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         logger.warning_once(statistics)
         FastLlamaModel.pre_patch()
 
@@ -813,10 +814,13 @@ class FastLlamaModel:
         patch_saving_functions(tokenizer)
 
         # Fix up config for transformers uploading PEFT
-        name = model.config._name_or_path
-        if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-            name = name[:len(name) - len(""-bnb-4bit"")]
-            model.config.update({""_name_or_path"" : name})
+        # Not necessary anymore since we require transformers>=4.37!
+        if False:
+            name = model.config._name_or_path
+            if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+                name = name[:len(name) - len(""-bnb-4bit"")]
+                model.config.update({""_name_or_path"" : name})
+            pass
         pass
 
         # Log Unsloth version for future fastpaths for inference
@@ -1019,11 +1023,13 @@ class FastLlamaModel:
 
         # Fix up config for transformers uploading PEFT
         for active_adapter in model.peft_config.keys():
-            name = model.peft_config[active_adapter].base_model_name_or_path
-            if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-                name = name[:len(name) - len(""-bnb-4bit"")]
-                model.peft_config[active_adapter].base_model_name_or_path = name
-            pass
+            # Not necessary since we requires transformers >= 4.37
+            if False:
+                name = model.peft_config[active_adapter].base_model_name_or_path
+                if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+                    name = name[:len(name) - len(""-bnb-4bit"")]
+                    model.peft_config[active_adapter].base_model_name_or_path = name
+                pass
             # Add revision to enable future fast inference paths
             model.peft_config[active_adapter].revision = f""unsloth""
         pass
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 5a7f844..0739631 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -34,7 +34,7 @@ def _get_model_name(model_name, load_in_4bit = True):
         logger.warning_once(
             f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
             f""4bit loading.\nThe minimum required version is 4.37.\n""\
-            f'Try `pip install ""git+https://github.com/huggingface/transformers.git""`\n'\
+            f'Try `pip install --upgrade ""transformers>=4.37""`\n'\
             f""to obtain the latest transformers build, then restart this session.\n""\
             f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
         )
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 7da9e10..56fc543 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -36,7 +36,7 @@ __INT_TO_FLOAT_MAPPER = \
     ),
     ""unsloth/zephyr-sft-bnb-4bit""    : (
         ""unsloth/zephyr-sft"",
-        ""alignment-handbook/zephyr-7b-sft-full"",
+        ""HuggingFaceH4/mistral-7b-sft-beta"",
     ),
     ""unsloth/tinyllama-bnb-4bit""     : (
         ""unsloth/tinyllama"",
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index bde68a0..2410572 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(
     bsz, q_len, _ = hidden_states.size()
 
     # Check for inference
-    if past_key_value is not None and q_len == 1:
+    if past_key_value is not None and q_len == 1 and bsz == 1:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -84,9 +84,9 @@ def MistralAttention_fast_forward(
     pass
 
     if past_key_value is not None:
-        # reuse k, v, self_attention
         K = torch.cat([past_key_value[0], K], dim = 2)
         V = torch.cat([past_key_value[1], V], dim = 2)
+    pass
     past_key_value = (K, V) if use_cache else None
 
     # Attention module
@@ -95,32 +95,33 @@ def MistralAttention_fast_forward(
         Q = Q.transpose(1, 2)
         K = K.transpose(1, 2)
         V = V.transpose(1, 2)
-        M = bsz * q_len
+        K_M = V_M = bsz * kv_seq_len
+        Q_M = bsz * q_len
 
         has_swa = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)
 
         # Group query attention
-        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)
-        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)
-        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
-        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+        V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+        K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+        V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
         if hidden_states.requires_grad:
-            K = K.reshape(bsz, q_len, n_heads, head_dim)
-            V = V.reshape(bsz, q_len, n_heads, head_dim)
+            K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)
+            V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)
 
             if has_swa:
-                Q = Q.view(1, M, n_heads, head_dim)
-                K = K.view(1, M, n_heads, head_dim)
-                V = V.view(1, M, n_heads, head_dim)
+                Q = Q.view(1, Q_M, n_heads, head_dim)
+                K = K.view(1, K_M, n_heads, head_dim)
+                V = V.view(1, V_M, n_heads, head_dim)
             pass
         else:
             # Xformers does support the forward pass though
             Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
 
             if has_swa:
-                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)
-                K = K.view(1, M, n_kv_heads, n_groups, head_dim)
-                V = V.view(1, M, n_kv_heads, n_groups, head_dim)
+                Q = Q.view(1, Q_M, n_kv_heads, n_groups, head_dim)
+                K = K.view(1, K_M, n_kv_heads, n_groups, head_dim)
+                V = V.view(1, V_M, n_kv_heads, n_groups, head_dim)
             pass
         pass
 
@@ -132,16 +133,16 @@ def MistralAttention_fast_forward(
         K = K.transpose(1, 2)
         V = V.transpose(1, 2)
         sw = getattr(self.config, ""sliding_window"", None)
-        sw = q_len if (sw is None or sw == ""null"") else sw
-        window = (-1, -1) if (q_len <= sw) else (sw, sw)
+        sw = kv_seq_len if (sw is None or sw == ""null"") else sw
+        window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)
         A = flash_attn_func(Q, K, V, causal = True, window_size = window)
     else:
         # Grouped query attention
         # if n_groups != 1:
-        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-        K = K.reshape(bsz, n_heads, q_len, head_dim)
-        V = V.reshape(bsz, n_heads, q_len, head_dim)
+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+        K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
+        V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
         # pass
         # Needs (batch_size, n_heads, seq_len, head_dim)
         # is_casual and attention_mask must not be both set!
@@ -278,7 +279,7 @@ class FastMistralModel(FastLlamaModel):
         statistics = \
            f""==((====))==  Unsloth: Fast Mistral patching release {__version__}\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
-           f""O^O/ \_/ \\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
+           f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
            f' ""-____-""     Apache 2 free license: http://github.com/unslothai/unsloth'
         logger.warning_once(statistics)
@@ -363,11 +364,13 @@ class FastMistralModel(FastLlamaModel):
         patch_saving_functions(tokenizer)
 
         # Fix up config for transformers uploading PEFT
-        name = model.config._name_or_path
-        if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-            name = name[:len(name) - len(""-bnb-4bit"")]
-            model.config.update({""_name_or_path"" : name})
-        pass
+        # Not necessary anymore since we require transformers>=4.37
+        if False:
+            name = model.config._name_or_path
+            if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+                name = name[:len(name) - len(""-bnb-4bit"")]
+                model.config.update({""_name_or_path"" : name})
+            pass
         
         # Log Unsloth version for future fastpaths for inference
         model.config.update({""unsloth_version"" : __version__})
diff --git a/unsloth/save.py b/unsloth/save.py
index 519d8e7..b4e1796 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -135,6 +135,17 @@ def unsloth_save_model(
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.9,
 ):
+    if save_method == ""merged_4bit"":
+        raise RuntimeError(
+            ""Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\n""\
+            ""to merge to GGUF or others later on. I suggest you to do this as a final step\n""\
+            ""if you're planning to do multiple saves.\n""\
+            ""If you are certain, change `save_method` to `merged_4bit_forced`.""
+        )
+    elif save_method == ""merged_4bit_forced"":
+        save_method = ""merged_4bit""
+    pass
+
     save_pretrained_settings = dict(locals())
     for deletion in (""model"", ""tokenizer"", ""save_method"", ""temporary_location"", ""maximum_memory_usage""):
         del save_pretrained_settings[deletion]
@@ -457,6 +468,8 @@ pass
 def install_llama_cpp_make_non_blocking():
     env = { **os.environ, ""LLAMA_CUBLAS"": ""1"", }
     n_jobs = max(int(psutil.cpu_count()*1.5), 1)
+    # Force make clean
+    os.system(""make clean -C llama.cpp"")
     full_command = [""make"", ""-j"", str(n_jobs), ""-C"", ""llama.cpp""]
     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
     return run_installer
@@ -487,8 +500,8 @@ pass
 
 
 def save_to_gguf(
-    model_directory : str = ""unsloth_finetuned_model"",
-    quantization_method    : str = ""fast_quantized"",
+    model_directory      : str = ""unsloth_finetuned_model"",
+    quantization_method  : str = ""fast_quantized"",
     _run_installer = None, # Non blocking install of llama.cpp
 ):
     from transformers.models.llama.modeling_llama import logger
@@ -566,7 +579,7 @@ def unsloth_save_pretrained_merged(
     self,
     save_directory       : Union[str, os.PathLike],
     tokenizer            = None,
-    save_method         : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
     is_main_process      : bool = True,
"
"diff --git a/blackwell/README.md b/blackwell/README.md
index fdf5d2b..84dfdc8 100644
--- a/blackwell/README.md
+++ b/blackwell/README.md
@@ -1,30 +1,32 @@
-## Unsloth Blackwell Compatibility
+# Unsloth Blackwell Compatibility
 
-### Overview
+## Overview
 
-`Blackwell` (`sm100+`) requires all dependent libraries to be compiled with `cuda 12.8`.  
+`Blackwell` (`sm100+`) requires all dependent libraries to be compiled with `cuda 12.8`.
 
 The core libs for running unsloth which have dependencies on `CUDA` version are:
 - `bitsandbytes` - already has wheels built with `CUDA 12.8` so `pip install` should work out of the box
 - `triton` - requires `triton>=3.3.1`
-- `torch` - requires installing with `pip install torch --extra-index-url https://download.pytorch.org/whl/cu128` 
+- `torch` - requires installing with `pip install torch --extra-index-url https://download.pytorch.org/whl/cu128`
 - `vllm` - safest is to use the nightly build: `uv pip install -U vllm --torch-backend=cu128 --extra-index-url https://wheels.vllm.ai/nightly`
 - `xformers` - as of 6/26, `xformers` wheels are not yet built with `sm100+` enabled as support was only recently [added](https://github.com/facebookresearch/xformers/commit/d9b3b6e2b38ca485c89507ef8ac1fbef2723cdfa) so will require a source build (see below).
-  
-### Installation
+
+## Installation
+
+### Using uv
 
 The installation order is important, since we want the overwrite bundled dependencies with specific versions (namely, `xformers` and `triton`).
 
-1) I prefer to use `uv` over `pip` as it's faster and better for resolving dependencies, especially for libraries which depend on `torch` but for which a specific `CUDA` version is required per this scenario.  
-    
+1) I prefer to use `uv` over `pip` as it's faster and better for resolving dependencies, especially for libraries which depend on `torch` but for which a specific `CUDA` version is required per this scenario.
+
     Install `uv`
-    
+
     ```bash
     curl -LsSf https://astral.sh/uv/install.sh | sh && source $HOME/.local/bin/env
     ```
-    
+
     Create a project dir and venv:
-    
+
     ```bash
     mkdir `unsloth-blackwell` && cd `unsloth-blackwell`
     uv venv .venv --python=3.12 --seed
@@ -32,7 +34,7 @@ The installation order is important, since we want the overwrite bundled depende
     ```
 
 2) Install `vllm`
-    
+
     ```bash
     uv pip install -U vllm --torch-backend=cu128 --extra-index-url https://wheels.vllm.ai/nightly
     ```
@@ -40,48 +42,136 @@ The installation order is important, since we want the overwrite bundled depende
     Note that we have to specify `cu128`, otherwise `vllm` will install `torch==2.7.0` but with `cu126`.
 
 3) Install `unsloth` dependencies
-    
+
     ```bash
     uv pip install unsloth unsloth_zoo bitsandbytes
     ```
 
 4) Download and build `xformers`
-    
+
     ```bash
     # First uninstall xformers installed by previous libraries
     uv pip uninstall xformers
-    
+
     # Clone and build
     git clone --depth=1 https://github.com/facebookresearch/xformers --recursive
     cd xformers
     export TORCH_CUDA_ARCH_LIST=""12.0""
     python setup.py install
     ```
-    
+
     Note that we have to explicitly set `TORCH_CUDA_ARCH_LIST=12.0`.
 
 5) Update `triton`
-    
+
     ```bash
     uv pip install -U triton>=3.3.1
     ```
-    
+
     `triton>=3.3.1` is required for `Blackwell` support.
 
-6) `transformers`  
-    `transformers >= 4.53.0` breaks `unsloth` inference.  Specifically, `transformers` with `gradient_checkpointing` enabled will automatically [switch off caching](https://github.com/huggingface/transformers/blob/67ddc82fbc7e52c6f42a395b4a6d278c55b77a39/src/transformers/modeling_layers.py#L52-L59).  
-     
+6) `transformers`
+    `transformers >= 4.53.0` breaks `unsloth` inference.  Specifically, `transformers` with `gradient_checkpointing` enabled will automatically [switch off caching](https://github.com/huggingface/transformers/blob/67ddc82fbc7e52c6f42a395b4a6d278c55b77a39/src/transformers/modeling_layers.py#L52-L59).
+
     When using `unsloth` `FastLanguageModel` to `generate` directly after training with `use_cache=True`, this will result in mismatch between expected and actual outputs [here](https://github.com/unslothai/unsloth/blob/bfa6a3678e2fb8097c5ece41d095a8051f099db3/unsloth/models/llama.py#L939).
-    
+
     Temporary solution is to switch off `gradient_checkpointing` (e.g., `model.disable_gradient_checkpointing()`) before generation if using `4.53.0` or stick with `4.52.4` for now:
-    
+
     ```bash
     uv pip install -U transformers==4.52.4
     ```
 
+
+### Using conda or mamba
+
+1) Install `conda/mamba`
+
+    ```bash
+    curl -L -O ""https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh""
+    ```
+
+    Run the installation script
+    ```bash
+    bash Miniforge3-$(uname)-$(uname -m).sh
+    ```
+
+    Create a conda or mamba environment
+    ```bash
+    conda create --name unsloth-blackwell python==3.12 -y
+    ```
+
+    Activate newly created environment
+    ```bash
+    conda activate unsloth-blackwell
+    ```
+
+2) Install `vllm`
+
+    Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your  `(unsloth-blackwell)user@machine:`
+
+    ```bash
+    pip install -U vllm --extra-index-url https://download.pytorch.org/whl/cu128 --extra-index-url https://wheels.vllm.ai/nightly
+    ```
+
+    Note that we have to specify `cu128`, otherwise `vllm` will install `torch==2.7.0` but with `cu126`.
+
+3) Install `unsloth` dependencies
+
+    Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your  `(unsloth-blackwell)user@machine:`
+
+    ```bash
+    pip install unsloth unsloth_zoo bitsandbytes
+    ```
+
+4) Download and build `xformers`
+
+    Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your  `(unsloth-blackwell)user@machine:`
+
+    ```bash
+    # First uninstall xformers installed by previous libraries
+    pip uninstall xformers
+
+    # Clone and build
+    git clone --depth=1 https://github.com/facebookresearch/xformers --recursive
+    cd xformers
+    export TORCH_CUDA_ARCH_LIST=""12.0""
+    python setup.py install
+    ```
+
+    Note that we have to explicitly set `TORCH_CUDA_ARCH_LIST=12.0`.
+
+5) Update `triton`
+
+    Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your  `(unsloth-blackwell)user@machine:`
+
+    ```bash
+    pip install -U triton>=3.3.1
+    ```
+
+    `triton>=3.3.1` is required for `Blackwell` support.
+
+6) `Transformers`
+    `transformers >= 4.53.0` breaks `unsloth` inference.  Specifically, `transformers` with `gradient_checkpointing` enabled will automatically [switch off caching](https://github.com/huggingface/transformers/blob/67ddc82fbc7e52c6f42a395b4a6d278c55b77a39/src/transformers/modeling_layers.py#L52-L59).
+
+    When using `unsloth` `FastLanguageModel` to `generate` directly after training with `use_cache=True`, this will result in mismatch between expected and actual outputs [here](https://github.com/unslothai/unsloth/blob/bfa6a3678e2fb8097c5ece41d095a8051f099db3/unsloth/models/llama.py#L939).
+
+    Temporary solution is to switch off `gradient_checkpointing` (e.g., `model.disable_gradient_checkpointing()`) before generation if using `4.53.0` or stick with `4.52.4` for now:
+
+    Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your  `(unsloth-blackwell)user@machine:`
+
+    ```bash
+    pip install -U transformers==4.52.4
+    ```
+
+
+If you are using mamba as your package just replace conda with mamba for all commands shown above.
+
+
+## Post Installation notes:
+
 After installation, your environment should look similar to `blackwell.requirements.txt`.
 
 Note, might need to downgrade `numpy<=2.2` after all the installs.
 
-### Test
+## Test
 Both `test_llama32_sft.py` and `test_qwen3_grpo.py` should run without issue if correct install. If not, check diff between your installed env and `blackwell.requirements.txt`.
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3c4d8f3..7f07bea 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1376,6 +1376,15 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
     @torch.inference_mode
     def _fast_generate(*args, **kwargs):
 
+        if hasattr(model, ""config"") and hasattr(model.config, ""max_position_embeddings""):
+            if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
+                if kwargs[""input_ids""].shape[-1] + kwargs[""max_new_tokens""] > model.config.max_position_embeddings:
+                    raise ValueError(
+                        f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {model.config.max_position_embeddings}!\n'\
+                        'You will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`.'
+                    )
+        pass
+
         # Set a flag for generation!
         internal_model = model
         while hasattr(internal_model, ""model""):
"
"diff --git a/README.md b/README.md
index 8a6c970..98f83e0 100644
--- a/README.md
+++ b/README.md
@@ -30,7 +30,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 | **Mistral 7b** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\* | 62% less |
 
 - This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
-- Our [raw text notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for text completion.
+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
 - Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.
 - \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.
 
@@ -86,9 +86,12 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 ### Conda Installation
 Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.
 ```bash
-conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia
+conda create --name unsloth_env python=3.10
+conda activate unsloth_env
 
-conda install xformers -c xformers -y
+conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia
+
+conda install xformers -c xformers
 
 pip install bitsandbytes
 
@@ -141,6 +144,7 @@ pip install --upgrade pip
 ```
 
 ##  Documentation
+- Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!
 - We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
 - We're in Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
 
@@ -162,7 +166,8 @@ fourbit_models = [
     ""unsloth/llama-2-13b-bnb-4bit"",
     ""unsloth/codellama-34b-bnb-4bit"",
     ""unsloth/tinyllama-bnb-4bit"",
-]
+] # Go to https://huggingface.co/unsloth for more 4-bit models!
+
 # Load Llama model
 model, tokenizer = FastLanguageModel.from_pretrained(
     model_name = ""unsloth/mistral-7b-bnb-4bit"", # Supports Llama, Mistral - replace this!
@@ -183,6 +188,8 @@ model = FastLanguageModel.get_peft_model(
     use_gradient_checkpointing = True,
     random_state = 3407,
     max_seq_length = max_seq_length,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
 )
 
 trainer = SFTTrainer(
@@ -205,6 +212,12 @@ trainer = SFTTrainer(
     ),
 )
 trainer.train()
+
+# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
+# (1) Saving to GGUF / merging to 16bit for vLLM
+# (2) Continued training from a saved LoRA adapter
+# (3) Adding an evaluation loop / OOMs
+# (4) Cutomized chat templates
 ```
 
 <a name=""DPO""></a>
diff --git a/pyproject.toml b/pyproject.toml
index 2fa2cb1..0497112 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -42,6 +42,7 @@ huggingface = [
     ""tqdm"",
     ""psutil"",
     ""wheel>=0.42.0"",
+    ""numpy"",
 ]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
@@ -83,22 +84,22 @@ cu121 = [
     ""bitsandbytes"",
     ""unsloth[cu121only]"",
 ]
-cu118_torch211 = [
+cu118-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu118onlytorch211]"",
 ]
-cu121_torch211 = [
+cu121-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch211]"",
 ]
-cu118_torch220 = [
+cu118-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu118onlytorch220]"",
 ]
-cu121_torch220 = [
+cu121-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch220]"",
@@ -112,18 +113,18 @@ conda = [
 colab = [
     ""unsloth[cu121]"",
 ]
-colab_ampere = [
+colab-ampere = [
     ""unsloth[cu121]"",
     ""packaging"",
     ""ninja"",
     ""flash-attn"",
 ]
-colab_torch211 = [
+colab-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch211]"",
 ]
-colab_ampere_torch211 = [
+colab-ampere-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch211]"",
@@ -131,12 +132,12 @@ colab_ampere_torch211 = [
     ""ninja"",
     ""flash-attn"",
 ]
-colab_torch220 = [
+colab-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch220]"",
 ]
-colab_ampere_torch220 = [
+colab-ampere-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch220]"",
@@ -144,7 +145,7 @@ colab_ampere_torch220 = [
     ""ninja"",
     ""flash-attn"",
 ]
-cu118_ampere = [
+cu118-ampere = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu118only]"",
@@ -152,7 +153,7 @@ cu118_ampere = [
     ""ninja"",
     ""flash-attn"",
 ]
-cu121_ampere = [
+cu121-ampere = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121only]"",
@@ -160,7 +161,7 @@ cu121_ampere = [
     ""ninja"",
     ""flash-attn"",
 ]
-cu118_ampere_torch211 = [
+cu118-ampere-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu118onlytorch211]"",
@@ -168,7 +169,7 @@ cu118_ampere_torch211 = [
     ""ninja"",
     ""flash-attn"",
 ]
-cu121_ampere_torch211 = [
+cu121-ampere-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch211]"",
@@ -176,7 +177,7 @@ cu121_ampere_torch211 = [
     ""ninja"",
     ""flash-attn"",
 ]
-cu118_ampere_torch220 = [
+cu118-ampere-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu118onlytorch220]"",
@@ -184,7 +185,7 @@ cu118_ampere_torch220 = [
     ""ninja"",
     ""flash-attn"",
 ]
-cu121_ampere_torch220 = [
+cu121-ampere-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch220]"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index d052b33..9dce29c 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -59,14 +59,38 @@ if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):
 import bitsandbytes as bnb
 import triton
 from triton.common.build import libcuda_dirs
+import os
+import re
+import numpy as np
+import subprocess
+
 try:
     cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32
     libcuda_dirs()
 except:
     warnings.warn(
-        ""Running `ldconfig /usr/lib64-nvidia` to link CUDA.""\
+        ""Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.""\
     )
-    os.system(""ldconfig /usr/lib64-nvidia"")
+
+    if os.path.exists(""/usr/lib64-nvidia""):
+        os.system(""ldconfig /usr/lib64-nvidia"")
+    elif os.path.exists(""/usr/local""):
+        # Sometimes bitsandbytes cannot be linked properly in Runpod for example
+        possible_cudas = subprocess.check_output([""ls"", ""-al"", ""/usr/local""]).decode(""utf-8"").split(""\n"")
+        find_cuda = re.compile(r""[\s](cuda\-[\d\.]{2,})$"")
+        possible_cudas = [find_cuda.search(x) for x in possible_cudas]
+        possible_cudas = [x.group(1) for x in possible_cudas if x is not None]
+
+        # Try linking cuda folder, or everything in local
+        if len(possible_cudas) == 0:
+            os.system(f""ldconfig /usr/local/"")
+        else:
+            find_number = re.compile(r""([\d\.]{2,})"")
+            latest_cuda = np.argsort([float(find_number.search(x).group(1)) for x in possible_cudas])[::-1][0]
+            latest_cuda = possible_cudas[latest_cuda]
+            os.system(f""ldconfig /usr/local/{latest_cuda}"")
+    pass
+
     importlib.reload(bnb)
     importlib.reload(triton)
     try:
@@ -75,9 +99,10 @@ except:
         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32
         libcuda_dirs()
     except:
-        raise ImportError(""CUDA is not linked properly.\n""\
+        raise ImportError(""Unsloth: CUDA is not linked properly.\n""\
                           ""We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.\n""\
-                          ""You need to run in your terminal `ldconfig /usr/lib64-nvidia` yourself, then import Unsloth."")
+                          ""You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\n""\
+                          ""Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version."")
 pass
 
 from .models import *
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c21da4e..c4de719 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -17,6 +17,7 @@ from typing import Union, Optional, List, Any, Callable
 import warnings
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 8a37be1..3ca6291 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -55,6 +55,7 @@ from peft import PeftModelForCausalLM
 from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
 from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 from ..save import patch_saving_functions
+import re, os, inspect, math, sys
 
 
 def original_apply_qkv(self, X):
@@ -782,30 +783,33 @@ pass
 # https://github.com/huggingface/transformers/pull/27931
 # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
 class LlamaRotaryEmbedding(torch.nn.Module):
+    # Fixes https://github.com/huggingface/transformers/pull/28837
+    # https://github.com/microsoft/DeepSpeed/issues/4932
+    # The precision of RoPE buffers is not correct, so we cast to int64.
     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
         super().__init__()
-
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))
-        self.register_buffer(""inv_freq"", inv_freq, persistent=False)
 
         # Build here to make `torch.jit.trace` work.
-        self._set_cos_sin_cache(
-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
-        )
+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
+        # in FP32. They are applied (multiplied) in FP32 as well.
         self.max_seq_len_cached = seq_len
-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
+        inv_freq = 1.0 / (
+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=""cpu"").float() / self.dim)
+        )
+        t = torch.arange(self.max_seq_len_cached, device=""cpu"", dtype=torch.int64).float()
 
-        freqs = torch.outer(t, self.inv_freq)
+        freqs = torch.outer(t, inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
         emb = torch.cat((freqs, freqs), dim=-1)
-        self.register_buffer(""cos_cached"", emb.cos().to(dtype), persistent=False)
-        self.register_buffer(""sin_cached"", emb.sin().to(dtype), persistent=False)
+        self.register_buffer(""cos_cached"", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+        self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
     pass
 
     def forward(self, x, seq_len=None):
@@ -823,7 +827,9 @@ pass
 
 class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
     """"""LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev""""""
-
+    # Fixes https://github.com/huggingface/transformers/pull/28837
+    # https://github.com/microsoft/DeepSpeed/issues/4932
+    # The precision of RoPE buffers is not correct, so we cast to int64.
     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):
         self.scaling_factor = scaling_factor
         super().__init__(dim, max_position_embeddings, base, device)
@@ -831,14 +837,17 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
         self.max_seq_len_cached = seq_len
-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
+        inv_freq = 1.0 / (
+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=""cpu"").float() / self.dim)
+        )
+        t = torch.arange(self.max_seq_len_cached, device=""cpu"", dtype=torch.int64).float()
         t = t / self.scaling_factor
 
-        freqs = torch.outer(t, self.inv_freq)
+        freqs = torch.outer(t, inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
         emb = torch.cat((freqs, freqs), dim=-1)
-        self.register_buffer(""cos_cached"", emb.cos().to(dtype), persistent=False)
-        self.register_buffer(""sin_cached"", emb.sin().to(dtype), persistent=False)
+        self.register_buffer(""cos_cached"", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+        self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
     pass
 pass
 
@@ -954,6 +963,125 @@ class FastLlamaModel:
             layer.self_attn.apply_o   = original_apply_o
         pass
 
+        # Patch Trainer
+        from transformers.trainer import Trainer
+        try:
+            if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)
+                Trainer._original_training_loop = inner_training_loop
+            else:
+                inner_training_loop = Trainer._original_training_loop
+        except:
+            raise RuntimeError(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        pass
+
+        import transformers.trainer
+        items_in_trainer = dir(transformers.trainer)
+        good_items = []
+        for item in items_in_trainer:
+            # TODO: Support Deepspeed
+            if item.startswith((""deepspeed"", ""xm"", ""met"", ""smp"")): continue
+            if item in inner_training_loop: good_items.append(item)
+        pass
+        exec(""from transformers.trainer import ("" + "", "".join(x for x in good_items) + "")"", globals())
+
+        start = re.search('logger\.info\([\""\'].+?Running training', inner_training_loop).span(0)[0]
+        end = inner_training_loop.find(""\n\n"", start)
+        original_debug = inner_training_loop[start:end]
+        spaces = re.search('\n([\s\t]{1,})', original_debug).group(0)[1:]
+        front_spaces = re.match('([\s\t]{1,})', inner_training_loop).group(0)
+
+        debug_info = """"""debug_info = \\
+        f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\n""\\
+        f""   \\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
+        f""O^O/ \\_/ \\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
+        f""\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
+        f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
+        logger.warning_once(debug_info)""""""
+
+        debug_info = debug_info.split('\n')
+        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
+        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)
+
+        debug_info = """"""n_total_devices = total_train_batch_size // \\
+            args.gradient_accumulation_steps // self._train_batch_size
+        if n_total_devices > 2:
+            logger.warning_once(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        debug_info =""""""
+        debug_info = debug_info.split('\n')
+        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
+        inner_training_loop = inner_training_loop.replace(""debug_info ="", debug_info, 1)
+
+        front_spaces = re.match(r""[\t\s]{1,}"", inner_training_loop).group(0)
+        inner_training_loop = re.sub(r""^"" + front_spaces, """", inner_training_loop, flags = re.MULTILINE)
+        inner_training_loop = inner_training_loop.replace(
+            ""train_dataloader = tpu_spmd_dataloader(train_dataloader)"",
+            ""raise RuntimeError('Unsloth: TPUs are not yet supported!')""
+        )
+        inner_training_loop = inner_training_loop.replace(
+            ""self.accelerator.free_memory()"",
+            ""self.accelerator.free_memory()\n"" + \
+            front_spaces + ""if self.is_deepspeed_enabled:""\
+            ""raise RuntimeError('Unsloth: Deepspeed is not yet supported!')\n"", 1,
+        )
+
+        check_batches = """"""train_dataloader = self.get_train_dataloader()
+        ga  = args.gradient_accumulation_steps
+        bsz = self._train_batch_size
+        total_batches = bsz * ga * args.world_size
+        n_total_devices = total_batches // ga // bsz
+        if n_total_devices > 2:
+            logger.warning_once(
+                ""Please consider a commercial license - Unsloth was designed for the GPU Poor.\\n""
+                ""The OSS currently works on 4 GPUs - we're a 2 person team, so please help fund\\n""
+                ""our development costs by supporting us through Ko-fi or buying a license! Thanks!"",
+            )
+            divisor = n_total_devices / 2
+            bsz = self._train_batch_size = max(int(bsz / divisor), 1)
+            if total_batches // ga // bsz > 2:
+                divisor = n_total_devices / 2
+                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)""""""
+        check_batches = check_batches.split('\n')
+        check_batches = ""\n"".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])
+        inner_training_loop = inner_training_loop.replace(
+            ""train_dataloader = self.get_train_dataloader()"",
+            check_batches, 1,
+        )
+        inner_training_loop = inner_training_loop.replace(
+            ""_inner_training_loop"",
+            ""_fast_inner_training_loop"", 1,
+        )
+        exec(inner_training_loop, globals())
+
+        Trainer._inner_training_loop = _fast_inner_training_loop
+        inner_training_loop = inner_training_loop.replace(
+            ""is_torch_tpu_available()"",
+            ""False"",
+        )
+        if ""n_total_devices >"" not in inner_training_loop:
+            raise RuntimeError(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        pass
+        inner_training_loop = inner_training_loop.replace(
+            ""is_sagemaker_mp_enabled()"",
+            ""False"",
+        )
+        Trainer._inner_training_loop = _fast_inner_training_loop
+
         # Save max_seq_length
         model.max_seq_length = max_position_embeddings
         internal_model = model
@@ -1073,7 +1201,7 @@ class FastLlamaModel:
         signature = str(inspect.signature(LoraConfig))
         SUPPORTS_LOFTQ  = ""loftq_config"" in signature
         SUPPORTS_RSLORA = ""use_rslora""   in signature
-
+        
         assert(max_seq_length <= model.max_seq_length)
 
         if lora_dropout != 0:
@@ -1200,6 +1328,28 @@ class FastLlamaModel:
             model.peft_config[active_adapter].revision = f""unsloth""
         pass
 
+        from transformers.trainer import Trainer 
+        if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
+            raise RuntimeError(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        pass
+
+        # Fix loftq issues
+        # loftq_config must not = None, but rather {}
+        all_configs = model.peft_config
+        for key, current_config in all_configs.items():
+            if hasattr(current_config, ""loftq_config"") and current_config.loftq_config is None:
+                new_args = current_config.__dict__
+                new_args[""loftq_config""] = {}
+                current_config = current_config.__class__(**new_args)
+                all_configs[key] = current_config
+            pass
+        pass
+
         # Do patching
         n_mlp = 0
         n_qkv = 0
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index f11a34a..e4b3561 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -118,9 +118,13 @@ class FastLanguageModel(FastLlamaModel):
             *args, **kwargs,
         )
 
-        # in case the model supports tagging, add the unsloth tag.
+        # In case the model supports tagging, add the unsloth tag.
         if hasattr(model, ""add_model_tags""):
-            model.add_model_tags([""unsloth""])
+            model.add_model_tags([""unsloth"",])
+        pass
+        if hasattr(tokenizer, ""add_model_tags""):
+            tokenizer.add_model_tags([""unsloth"",])
+        pass
 
         if load_in_4bit:
             # Fix up bitsandbytes config
@@ -143,7 +147,7 @@ class FastLanguageModel(FastLlamaModel):
 
         if is_peft:
             # Now add PEFT adapters
-            model = PeftModel.from_pretrained(model, old_model_name)
+            model = PeftModel.from_pretrained(model, old_model_name, token = token)
             # Patch it as well!
             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)
         pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 1c90a45..323358f 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -42,6 +42,10 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/tinyllama"",
         ""TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"",
     ),
+    ""unsloth/tinyllama-chat-bnb-4bit"" : (
+        ""unsloth/tinyllama-chat"",
+        ""TinyLlama/TinyLlama-1.1B-Chat-v1.0"",
+    ),
     ""unsloth/mistral-7b-instruct-v0.1-bnb-4bit"" : (
         ""mistralai/Mistral-7B-Instruct-v0.1"",
     ),
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 615e436..0e36023 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -368,6 +368,140 @@ class FastMistralModel(FastLlamaModel):
             layer.self_attn.apply_o   = original_apply_o
         pass
 
+        # Patch Trainer
+        from transformers.trainer import Trainer
+        if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
+            try:
+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)
+            except:
+                raise RuntimeError(
+                    ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
+                    ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                    ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
+                    ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+                )
+            pass
+        pass
+
+        # Patch Trainer
+        from transformers.trainer import Trainer
+        try:
+            if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)
+                Trainer._original_training_loop = inner_training_loop
+            else:
+                inner_training_loop = Trainer._original_training_loop
+        except:
+            raise RuntimeError(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        pass
+
+        import transformers.trainer
+        items_in_trainer = dir(transformers.trainer)
+        good_items = []
+        for item in items_in_trainer:
+            # TODO: Support Deepspeed
+            if item.startswith((""deepspeed"", ""xm"", ""met"", ""smp"")): continue
+            if item in inner_training_loop: good_items.append(item)
+        pass
+        exec(""from transformers.trainer import ("" + "", "".join(x for x in good_items) + "")"", globals())
+
+        start = re.search('logger\.info\([\""\'].+?Running training', inner_training_loop).span(0)[0]
+        end = inner_training_loop.find(""\n\n"", start)
+        original_debug = inner_training_loop[start:end]
+        spaces = re.search('\n([\s\t]{1,})', original_debug).group(0)[1:]
+        front_spaces = re.match('([\s\t]{1,})', inner_training_loop).group(0)
+
+        debug_info = """"""debug_info = \\
+        f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\n""\\
+        f""   \\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
+        f""O^O/ \\_/ \\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
+        f""\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
+        f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
+        logger.warning_once(debug_info)""""""
+
+        debug_info = debug_info.split('\n')
+        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
+        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)
+
+        debug_info = """"""n_total_devices = total_train_batch_size // \\
+            args.gradient_accumulation_steps // self._train_batch_size
+        if n_total_devices > 2:
+            logger.warning_once(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        debug_info =""""""
+        debug_info = debug_info.split('\n')
+        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
+        inner_training_loop = inner_training_loop.replace(""debug_info ="", debug_info, 1)
+
+        front_spaces = re.match(r""[\t\s]{1,}"", inner_training_loop).group(0)
+        inner_training_loop = re.sub(r""^"" + front_spaces, """", inner_training_loop, flags = re.MULTILINE)
+        inner_training_loop = inner_training_loop.replace(
+            ""train_dataloader = tpu_spmd_dataloader(train_dataloader)"",
+            ""raise RuntimeError('Unsloth: TPUs are not yet supported!')""
+        )
+        inner_training_loop = inner_training_loop.replace(
+            ""self.accelerator.free_memory()"",
+            ""self.accelerator.free_memory()\n"" + \
+            front_spaces + ""if self.is_deepspeed_enabled:""\
+            ""raise RuntimeError('Unsloth: Deepspeed is not yet supported!')\n"", 1,
+        )
+
+        check_batches = """"""train_dataloader = self.get_train_dataloader()
+        ga  = args.gradient_accumulation_steps
+        bsz = self._train_batch_size
+        total_batches = bsz * ga * args.world_size
+        n_total_devices = total_batches // ga // bsz
+        if n_total_devices > 2:
+            logger.warning_once(
+                ""Please consider a commercial license - Unsloth was designed for the GPU Poor.\\n""
+                ""The OSS currently works on 4 GPUs - we're a 2 person team, so please help fund\\n""
+                ""our development costs by supporting us through Ko-fi or buying a license! Thanks!"",
+            )
+            divisor = n_total_devices / 2
+            bsz = self._train_batch_size = max(int(bsz / divisor), 1)
+            if total_batches // ga // bsz > 2:
+                divisor = n_total_devices / 2
+                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)""""""
+        check_batches = check_batches.split('\n')
+        check_batches = ""\n"".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])
+        inner_training_loop = inner_training_loop.replace(
+            ""train_dataloader = self.get_train_dataloader()"",
+            check_batches, 1,
+        )
+        inner_training_loop = inner_training_loop.replace(
+            ""_inner_training_loop"",
+            ""_fast_inner_training_loop"", 1,
+        )
+        exec(inner_training_loop, globals())
+
+        Trainer._inner_training_loop = _fast_inner_training_loop
+        inner_training_loop = inner_training_loop.replace(
+            ""is_torch_tpu_available()"",
+            ""False"",
+        )
+        if ""n_total_devices >"" not in inner_training_loop:
+            raise RuntimeError(
+                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
+                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
+                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+            )
+        pass
+        inner_training_loop = inner_training_loop.replace(
+            ""is_sagemaker_mp_enabled()"",
+            ""False"",
+        )
+        Trainer._inner_training_loop = _fast_inner_training_loop
+
         # Save max_seq_length
         max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)
         model.max_seq_length = max_position_embeddings
diff --git a/unsloth/save.py b/unsloth/save.py
index dc83f27..83e13bd 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -140,17 +140,28 @@ def unsloth_save_model(
 
     # Push to hub
     use_temp_dir         : Optional[bool] = None,
-    commit_message       : Optional[str] = None,
+    commit_message       : Optional[str] = ""Trained with Unsloth"",
     private              : Optional[bool] = None,
     create_pr            : bool = False,
     revision             : str = None,
-    commit_description   : str = None,
+    commit_description   : str = ""Upload model trained with Unsloth 2x faster"",
     tags                 : List[str] = None,
 
     # Our functions
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.9,
 ):
+    if commit_message is None: commit_message = """"
+    if ""Unsloth"" not in commit_message:
+        commit_message += "" (Trained with Unsloth)""
+    commit_message = commit_message.lstrip()
+
+    if commit_description is None:
+        commit_description = ""Upload model trained with Unsloth 2x faster""
+    elif ""Unsloth 2x faster"" not in commit_description:
+        commit_description += "" (Trained with Unsloth 2x faster)""
+    pass
+
     if save_method == ""merged_4bit"":
         raise RuntimeError(
             ""Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\n""\
@@ -202,7 +213,7 @@ def unsloth_save_model(
     pass
     save_pretrained_settings[""tags""] = tags
 
-    if (save_method == ""lora"") and push_to_hub:
+    if ((save_method == ""lora"") or (save_method == ""merged_4bit"")) and push_to_hub:
         if token is None:
             raise RuntimeError(
                 ""Unsloth: Pushing to HF requires a token. Pass `token = 'hf_....'`\n""\
@@ -210,7 +221,20 @@ def unsloth_save_model(
             )
         pass
 
-        model.push_to_hub(
+        if save_method == ""lora"":
+            print(""Unsloth: Saving LoRA adapters. Please wait..."")
+        elif save_method == ""merged_4bit"":
+            print(""Unsloth: Saving 4bit Bitsandbytes model. Please wait..."")
+        pass
+
+        # Update model tag
+        _ = upload_to_huggingface(
+            model, save_directory, token,
+            ""finetuned"", ""trl"", file_location = None,
+            old_username = None, private = private,
+        )
+
+        model.original_push_to_hub(
             repo_id            = save_directory,
             use_temp_dir       = use_temp_dir,
             commit_message     = commit_message,
@@ -224,7 +248,7 @@ def unsloth_save_model(
             tags               = tags,
         )
         if tokenizer is not None:
-            tokenizer.push_to_hub(
+            tokenizer.original_push_to_hub(
                 repo_id            = save_directory,
                 use_temp_dir       = use_temp_dir,
                 commit_message     = commit_message,
@@ -238,31 +262,11 @@ def unsloth_save_model(
                 tags               = tags,
             )
         pass
-        return save_directory
-    pass
-
-    # Update model tag
-    username = """"
-    if push_to_hub:
-        username = upload_to_huggingface(
-            model, save_directory, token,
-            ""finetuned"", ""trl"", file_location = None,
-        )
-    pass
-
-    # If push_to_hub, we must remove the .../ part of a repo
-    if push_to_hub and ""/"" in save_directory:
-
-        # +1 solves absolute path issues
-        new_save_directory = save_directory[save_directory.find(""/"")+1:]
-
-        logger.warning_once(
-            f""Unsloth: You are pushing to hub, but you passed your HF username.\n""\
-            f""We shall truncate {save_directory} to {new_save_directory}""
-        )
 
-        save_pretrained_settings[""save_directory""] = new_save_directory
-        save_directory = new_save_directory
+        if hasattr(model, ""config""):
+            print(f""Saved {save_method} model to https://huggingface.co/"" + save_directory)
+        pass
+        return save_directory
     pass
 
     # Tokenizer has different saving arguments
@@ -292,13 +296,25 @@ def unsloth_save_model(
         # Do general saving
         # Edit save_pretrained_settings
         # [TODO] _create_repo has errors due to **kwargs getting accepted
-        for deletion in \
-            (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",):
+        # commit_description does not seem to work?
+        what_to_delete = (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",) \
+            if save_pretrained_settings[""push_to_hub""] is False else \
+            (""use_temp_dir"", ""create_pr"", ""revision"", ""tags"", ""commit_description"",)
+        for deletion in what_to_delete:
             del save_pretrained_settings[deletion]
         pass
         if hasattr(model, ""add_model_tags""):
             model.add_model_tags([""unsloth"",])
 
+        # Update model tag
+        if push_to_hub:
+             _ = upload_to_huggingface(
+                model, save_pretrained_settings[""save_directory""], token,
+                ""finetuned"", ""trl"", file_location = None,
+                old_username = None, private = private,
+            )
+        pass
+
         if tokenizer is not None:
             print(""Unsloth: Saving tokenizer..."", end = """")
             tokenizer.save_pretrained(**tokenizer_save_settings)
@@ -310,10 +326,33 @@ def unsloth_save_model(
         if save_method != ""lora"": print("" This might take 10 minutes for Llama-7b..."", end = """")
 
         model.save_pretrained(**save_pretrained_settings)
+
+        if push_to_hub and hasattr(model, ""config""):
+            print(""Saved to https://huggingface.co/"" + save_pretrained_settings[""save_directory""])
+        pass
+
         print("" Done."")
         return save_directory
     pass
 
+    # If push_to_hub, we must remove the .../ part of a repo
+    username = None
+    if push_to_hub and ""/"" in save_directory:
+
+        # +1 solves absolute path issues
+        username = save_directory[:save_directory.find(""/"")]
+        new_save_directory = save_directory[save_directory.find(""/"")+1:]
+
+        logger.warning_once(
+            f""Unsloth: You are pushing to hub, but you passed your HF username = {username}.\n""\
+            f""We shall truncate {save_directory} to {new_save_directory}""
+        )
+
+        save_pretrained_settings[""save_directory""] = new_save_directory
+        tokenizer_save_settings [""save_directory""] = new_save_directory
+        save_directory = new_save_directory
+    pass
+
     print(""Unsloth: Merging 4bit and LoRA weights to 16bit..."")
 
     # Determine max RAM usage minus sharding
@@ -339,7 +378,7 @@ def unsloth_save_model(
         logger.warning_once(
             f""Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\n""\
             f""We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n""\
-            f""To force `safe_serialization`, set it to None instead."",
+            f""To force `safe_serialization`, set it to `None` instead."",
         )
         safe_serialization = False
         save_function = fast_save_pickle
@@ -413,13 +452,26 @@ def unsloth_save_model(
     # Edit save_pretrained_settings
     # [TODO] _create_repo has errors due to **kwargs getting accepted
     save_pretrained_settings[""state_dict""] = state_dict
-    for deletion in \
-        (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",):
+    
+    # commit_description does not seem to work?
+    what_to_delete = (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",) \
+        if not push_to_hub else \
+        (""use_temp_dir"", ""create_pr"", ""revision"", ""tags"", ""commit_description"",)
+    for deletion in what_to_delete:
         del save_pretrained_settings[deletion]
     pass
     if hasattr(model, ""add_model_tags""):
         model.add_model_tags([""unsloth"",])
 
+    # Update model tag
+    if push_to_hub:
+        _ = upload_to_huggingface(
+            model, save_pretrained_settings[""save_directory""], token,
+            ""finetuned"", ""trl"", file_location = None,
+            old_username = username, private = private,
+        )
+    pass
+
     if tokenizer is not None:
         print(""Unsloth: Saving tokenizer..."", end = """")
         tokenizer.save_pretrained(**tokenizer_save_settings)
@@ -452,9 +504,8 @@ def unsloth_save_model(
     model.config = old_config
     print(""Done."")
 
-    # Print location
-    if push_to_hub:
-        print(f""Saved to https://huggingface.co/{username}/{save_directory.lstrip('/')}"")
+    if push_to_hub and hasattr(model, ""config""):
+        print(f""Saved merged model to https://huggingface.co/{username}/{save_directory.lstrip('/')}"")
     pass
 
     save_pretrained_settings[""state_dict""] = None
@@ -478,7 +529,7 @@ def unsloth_save_model(
     for _ in range(3):
         torch.cuda.empty_cache()
         gc.collect()
-    return save_directory
+    return save_directory, username
 pass
 
 
@@ -494,7 +545,7 @@ def install_llama_cpp_make_non_blocking():
     n_jobs = max(int(psutil.cpu_count()*1.5), 1)
     # Force make clean
     os.system(""make clean -C llama.cpp"")
-    full_command = [""make"", ""all"", ""-j"", str(n_jobs), ""-C"", ""llama.cpp""]
+    full_command = [""make"", ""all"", ""-j""+str(n_jobs), ""-C"", ""llama.cpp""]
     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
     return run_installer
 pass
@@ -507,10 +558,44 @@ def install_python_non_blocking(packages = []):
 pass
 
 
+def install_llama_cpp_old(version = -10):
+    # Download the 10th latest release since the latest might be broken!
+    # FALLBACK mechanism
+    releases = subprocess.check_output([""git"", ""ls-remote"", ""--tags"", ""https://github.com/ggerganov/llama.cpp.git""])
+    releases = releases.decode(""utf-8"").replace(""\t"", "" "").split(""\n"")
+    for i, x in enumerate(releases):
+        if ""refs/tags/b"" not in x: break
+    releases = releases[:i]
+    latest = releases[-1]
+    version = releases[version].split("" "")[0]
+
+    # Clone a specific commit
+    commands = [
+        ""git clone https://github.com/ggerganov/llama.cpp"",
+        f""cd llama.cpp && git reset --hard {version} && git clean -df && ""\
+        f""make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}"",
+        ""pip install gguf protobuf"",
+    ]
+    for command in commands:
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
+            for line in sp.stdout:
+                print(line.decode(""utf-8""), flush = True, end = """")
+        pass
+    pass
+    # Check if successful
+    if not os.path.exists(""llama.cpp/quantize""):
+        raise RuntimeError(
+            ""Unsloth: llama.cpp GGUF seems to be too buggy to install.\n""\
+            ""File a report to llama.cpp's main repo since this is not an Unsloth issue.""
+        )
+    pass
+pass
+
+
 def install_llama_cpp_blocking():
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
-        f""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j {psutil.cpu_count()*2}"",
+        f""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}"",
         ""pip install gguf protobuf"",
     ]
     if os.path.exists(""llama.cpp""): return
@@ -563,10 +648,13 @@ def save_to_gguf(
 
     print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
     if _run_installer is not None:
-        _run_installer.wait()
+        error = _run_installer.wait()
     else:
+        error = 0
         install_llama_cpp_blocking()
     pass
+    # Check if successful. If not install 10th latest release
+    if error != 0 or not os.path.exists(""llama.cpp/quantize""): install_llama_cpp_old(-10)
 
     if   quantization_method == ""f32"":  first_conversion = ""f32""
     elif quantization_method == ""f16"":  first_conversion = ""f16""
@@ -580,15 +668,18 @@ def save_to_gguf(
             first_conversion = ""f16""
         pass
     pass
-    print(f""Unsloth: [1] Converting HF into {first_conversion} GGUF format. This will take 3 minutes..."")
 
     n_cpus = psutil.cpu_count()*2
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
     
     final_location = f""./{model_directory}-unsloth.{first_conversion.upper()}.gguf""
 
+    print(f""Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\n""\
+          f""The output location will be {final_location}\n""\
+          ""This will take 3 minutes..."")
+
     command = f""python llama.cpp/convert.py {model_directory} ""\
-        f""--outfile {final_location} ""\
+        f""--outfile {final_location} --vocab-type hfft ""\
         f""--outtype {first_conversion} --concurrency {n_cpus}""
 
     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:
@@ -601,7 +692,8 @@ def save_to_gguf(
     # Check if quantization succeeded!
     if not os.path.isfile(final_location):
         raise RuntimeError(
-            ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
+            f""Unsloth: Quantization failed for {final_location}\n""\
+            ""You might have to compile llama.cpp yourself, then run this again.\n""\
             ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
             ""You must run this in the same folder as you're saving your model.\n""\
             ""git clone https://github.com/ggerganov/llama.cpp\n""\
@@ -662,7 +754,7 @@ def unsloth_save_pretrained_merged(
     save_peft_format     : bool = True,
     tags                 : List[str] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
-    maximum_memory_usage : float = 0.85,   
+    maximum_memory_usage : float = 0.85,
 ):
     """"""
         Same as .save_pretrained(...) except 4bit weights are auto
@@ -695,14 +787,14 @@ def unsloth_push_to_hub_merged(
     tokenizer            = None,
     save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
     use_temp_dir         : Optional[bool] = None,
-    commit_message       : Optional[str] = None,
+    commit_message       : Optional[str] = ""Trained with Unsloth"",
     private              : Optional[bool] = None,
     token                : Union[bool, str, None] = None,
     max_shard_size       : Union[int, str, None] = ""5GB"",
     create_pr            : bool = False,
     safe_serialization   : bool = True,
     revision             : str = None,
-    commit_description   : str = None,
+    commit_description   : str = ""Upload model trained with Unsloth 2x faster"",
     tags                 : Optional[List[str]] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.85,
@@ -760,15 +852,27 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/
 [<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)
 """"""
 
-def upload_to_huggingface(model, save_directory, token, method, extra = """", file_location = None):
+def upload_to_huggingface(
+    model,
+    save_directory,
+    token,
+    method,
+    extra = """",
+    file_location = None,
+    old_username = None,
+    private = None,
+):
     # Check for username
     username = """"
     save_directory = save_directory.lstrip(""./"")
     if ""/"" not in save_directory:
         from huggingface_hub import whoami
         try: 
-            username = whoami()['name']
-            save_directory = f""{save_directory}/{username}""
+            username = whoami(token = token)[""name""]
+            if type(old_username) is str and username != old_username:
+                username = old_username
+            pass
+            save_directory = f""{username}/{save_directory}""
         except:
             raise RuntimeError(f""Unsloth: {save_directory} is not a Huggingface directory."")
     else:
@@ -776,24 +880,28 @@ def upload_to_huggingface(model, save_directory, token, method, extra = """", file
     pass
 
     from huggingface_hub import create_repo
-    create_repo(
-        repo_id   = save_directory,
-        token     = token,
-        repo_type = ""model"",
-        exist_ok  = True,
-    )
-
-    # Create model card
-    from huggingface_hub import ModelCard
-    content = MODEL_CARD.format(
-        username   = username,
-        base_model = model.config._name_or_path,
-        model_type = model.config.model_type,
-        method     = """",
-        extra      = extra,
-    )
-    card = ModelCard(content)
-    card.push_to_hub(save_directory, token = token)
+    try:
+        create_repo(
+            repo_id   = save_directory,
+            token     = token,
+            repo_type = ""model"",
+            exist_ok  = False,
+            private   = private,
+        ) 
+
+        # Create model card
+        from huggingface_hub import ModelCard
+        content = MODEL_CARD.format(
+            username   = username,
+            base_model = model.config._name_or_path,
+            model_type = model.config.model_type,
+            method     = """",
+            extra      = extra,
+        )
+        card = ModelCard(content)
+        card.push_to_hub(save_directory, token = token)
+    except:
+        pass
 
     if file_location is not None:
         # Now upload file
@@ -811,6 +919,7 @@ def upload_to_huggingface(model, save_directory, token, method, extra = """", file
             path_in_repo    = uploaded_location,
             repo_id         = save_directory,
             repo_type       = ""model"",
+            commit_message  = ""(Trained with Unsloth)"",
         )
 
         # We also upload a config.json file
@@ -823,6 +932,7 @@ def upload_to_huggingface(model, save_directory, token, method, extra = """", file
             path_in_repo    = ""config.json"",
             repo_id         = save_directory,
             repo_type       = ""model"",
+            commit_message  = ""(Trained with Unsloth)"",
         )
         os.remove(""_temporary_unsloth_config.json"")
     pass
@@ -838,6 +948,7 @@ def unsloth_save_pretrained_gguf(
     first_conversion     : str = ""f16"",
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
+    private              : Optional[bool] = None,
     is_main_process      : bool = True,
     state_dict           : Optional[dict] = None,
     save_function        : Callable = torch.save,
@@ -847,7 +958,7 @@ def unsloth_save_pretrained_gguf(
     save_peft_format     : bool = True,
     tags                 : List[str] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
-    maximum_memory_usage : float = 0.85,   
+    maximum_memory_usage : float = 0.85,
 ):
     """"""
         Same as .save_pretrained(...) except 4bit weights are auto
@@ -898,11 +1009,11 @@ def unsloth_save_pretrained_gguf(
         python_install = install_python_non_blocking([""gguf"", ""protobuf""])
         git_clone.wait()
         makefile  = install_llama_cpp_make_non_blocking()
-        new_save_directory = unsloth_save_model(**arguments)
+        new_save_directory, old_username = unsloth_save_model(**arguments)
         python_install.wait()
     else:
         try:
-            new_save_directory = unsloth_save_model(**arguments)
+            new_save_directory, old_username = unsloth_save_model(**arguments)
             makefile = None
         except:
             # Retry by recloning llama.cpp
@@ -910,7 +1021,7 @@ def unsloth_save_pretrained_gguf(
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             git_clone.wait()
             makefile  = install_llama_cpp_make_non_blocking()
-            new_save_directory = unsloth_save_model(**arguments)
+            new_save_directory, old_username = unsloth_save_model(**arguments)
             python_install.wait()
         pass
     pass
@@ -924,12 +1035,12 @@ def unsloth_save_pretrained_gguf(
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
         username = upload_to_huggingface(
             self, save_directory, token,
-            ""GGUF converted"", ""gguf"", file_location,
+            ""GGUF converted"", ""gguf"", file_location, old_username, private,
         )
         link = f""{username}/{new_save_directory.lstrip('/.')}"" \
             if username not in new_save_directory else \
             new_save_directory.lstrip('/.')
-        print(f""Saved to https://huggingface.co/{link}"")
+        print(f""Saved GGUF to https://huggingface.co/{link}"")
     pass
 pass
 
@@ -941,14 +1052,14 @@ def unsloth_push_to_hub_gguf(
     quantization_method  : str = ""fast_quantized"",
     first_conversion     : str = ""f16"",
     use_temp_dir         : Optional[bool] = None,
-    commit_message       : Optional[str] = None,
+    commit_message       : Optional[str] = ""Trained with Unsloth"",
     private              : Optional[bool] = None,
     token                : Union[bool, str, None] = None,
     max_shard_size       : Union[int, str, None] = ""5GB"",
     create_pr            : bool = False,
     safe_serialization   : bool = True,
     revision             : str = None,
-    commit_description   : str = None,
+    commit_description   : str = ""Upload model trained with Unsloth 2x faster"",
     tags                 : Optional[List[str]] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.85,
@@ -998,19 +1109,19 @@ def unsloth_push_to_hub_gguf(
         python_install = install_python_non_blocking([""gguf"", ""protobuf""])
         git_clone.wait()
         makefile  = install_llama_cpp_make_non_blocking()
-        new_save_directory = unsloth_save_model(**arguments)
+        new_save_directory, old_username = unsloth_save_model(**arguments)
         python_install.wait()
     else:
         try:
-            new_save_directory = unsloth_save_model(**arguments)
+            new_save_directory, old_username = unsloth_save_model(**arguments)
             makefile = None
         except:
             # Retry by recloning llama.cpp
             git_clone = install_llama_cpp_clone_non_blocking()
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             git_clone.wait()
-            makefile  = install_llama_cpp_make_non_blocking()
-            new_save_directory = unsloth_save_model(**arguments)
+            makefile = install_llama_cpp_make_non_blocking()
+            new_save_directory, old_username = unsloth_save_model(**arguments)
             python_install.wait()
         pass
     pass
@@ -1023,12 +1134,12 @@ def unsloth_push_to_hub_gguf(
     print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
     username = upload_to_huggingface(
         self, repo_id, token,
-        ""GGUF converted"", ""gguf"", file_location,
+        ""GGUF converted"", ""gguf"", file_location, old_username, private,
     )
     link = f""{username}/{new_save_directory.lstrip('/.')}"" \
         if username not in new_save_directory else \
         new_save_directory.lstrip('/.')
-    print(f""Saved to https://huggingface.co/{link}"")
+    print(f""Saved GGUF to https://huggingface.co/{link}"")
 pass
 
 
@@ -1038,31 +1149,17 @@ def patch_saving_functions(model):
     import types
     from typing import Callable, Optional, Union, List
 
-    if hasattr(model, ""_original_push_to_hub""): return
-
-    # First check if this has already been called, and revert it
-    original_model = model
-    while True:
-        if hasattr(original_model, ""_original_push_to_hub""):
-            original_model.push_to_hub = original_model._original_push_to_hub
-            del original_model._original_push_to_hub
-            if hasattr(original_model, ""push_to_hub_merged""):     del original_model.push_to_hub_merged
-            if hasattr(original_model, ""save_pretrained_merged""): del original_model.save_pretrained_merged
-            if hasattr(original_model, ""push_to_hub_gguf""):       del original_model.push_to_hub_gguf
-            if hasattr(original_model, ""save_pretrained_gguf""):   del original_model.save_pretrained_gguf
-        pass
-
-        if hasattr(original_model, ""model""): original_model = original_model.model
-        else: break
+    # And now re add our saving methods!
+    if model.push_to_hub.__name__ == ""unsloth_push_to_hub"":
+        original_push_to_hub = model.original_push_to_hub
+    else:
+        original_push_to_hub = model.push_to_hub
     pass
 
-    # And now re add our saving methods!
-    original_push_to_hub = model.push_to_hub
     signature = str(inspect.signature(original_push_to_hub)).replace(""NoneType"", ""None"")
     signature = signature[1:]
     signature = re.sub(""<function save at .+?>"", ""torch.save"", signature)
     docs = original_push_to_hub.__doc__.encode(""utf-8"").decode(""utf-8"")
-    model._original_push_to_hub = original_push_to_hub
 
     push_to_hub_text = f'''def unsloth_push_to_hub(self, {signature}:
     """"""
@@ -1077,11 +1174,45 @@ def patch_saving_functions(model):
         arguments[""tags""] = [""unsloth"",]
     elif hasattr(self, ""add_model_tags""):
         self.add_model_tags([""unsloth"",])
+
+    if ""commit_message"" in arguments:
+        commit_message = arguments[""commit_message""]
+        if commit_message is not None:
+            if not commit_message.endswith("" ""): commit_message += "" ""
+            if ""Unsloth"" not in commit_message:
+                commit_message += ""(Trained with Unsloth)""
+        else:
+            commit_message = ""Upload model trained with Unsloth""
+        arguments[""commit_message""] = commit_message
+
+    if ""commit_description"" in arguments:
+        commit_description = arguments[""commit_description""]
+        if commit_description is not None:
+            if not commit_description.endswith("" ""): commit_description += "" ""
+            if ""Unsloth"" not in commit_description:
+                commit_description += ""(Trained with Unsloth 2x faster)""
+        else:
+            commit_description = ""Upload model trained with Unsloth 2x faster""
+        arguments[""commit_description""] = commit_description
+
+    # Update model tag
+    if hasattr(self, ""config""):
+        _ = upload_to_huggingface(
+            self, arguments[""repo_id""], arguments[""token""],
+            ""finetuned"", ""trl"", file_location = None,
+            old_username = None, private = arguments[""private""],
+        )
+    pass
+
     try:
-        return self._original_push_to_hub(**arguments)
+        self.original_push_to_hub(**arguments)
     except:
         del arguments[""tags""]
-        return self._original_push_to_hub(**arguments)
+        self.original_push_to_hub(**arguments)
+    pass
+
+    if hasattr(self, ""config""):
+        print(""Saved model to https://huggingface.co/"" + arguments[""repo_id""])
     pass
     '''
     exec(push_to_hub_text, globals())
@@ -1089,12 +1220,12 @@ def patch_saving_functions(model):
     original_model = model
     while True:
 
-        if not hasattr(original_model, ""_original_push_to_hub""):
-            original_model._original_push_to_hub = original_model.push_to_hub
+        if original_model.push_to_hub.__name__ != ""unsloth_push_to_hub"":
+            original_model.original_push_to_hub = original_model.push_to_hub
             original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)
-
             if hasattr(original_model, ""add_model_tags""):
                 original_model.add_model_tags([""unsloth"",])
+            pass
         pass
 
         if hasattr(original_model, ""model""): original_model = original_model.model
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index f0193c7..0c07035 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -73,14 +73,13 @@ def _cross_entropy_forward(
     mask = col_offsets < VOCAB_SIZE
 
     label_idx = tl.load(labels_ptr).to(tl.int32)
-    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
 
     # Go logit scaling for Cohere: t * x
     if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits.to(tl.float32) / SOFTCAP).to(logits.dtype)
-
-    logits = logits.to(tl.float32)
+    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
+    
     c = tl.max(logits, 0)
     logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
@@ -152,14 +151,13 @@ def _chunked_cross_entropy_forward(
     mask = col_offsets < VOCAB_SIZE
 
     label_idx = tl.load(labels_ptr).to(tl.int32)
-    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
 
     # Go logit scaling for Cohere: t * x
     if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits.to(tl.float32) / SOFTCAP).to(logits.dtype)
+    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
 
-    logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
     logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
@@ -229,7 +227,7 @@ def _cross_entropy_backward(
     else:
         dloss = 0.0
 
-    x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+    x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
 
     # Do logit scaling for Cohere
     if DO_LOGIT_SCALING:
@@ -241,12 +239,12 @@ def _cross_entropy_backward(
     partial = x
     if DO_SOFTCAPPING:
         # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
-        partial = triton_tanh(x.to(tl.float32) / SOFTCAP).to(x.dtype)
+        partial = triton_tanh(x / SOFTCAP)
         x = SOFTCAP * partial
     pass
 
     logsumexp = tl.load(logsumexp_ptr + row_idx)
-    y = tl.exp(x.to(tl.float32) - logsumexp)
+    y = tl.exp(x - logsumexp)
     y = tl.where(
         col_offsets == label_idx,
         y - 1.0, # exp(x - logsumexp) - 1
@@ -337,6 +335,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         return losses
     pass
 
+
     @staticmethod
     def backward(ctx, dlosses):
         logits, logsumexp, labels = ctx.saved_tensors
@@ -345,6 +344,8 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         n_rows, vocab_size = logits.shape
 
         BLOCK_SIZE : int = 4096
+        div : int
+        mod : int
         div, mod = divmod(vocab_size, BLOCK_SIZE)
         n_blocks : int = div + (mod != 0)
 
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 6bd8a6f..5135766 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -45,10 +45,9 @@ def fast_geglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     mlp_size = self.config.intermediate_size
-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
 
-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
+    gate = fast_linear_forward(self.gate_proj, X)
+    up   = fast_linear_forward(self.  up_proj, X)
     gate = torch_nn_functional_gelu(gate, approximate = ""tanh"")
     gate *= up
 
@@ -83,20 +82,30 @@ def GemmaDecoderLayer_fast_forward(
     padding_mask:         Optional[torch.LongTensor] = None,
     *args, **kwargs,
 ):
-    if past_key_value is not None:
+    if use_cache: #past_key_value is not None:
         do_prefill = not hasattr(self.self_attn, ""paged_attention"")
         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda"")
 
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)
-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
-            self.self_attn,
-            hidden_states,
-            past_key_value,
-            position_ids,
-            do_prefill = do_prefill,
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
         )
+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+        #     self.self_attn,
+        #     hidden_states,
+        #     past_key_value,
+        #     position_ids,
+        #     do_prefill = do_prefill,
+        # )
         hidden_states += residual
 
         # Fully Connected
@@ -129,13 +138,8 @@ def GemmaDecoderLayer_fast_forward(
     pass
 
     outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
     return outputs
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bfbd10e..b802403 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -74,7 +74,7 @@ pass
 
 
 from math import sqrt as math_sqrt
-KV_CACHE_INCREMENT = 128 # KV Cache update size
+KV_CACHE_INCREMENT = 256 # KV Cache update size
 
 def LlamaAttention_fast_forward_inference(
     self,
@@ -82,6 +82,7 @@ def LlamaAttention_fast_forward_inference(
     past_key_value: Optional[Tuple[torch.Tensor]],
     position_ids,
     do_prefill = False,
+    attention_mask = None,
 ):
     """"""
         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
@@ -167,12 +168,12 @@ def LlamaAttention_fast_forward_inference(
     Kn *= cos; Kn.addcmul_(RH_K, sin);
     
     # New KV cache
-    # Kn = torch.cat([K1, Kn], dim = 2)
-    # Vn = torch.cat([V1, Vn], dim = 2)
-    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
-    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
-    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
-    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+    Kn = torch.cat([K1, Kn], dim = 2)
+    Vn = torch.cat([V1, Vn], dim = 2)
+    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
 
     # Handle sliding windows
     sliding_window = getattr(self.config, ""sliding_window"", None)
@@ -200,6 +201,7 @@ def LlamaAttention_fast_forward_inference(
     # Attention
     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
     A *= self.scalar
+    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
     A = torch.matmul(A, Vnn, out = Qn)
     A = A.transpose(1, 2)
@@ -215,10 +217,9 @@ def fast_swiglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     mlp_size = self.config.intermediate_size
-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
 
-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
+    gate = fast_linear_forward(self.gate_proj, X)
+    up   = fast_linear_forward(self.  up_proj, X)
     gate = torch_nn_functional_silu(gate, inplace = True)
     gate *= up
 
@@ -375,19 +376,30 @@ def LlamaDecoderLayer_fast_forward(
             (see `past_key_values`).
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
-    if past_key_value is not None:
+    if use_cache: #past_key_value is not None:
         do_prefill = not hasattr(self.self_attn, ""paged_attention"")
 
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
-            self.self_attn,
-            hidden_states,
-            past_key_value,
-            position_ids,
-            do_prefill = do_prefill,
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
         )
+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+        #     self.self_attn,
+        #     hidden_states,
+        #     past_key_value,
+        #     position_ids,
+        #     do_prefill = do_prefill,
+        #     attention_mask = attention_mask,
+        # )
         hidden_states += residual
 
         # Fully Connected
@@ -418,13 +430,8 @@ def LlamaDecoderLayer_fast_forward(
     pass
 
     outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
     return outputs
 pass
 
@@ -602,9 +609,8 @@ def LlamaModel_fast_forward(
     pass
 
     for idx, decoder_layer in enumerate(self.layers):
-        if output_hidden_states:
-            all_hidden_states += (hidden_states,)
 
+        if output_hidden_states: all_hidden_states += (hidden_states,)
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
         if self.gradient_checkpointing and self.training:
@@ -636,22 +642,15 @@ def LlamaModel_fast_forward(
                 use_cache=use_cache,
                 padding_mask=padding_mask,
             )
+        pass
 
         hidden_states = layer_outputs[0]
-
-        if use_cache:
-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
-
-        if output_attentions:
-            all_self_attns += (layer_outputs[1],)
+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+        if output_attentions: all_self_attns += (layer_outputs[1],)
     pass
-    
     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
 
-    # add hidden states from the last decoder layer
-    if output_hidden_states:
-        all_hidden_states += (hidden_states,)
-
+    if output_hidden_states: all_hidden_states += (hidden_states,)
     next_cache = next_decoder_cache if use_cache else None
     if not return_dict:
         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
@@ -670,12 +669,29 @@ def LlamaModel_fast_forward_inference(
     self,
     input_ids,
     past_key_values,
+    attention_mask = None,
 ):
     # Fix out of bounds tokenization
     input_ids = input_ids[:,:self.max_seq_length]
 
     hidden_states = self.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
+    bsz, q_len, hd = hidden_states.shape
+    seq_len = past_key_values[0][0].shape[-2]
+
+    # Must use attention mask for batched processing
+    sliding_window = getattr(self.config, ""sliding_window"", None)
+    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):
+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+            sliding_window = sliding_window,
+        )
+    else:
+        attention_mask = None
+    pass
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.layers):
@@ -686,7 +702,9 @@ def LlamaModel_fast_forward_inference(
             decoder_layer.self_attn,
             hidden_states,
             past_key_values[idx],
-            None,
+            position_ids = None,
+            do_prefill = False,
+            attention_mask = attention_mask,
         )
         hidden_states += residual
 
@@ -726,11 +744,12 @@ def CausalLM_fast_forward(fast_forward_inference):
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
 
-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, ""paged_attention""):
             outputs = fast_forward_inference(
                 self.model,
                 input_ids,
                 past_key_values,
+                attention_mask = attention_mask,
             )
         else:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index c609d2e..fcc1ab6 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(
     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
     self.model._has_no_labels = labels is None
 
-    if past_key_values is not None and \
+    if False and past_key_values is not None and \
         hasattr(self.model.layers[0].self_attn, ""paged_attention""):
         outputs = LlamaModel_fast_forward_inference(
             self.model,
             input_ids,
             past_key_values,
+            attention_mask = attention_mask,
         )
     else:
         outputs = self.model(
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 5102d8f..d5651d5 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -31,6 +31,10 @@ import numpy as np
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
 # Thank you for your understanding and we appreciate it immensely!
+
+# Fixes https://github.com/unslothai/unsloth/issues/1266
+os.environ[""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""] = ""python""
+
 if ""CUDA_VISIBLE_DEVICES"" in os.environ:
     os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
     devices = os.environ[""CUDA_VISIBLE_DEVICES""]
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index a417982..cd59e03 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -218,7 +218,13 @@ class FastLanguageModel(FastLlamaModel):
                     f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 ) 
-            raise RuntimeError(autoconfig_error or peft_error)
+            # Create a combined error message showing both failures
+            combined_error = (
+                ""Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\n""
+                f""AutoConfig error: {autoconfig_error}\n\n""
+                f""PeftConfig error: {peft_error}\n\n""
+            )
+            raise RuntimeError(combined_error)
         pass
 
         # Get base model for PEFT:
@@ -597,7 +603,13 @@ class FastModel(FastBaseModel):
                     f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 ) 
-            raise RuntimeError(autoconfig_error or peft_error)
+            # Create a combined error message showing both failures
+            combined_error = (
+                ""Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\n""
+                f""AutoConfig error: {autoconfig_error}\n\n""
+                f""PeftConfig error: {peft_error}\n\n""
+            )
+            raise RuntimeError(combined_error)
         pass
 
         # Get base model for PEFT:
"
"diff --git a/README.md b/README.md
index 759057f..412e3ff 100644
--- a/README.md
+++ b/README.md
@@ -59,7 +59,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 - No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
 - Works on **Linux** and **Windows** via WSL.
 - Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
-- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!
+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for up to **30x faster training**!
 - If you trained a model with Unsloth, you can use this cool sticker! &nbsp; <img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png"" height=""50"" align=""center"" />
 
 
diff --git a/pyproject.toml b/pyproject.toml
index 05c4191..3ea50ca 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,17 +33,17 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
+    ""tyro"",
     ""transformers>=4.38.2"",
     ""datasets>=2.16.0"",
     ""sentencepiece"",
-    ""accelerate>=0.26.1"",
-    ""trl>=0.7.9"",
-    ""peft>=0.7.1"",
     ""tqdm"",
     ""psutil"",
     ""wheel>=0.42.0"",
     ""numpy"",
-    ""triton"",
+    ""accelerate>=0.26.1"",
+    ""trl>=0.7.9"",
+    ""peft>=0.7.1"",
 ]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
@@ -128,17 +128,12 @@ cu121-torch220 = [
 kaggle = [
     ""unsloth[huggingface]"",
 ]
-conda = [
+kaggle-new = [
     ""unsloth[huggingface]"",
+    ""bitsandbytes"",
 ]
-colab = [
-    ""unsloth[cu121]"",
-]
-colab-ampere = [
-    ""unsloth[cu121]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn"",
+conda = [
+    ""unsloth[huggingface]"",
 ]
 colab-torch211 = [
     ""unsloth[huggingface]"",
@@ -166,6 +161,29 @@ colab-ampere-torch220 = [
     ""ninja"",
     ""flash-attn"",
 ]
+colab-new = [
+    ""tyro"",
+    ""transformers>=4.38.2"",
+    ""datasets>=2.16.0"",
+    ""sentencepiece"",
+    ""tqdm"",
+    ""psutil"",
+    ""wheel>=0.42.0"",
+    ""numpy"",
+]
+colab-no-deps = [
+    ""accelerate>=0.26.1"",
+    ""trl>=0.7.9"",
+    ""peft>=0.7.1"",
+    ""xformers"",
+    ""bitsandbytes"",
+]
+colab-ampere = [
+    ""unsloth[colab-ampere-torch220]"",
+    ""packaging"",
+    ""ninja"",
+    ""flash-attn"",
+]
 cu118-ampere = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 5487e9d..7d205eb 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -259,8 +259,10 @@ def get_chat_template(
         assert(""Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported."")
     pass
 
-    if tokenizer.__class__.__name__.startswith(""Gemma"") and chat_template == ""chatml"":
-        chat_template = ""gemma_chatml""
+    IS_GEMMA = False
+    if tokenizer.__class__.__name__.startswith(""Gemma""):
+        if chat_template == ""chatml"": chat_template = ""gemma_chatml""
+        IS_GEMMA = True
     pass
 
     old_padding_side = tokenizer.padding_side
@@ -338,6 +340,12 @@ def get_chat_template(
         .replace(""'user'"",      ""'"" + mapping[""user""]      + ""'"")\
         .replace(""'assistant'"", ""'"" + mapping[""assistant""] + ""'"")
 
+    # Careful on Gemma
+    # bos_token is a must or else losses become too high
+    if IS_GEMMA and not chat_template.startswith(""{{ bos_token }}""):
+        chat_template = ""{{ bos_token }}"" + chat_template
+    pass
+
     _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)
     tokenizer.padding_side  = old_padding_side
     tokenizer.chat_template = chat_template
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index bcd0e1a..7bfec43 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -154,6 +154,7 @@ def GemmaModel_fast_forward_inference(
     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda"")
 
     hidden_states = self.embed_tokens(input_ids)
+    hidden_states = hidden_states.to(self.config.torch_dtype)
     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32
     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32
     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index e949337..d83d9b7 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -509,7 +509,10 @@ def LlamaModel_fast_forward(
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
-    # Mormalized from Gemma
+    # Downcast to the correct dtype ie float32 to float16
+    inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
+
+    # Normalized from Gemma
     IS_GEMMA = self.config.model_type == ""gemma""
     train_embed_tokens = self.embed_tokens.weight.requires_grad
 
@@ -665,6 +668,7 @@ def LlamaModel_fast_forward_inference(
     input_ids = input_ids[:,:self.max_seq_length]
 
     hidden_states = self.embed_tokens(input_ids)
+    hidden_states = hidden_states.to(self.config.torch_dtype)
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.layers):
@@ -1334,6 +1338,7 @@ class FastLlamaModel:
                     ""We shall do it for you!""
                 )
                 train_lm_head = True
+                model.model.embed_tokens.to(torch.float32, non_blocking = True)
 
             elif module == ""embed_tokens"":
                 logger.warning_once(
@@ -1341,6 +1346,7 @@ class FastLlamaModel:
                     ""We shall do it for you!""
                 )
                 train_embed_tokens = True
+                model.lm_head.to(torch.float32, non_blocking = True)
 
             else:
                 assert(module in accepted_modules)
@@ -1477,12 +1483,12 @@ class FastLlamaModel:
                 if  hasattr(gate_proj, ""lora_A"") and \
                     hasattr(  up_proj, ""lora_A"") and \
                     hasattr(down_proj, ""lora_A"") and \
-                    ((gate_proj.base_layer if hasattr(gate_proj, ""base_layer"") else gate_proj).bias is None) and \
-                    ((  up_proj.base_layer if hasattr(  up_proj, ""base_layer"") else   up_proj).bias is None) and \
-                    ((down_proj.base_layer if hasattr(down_proj, ""base_layer"") else down_proj).bias is None) and \
-                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, ""lora_magnitude_vector"") else None) is None) and \
-                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, ""lora_magnitude_vector"") else None) is None) and \
-                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, ""lora_magnitude_vector"") else None) is None):
+                    (getattr(gate_proj, ""base_layer"", gate_proj).bias is None) and \
+                    (getattr(  up_proj, ""base_layer"",   up_proj).bias is None) and \
+                    (getattr(down_proj, ""base_layer"", down_proj).bias is None) and \
+                    (getattr(gate_proj, ""lora_magnitude_vector"", None) is None) and \
+                    (getattr(  up_proj, ""lora_magnitude_vector"", None) is None) and \
+                    (getattr(down_proj, ""lora_magnitude_vector"", None) is None):
 
                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)
@@ -1501,12 +1507,12 @@ class FastLlamaModel:
                 if  hasattr(q_proj, ""lora_A"") and \
                     hasattr(k_proj, ""lora_A"") and \
                     hasattr(v_proj, ""lora_A"") and \
-                    ((q_proj.base_layer if hasattr(q_proj, ""base_layer"") else q_proj).bias is None) and \
-                    ((k_proj.base_layer if hasattr(k_proj, ""base_layer"") else k_proj).bias is None) and \
-                    ((v_proj.base_layer if hasattr(v_proj, ""base_layer"") else v_proj).bias is None) and \
-                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, ""lora_magnitude_vector"") else None) is None) and \
-                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, ""lora_magnitude_vector"") else None) is None) and \
-                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, ""lora_magnitude_vector"") else None) is None):
+                    (getattr(q_proj, ""base_layer"", q_proj).bias is None) and \
+                    (getattr(q_proj, ""base_layer"", k_proj).bias is None) and \
+                    (getattr(q_proj, ""base_layer"", v_proj).bias is None) and \
+                    (getattr(q_proj, ""lora_magnitude_vector"", None) is None) and \
+                    (getattr(k_proj, ""lora_magnitude_vector"", None) is None) and \
+                    (getattr(v_proj, ""lora_magnitude_vector"", None) is None):
 
                     layer.self_attn.apply_qkv = apply_lora_qkv
                     n_qkv += 1
@@ -1520,8 +1526,8 @@ class FastLlamaModel:
                 # O attention patching
                 o_proj = layer.self_attn.o_proj
                 if hasattr(o_proj, ""lora_A"") and \
-                    ((o_proj.base_layer if hasattr(o_proj, ""base_layer"") else o_proj).bias is None) and \
-                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, ""lora_magnitude_vector"") else None) is None):
+                    (getattr(o_proj, ""base_layer"", o_proj).bias is None) and \
+                    (getattr(o_proj, ""lora_magnitude_vector"", None) is None):
 
                     layer.self_attn.apply_o = apply_lora_o
                     n_o += 1
diff --git a/unsloth/save.py b/unsloth/save.py
index 7543eef..dd0dc26 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -632,6 +632,7 @@ pass
 
 
 def save_to_gguf(
+    model_type           : str,
     model_directory      : str = ""unsloth_finetuned_model"",
     quantization_method  : str = ""fast_quantized"",
     first_conversion     : str = ""f16"",
@@ -639,10 +640,18 @@ def save_to_gguf(
 ):
     from transformers.models.llama.modeling_llama import logger
 
+    # Careful convert.py is only for Llama / Mistral based archs
+    use_fast_convert = False
+    if   model_type == ""llama"":   use_fast_convert = True
+    elif model_type == ""mistral"": use_fast_convert = True
+    pass
+    logger.warning_once(f""Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}."")
+
     if   quantization_method == ""not_quantized"":  quantization_method = ""f16""
     elif quantization_method == ""fast_quantized"": quantization_method = ""q8_0""
     elif quantization_method == ""quantized"":      quantization_method = ""q4_k_m""
     elif quantization_method is None:             quantization_method = ""q8_0""
+    pass
 
     if quantization_method not in ALLOWED_QUANTS.keys():
         error = f""Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\n""
@@ -692,6 +701,12 @@ def save_to_gguf(
         pass
     pass
 
+    # Non llama/mistral needs can only use f32 or f16
+    if not use_fast_convert and (first_conversion != ""f16"" or first_conversion != ""f32""):
+        logger.warning_once(""Unsloth: We must use f16 for non Llama and Mistral models."")
+        first_conversion = ""f16""
+    pass
+
     n_cpus = psutil.cpu_count()
     if n_cpus is None: n_cpus = 1
     n_cpus *= 2
@@ -703,9 +718,15 @@ def save_to_gguf(
           f""The output location will be {final_location}\n""\
           ""This will take 3 minutes..."")
 
-    command = f""python llama.cpp/convert.py {model_directory} ""\
-        f""--outfile {final_location} --vocab-type hfft ""\
-        f""--outtype {first_conversion} --concurrency {n_cpus}""
+    if use_fast_convert:
+        command = f""python llama.cpp/convert.py {model_directory} ""\
+            f""--outfile {final_location} --vocab-type hfft ""\
+            f""--outtype {first_conversion} --concurrency {n_cpus}""
+    else:
+        command = f""python llama.cpp/convert-hf-to-gguf.py {model_directory} ""\
+            f""--outfile {final_location} ""\
+            f""--outtype {first_conversion}""
+    pass
 
     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:
         for line in sp.stdout:
@@ -1054,7 +1075,8 @@ def unsloth_save_pretrained_gguf(
     for _ in range(3):
         gc.collect()
 
-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)
+    model_type = self.config.model_type
+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)
 
     if push_to_hub:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
@@ -1154,7 +1176,8 @@ def unsloth_push_to_hub_gguf(
     for _ in range(3):
         gc.collect()
 
-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)
+    model_type = self.config.model_type
+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)
 
     print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
     username = upload_to_huggingface(
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 4e7a71a..07999ea 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -266,6 +266,20 @@ llama3_template_eos_token = ""eos_token""
 CHAT_TEMPLATES[""llama-3""] = (llama3_template, llama3_template_eos_token,)
 
 
+# Phi-3
+phi3_template = \
+    ""{{ bos_token }}""\
+    ""{% for message in messages %}""\
+        ""{% if (message['role'] == 'user') %}""\
+            ""{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}""\
+        ""{% elif (message['role'] == 'assistant') %}""\
+            ""{{message['content'] + '<|end|>' + '\n'}}""\
+        ""{% endif %}""\
+    ""{% endfor %}""
+phi3_template_eos_token = ""<|end|>""
+CHAT_TEMPLATES[""phi-3""] = (phi3_template, phi3_template_eos_token,)
+
+
 def get_chat_template(
     tokenizer,
     chat_template = ""chatml"",
@@ -595,4 +609,12 @@ def test_chat_templates():
     correct_tokenizer.chat_template = template
     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
     assert(correct_prompt == our_prompt)
+
+    # Phi-3
+    template = phi3_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""microsoft/Phi-3-mini-4k-instruct"")
+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    assert(correct_prompt == our_prompt)
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 9c4ae8f..49f054e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -144,24 +144,60 @@ pass
 
 
 def patch_tokenizer(model, tokenizer):
+    """"""
+        Phi3's pad_token isn't set. We set it to <|placeholder...
+        Llama-3 is <|reserved...
+        Llama-2 is <unk>
+        Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!
+        Fixes https://github.com/unslothai/unsloth/issues/5
+    """"""
+    possible_reserved_tokens = (""<|reserved"", ""<|placeholder"",)
+
     if model is not None:
         model.config.update({""unsloth_version"" : __version__})
-    if not hasattr(tokenizer, ""pad_token"") or tokenizer.pad_token is None:
-        # Fixes https://github.com/unslothai/unsloth/issues/5
-        if hasattr(tokenizer, ""unk_token"") and tokenizer.unk_token is not None:
-            tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token})
-            tokenizer.pad_token = tokenizer.unk_token
-        else:
-            name = model.config._name_or_path if model is not None else ""Model""
-            logger.warning_once(
-                f""{name} does not have a padding or unknown token!\n""\
-                f""Will use the EOS token of id {tokenizer.eos_token_id} as padding.""
+
+    bad_pad_token = False
+    if hasattr(tokenizer, ""pad_token"") and tokenizer.pad_token is not None:
+        # Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!
+        bad_pad_token = tokenizer.eos_token == tokenizer.pad_token
+    elif hasattr(tokenizer, ""pad_token"") and tokenizer.pad_token is None:
+        bad_pad_token = True
+    else:
+        bad_pad_token = False
+    pass
+
+    if bad_pad_token:
+        # Find a better pad token
+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
+        possible_pad_token = None
+        for added_token in added_tokens[::-1]:
+            if added_token.startswith(possible_reserved_tokens):
+                possible_pad_token = added_token
+                break
+            pass
+        pass
+        if possible_pad_token is None:
+            # Try unk_token
+            possible_pad_token = tokenizer.unk_token
+        pass
+        if possible_pad_token is None:
+            # Failure!!
+            raise RuntimeError(
+                ""Unsloth: Tokenizer's pad_token cannot be = eos_token, and we couldn't find a\n""\
+                ""replacement of either <|reserved... or <|placeholder...""
             )
-            assert(hasattr(tokenizer, ""eos_token""))
-            tokenizer.add_special_tokens({""pad_token"" : tokenizer.eos_token})
-            tokenizer.pad_token = tokenizer.eos_token
+        pass
+
+        name = model.config._name_or_path if model is not None else ""Model""
+        logger.warning_once(
+            f""{name} does not have a padding token! Will use pad_token = {possible_pad_token}.""
+        )
+        
+        # Edit pad_token
+        tokenizer.add_special_tokens({""pad_token"" : possible_pad_token})
+        tokenizer.pad_token = possible_pad_token
         if model is not None:
-            config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
+            config = model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
     pass
     return model, tokenizer
 pass
diff --git a/unsloth/save.py b/unsloth/save.py
index 868d25d..b825b10 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -18,6 +18,7 @@ from peft.tuners.lora import Linear as Peft_Linear
 from typing import Optional, Callable, Union, List
 import torch
 import os
+import shutil
 import pickle
 import gc
 from transformers.models.llama.modeling_llama import logger
@@ -87,6 +88,24 @@ def print_quantization_methods():
 pass
 
 
+def check_if_sentencepiece_model(model, temporary_location = ""_unsloth_sentencepiece_temp""):
+    if not hasattr(model, ""_saved_temp_tokenizer""): return False
+
+    temp_tokenizer = model._saved_temp_tokenizer
+    sentencepiece_model = False
+    file_location = f""{temporary_location}/{temp_tokenizer.name_or_path}""
+    if not os.path.exists(file_location):
+        os.makedirs(file_location)
+    pass
+    temp_tokenizer.save_pretrained(file_location)
+    if os.path.isfile(f""{file_location}/tokenizer.model""):
+        sentencepiece_model = True
+    pass
+    shutil.rmtree(file_location)
+    return sentencepiece_model
+pass
+
+
 def _free_cached_model(model):
     from huggingface_hub import scan_cache_dir
     cached_repos = list(scan_cache_dir().repos)
@@ -840,6 +859,7 @@ pass
 
 def save_to_gguf(
     model_type           : str,
+    is_sentencepiece     : bool = False,
     model_directory      : str = ""unsloth_finetuned_model"",
     quantization_method  : str = ""fast_quantized"",
     first_conversion     : str = ""f16"",
@@ -856,7 +876,8 @@ def save_to_gguf(
 
     # Careful convert.py is only for Llama / Mistral based archs
     use_fast_convert = False
-    if   model_type == ""llama"":   use_fast_convert = True
+    if not is_sentencepiece:      use_fast_convert = False # Llama-3
+    elif model_type == ""llama"":   use_fast_convert = True
     elif model_type == ""mistral"": use_fast_convert = True
     pass
     logger.warning_once(f""Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}."")
@@ -951,7 +972,7 @@ def save_to_gguf(
             f""--outtype {first_conversion} --concurrency {n_cpus}""
     else:
         # Need to fix convert-hf-to-gguf.py for some models!
-        _fix_gemma_gguf()
+        # _fix_gemma_gguf()
 
         command = f""python llama.cpp/convert-hf-to-gguf.py {model_directory} ""\
             f""--outfile {final_location} ""\
@@ -1353,7 +1374,10 @@ def unsloth_save_pretrained_gguf(
         gc.collect()
 
     model_type = self.config.model_type
-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)
+    is_sentencepiece_model = check_if_sentencepiece_model(self)
+    file_location = save_to_gguf(model_type, is_sentencepiece_model, 
+        new_save_directory, quantization_method, first_conversion, makefile,
+    )
 
     if push_to_hub:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
@@ -1473,7 +1497,10 @@ def unsloth_push_to_hub_gguf(
         gc.collect()
 
     model_type = self.config.model_type
-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)
+    is_sentencepiece_model = check_if_sentencepiece_model(self)
+    file_location = save_to_gguf(model_type, is_sentencepiece_model, 
+        new_save_directory, quantization_method, first_conversion, makefile,
+    )
 
     print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
     username = upload_to_huggingface(
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index e03f50b..34a61e2 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -153,6 +153,8 @@ from transformers.training_args import logger as transformers_training_args_logg
 transformers_training_args_logger.addFilter(HideLoggingMessage(""The speedups""))
 # torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED.
 transformers_training_args_logger.addFilter(HideLoggingMessage(""torch.distributed""))
+# average_tokens_across_devices is set to True but it is invalid when world size is1
+transformers_training_args_logger.addFilter(HideLoggingMessage(""average_tokens_across_devices""))
 del transformers_training_args_logger
 
 # No label_names provided for model class
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index b0f485d..8dca483 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -722,6 +722,8 @@ class FastBaseModel:
         pass
         # Must disable returning hidden states in the case for GRPO
         os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""0""
+        # Must enable returning logits
+        os.environ[""UNSLOTH_RETURN_LOGITS""] = ""1""
         return model
     pass
 
@@ -760,6 +762,8 @@ class FastBaseModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = True
         pass
+        # Can re-enable not returning logits
+        os.environ[""UNSLOTH_RETURN_LOGITS""] = ""0""
         return model
     pass
 pass
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index a3b3e68..d18aaac 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -17,6 +17,27 @@ from packaging.version import Version
 import os, re, subprocess, inspect
 import numpy as np
 
+# Check if modules that need patching are already imported
+critical_modules = ['trl', 'transformers', 'peft']
+already_imported = [mod for mod in critical_modules if mod in sys.modules]
+
+# This check is critical because Unsloth optimizes these libraries by modifying
+# their code at import time. If they're imported first, the original (slower, 
+# more memory-intensive) implementations will be used instead of Unsloth's
+# optimized versions, potentially causing OOM errors or slower training.
+
+if already_imported:
+    # stacklevel=2 makes warning point to user's import line rather than this library code,
+    # showing them exactly where to fix the import order in their script
+    warnings.warn(
+        f""WARNING: Unsloth should be imported before {', '.join(already_imported)} ""
+        f""to ensure all optimizations are applied. Your code may run slower or encounter ""
+        f""memory issues without these optimizations.\n\n""
+        f""Please restructure your imports with 'import unsloth' at the top of your file."",
+        stacklevel = 2,
+    )
+pass
+
 # Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 784ca9c..779ff83 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -35,6 +35,7 @@ except:
     MistralSdpaAttention   = MistralAttention
     MistralFlashAttention2 = MistralAttention
 pass
+from unsloth_zoo.utils import Version, _get_dtype
 
 
 def MistralAttention_fast_forward(
@@ -183,6 +184,7 @@ def MistralForCausalLM_fast_forward(
     output_hidden_states: Optional[bool] = None,
     return_dict: Optional[bool] = None,
     num_logits_to_keep: Optional[int] = 0,
+    logits_to_keep: Optional[int] = 0,
     *args, **kwargs,
 ) -> Union[Tuple, CausalLMOutputWithPast]:
 
@@ -194,7 +196,6 @@ def MistralForCausalLM_fast_forward(
         elif q_len <= sliding_window:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
         else:
-            # Fix from https://github.com/Rypo
             causal_mask = xformers.attn_bias.BlockDiagonalCausalMask\
                 .from_seqlens([q_len]*bsz)\
                 .make_local_attention(window_size = sliding_window)
@@ -219,20 +220,35 @@ def MistralForCausalLM_fast_forward(
         )
     else:
         outputs = self.model(
-            input_ids=input_ids,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
+            input_ids = input_ids,
+            causal_mask = causal_mask,
+            attention_mask = attention_mask,
+            position_ids = position_ids,
+            past_key_values = past_key_values,
+            inputs_embeds = inputs_embeds,
+            use_cache = use_cache,
+            output_attentions = output_attentions,
+            output_hidden_states = output_hidden_states,
+            return_dict = return_dict,
         )
     pass
 
     hidden_states = outputs[0]
+
+    # If we are in GRPO mode, return raw hidden states
+    if os.environ.get(""UNSLOTH_RETURN_HIDDEN_STATES"", ""0"") == ""1"":
+        num_logits_to_keep = max(num_logits_to_keep, logits_to_keep)
+        if num_logits_to_keep != 0:
+            hidden_states = hidden_states[:, -num_logits_to_keep:, :]
+        return CausalLMOutputWithPast(
+            loss = None,
+            logits = hidden_states,
+            past_key_values = outputs.past_key_values,
+            hidden_states = outputs.hidden_states,
+            attentions = outputs.attentions,
+        )
+    pass
+
     bsz, q_len, hd = hidden_states.shape
     lm_head = self.lm_head.weight
     if bsz == 1 and q_len == 1:
@@ -241,9 +257,37 @@ def MistralForCausalLM_fast_forward(
     elif num_logits_to_keep != 0:
         logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
     else:
+        RETURN_LOGITS = os.environ.get(""UNSLOTH_RETURN_LOGITS"", ""0"") == ""1""
+        # < 1024 Normal Unsloth uses less VRAM!
+        if bsz * q_len <= 1024: RETURN_LOGITS = True
+
+        if not RETURN_LOGITS and HAS_CUT_CROSS_ENTROPY and labels is not None:
+            n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
+            logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
+            loss = fused_linear_cross_entropy(
+                hidden_states = hidden_states,
+                lm_weight = lm_head,
+                labels = labels,
+                num_items_in_batch = n_items,
+                logit_softcapping = logit_softcapping,
+            )
+
+            if not return_dict:
+                output = (logits,) + outputs[1:]
+                return (loss,) + output if loss is not None else output
+            
+            output = CausalLMOutputWithPast(
+                loss = loss,
+                logits = EMPTY_LOGITS,
+                past_key_values = outputs.past_key_values,
+                hidden_states = outputs.hidden_states,
+                attentions = outputs.attentions,
+            )
+            return output
+        pass
         logits = self.lm_head(hidden_states.to(lm_head.dtype))
     pass
-    logits = logits.to(self.config.torch_dtype)
+    logits = logits.to(_get_dtype(self.config.torch_dtype))
 
     loss = None
     if labels is not None:
@@ -252,7 +296,7 @@ def MistralForCausalLM_fast_forward(
             # Fixes https://github.com/unslothai/unsloth/issues/10
             self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
         pass
-        
+
         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
         loss = fast_cross_entropy_loss(
             logits  = shift_logits,
@@ -266,11 +310,11 @@ def MistralForCausalLM_fast_forward(
         return (loss,) + output if loss is not None else output
 
     return CausalLMOutputWithPast(
-        loss=loss,
-        logits=logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
+        loss = loss,
+        logits = logits,
+        past_key_values = outputs.past_key_values,
+        hidden_states = outputs.hidden_states,
+        attentions = outputs.attentions,
     )
 pass
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 994f97a..8677879 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -178,20 +178,20 @@ from transformers.models.llama.modeling_llama import logger
 # Get Xformers
 from xformers import __version__ as xformers_version
 # Temporarily disable 0.0.27 and higher - inference issues
-if Version(xformers_version) >= Version(""0.0.27""):
-    raise ImportError(
-        ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
-        ""then press Disconnect Runtime and then Restart it.\n""\
-        ""\n""\
-        ""%%capture\n""
-        ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
-        '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
-        '!pip install --no-deps ""xformers<0.0.27"" ""trl<0.9.0"" peft accelerate bitsandbytes\n'\
-        '\n'\
-        f""Otherwise in local machines, your xformers version of {xformers_version} is too new.\n""\
-        'Please downgrade xformers via `pip install --force-reinstall ""xformers<0.0.27""'
-    )
-pass
+# if Version(xformers_version) >= Version(""0.0.27""):
+#     raise ImportError(
+#         ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
+#         ""then press Disconnect Runtime and then Restart it.\n""\
+#         ""\n""\
+#         ""%%capture\n""
+#         ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
+#         '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
+#         '!pip install --no-deps ""xformers<0.0.27"" ""trl<0.9.0"" peft accelerate bitsandbytes\n'\
+#         '\n'\
+#         f""Otherwise in local machines, your xformers version of {xformers_version} is too new.\n""\
+#         'Please downgrade xformers via `pip install --force-reinstall ""xformers<0.0.27""'
+#     )
+# pass
 
 if   Version(torch_version) < Version(""2.2.0"") and Version(xformers_version) >= Version(""0.0.24""):
     raise ImportError(
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index f22e81e..fb6d5c5 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -42,6 +42,7 @@ def __get_model_name(
     FLOAT_TO_INT_MAPPER = None,
 ):
 
+    model_name = str(model_name)
     if not SUPPORTS_FOURBIT and model_name.lower() in INT_TO_FLOAT_MAPPER:
         model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
         logger.warning_once(
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 462555f..254a68a 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -67,6 +67,7 @@ __INT_TO_FLOAT_MAPPER = \
         ""codellama/CodeLlama-7b-hf"",
     ),
     ""unsloth/codellama-13b-bnb-4bit"" : (
+        ""unsloth/codellama-13b"",
         ""codellama/CodeLlama-13b-hf"",
     ),
     ""unsloth/yi-6b-bnb-4bit"" : (
"
"diff --git a/pyproject.toml b/pyproject.toml
index f9a33a8..cddc425 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.5.10"",
+    ""unsloth_zoo>=2025.5.11"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.5.9"",
+    ""unsloth_zoo>=2025.5.11"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 9325428..4bfaf6a 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.5.8""
+__version__ = ""2025.5.9""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 24cd5e6..8b6dd16 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -358,7 +358,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""from unsloth_zoo.vision_utils import UnslothVisionDataCollator\n""\
         ""if not isinstance(data_collator, UnslothVisionDataCollator):\n""\
         ""    if isinstance(data_collator, DataCollatorForSeq2Seq) and 'labels' not in train_dataset.column_names:\n""\
-        ""        data_collator = TransformersDataCollatorForLanguageModeling(__tokenizer, mlm = False)\n""\
+        ""        data_collator = TransformersDataCollatorForLanguageModeling(__tokenizer, mlm = False, mlm_probability = 0.0)\n""\
         ""    elif isinstance(data_collator, TransformersDataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
         ""        data_collator = DataCollatorForSeq2Seq(__tokenizer)\n""\
         ""else:\n""\
@@ -374,7 +374,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""        if isinstance(data_collator, DataCollatorForSeq2Seq):\n""\
         ""            data_collator = DataCollatorForSeq2Seq(__tokenizer.tokenizer)\n""\
         ""        else:\n""\
-        ""            data_collator = TransformersDataCollatorForLanguageModeling(__tokenizer.tokenizer, mlm = False)\n""
+        ""            data_collator = TransformersDataCollatorForLanguageModeling(__tokenizer.tokenizer, mlm = False, mlm_probability = 0.0)\n""
         extra_args += pad_check
     pass
 
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index e6d09b7..bfd45a3 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -330,7 +330,7 @@ def unsloth_save_model(
             old_username = None, private = private,
         )
 
-        getattr(model, ""original_push_to_hub"", tokenizer.push_to_hub)\
+        getattr(model, ""original_push_to_hub"", model.push_to_hub)\
         (
             repo_id            = save_directory,
             use_temp_dir       = use_temp_dir,
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 480d22a..2587c5a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -99,7 +99,7 @@ torch_nn_functional_softmax = torch.nn.functional.softmax
 SDPA_HAS_GQA = ""enable_gqa"" in scaled_dot_product_attention.__doc__
 
 # Fix new HF's inference code
-def _fast_prepare_inputs_for_generation(self, input_ids, **kwargs,):
+def _fast_prepare_inputs_for_generation(self, input_ids, attention_mask=None, **kwargs,):
     past_key_values = kwargs.get(""past_key_values"", None)
     if past_key_values is not None:
         # Check for uninitialized DynamicCache
@@ -107,11 +107,38 @@ def _fast_prepare_inputs_for_generation(self, input_ids, **kwargs,):
             past_key_values = None
             kwargs[""past_key_values""] = None
         else:
+            bs, cache_length = input_ids.shape
             input_ids = input_ids[:,[-1]]
-            kwargs[""attention_mask""] = kwargs[""attention_mask""][:,[-1]]
+            
+            # Get to the base model
+            base_model = self
+            if hasattr(base_model, 'base_model_prefix'):
+                base_model = getattr(base_model, base_model.base_model_prefix)
+                
+            if hasattr(base_model, ""_prepare_4d_causal_attention_mask_with_cache_position""):
+                attention_mask = base_model._prepare_4d_causal_attention_mask_with_cache_position(
+                    attention_mask,
+                    sequence_length=1,
+                    target_length=cache_length,
+                    dtype=self.dtype,
+                    device=input_ids.device,
+                    cache_position=torch.arange(cache_length, cache_length+1, device=input_ids.device),
+                    batch_size=bs,
+                    config=self.config,
+                    past_key_values=past_key_values,
+                )
+            else:
+                attention_mask = attention_mask[:,[-1]]
+                logger.warning_once(
+                    f""{self.__class__.__name__} has no `_prepare_4d_causal_attention_mask_with_cache_position` method ""
+                    ""defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're ""
+                    ""writing code, see Llama for an example implementation. If you're a user, please report this ""
+                    ""issue on GitHub.""
+                )
+
     if ""cache_position"" in kwargs:
         kwargs[""position_ids""] = kwargs[""cache_position""]
-    return { ""input_ids"" : input_ids, **kwargs, }
+    return { ""input_ids"" : input_ids, ""attention_mask"": attention_mask, **kwargs, }
 pass
 
 
"
"diff --git a/README.md b/README.md
index fc28260..b54c14f 100644
--- a/README.md
+++ b/README.md
@@ -10,7 +10,7 @@
 <a href=""https://discord.gg/u54VK8m8tk""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
 <a href=""https://ko-fi.com/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png"" height=""48""></a>
 
-### Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory!
+### Finetune Mistral, Gemma, Llama 2-5x faster with 80% less memory!
 
 ![](https://i.ibb.co/sJ7RhGG/image-41.png)
 
@@ -22,21 +22,28 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 
 | Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
 |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
-| **Gemma 7b**      | [ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 58% less |
-| **Mistral 7b**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
-| **Llama-2 7b**      | [ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |
-| **TinyLlama**  | [ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |
-| **CodeLlama 34b** A100   | [ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |
-| **Mistral 7b** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\* | 62% less |
-| **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |
-
+| **Gemma 7b**      | [ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |
+| **Mistral 7b**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |
+| **Llama-2 7b**      | [ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 60% less |
+| **TinyLlama**  | [ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 82% less |
+| **CodeLlama 34b** A100   | [ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 49% less |
+| **Mistral 7b** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\* | 73% less |
+| **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
+
+- Benchmarking compared to FA2 + Hugging Face combined.
 - This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
 - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
 - \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.
 
 ##  Unsloth.ai News
--  [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) on 6T tokens now works. And [Gemma 2b notebook](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)
--  Added [conversational notebooks](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) and [raw text notebooks](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing)
+-  NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you're using our notebooks. To enable, simply change 1 line:
+```python
+model = FastLanguageModel.get_peft_model(
+    model,
+    use_gradient_checkpointing = ""unsloth"", # <<<<<<<
+)
+```
+-  NEW! [CodeGemma](https://colab.research.google.com/drive/19lwcRk_ZQ_ZtX-qzFP3qZBBHZNcMD1hh?usp=sharing) now works along with [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) and [Gemma 2b](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)
 -  [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models
 -  [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO
 -  We did a [blog](https://huggingface.co/blog/unsloth-trl) with Hugging Face and are in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)
@@ -46,9 +53,9 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 | Type                            | Links                               |
 | ------------------------------- | --------------------------------------- |
 |  **Wiki & FAQ**              | [Read Our Wiki](https://github.com/unslothai/unsloth/wiki) |
+| <img height=""14"" src=""https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg"" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
 |  **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |
 |  **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|
-| <img height=""14"" src=""https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg"" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
 |  **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)
 |  **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|
 |  **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|
@@ -413,7 +420,8 @@ Two Tesla T4s on Kaggle
 ![](https://i.ibb.co/sJ7RhGG/image-41.png)
 <br>
 
-### Credits
-1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
-2. [152334H](https://github.com/152334H) for experimental DPO support
-3. [atgctg](https://github.com/atgctg) for syntax highlighting
+### Thank You to
+- [HuyNguyen-hust](https://github.com/HuyNguyen-hust) for making [RoPE Embeddings 28% faster](https://github.com/unslothai/unsloth/pull/238)
+- [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
+- [152334H](https://github.com/152334H) for experimental DPO support
+- [atgctg](https://github.com/atgctg) for syntax highlighting
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f08b476..ca33509 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -324,6 +324,13 @@ def LlamaAttention_fast_forward_inference(
     #     Knn, Vnn = Knn, Vnn
     # pass
 
+    # when qlen==vlen and attn_mask is None, we should use causal attention
+    Q_len = Qn.shape[-2]
+    K_len = Knn.shape[-2]
+    if attention_mask is None and Q_len == K_len:
+        is_causal = True
+    else:
+        is_causal = False
     # Attention
     if bsz == 1:
         Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
@@ -524,11 +531,18 @@ def LlamaAttention_fast_forward(
         V = V.transpose(1, 2)
         A = flash_attn_func(Q, K, V, causal = True)
     else:
+        # when qlen==vlen and attn_mask is None, we should use causal attention
+        Q_len = Q.shape[-2]
+        K_len = K.shape[-2]
+        if attention_mask is None and Q_len == K_len:
+            is_causal = True
+        else:
+            is_causal = False
         # Grouped query attention
         if SDPA_HAS_GQA:
             # Needs (batch_size, n_heads, seq_len, head_dim)
             # is_casual and attention_mask must not be both set!
-            A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False, enable_gqa = n_groups != 1)
+            A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = is_causal, enable_gqa = n_groups != 1)
             # Go back to (batch_size, seq_len, n_heads, head_dim)
             A = A.transpose(1, 2)#.contiguous()
         else:
@@ -543,7 +557,7 @@ def LlamaAttention_fast_forward(
             Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
             # Needs (batch_size, n_heads, seq_len, head_dim)
             # is_casual and attention_mask must not be both set!
-            A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)
+            A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = is_causal)
             # Go back to (batch_size, seq_len, n_heads, head_dim)
             A = A.transpose(1, 2).contiguous()
         pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 80cb195..09e035e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -180,12 +180,14 @@ def patch_tokenizer(model, tokenizer):
             # Try unk_token
             possible_pad_token = tokenizer.unk_token
         pass
+
         if possible_pad_token is None:
-            # Failure!!
-            raise RuntimeError(
-                ""Unsloth: Tokenizer's pad_token cannot be = eos_token, and we couldn't find a\n""\
-                ""replacement of either <|reserved... or <|placeholder...""
-            )
+            # Failure to find a good replacement!! We shall manually add one!
+            new_pad_token = ""<|PAD_TOKEN|>""
+            while new_pad_token in tokenizer.get_vocab():
+                new_pad_token += ""#""
+            pass
+            possible_pad_token = new_pad_token
         pass
 
         name = model.config._name_or_path if model is not None else ""Model""
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 44998b4..f08c403 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1005,7 +1005,7 @@ class FastLlamaModel:
     @staticmethod
     def from_pretrained(
         model_name     = ""unsloth/llama-2-7b-bnb-4bit"",
-        max_seq_length = 4096,
+        max_seq_length = None,
         dtype          = None,
         load_in_4bit   = True,
         token          = None,
@@ -1050,6 +1050,11 @@ class FastLlamaModel:
         model_max_seq_length = \
             AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings
 
+        # If max_seq_length is not specified, use maximum fron config
+        if max_seq_length is None:
+            max_seq_length = model_max_seq_length
+        pass
+
         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):
             rope_scaling = max_seq_length / model_max_seq_length
             logger.warning_once(
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 2b3bf47..86a0f5d 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -67,8 +67,8 @@ pass
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
-        model_name     = ""unsloth/mistral-7b-bnb-4bit"",
-        max_seq_length = 4096,
+        model_name     = ""unsloth/llama-3-8b-bnb-4bit"",
+        max_seq_length = None,
         dtype          = None,
         load_in_4bit   = True,
         token          = None,
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 902177c..4594919 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -290,7 +290,7 @@ class FastMistralModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
         model_name     = ""unsloth/mistral-7b-bnb-4bit"",
-        max_seq_length = 4096,
+        max_seq_length = None,
         dtype          = None,
         load_in_4bit   = True,
         token          = None,
@@ -340,6 +340,11 @@ class FastMistralModel(FastLlamaModel):
         model_config = AutoConfig.from_pretrained(model_name, token = token)
         model_max_seq_length = model_config.max_position_embeddings
 
+        # If max_seq_length is not specified, use maximum fron config
+        if max_seq_length is None:
+            max_seq_length = model_max_seq_length
+        pass
+
         # Mistral does NOT support RoPE Scaling sadly so we have to error out.
         if max_seq_length > model_max_seq_length:
             raise RuntimeError(
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 87cba84..7a1d2dd 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -304,6 +304,7 @@ def fix_sentencepiece_gguf(saved_location):
     if len(added_tokens_json) == 0: return
 
     added_tokens_json = dict(sorted(added_tokens_json.items(), key = lambda item: item[1]))
+    new_size = sentence_piece_size + len(added_tokens_json)
 
     # Confirm added_tokens_json is correct
     added_tokens_ids = np.array(list(added_tokens_json.values()))
@@ -312,7 +313,11 @@ def fix_sentencepiece_gguf(saved_location):
     if (added_tokens_ids.min() != sentence_piece_size): return
 
     # Edit sentence piece tokens with added_tokens_json
-    logger.warning(""Unsloth: Extending tokenizer.model with added_tokens.json!"")
+    logger.warning(
+        f""Unsloth: Extending {saved_location}/tokenizer.model with added_tokens.json.\n""\
+        f""Originally tokenizer.model is of size ({sentence_piece_size}).\n""\
+        f""But we need to extend to sentencepiece vocab size ({new_size}).""
+    )
     new_tokens = deepcopy(tokenizer_file.pieces[-len(added_tokens_ids):])
     for new_token, added_token in zip(new_tokens, added_tokens_json.keys()):
         new_token.piece = added_token.encode(""utf-8"")
@@ -357,7 +362,10 @@ def load_correct_tokenizer(
             padding_side      = padding_side,
             token             = token,
             trust_remote_code = trust_remote_code,
+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373
             use_fast          = False,
+            legacy            = False,
+            from_slow         = True,
             cache_dir         = cache_dir,
         )
     except:
@@ -512,7 +520,10 @@ def check_tokenizer(
                     model_max_length = model_max_length,
                     padding_side = padding_side,
                     token = token,
+                    # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373
                     use_fast = False,
+                    legacy = False,
+                    from_slow = True,
                     cache_dir = cache_dir,
                 )
                 return check_tokenizer(
@@ -725,7 +736,8 @@ def fix_sft_trainer_tokenizer():
         ""test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\n""\
         ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
         ""chat_template = '' if chat_template is None else chat_template\n""\
-        ""has_bos_token_already = test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template\n""\
+        ""has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) ""\
+        ""if getattr(tokenizer, 'bos_token', None) is not None else False\n""\
         ""add_special_tokens = False if has_bos_token_already else add_special_tokens\n\n""
 
         check_text = check_text.split(""\n"")
"
"diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index c2de979..26f632e 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -33,7 +33,11 @@ from .fast_lora import (
 )
 from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora
 
-from .flex_attention import HAS_FLEX_ATTENTION, slow_attention_softcapping
+from .flex_attention import (
+    HAS_FLEX_ATTENTION,
+    slow_attention_softcapping,
+    slow_inference_attention_softcapping,
+)
 
 if HAS_FLEX_ATTENTION:
     from .flex_attention import (
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index a992a02..9cf999e 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -80,3 +80,40 @@ def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
     return A
 pass
 
+
+torch_matmul = torch.matmul
+torch_tanh   = torch.tanh
+torch_nn_functional_softmax = torch.nn.functional.softmax
+def slow_inference_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
+    n_heads    = self.num_heads
+    head_dim   = self.head_dim
+    n_kv_heads = self.num_key_value_heads
+    n_groups   = self.num_key_value_groups
+    
+    # Grouped query attention
+    K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+    V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+    K = K.reshape(bsz, n_heads, q_len, head_dim)
+    V = V.reshape(bsz, n_heads, q_len, head_dim)
+
+    # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
+    # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
+    # We default to using the config file itself
+    # s = self.config.hidden_size // self.config.num_attention_heads
+    s = self.config.query_pre_attn_scalar
+    t = self.config.attn_logit_softcapping
+
+    Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
+    A = torch_matmul(Q, K.transpose(2, 3))
+
+    # Logit softcapping
+    A /= t; torch_tanh(A, out = A); A *= t;
+    A += causal_mask[:q_len, :q_len]
+    # Much slower in torch compile!
+    # A.masked_fill_(causal_mask[:q_len, :q_len], -float(""inf""))
+    A = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
+    A = torch_matmul(A, V)
+    A = A.transpose(1, 2).contiguous()
+    A = A.reshape(bsz, q_len, n_heads*head_dim)
+    return A
+pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ea9a0c5..242d234 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -39,6 +39,8 @@ __all__ = [
     ""create_boolean_mask"",
     ""torch_amp_custom_fwd"",
     ""torch_amp_custom_bwd"",
+    ""accelerate_old_send_to_device"",
+    ""accelerate_new_send_to_device"",
 ]
 
 import torch
@@ -287,6 +289,7 @@ if Version(xformers_version) >= Version(""0.0.27""):
     import accelerate.utils.operations
     if hasattr(accelerate.utils.operations, ""send_to_device"") and \
         accelerate.utils.operations.send_to_device.__name__ != ""_fixed_send_to_device"":
+        accelerate_old_send_to_device = accelerate.utils.operations.send_to_device
         from accelerate.utils.operations import *
         send_to_device = inspect.getsource(accelerate.utils.operations.send_to_device)
         send_to_device = re.sub(
@@ -296,6 +299,7 @@ if Version(xformers_version) >= Version(""0.0.27""):
         ).replace(""def send_to_device"", ""def _fixed_send_to_device"")
         exec(send_to_device)
         # accelerate.utils.operations.send_to_device = _fixed_send_to_device
+        accelerate_new_send_to_device = _fixed_send_to_device
     pass
 pass
 # =============================================
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 6858f52..218849e 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -157,7 +157,10 @@ def Gemma2Attention_fast_forward(
         A = A.reshape(bsz, q_len, n_heads*head_dim)
     else:
         mask = causal_mask if attention_mask is None else attention_mask
-        A = slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, kv_seq_len)
+        fx = slow_inference_attention_softcapping \
+            if ""_flag_for_generation"" in kwargs else \
+            slow_attention_softcapping
+        A = fx(Q, K, V, causal_mask, self, bsz, kv_seq_len)
     pass
     A = self.apply_o(self, A)
     return A, None, past_key_value
@@ -192,6 +195,7 @@ def Gemma2DecoderLayer_fast_forward(
             output_attentions=output_attentions,
             use_cache=use_cache,
             padding_mask=padding_mask,
+            _flag_for_generation=True,
         )
         hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)
         hidden_states += residual
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 5ccf906..3fcb8a7 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -953,6 +953,8 @@ def CausalLM_fast_forward(fast_forward_inference):
         if bsz == 1 and q_len == 1:
             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
             logits = logits.unsqueeze(0).unsqueeze(0)
+        elif num_logits_to_keep != 0:
+            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
         else:
             logits = self.lm_head(hidden_states.to(lm_head.dtype))
         pass
@@ -1368,8 +1370,14 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
         pass
         internal_model._flag_for_generation = True
 
+        # Must patch accelerate for Xformers
+        import accelerate.utils.operations
+        accelerate.utils.operations.send_to_device = accelerate_new_send_to_device
+
         # For newer HF
         kwargs[""cache_implementation""] = ""dynamic""
+        # For num_logits_to_keep
+        kwargs[""num_logits_to_keep""] = 1
 
         # Remove token_type_ids
         kwargs.pop(""token_type_ids"", None)
@@ -1402,6 +1410,9 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
         pass
         if hasattr(internal_model, ""_flag_for_generation""): del internal_model._flag_for_generation
 
+        # Return accelerate back
+        accelerate.utils.operations.send_to_device = accelerate_old_send_to_device
+
         return output
     pass
     return _fast_generate
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 207540a..480d22a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1731,7 +1731,7 @@ class FastLlamaModel:
         except: pass
         has_rope_scaling = True
 
-        # If max_seq_length is not specified, use maximum fron config
+        # If max_seq_length is not specified, use maximum from config
         if max_seq_length is None:
             max_seq_length = model_max_seq_length
         pass
@@ -1753,7 +1753,7 @@ class FastLlamaModel:
             # Warn RoPE scaling isn't allowed
             if not has_rope_scaling:
                 raise RuntimeError(
-                    ""However, {model_name} doesn't support RoPE Scaling!\n""\
+                    f""However, {model_name} doesn't support RoPE Scaling!\n""\
                     ""Please file a feature request at https://github.com/unslothai/unsloth.""
                 )
             pass
"
"diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index cf9c165..f13f7ef 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -536,7 +536,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
     if ""args.use_vllm"" in init and ""model"" in init and ""args"" in init:
         # .*? matches first match. .+? matches final match.
         replacer = re.findall(
-            ""def __init__\(.*?\).*?\:\n"",
+            r""def __init__\(.*?\).*?\:\n"",
             init,
             flags = re.MULTILINE | re.DOTALL,
         )
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 91bb020..2666912 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -945,7 +945,7 @@ def patch_sft_trainer_tokenizer():
         if replacer is None:
             # .*? matches first match. .+? matches final match.
             replacer = re.findall(
-                f""def {function_name}\(.*?\).*?\:\n"",
+                f""def {function_name}"" + r""\(.*?\).*?\:\n"",
                 function,
                 flags = re.MULTILINE | re.DOTALL,
             )
"
"diff --git a/pyproject.toml b/pyproject.toml
index d17859c..3d381fa 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.5"",
+    ""unsloth_zoo>=2025.7.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.5"",
+    ""unsloth_zoo>=2025.7.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
diff --git a/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
new file mode 100644
index 0000000..0bf548b
--- /dev/null
+++ b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
@@ -0,0 +1,255 @@
+# -*- coding: utf-8 -*-
+
+from unsloth import FastVisionModel
+
+import torch
+from qwen_vl_utils import process_vision_info
+import os
+from datasets import load_dataset
+from trl import SFTTrainer, SFTConfig
+
+import sys
+from pathlib import Path
+
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.ocr_eval import OCRModelEvaluator
+
+
+## Dataset Preparation
+from datasets import load_dataset
+
+dataset = load_dataset(""lbourdois/OCR-liboaccn-OPUS-MIT-5M-clean"", 'en', split=""train"")
+# To select the first 2000 examples
+train_dataset = dataset.select(range(2000))
+
+# To select the next 200 examples for evaluation
+eval_dataset = dataset.select(range(2000, 2200))
+
+# Convert dataset to OAI messages
+def format_data(sample):
+    return {""messages"": [
+                {
+                    ""role"": ""system"",
+                    ""content"": [{""type"": ""text"", ""text"": system_message}],
+                },
+                {
+                    ""role"": ""user"",
+                    ""content"": [
+                        {
+                            ""type"": ""text"",
+                            ""text"": sample[""question""],
+                        },{
+                            ""type"": ""image"",
+                            ""image"": sample[""image""],
+                        }
+                    ],
+                },
+                {
+                    ""role"": ""assistant"",
+                    ""content"": [{""type"": ""text"", ""text"": sample[""answer""]}],
+                },
+            ],
+        }
+
+system_message = ""You are an expert french ocr system.""
+# Convert dataset to OAI messages
+# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes
+train_dataset = [format_data(sample) for sample in train_dataset]
+eval_dataset = [format_data(sample) for sample in eval_dataset]
+
+## Setup OCR main evaluation function and helpers
+import os
+import torch
+from tqdm import tqdm
+import pandas as pd
+from jiwer import wer, cer
+from qwen_vl_utils import process_vision_info
+
+#
+ocr_evaluator = OCRModelEvaluator()
+model_comparison_results = {}
+
+## Finetuning Setup and Run
+# Load Base Model
+
+model, tokenizer = FastVisionModel.from_pretrained(
+    model_name = ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
+    max_seq_length = 2048, # Choose any for long context!
+    load_in_4bit = True,  # 4 bit quantization to reduce memory
+    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
+    full_finetuning = False, # [NEW!] We have full finetuning now!
+)
+
+# benchmark base model performance
+model_name = ""Unsloth Base model""
+FastVisionModel.for_inference(model)
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_base_model_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+## Lora Finetuning
+model = FastVisionModel.get_peft_model(
+    model,
+    finetune_vision_layers     = True, # Turn off for just text!
+    finetune_language_layers   = True,  # Should leave on!
+    finetune_attention_modules = True,  # Attention good for GRPO
+    finetune_mlp_modules       = True,  # SHould leave on always!
+
+    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    #target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                      #""gate_proj"", ""up_proj"", ""down_proj"",],
+    lora_alpha = 32,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+)
+
+from unsloth import is_bf16_supported
+from unsloth.trainer import UnslothVisionDataCollator
+FastVisionModel.for_training(model) # Enable for training!
+model.config.use_cache = False
+
+
+trainer = SFTTrainer(
+    model = model,
+    tokenizer = tokenizer,
+    data_collator = UnslothVisionDataCollator(model, tokenizer),
+    train_dataset = train_dataset,
+    args = SFTConfig(
+        #per_device_train_batch_size = 4,
+        #gradient_accumulation_steps = 8,
+        per_device_train_batch_size = 2,
+        gradient_accumulation_steps = 4,
+        gradient_checkpointing=True,
+        gradient_checkpointing_kwargs = {""use_reentrant"": False}, # use reentrant checkpointing
+        max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper
+        warmup_ratio=0.03,
+        #num_train_epochs = 2, # Set this instead of max_steps for full training runs
+        max_steps=60,
+        learning_rate = 2e-4,
+        fp16 = not is_bf16_supported(),
+        bf16 = is_bf16_supported(),
+        logging_steps = 5,
+        save_strategy=""epoch"",
+        optim = ""adamw_torch_fused"",
+        weight_decay = 0.01,
+        lr_scheduler_type = ""linear"",
+        seed = 3407,
+        output_dir = ""unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"",
+        report_to = ""none"",     # For Weights and Biases
+
+        # You MUST put the below items for vision finetuning:
+        remove_unused_columns = False,
+        dataset_text_field = """",
+        dataset_kwargs = {""skip_prepare_dataset"": True},
+        dataset_num_proc = 4,
+        max_seq_length = 2048,
+    ),
+)
+
+# run training
+trainer_stats = trainer.train()
+
+model.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"", tokenizer)
+tokenizer.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
+
+## Measure Adapter Performance
+
+# benchmark lora model performance
+model_name = ""Unsloth lora adapter model""
+FastVisionModel.for_inference(model)
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_lora_model_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+## Merge Model
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+base = find_lora_base_model(model)
+
+print((base.__class__.__name__))
+
+# merge default 16 bits
+model.save_pretrained_merged(save_directory=""qwen2.5-ocr-merged-finetune-merge-16bit"", tokenizer=tokenizer)
+
+
+## Benchmark merged model performance
+
+### 16 bits merged model
+
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=False)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-16bits""
+model.config.use_cache = True
+
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_16bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# load 16bits-merged model in 4 bits
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=True, load_in_8bit=False)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-4bits""
+model.config.use_cache = True
+
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_4bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# load model in 8 bits
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=True)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-8bits""
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_8bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# """"""### 4 bits merged model""""""
+#
+# # load 4bits-merged model in 4 bits
+# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=True, load_in_8bit=False)
+#
+# # benchmark 4bit loaded, 4bits merged model performance
+# model_name = ""Unsloth 4bits-merged model load-4bits""
+#
+# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_4bits_results"")
+# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+#
+# # load model in 8 bits
+# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=False, load_in_8bit=True)
+#
+# # benchmark 8bit loaded, 4bits merged model performance
+# model_name = ""Unsloth 4bits-merged model load-8bits""
+#
+# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_8bits_results"")
+# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# Model comparison report
+#print model comparison
+ocr_evaluator.print_model_comparison()
+
+
+
+# Final cleanup
+print(""\n Cleaning up temporary files..."")
+safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
+safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"")
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./qwen2.5-ocr-merged-finetune-merge-16bit"")
+
+print(""\n Pipeline completed successfully!"")
+print(""="" * 80)
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index e965b29..7fb0093 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -222,7 +222,7 @@ elif DEVICE_TYPE == ""xpu"":
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.7.1""):
+    if Version(unsloth_zoo_version) < Version(""2025.7.7""):
         print(
             ""Unsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\n""\
             ""Do this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index bfd7c8c..1774ed3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.4""
+__version__ = ""2025.7.5""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index f559c6c..28fa163 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -618,6 +618,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen2.5-VL-7B-Instruct"",
         ""unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"",
     ),
+    ""unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-VL-32B-Instruct"",
+        ""Qwen/Qwen2.5-VL-32B-Instruct"",
+        ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
+    ),
     ""unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-VL-72B-Instruct"",
         ""Qwen/Qwen2.5-VL-72B-Instruct"",
"
"diff --git a/images/Merge.png b/images/Merge.png
new file mode 100644
index 0000000..a2df048
Binary files /dev/null and b/images/Merge.png differ
diff --git a/images/ollama.png b/images/ollama.png
new file mode 100644
index 0000000..fa83bb4
Binary files /dev/null and b/images/ollama.png differ
diff --git a/pyproject.toml b/pyproject.toml
index c2cf4ae..581b86a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -34,7 +34,7 @@ exclude = [""images*""]
 [project.optional-dependencies]
 huggingface = [
     ""tyro"",
-    ""transformers>=4.38.2"",
+    ""transformers>=4.42.3"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -185,9 +185,9 @@ colab-ampere-torch220 = [
 ]
 colab-new = [
     ""tyro"",
-    ""transformers>=4.38.2"",
+    ""transformers>=4.42.3"",
     ""datasets>=2.16.0"",
-    ""sentencepiece"",
+    ""sentencepiece>=0.2.0"",
     ""tqdm"",
     ""psutil"",
     ""wheel>=0.42.0"",
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 2605779..dc1ad26 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -19,14 +19,17 @@ from .utils import calculate_settings, MAX_FUSED_SIZE
 from transformers.models.llama.modeling_llama import logger
 
 
+@triton.heuristics({""DO_SOFTCAPPING"": lambda args: args[""DO_SOFTCAPPING""],})
 @triton.jit
 def _cross_entropy_forward(
     logits_ptr, logits_row_stride,
     loss_ptr,
     logsumexp_ptr,
     labels_ptr,
-    VOCAB_SIZE : tl.constexpr,
-    BLOCK_SIZE : tl.constexpr,
+    VOCAB_SIZE     : tl.constexpr,
+    BLOCK_SIZE     : tl.constexpr,
+    DO_SOFTCAPPING : tl.constexpr,
+    SOFTCAP        : tl.constexpr,
 ):
     """"""
         Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]
@@ -58,13 +61,19 @@ def _cross_entropy_forward(
     mask = col_offsets < VOCAB_SIZE
 
     label_idx = tl.load(labels_ptr).to(tl.int32)
-    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
+    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+    # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
+    if DO_SOFTCAPPING: logits = SOFTCAP * tl.math.tanh(logits / SOFTCAP)
+
+    logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
     logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
     if label_idx != -100:
-        x = tl.load(logits_ptr + label_idx).to(tl.float32)
-        loss = logsumexp - x
+        x = tl.load(logits_ptr + label_idx)
+        # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
+        if DO_SOFTCAPPING: x = SOFTCAP * tl.math.tanh(x / SOFTCAP)
+        loss = logsumexp - x.to(tl.float32)
     else:
         loss = 0.0
     tl.store(logsumexp_ptr, logsumexp)
@@ -72,15 +81,18 @@ def _cross_entropy_forward(
 pass
 
 
+@triton.heuristics({""DO_SOFTCAPPING"": lambda args: args[""DO_SOFTCAPPING""],})
 @triton.jit
 def _chunked_cross_entropy_forward(
     logits_ptr, logits_row_stride,
     loss_ptr,
     logsumexp_ptr,
     labels_ptr,
-    VOCAB_SIZE : tl.constexpr,
-    N_CHUNKS   : tl.constexpr,
-    BLOCK_SIZE : tl.constexpr,
+    VOCAB_SIZE     : tl.constexpr,
+    N_CHUNKS       : tl.constexpr,
+    BLOCK_SIZE     : tl.constexpr,
+    DO_SOFTCAPPING : tl.constexpr,
+    SOFTCAP        : tl.constexpr,
 ):
     """"""
         256K vocab divided in 4 chunks
@@ -117,7 +129,11 @@ def _chunked_cross_entropy_forward(
     mask = col_offsets < VOCAB_SIZE
 
     label_idx = tl.load(labels_ptr).to(tl.int32)
-    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
+    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+    # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
+    if DO_SOFTCAPPING: logits = SOFTCAP * tl.math.tanh(logits / SOFTCAP)
+
+    logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
     logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
@@ -126,7 +142,9 @@ def _chunked_cross_entropy_forward(
         # Do the -x separately
         if label_idx != -100:
             x = tl.load(logits_ptr + label_idx).to(tl.float32)
-            loss = -1.0 * x
+            # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
+            if DO_SOFTCAPPING: x = SOFTCAP * tl.math.tanh(x / SOFTCAP)
+            loss = -1.0 * x.to(tl.float32)
         else:
             loss = 0.0
         tl.store(loss_ptr, loss)
@@ -135,14 +153,17 @@ def _chunked_cross_entropy_forward(
 pass
 
 
+@triton.heuristics({""DO_SOFTCAPPING"": lambda args: args[""DO_SOFTCAPPING""],})
 @triton.jit
 def _cross_entropy_backward(
     logits_ptr, logits_row_stride,
     dloss_ptr,   dloss_row_stride,
     logsumexp_ptr,
     labels_ptr,
-    VOCAB_SIZE : tl.constexpr,
-    BLOCK_SIZE : tl.constexpr,
+    VOCAB_SIZE     : tl.constexpr,
+    BLOCK_SIZE     : tl.constexpr,
+    DO_SOFTCAPPING : tl.constexpr,
+    SOFTCAP        : tl.constexpr,
 ):
     """"""
         CE_i = -y log(P) = y * (log[sum(exp(x))] - x)
@@ -173,15 +194,27 @@ def _cross_entropy_backward(
     else:
         dloss = 0.0
 
-    x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
+    x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+    # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
+    if DO_SOFTCAPPING:
+        # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
+        partial = tl.math.tanh(x / SOFTCAP)
+        x = SOFTCAP * partial
+    pass
+
     logsumexp = tl.load(logsumexp_ptr + row_idx)
-    y = tl.exp(x - logsumexp)
+    y = tl.exp(x.to(tl.float32) - logsumexp)
     y = tl.where(
         col_offsets == label_idx,
         y - 1.0, # exp(x - logsumexp) - 1
         y,       # exp(x - logsumexp)
     )
 
+    if DO_SOFTCAPPING:
+        # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
+        y = y * (1.0 - partial*partial)
+    pass
+
     # If y == 0: dC/dx = 0 ==> we already masked it to be = 0, so dloss = 0.
     tl.store(logits_ptr + col_offsets, dloss * y, mask = mask)
 pass
@@ -191,40 +224,46 @@ MAX_FUSED_SIZE = 65536 # 2**16
 
 class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
-    def forward(ctx, logits, labels):
+    def forward(ctx, logits, labels, logit_softcapping = 0):
         n_rows, vocab_size = logits.shape
 
         div, mod = divmod(vocab_size, MAX_FUSED_SIZE)
         n_chunks = div + (mod != 0)
-        losses = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
+        losses = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
+
+        DO_SOFTCAPPING = (logit_softcapping != 0)
 
         if n_chunks == 1:
             # For small vocabs <= 65336 like Llama, Mistral
             BLOCK_SIZE, num_warps = calculate_settings(vocab_size)
-            logsumexp = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
+            logsumexp = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
 
             _cross_entropy_forward[(n_rows,)](
                 logits, logits.stride(0),
                 losses,
                 logsumexp,
                 labels,
-                VOCAB_SIZE = vocab_size,
-                BLOCK_SIZE = BLOCK_SIZE,
-                num_warps  = num_warps,
+                VOCAB_SIZE     = vocab_size,
+                BLOCK_SIZE     = BLOCK_SIZE,
+                DO_SOFTCAPPING = DO_SOFTCAPPING,
+                SOFTCAP        = logit_softcapping,
+                num_warps      = num_warps,
             )
         else:
             # For large vocabs > 65336 like Gemma 256K
-            logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = ""cuda"")
+            logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = ""cuda:0"")
 
             _chunked_cross_entropy_forward[(n_rows, n_chunks,)](
                 logits, logits.stride(0),
                 losses,
                 logsumexp,
                 labels,
-                VOCAB_SIZE = vocab_size,
-                N_CHUNKS   = n_chunks,
-                BLOCK_SIZE = MAX_FUSED_SIZE,
-                num_warps  = 32,
+                VOCAB_SIZE     = vocab_size,
+                N_CHUNKS       = n_chunks,
+                BLOCK_SIZE     = MAX_FUSED_SIZE,
+                DO_SOFTCAPPING = DO_SOFTCAPPING,
+                SOFTCAP        = logit_softcapping,
+                num_warps      = 32,
             )
             # logsumexp(chunked_logsumexp) - x
             # Do the -x separately
@@ -234,6 +273,8 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         pass
 
         ctx.save_for_backward(logits, logsumexp, labels)
+        ctx.DO_SOFTCAPPING    = DO_SOFTCAPPING
+        ctx.logit_softcapping = logit_softcapping
         return losses
     pass
 
@@ -251,16 +292,18 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
             dlosses, dlosses.stride(0),
             logsumexp,
             labels,
-            VOCAB_SIZE = vocab_size,
-            BLOCK_SIZE = BLOCK_SIZE,
-            num_warps  = 8,
+            VOCAB_SIZE     = vocab_size,
+            BLOCK_SIZE     = BLOCK_SIZE,
+            DO_SOFTCAPPING = ctx.DO_SOFTCAPPING,
+            SOFTCAP        = ctx.logit_softcapping,
+            num_warps      = 8,
         )
         return logits, None, None,
     pass
 pass
 
 
-def fast_cross_entropy_loss(logits, labels):
+def fast_cross_entropy_loss(logits, labels, logit_softcapping = 0):
     """"""
     Arguments:
         logits: (batch, seq_len, vocab_size)
@@ -274,6 +317,7 @@ def fast_cross_entropy_loss(logits, labels):
     loss = Fast_CrossEntropyLoss.apply(
         logits.view(batch*seq_len, d),
         labels.view(-1),
+        logit_softcapping,
     )
     n_items = torch.count_nonzero(labels != -100)
     return loss.sum() / n_items
diff --git a/unsloth/kernels/geglu.py b/unsloth/kernels/geglu.py
index df80fcb..006e8c0 100644
--- a/unsloth/kernels/geglu.py
+++ b/unsloth/kernels/geglu.py
@@ -41,7 +41,7 @@ pass
 def geglu_exact_forward_kernel(gate, up):
     batch, seq_len, hd = gate.shape
     n_elements = gate.numel()
-    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda"")
+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda:0"")
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
     _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
@@ -133,7 +133,7 @@ pass
 def geglu_approx_forward_kernel(gate, up):
     batch, seq_len, hd = gate.shape
     n_elements = gate.numel()
-    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda"")
+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda:0"")
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
     _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 4db89b7..f26e596 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -119,7 +119,7 @@ def _gemma_rms_layernorm_forward(
     W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)
 
     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols
-    inv_var = 1.0 / tl.sqrt(row_var + eps) # Must be 1/sqrt to match Deepmind's impl
+    inv_var = tl.math.rsqrt(row_var + eps)
     tl.store(r, inv_var)
     normed = X_row * inv_var
     output = normed * (W_row + 1.0)
@@ -137,8 +137,8 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         n_rows, n_cols = X.shape
         BLOCK_SIZE, num_warps = calculate_settings(n_cols)
 
-        Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = ""cuda"")
-        r = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
+        Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = ""cuda:0"")
+        r = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
 
         fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward
         fx[(n_rows,)](
diff --git a/unsloth/kernels/swiglu.py b/unsloth/kernels/swiglu.py
index ff6b162..f81b7aa 100644
--- a/unsloth/kernels/swiglu.py
+++ b/unsloth/kernels/swiglu.py
@@ -41,7 +41,7 @@ pass
 def swiglu_fg_kernel(e, g):
     batch, seq_len, hd = e.shape
     n_elements = e.numel()
-    h = torch.empty((batch, seq_len, hd), dtype = e.dtype, device = ""cuda"")
+    h = torch.empty((batch, seq_len, hd), dtype = e.dtype, device = ""cuda:0"")
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
     _fg_kernel[grid](e, g, h, n_elements, BLOCK_SIZE = 1024,)
     return h
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index ddee198..935f1d4 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -105,14 +105,14 @@ def fast_dequantize(W, quant_state = None, out = None):
 
     # Create weight matrix
     if out is None:
-        out = torch.empty(shape, dtype = dtype, device = ""cuda"")
+        out = torch.empty(shape, dtype = dtype, device = ""cuda:0"")
     else:
         assert(out.shape == shape)
         assert(out.dtype == dtype)
 
     # NF4 dequantization of statistics
     n_elements_absmax = absmax.numel()
-    out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda"")
+    out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"")
 
     # Do dequantization
     ptr_out_absmax = get_ptr(out_absmax)
@@ -161,7 +161,7 @@ def fast_gemv(X, W, quant_state, out = None):
     bout = shape[0]
 
     if out is None:
-        out = torch.empty((1, 1, bout,), dtype = dtype, device = ""cuda"")
+        out = torch.empty((1, 1, bout,), dtype = dtype, device = ""cuda:0"")
     # else:
     #     assert(out.shape == (1, 1, bout,))
     # pass
@@ -179,7 +179,7 @@ def fast_gemv(X, W, quant_state, out = None):
     ldb = ctypes.c_int32(ldb)
     ldc = ctypes.c_int32(ldc)
 
-    df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda"")
+    df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda:0"")
     cdequantize_blockwise_fp32(
         get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
         ctypes.c_int(blocksize2), ctypes.c_int(df.numel()),
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7a6954c..73aa0c6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -21,6 +21,12 @@ warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""transformers"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""accelerate"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""multiprocessing"")
+
+# Stop ""Special tokens have been added in the vocabulary, ...""
+import logging
+logging.getLogger(""transformers.tokenization_utils_base"").setLevel(logging.CRITICAL+1)
+
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
@@ -31,7 +37,7 @@ import numpy as np
 import os
 import psutil
 
-__version__ = ""2024.6""
+__version__ = ""2024.7""
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
@@ -80,8 +86,49 @@ __all__ = [
     ""offload_output_embeddings"",
     ""is_bfloat16_supported"",
     ""unsloth_offloaded_gradient_checkpoint"",
+    ""torch_compile_options"",
 ]
 
+# Just remove max_autotune_gemm warning
+import functools
+@functools.lru_cache(None)
+def is_big_gpu(index):
+    sms = torch.cuda.get_device_properties(index).multi_processor_count
+    if sms < 80:  # V100
+        # log.warning(""not enough SMs to use max_autotune_gemm mode"")
+        return False
+    return True
+import torch._inductor.utils
+torch._inductor.utils.is_big_gpu = is_big_gpu
+
+
+# Torch compile arguments
+torch_compile_arguments = [
+    ""config.dce = True"",
+    ""config.memory_planning = True"",
+    ""config.memory_pool = 'combined'"",
+    ""config.coordinate_descent_tuning = True"",
+    ""config.max_autotune_gemm = False"", # GEMM is unnecessary
+    ""config.autotune_multi_device = False"",
+    ""config.max_autotune_gemm_backends = 'ATEN'"", # Not much faster
+    ""config.aggressive_fusion = False"", # Careful changes results!
+    ""config.cuda.enable_cuda_lto = True"",
+    ""config.cuda.use_fast_math = True"",
+    ""config.cuda.compile_opt_level = '-O2'"",
+]
+import torch._inductor.config as config
+for _try_compile_argument in torch_compile_arguments:
+    try:    exec(_try_compile_argument)
+    except: pass
+pass
+torch_compile_options = {
+    ""epilogue_fusion""   : True,
+    ""max_autotune""      : True,
+    ""shape_padding""     : True,
+    ""trace.enabled""     : False, # Output Triton kernel outputs!
+    ""triton.cudagraphs"" : False,
+}
+
 
 def prepare_model_for_kbit_training(
     model                      : Any,
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 9937489..4c4515b 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -247,6 +247,8 @@ class FastGemmaModel(FastLlamaModel):
         GemmaModel          .forward = LlamaModel_fast_forward
         GemmaForCausalLM    .forward = CausalLM_fast_forward(GemmaModel_fast_forward_inference)
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+        fix_prepare_inputs_for_generation(GemmaForCausalLM)
+
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
         # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
new file mode 100644
index 0000000..0669e42
--- /dev/null
+++ b/unsloth/models/gemma2.py
@@ -0,0 +1,538 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .llama import *
+from ._utils import __version__
+from .gemma import (
+    GemmaFixedRotaryEmbedding,
+    fast_geglu_inference,
+)
+from transformers.models.gemma2.modeling_gemma2 import (
+    Gemma2Attention,
+    Gemma2DecoderLayer,
+    Gemma2Model,
+    Gemma2ForCausalLM,
+    Gemma2RotaryEmbedding,
+    apply_rotary_pos_emb,
+    repeat_kv,
+)
+from transformers.models.gemma2.modeling_gemma2 import *
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
+# For Pytorch 2.1.1
+try:
+    from transformers.models.gemma2.modeling_gemma2 import (
+        Gemma2SdpaAttention,
+        Gemma2FlashAttention2,
+    )
+except:
+    Gemma2SdpaAttention   = Gemma2Attention
+    Gemma2FlashAttention2 = Gemma2Attention
+pass
+
+
+# [TODO] We must randomnly use torch.compile?
+# I checked the gradients and formulas and I'm sure it's correct.
+# I'm stumped :(
+@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
+def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
+    old_dtype = X.dtype
+    X = X.float()
+    X = X * torch.rsqrt(X.square().mean(-1, keepdim = True) + layernorm.eps) * \
+        (1.0 + layernorm.weight.float())
+    return X.to(old_dtype)
+pass
+
+
+# Logit softcapping
+@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
+def gemma2_attention(Q, K, V, causal_mask, self, bsz, q_len):
+    n_heads    = self.num_heads
+    head_dim   = self.head_dim
+    n_kv_heads = self.num_key_value_heads
+    n_groups   = self.num_key_value_groups
+    
+    # Grouped query attention
+    K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+    V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+    K = K.reshape(bsz, n_heads, q_len, head_dim)
+    V = V.reshape(bsz, n_heads, q_len, head_dim)
+
+    s = self.config.hidden_size // self.config.num_attention_heads
+    t = self.config.attn_logit_softcapping
+
+    Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
+    A = torch.matmul(Q, K.transpose(2, 3))
+    A = t * torch.tanh(A / t) # Logit softcapping
+    A += causal_mask[:q_len, :q_len]
+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
+    A = torch.matmul(A, V)
+    A = A.transpose(1, 2).contiguous()
+    A = A.reshape(bsz, q_len, n_heads*head_dim)
+    return A
+pass
+
+
+# Logit softcapping
+def Gemma2Attention_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    bool = False,
+    use_cache:            bool = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    *args, **kwargs,
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+    
+    # Clear inference
+    if hasattr(self, ""paged_attention""):
+        del self.paged_attention_K
+        del self.paged_attention_V
+        del self.paged_attention
+        del self.temp_QA
+        del self.temp_KV
+        del self.RH_Q
+        del self.attention
+    pass
+
+    bsz, q_len, _ = hidden_states.size()
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    assert(n_kv_heads * n_groups == n_heads)
+
+    Q, K, V = self.apply_qkv(self, hidden_states)
+    Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)
+    K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+    V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+
+    kv_seq_len = K.shape[-2]
+    if past_key_value is not None:
+        kv_seq_len += past_key_value[0].shape[-2]
+
+    if position_ids is None:
+        cos = self.rotary_emb.cos_cached
+        sin = self.rotary_emb.sin_cached
+        Q, K = fast_rope_embedding(Q, K, cos, sin)
+    else:
+        cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
+        Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
+    pass
+
+    if past_key_value is not None:
+        K = torch.cat([past_key_value[0], K], dim = 2)
+        V = torch.cat([past_key_value[1], V], dim = 2)
+    pass
+    past_key_value = (K, V) if use_cache else None
+
+    A = gemma2_attention(Q, K, V, causal_mask, self, bsz, kv_seq_len)
+    A = self.apply_o(self, A)
+    return A, None, past_key_value
+pass
+
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590
+def Gemma2DecoderLayer_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    Optional[bool] = False,
+    use_cache:            Optional[bool] = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    *args, **kwargs,
+):
+    if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda:0"")
+
+        # Self Attention
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+        hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)
+        hidden_states += residual
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference_gemma(self. pre_feedforward_layernorm, hidden_states, out_weight)
+        hidden_states = fast_geglu_inference(self.mlp, hidden_states)
+        hidden_states = fast_rms_layernorm_inference_gemma(self.post_feedforward_layernorm, hidden_states, out_weight)
+        hidden_states += residual
+    else:
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_gemma2_compiled(self.input_layernorm, hidden_states, gemma = True)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+        hidden_states = fast_rms_layernorm_gemma2_compiled(self.post_attention_layernorm, hidden_states, gemma = True)
+        hidden_states = residual + hidden_states
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_gemma2_compiled(self. pre_feedforward_layernorm, hidden_states, gemma = True)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = fast_rms_layernorm_gemma2_compiled(self.post_feedforward_layernorm, hidden_states, gemma = True)
+        hidden_states = residual + hidden_states
+    pass
+
+    outputs = (hidden_states,)
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
+    return outputs
+pass
+
+
+from math import sqrt as math_sqrt
+KV_CACHE_INCREMENT = 256 # KV Cache update size
+torch_nn_functional_softmax = torch.nn.functional.softmax
+
+def Gemma2Attention_fast_forward_inference(
+    self,
+    hidden_states:  torch.Tensor,
+    past_key_value: Optional[Tuple[torch.Tensor]],
+    position_ids,
+    do_prefill = False,
+    attention_mask = None,
+    use_sliding_window = False,
+):
+    Xn = hidden_states
+    bsz, _, hd = hidden_states.size()
+    K1, V1 = past_key_value
+    dtype = Xn.dtype
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    attention_size = n_heads*head_dim
+    # assert(n_kv_heads * n_groups == n_heads)
+    seq_len = K1.shape[-2]
+    kv_seq_len = seq_len + 1
+
+    # Prefill phase
+    # if not hasattr(self, ""paged_attention""):
+    if do_prefill:
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = ""cuda:0"")
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
+        self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
+        self.scalar = 1.0 / math_sqrt(self.config.hidden_size // self.config.num_attention_heads)
+        self.half_head_dim = head_dim // 2
+        self.           t =       self.config.attn_logit_softcapping
+        self.reciprocal_t = 1.0 / self.config.attn_logit_softcapping
+    elif kv_seq_len >= self.paged_attention.shape[0]:
+        self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.attention.resize_((bsz, n_heads, 1, self.attention.shape[-1]+KV_CACHE_INCREMENT))
+    pass
+
+    Qn = fast_linear_forward(self.q_proj, Xn, out = self.temp_QA[0])
+    Kn = fast_linear_forward(self.k_proj, Xn, out = self.temp_KV[0])
+    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])
+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)
+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+
+    # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
+    # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)
+    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)
+    h = self.half_head_dim
+
+    RH_Q = self.RH_Q
+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    Qn *= cos
+    Qn.addcmul_(RH_Q, sin)
+
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]
+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]
+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    Kn *= cos
+    Kn.addcmul_(RH_K, sin)
+    
+    # New KV cache
+    # Kn = torch.cat([K1, Kn], dim = 2)
+    # Vn = torch.cat([V1, Vn], dim = 2)
+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+
+    # Handle sliding windows
+    sliding_window = self.config.sliding_window
+    if use_sliding_window and kv_seq_len > sliding_window:
+        # From https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L193
+        slicing_tokens = 1 - sliding_window
+        Knn = Kn[:, :, slicing_tokens:, :]#.contiguous()
+        Vnn = Vn[:, :, slicing_tokens:, :]#.contiguous()
+    else:
+        Knn, Vnn = Kn, Vn
+    pass
+
+    # Grouped query attention
+    _, _, cached_len, _ = Knn.shape
+    if n_groups != 1:
+        Knn = Knn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vnn = Vnn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)
+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)
+    pass
+    # else:
+    #     Knn, Vnn = Knn, Vnn
+    # pass
+
+    # Attention
+    # if bsz == 1:
+    Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
+    # It seems like doing (Q * scalar) @ K is better than (Q @ K) * scalar to stop overflows
+    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+    # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
+
+    A *= self.reciprocal_t; torch.tanh(A, out = A); A *= self.t;  # Logit softcapping
+
+    A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
+    A = torch.matmul(A, Vnn, out = Qn)
+    # else:
+    #     A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+    # pass
+    A = A.transpose(1, 2)
+    A = A.reshape(bsz, 1, attention_size)
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])
+    return A, (Kn, Vn)
+pass
+
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
+# @torch.inference_mode
+def Gemma2Model_fast_forward_inference(
+    self,
+    input_ids,
+    past_key_values,
+    position_ids,
+    attention_mask = None,
+):
+    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda:0"")
+    input_ids = input_ids[:,:self.max_seq_length]
+    hidden_states = self.model.embed_tokens(input_ids)
+    hidden_states = hidden_states.to(self.config.torch_dtype)
+    # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32
+    # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32
+    hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)
+
+    bsz, q_len, hd = hidden_states.shape
+    seq_len = past_key_values[0][0].shape[-2]
+    if bsz != 1:
+        SWA = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+            sliding_window = self.config.sliding_window,
+        )
+        GA = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+        )
+    else:
+        SWA = attention_mask
+        GA  = attention_mask
+    pass
+
+    next_decoder_cache = []
+    for idx, decoder_layer in enumerate(self.model.layers):
+
+        use_sliding_window = idx % 2 == 0
+
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)
+        hidden_states, present_key_value = Gemma2Attention_fast_forward_inference(
+            decoder_layer.self_attn,
+            hidden_states = hidden_states,
+            past_key_value = past_key_values[idx],
+            position_ids = position_ids,
+            attention_mask = SWA if use_sliding_window else GA,
+            do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
+            use_sliding_window = use_sliding_window,
+        )
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)
+        hidden_states += residual
+
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer. pre_feedforward_layernorm, hidden_states, out_weight)
+        hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_feedforward_layernorm, hidden_states, out_weight)
+        hidden_states += residual
+
+        next_decoder_cache.append(present_key_value)
+    pass
+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)
+
+    return BaseModelOutputWithPast(
+        last_hidden_state = hidden_states,
+        past_key_values = next_decoder_cache,
+        hidden_states = [],
+        attentions = [],
+    )
+pass
+
+
+class FastGemma2Model(FastLlamaModel):
+
+    @staticmethod
+    def pre_patch():
+        Gemma2Attention      .forward = Gemma2Attention_fast_forward
+        Gemma2SdpaAttention  .forward = Gemma2Attention_fast_forward
+        Gemma2FlashAttention2.forward = Gemma2Attention_fast_forward
+        Gemma2DecoderLayer   .forward = Gemma2DecoderLayer_fast_forward
+        Gemma2Model          .forward = LlamaModel_fast_forward
+        Gemma2ForCausalLM    .forward = CausalLM_fast_forward(Gemma2Model_fast_forward_inference)
+        PeftModelForCausalLM .forward = PeftModelForCausalLM_fast_forward
+        fix_prepare_inputs_for_generation(Gemma2ForCausalLM)
+        
+        # Solves https://github.com/unslothai/unsloth/issues/168
+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+        # https://github.com/huggingface/transformers/pull/27931
+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+        import transformers.models.gemma2.modeling_gemma2
+        transformers.models.gemma2.modeling_gemma2.Gemma2RotaryEmbedding = GemmaFixedRotaryEmbedding
+        return
+    pass
+
+
+    @staticmethod
+    def post_patch(model):
+        # Patch model for Gemma
+        layers = model.model.layers
+
+        # Torch.compile fails on embedding matrix??
+        # Workaround randomnly fixes it for torch versions < 2.2
+        model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)
+        model.config.update({""unsloth_version"" : __version__})
+
+        # We also do this for the lm_head
+        lm_head = torch.nn.Linear(1, 1, bias = None)
+        del lm_head.weight
+        lm_head.weight = model.lm_head.weight
+        lm_head.in_features  = lm_head.weight.shape[1]
+        lm_head.out_features = lm_head.weight.shape[0]
+        model.lm_head = lm_head
+
+        # Gemma has tied weights! This means lm_head == embed_tokens
+        if model.model.embed_tokens.weight.data_ptr() != model.lm_head.weight.data_ptr():
+            lm_head = torch.nn.Linear(1, 1, bias = None)
+            del lm_head.weight
+            lm_head.weight = model.model.embed_tokens.weight
+            lm_head.in_features  = lm_head.weight.shape[1]
+            lm_head.out_features = lm_head.weight.shape[0]
+            model.lm_head = lm_head
+        pass
+
+        # Also patch all dtypes - BnB seems to not allocate the correct type?
+        # BnB default dtype seems to be float16!
+        correct_dtype = lm_head.weight.dtype
+
+        for name, module in model.named_modules():
+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
+                weight = module.weight
+                quant_state = weight.quant_state
+
+                if type(quant_state) is list:
+                    # BnB seems to have float16 as default!
+                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
+                else:
+                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+                    quant_state.dtype = correct_dtype
+                pass
+            pass
+            # Downcast RoPE embedding to correct data type
+            # RoPE must be done in float32 for Gemma
+            # if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
+            #     and (module.cos_cached.dtype != correct_dtype):
+
+            #     module.cos_cached = module.cos_cached.to(correct_dtype)
+            #     module.sin_cached = module.sin_cached.to(correct_dtype)
+            #     pass
+            # pass
+        pass
+
+        # Add 1 to weight
+        # return output * (1 + self.weight)
+        # https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py#L89
+        from transformers.models.gemma2.modeling_gemma2 import Gemma2RMSNorm
+
+        # Freeze all parameters except LoRA
+        # We do this first since += 1 seems to not be liked by requires_grad = True
+        for name, param in model.named_parameters():
+            if "".lora_A."" in name or "".lora_B."" in name:
+                param.requires_grad_(True)
+            else:
+                param.requires_grad_(False)
+        pass
+
+        # Patch RMS Layernorm
+        for name, module in model.named_modules():
+            if isinstance(module, Gemma2RMSNorm):
+                # Must be in float32
+                # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36
+                # module = module.to(torch.float32)
+                # Leave + 1 to Triton kernel itself
+                # module.weight += 1.0 # return output * (1 + self.weight)
+                if not hasattr(module, ""variance_epsilon""):
+                    module.variance_epsilon = module.eps # Gemma doesn't use variance_epsilon
+        pass
+
+        # Clear deleted GPU items
+        import gc
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        return model
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2368a37..e19b857 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -15,6 +15,8 @@
 import torch
 import gc
 from typing import Optional, Tuple, List, Union
+from ._utils import *
+from ._utils import __version__
 from torch.nn.functional import scaled_dot_product_attention
 from transformers.models.llama.modeling_llama import (
     logger,
@@ -25,8 +27,6 @@ from transformers.modeling_attn_mask_utils import (
     _prepare_4d_causal_attention_mask_for_sdpa,
 )
 from ..kernels import *
-from ._utils import *
-from ._utils import __version__
 from ..tokenizer_utils import *
 if HAS_FLASH_ATTENTION:
     from flash_attn import flash_attn_func
@@ -78,6 +78,24 @@ from math import sqrt as math_sqrt
 KV_CACHE_INCREMENT = 256 # KV Cache update size
 torch_nn_functional_softmax = torch.nn.functional.softmax
 
+# Fix new HF's inference code
+def _fast_prepare_inputs_for_generation(self, input_ids, **kwargs,):
+    if ""past_key_values"" in kwargs:
+        input_ids = input_ids[:,[-1]]
+        kwargs[""attention_mask""] = kwargs[""attention_mask""][:,[-1]]
+    kwargs[""position_ids""] = kwargs[""cache_position""]
+    return { ""input_ids"" : input_ids, **kwargs, }
+pass
+
+
+def fix_prepare_inputs_for_generation(module):
+    # Fix prepare_inputs_for_generation
+    if hasattr(module, ""prepare_inputs_for_generation""):
+        module.prepare_inputs_for_generation = _fast_prepare_inputs_for_generation
+    pass
+pass
+
+
 def LlamaAttention_fast_forward_inference(
     self,
     hidden_states:  torch.Tensor,
@@ -542,7 +560,8 @@ def LlamaModel_fast_forward(
     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
 
     # Normalized from Gemma
-    IS_GEMMA = self.config.model_type == ""gemma""
+    IS_GEMMA  = self.config.model_type.startswith(""gemma"")
+    IS_GEMMA2 = self.config.model_type.startswith(""gemma2"")
     train_embed_tokens = self.embed_tokens.weight.requires_grad
 
     if IS_GEMMA:
@@ -642,17 +661,38 @@ def LlamaModel_fast_forward(
             offloaded_gradient_checkpointing = True
     pass
 
+    # Gemma2 has alternating SWA and global attn
+    if IS_GEMMA2 and not hasattr(self, ""SWA_mask""):
+        from transformers.modeling_attn_mask_utils import AttentionMaskConverter
+        n = self.config.max_position_embeddings
+        self.SWA_mask = AttentionMaskConverter(
+            is_causal = True,
+            sliding_window = self.config.sliding_window,
+        )\
+            .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
+            .squeeze(0).squeeze(0)
+
+        self.GA_mask = AttentionMaskConverter(
+            is_causal = True,
+        )\
+            .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
+            .squeeze(0).squeeze(0)
+    pass
+
     # Go through every layer!
     for idx, decoder_layer in enumerate(self.layers):
 
         if output_hidden_states: all_hidden_states += (hidden_states,)
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
+        mask = causal_mask
+        if IS_GEMMA2: mask = self.SWA_mask if (idx % 2 == 0) else self.GA_mask
+
         if offloaded_gradient_checkpointing:
             hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(
                 decoder_layer,
                 hidden_states,
-                causal_mask,
+                mask,
                 attention_mask,
                 position_ids,
                 past_key_values,
@@ -670,7 +710,7 @@ def LlamaModel_fast_forward(
             layer_outputs = torch.utils.checkpoint.checkpoint(
                 create_custom_forward(decoder_layer),
                 hidden_states,
-                causal_mask,
+                mask,
                 attention_mask,
                 position_ids,
                 use_reentrant = True,
@@ -681,7 +721,7 @@ def LlamaModel_fast_forward(
         else:
             layer_outputs = decoder_layer(
                 hidden_states,
-                causal_mask=causal_mask,
+                causal_mask=mask,
                 attention_mask=attention_mask,
                 position_ids=position_ids,
                 past_key_value=past_key_value,
@@ -838,6 +878,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         logits = logits.to(self.config.torch_dtype)
 
         loss = None
+        logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
         if labels is not None:
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
@@ -849,7 +890,12 @@ def CausalLM_fast_forward(fast_forward_inference):
             loss = fast_cross_entropy_loss(
                 logits = shift_logits,
                 labels = shift_labels,
+                logit_softcapping = logit_softcapping,
             )
+        elif logit_softcapping != 0:
+            logits *= (1.0 / logit_softcapping)
+            torch.tanh(logits, out = logits)
+            logits *= logit_softcapping
         pass
 
         if not return_dict:
@@ -983,11 +1029,22 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
         pass
         internal_model._flag_for_generation = True
 
+        # For newer HF
+        kwargs[""cache_implementation""] = ""dynamic""
+
+        # Set pad token
+        old_pad_token_id = getattr(model.config, ""pad_token_id"", None)
+        old_eos_token_id = getattr(model.config, ""eos_token_id"", None)
+        model.config.pad_token_id = old_eos_token_id
+
         # Autocasted
         with torch.autocast(device_type = device_type, dtype = dtype):
             output = generate(*args, **kwargs)
         pass
 
+        # Revert
+        model.config.pad_token_id = old_pad_token_id
+
         # Unset a flag for generation!
         internal_model = model
         while hasattr(internal_model, ""model""):
@@ -1013,6 +1070,7 @@ class FastLlamaModel:
         LlamaModel          .forward = LlamaModel_fast_forward
         LlamaForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+        fix_prepare_inputs_for_generation(LlamaForCausalLM)
 
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
@@ -1056,7 +1114,7 @@ class FastLlamaModel:
            f""==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
-           f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
+           f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
         model_patcher.pre_patch()
@@ -1200,11 +1258,11 @@ class FastLlamaModel:
             'nvidia-smi --query-gpu=memory.used --format=csv', shell = True)
         output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)
         output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)
-        if output > 1: raise RuntimeError(
-            'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\
+        if output > 1: print(
+            '********************\\nUnsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\
             'enabling it will require much more work, so we have to prioritize. Please understand!\\n'\\
-            'We do have a separate beta version, which you can contact us about!\\n'\\
-            'Thank you for your understanding and we appreciate it immensely!')
+            '********************\\nWe do have a separate beta version, which you can contact us about!\\n'\\
+            '********************\\nThank you for your understanding and we appreciate it immensely!')
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()""""""
@@ -1760,6 +1818,7 @@ class FastLlamaModel:
         elif model_type == ""mistral"": apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""qwen2"":   apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu_approx
+        elif model_type == ""gemma2"":  apply_lora_mlp = apply_lora_mlp_geglu_approx
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
         pass
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index d87af0a..9134d4a 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -26,8 +26,11 @@ major, minor = transformers_version.split(""."")[:2]
 major, minor = int(major), int(minor)
 SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)
 SUPPORTS_GEMMA   = (major > 4) or (major == 4 and minor >= 38)
+SUPPORTS_GEMMA2  = (major > 4) or (major == 4 and minor >= 42)
 if SUPPORTS_GEMMA:
-    from .gemma import FastGemmaModel
+    from .gemma  import FastGemmaModel
+if SUPPORTS_GEMMA2:
+    from .gemma2 import FastGemma2Model
 del major, minor
 
 
@@ -138,6 +141,15 @@ class FastLanguageModel(FastLlamaModel):
                     f""to obtain the latest transformers build, then restart this session.""\
                 )
             dispatch_model = FastGemmaModel
+        elif model_type == ""gemma2"":
+            if not SUPPORTS_GEMMA2:
+                raise RuntimeError(
+                    f""Unsloth: Your transformers version of {transformers_version} does not support Gemma2.\n""\
+                    f""The minimum required version is 4.43.\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.43""`\n'\
+                    f""to obtain the latest transformers build, then restart this session.""\
+                )
+            dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
         else:
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 4b40065..cec7332 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -191,6 +191,14 @@ __INT_TO_FLOAT_MAPPER = \
     ""mistralai/Codestral-22B-v0.1"" : (
         ""mistral-community/Codestral-22B-v0.1"",
     ),
+    ""unsloth/gemma-2-9b-bnb-4bit"" : (
+        ""unsloth/gemma-2-9b"",
+        ""google/gemma-2-9b"",
+    ),
+    ""unsloth/gemma-2-27b-bnb-4bit"" : (
+        ""unsloth/gemma-2-27b"",
+        ""google/gemma-2-27b"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index d8bd85d..e0b51a1 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -275,7 +275,8 @@ class FastMistralModel(FastLlamaModel):
         MistralModel          .forward = LlamaModel_fast_forward
         MistralForCausalLM    .forward = MistralForCausalLM_fast_forward
         PeftModelForCausalLM  .forward = PeftModelForCausalLM_fast_forward
-
+        fix_prepare_inputs_for_generation(MistralForCausalLM)
+        
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
         # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
diff --git a/unsloth/models/qwen2.py b/unsloth/models/qwen2.py
index 984bf7c..5b9fff5 100644
--- a/unsloth/models/qwen2.py
+++ b/unsloth/models/qwen2.py
@@ -43,6 +43,7 @@ class FastQwen2Model(FastLlamaModel):
         Qwen2Model          .forward = LlamaModel_fast_forward
         Qwen2ForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+        fix_prepare_inputs_for_generation(Qwen2ForCausalLM)
 
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 50b0927..8727ca0 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -963,11 +963,11 @@ def patch_sft_trainer_tokenizer():
     ""    'nvidia-smi --query-gpu=memory.used --format=csv', shell = True)\n""\
     ""output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)\n""\
     ""output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)\n""\
-    ""if output > 1: raise RuntimeError(\n""\
-    ""    'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\\n""\
+    ""if output > 1: print(\n""\
+    ""    '********************\\nUnsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\\n""\
     ""    'enabling it will require much more work, so we have to prioritize. Please understand!\\n'\\\n""\
-    ""    'We do have a separate beta version, which you can contact us about!\\n'\\\n""\
-    ""    'Thank you for your understanding and we appreciate it immensely!')\n""\
+    ""    '********************\\nWe do have a separate beta version, which you can contact us about!\\n'\\\n""\
+    ""    '********************\\nThank you for your understanding and we appreciate it immensely!')\n""\
     ""for _ in range(3):\n""\
     ""    gc.collect()\n""\
     ""    torch.cuda.empty_cache()\n""\
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 49b8ba3..7a6954c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -372,11 +372,6 @@ def prepare_n_gradient_checkpoints(
 pass
 
 
-# Unsloth only works on NVIDIA GPUs for now
-device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
-device = f""cuda:{device if device.isdigit() else '0'}""
-
 class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     """"""
     Saves VRAM by smartly offloading to RAM.
@@ -398,7 +393,7 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     @torch.cuda.amp.custom_bwd
     def backward(ctx, dY):
         (hidden_states,) = ctx.saved_tensors
-        hidden_states = hidden_states.to(device, non_blocking = True).detach()
+        hidden_states = hidden_states.to(""cuda:0"", non_blocking = True).detach()
         hidden_states.requires_grad = True
         with torch.enable_grad():
             (output,) = ctx.forward_function(hidden_states, *ctx.args)
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 0cc047d..9937489 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -38,9 +38,6 @@ except:
     GemmaFlashAttention2 = GemmaAttention
 pass
 
-import os
-device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
 
 torch_nn_functional_gelu = torch.nn.functional.gelu
 def fast_geglu_inference(self, X):
@@ -48,7 +45,7 @@ def fast_geglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     # mlp_size = self.config.intermediate_size
-    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = device)
+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
 
     gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
@@ -75,7 +72,7 @@ def GemmaDecoderLayer_fast_forward(
     *args, **kwargs,
 ):
     if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
-        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = device)
+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda:0"")
 
         # Self Attention
         residual = hidden_states
@@ -137,7 +134,7 @@ def GemmaModel_fast_forward_inference(
     position_ids,
     attention_mask = None,
 ):
-    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = device)
+    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda:0"")
     input_ids = input_ids[:,:self.max_seq_length]
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
@@ -220,8 +217,8 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
 
         emb = torch.cat((radians_new, radians_new), dim = -1)
         # We must do RoPE in float32!
-        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)
-        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)
+        cos = emb.cos().to(device = ""cuda:0"", non_blocking = True)#, dtype = dtype)
+        sin = emb.sin().to(device = ""cuda:0"", non_blocking = True)#, dtype = dtype)
         self.register_buffer(""cos_cached"", cos, persistent = False)
         self.register_buffer(""sin_cached"", sin, persistent = False)
     pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9db7fcf..2d8e6a0 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -74,11 +74,6 @@ def original_apply_o(self, X):
     return O
 pass
 
-import os # Unsloth only works on NVIDIA GPUs for now
-device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
-device = f""cuda:{device if device.isdigit() else '0'}""
-
 from math import sqrt as math_sqrt
 KV_CACHE_INCREMENT = 256 # KV Cache update size
 torch_nn_functional_softmax = torch.nn.functional.softmax
@@ -136,15 +131,15 @@ def LlamaAttention_fast_forward_inference(
     # Prefill phase
     # if not hasattr(self, ""paged_attention""):
     if do_prefill:
-        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = device)
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = ""cuda:0"")
         self.paged_attention_K = self.paged_attention[:,0]
         self.paged_attention_V = self.paged_attention[:,1]
         self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
         self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
-        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
-        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
-        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
-        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
     elif kv_seq_len >= self.paged_attention.shape[0]:
@@ -174,7 +169,7 @@ def LlamaAttention_fast_forward_inference(
     Qn *= cos
     Qn.addcmul_(RH_Q, sin)
 
-    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = device)
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
     RH_K[:,:,:,:h] = Kn[:,:,:,h:]
     RH_K[:,:,:,h:] = Kn[:,:,:,:h]
     torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
@@ -236,7 +231,7 @@ def fast_swiglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     # mlp_size = self.config.intermediate_size
-    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = device)
+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
 
     gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
@@ -526,7 +521,7 @@ def LlamaModel_fast_forward(
         position_ids = torch.arange(
             past_key_values_length, seq_length + past_key_values_length,
             dtype  = torch.int32,
-            device = device,
+            device = ""cuda:0"",
         )
         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
     elif position_ids is not None:
@@ -846,11 +841,8 @@ def CausalLM_fast_forward(fast_forward_inference):
         if labels is not None:
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
-                device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-                device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
-                device = f""cuda:{device if device.isdigit() else '0'}""
                 # Fixes https://github.com/unslothai/unsloth/issues/10
-                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = device)
+                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
             pass
             
             shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
@@ -1471,7 +1463,7 @@ class FastLlamaModel:
                     print(""Unsloth: Casting embed_tokens to float32"")
 
                     model.model.model.embed_tokens.modules_to_save.default\
-                        .to(device = device, dtype = torch.float32, non_blocking = True)
+                        .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
                     model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old embed_tokens to CPU - should be disk!
@@ -1484,7 +1476,7 @@ class FastLlamaModel:
                     print(""Unsloth: Casting lm_head to float32"")
 
                     model.model.lm_head.modules_to_save.default\
-                        .to(device = device, dtype = torch.float32, non_blocking = True)
+                        .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
                     model.model.lm_head.modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old lm_head to CPU - should be disk!
@@ -1713,7 +1705,7 @@ class FastLlamaModel:
             print(""Unsloth: Casting embed_tokens to float32"")
             assert(hasattr(model.model.model.embed_tokens, ""modules_to_save""))
             model.model.model.embed_tokens.modules_to_save.default\
-                .to(device = device, dtype = torch.float32, non_blocking = True)
+                .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
             model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
         pass
 
@@ -1721,7 +1713,7 @@ class FastLlamaModel:
             print(""Unsloth: Casting lm_head to float32"")
             assert(hasattr(model.model.lm_head, ""modules_to_save""))
             model.model.lm_head.modules_to_save.default\
-                .to(device = device, dtype = torch.float32, non_blocking = True)
+                .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
             model.model.lm_head.modules_to_save.default.requires_grad_(True)
         pass
 
@@ -1902,10 +1894,7 @@ class FastLlamaModel:
         # Patch cross entropy loss labels
         # Fixes https://github.com/unslothai/unsloth/issues/10
         max_seq_length = model.max_seq_length
-        device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-        device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
-        device = f""cuda:{device if device.isdigit() else '0'}""
-        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = device)
+        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda:0"")
         model.model.extra_ignored_labels = extra_ignored_labels
         internal_model = model
         while hasattr(internal_model, ""model""):
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 832189b..d8bd85d 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -239,11 +239,8 @@ def MistralForCausalLM_fast_forward(
     if labels is not None:
         shift_logits = logits
         if not hasattr(self, ""extra_ignored_labels""):
-            device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-            device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
-            device = f""cuda:{device if device.isdigit() else '0'}""
             # Fixes https://github.com/unslothai/unsloth/issues/10
-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = device)
+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
         pass
         
         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
"
"diff --git a/README.md b/README.md
index 4580e83..9c44e26 100644
--- a/README.md
+++ b/README.md
@@ -1,23 +1,25 @@
 <div class=""align-center"">
   <img src=""./images/unsloth new logo.png"" width=""400"" />
   <a href=""https://discord.gg/u54VK8m8tk""><img src=""./images/Discord.png"" width=""180""></a>
+  <a href=""https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing""><img src=""./images/try live demo green.png"" width=""130""></a>
 </div>
 
-
-## 80% faster 50% less memory local QLoRA finetuning
+## 2-5x faster 50% less memory local LLM finetuning
 * Manual autograd engine - hand derived backprop steps.
-* QLoRA / LoRA 80% faster, 50% less memory.
-* All kernels written in OpenAI's Triton language.
+* 2x to 5x faster than QLoRA. 50% less memory usage.
+* All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language.
 * 0% loss in accuracy - no approximation methods - all exact.
-* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+. Tesla T4, RTX 20, 30, 40 series, A100, H100s
-* Flash Attention support via Xformers.
-* Supports 4bit and 16bit LoRA finetuning.
+* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)
+* [Flash Attention v2](https://github.com/Dao-AILab/flash-attention) support via [Xformers](https://github.com/facebookresearch/xformers).
+* **NEW!** Works on **Linux** and **Windows** via WSL.
+* **NEW!** Experimental support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)!
+* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
 * Train Slim Orca **fully locally in 260 hours from 1301 hours (5x faster).**
 * Open source version trains 5x faster or you can check out [Unsloth Pro and Max](https://unsloth.ai/) codepaths for **30x faster training**!
   
 <div class=""align-center"">
   <img src=""./images/Slim Orca 2GPUs.png"" width=""400"" />
-  <img src=""./images/LAION%202GPU.svg"" width=""400"" />
+  <img src=""./images/LAION 2GPU.png"" width=""400"" />
 </div>
 
 1. Try our Colab examples for [the Alpaca 52K dataset](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) or [the Slim Orca 518K dataset](https://colab.research.google.com/drive/1VNqLARpE8N8eYwNrUSDoHVjtbR9W0_c7?usp=sharing).
@@ -49,7 +51,13 @@ pip install --upgrade --force-reinstall --no-cache-dir torch triton \
 ```
 Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.
 
-# Alpaca Example
+4. If you get errors, try the below first, then go back to step 1:
+```
+pip install --upgrade pip
+```
+
+# Documentation
+We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
 ```
 from unsloth import FastLlamaModel
 import torch
@@ -59,7 +67,7 @@ load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False
 
 # Load Llama model
 model, tokenizer = FastLlamaModel.from_pretrained(
-    model_name = ""unsloth/llama-2-7b"", # Supports any llama model
+    model_name = ""unsloth/llama-2-7b"", # Supports any llama model eg meta-llama/Llama-2-7b-hf
     max_seq_length = max_seq_length,
     dtype = dtype,
     load_in_4bit = load_in_4bit,
@@ -80,12 +88,16 @@ model = FastLlamaModel.get_peft_model(
     max_seq_length = max_seq_length,
 )
 
-trainer = .... Use Huggingface's Trainer and dataset loading
+trainer = .... Use Huggingface's Trainer and dataset loading (TRL, transformers etc)
 ```
 
 If you trained a model with Unsloth, we made a cool sticker!!
 <img src=""./images/unsloth made with love.png"" width=""200"" />
 
+# DPO (Direct Preference Optimization) Experimental support
+[152334H](https://github.com/152334H) hacked Unsloth to work with DPO via TRL!
+1. Hack the model's `config.json` to be llama model. [Example gist](https://gist.github.com/152334H/d8a68b51b83bac008a02e69ecc81d5c1).
+2. Use Unsloth for DPO for both base and reference models. [Example gist](https://gist.github.com/152334H/4847f3a8cca12894877e6b30698b0b64).
 
 # Future Milestones and limitations
 1. Support sqrt gradient checkpointing which further slashes memory usage by 25%.
@@ -94,6 +106,9 @@ If you trained a model with Unsloth, we made a cool sticker!!
 # Performance comparisons on 1 Tesla T4 GPU:
 **Time taken for 1 epoch**
 
+One Tesla T4 on Google Colab
+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = ""adamw_8bit"", schedule = ""linear"", schedule_steps = 10`
+
 | System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
 | --- | --- | --- | --- | --- | --- |
 | Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |
@@ -113,19 +128,28 @@ If you trained a model with Unsloth, we made a cool sticker!!
 # Performance comparisons on 2 Tesla T4 GPUs via DDP:
 **Time taken for 1 epoch**
 
-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
+Two Tesla T4s on Kaggle
+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = ""adamw_8bit"", schedule = ""linear"", schedule_steps = 10`
+
+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |
 | --- | --- | --- | --- | --- | --- |
-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m |
-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) |
-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) |
+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |
+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |
+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |
 
 **Peak Memory Usage on a Multi GPU System (2 GPUs)**
 
-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |
+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |
 | --- | --- | --- | --- | --- | --- |
-| Huggingface | 2 T4 | 8.4GB \| 6GB | 7.2GB \| 5.3GB | 14.3GB \| 6.6GB | 10.9GB \| 5.9GB |
-| Unsloth Pro | 2 T4 | 7.7GB \| 4.9GB | 7.5GB \| 4.9GB | 8.5GB \| 4.9GB | 6.2GB \| 4.7GB |
-| Unsloth Max | 2 T4 | 10.5GB \| 5GB | 10.6GB \| 5GB | 10.6GB \| 5GB | 10.5GB \| 5GB |
+| Huggingface | 2 T4 | 8.4GB \| 6GB | 7.2GB \| 5.3GB | 14.3GB \| 6.6GB | 10.9GB \| 5.9GB * |
+| Unsloth Pro | 2 T4 | 7.7GB \| 4.9GB | 7.5GB \| 4.9GB | 8.5GB \| 4.9GB | 6.2GB \| 4.7GB * |
+| Unsloth Max | 2 T4 | 10.5GB \| 5GB | 10.6GB \| 5GB | 10.6GB \| 5GB | 10.5GB \| 5GB * |
+
+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.
+
+### For replication of timings:
+* [Huggingface LAION DDP reference implementation](https://www.kaggle.com/code/danielhanchen/huggingface-original-laion-oig) 60 steps on DDP Kaggle 2 Tesla T4 GPUs takes 40 minutes and 46 seconds
+* [Unsloth LAION DDP fast implementation](https://www.kaggle.com/code/danielhanchen/unsloth-laion-chip2-kaggle) 60 steps on DDP Kaggle 2 Tesla T4 GPUs - **Unsloth only uses 1 GPU whilst Pro plans use more.** takes 4 minutes and 34 seconds **(8.64x speedup)**
 
 # Troubleshooting
 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:
@@ -136,4 +160,8 @@ If you trained a model with Unsloth, we made a cool sticker!!
 
 3. If it doesn't install - maybe try updating `pip`.
 
+# Credits
+1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
+2. [152334H](https://github.com/152334H) for experimental DPO support
+
 <img src=""./images/unsloth loading page render.png"" width=""300"" />
diff --git a/images/Discord.png b/images/Discord.png
index 00b4d53..e1a0e14 100644
Binary files a/images/Discord.png and b/images/Discord.png differ
diff --git a/images/LAION 2GPU.png b/images/LAION 2GPU.png
new file mode 100644
index 0000000..d154a52
Binary files /dev/null and b/images/LAION 2GPU.png differ
diff --git a/images/LAION 2GPU.svg b/images/LAION 2GPU.svg
deleted file mode 100644
index 7240356..0000000
--- a/images/LAION 2GPU.svg	
+++ /dev/null
@@ -1,1518 +0,0 @@
-<?xml version=""1.0"" encoding=""utf-8"" standalone=""no""?>
-<!DOCTYPE svg PUBLIC ""-//W3C//DTD SVG 1.1//EN""
-  ""http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"">
-<svg xmlns:xlink=""http://www.w3.org/1999/xlink"" width=""720pt"" height=""432pt"" viewBox=""0 0 720 432"" xmlns=""http://www.w3.org/2000/svg"" version=""1.1"">
- <metadata>
-  <rdf:RDF xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:cc=""http://creativecommons.org/ns#"" xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"">
-   <cc:Work>
-    <dc:type rdf:resource=""http://purl.org/dc/dcmitype/StillImage""/>
-    <dc:date>2023-11-30T13:10:10.501490</dc:date>
-    <dc:format>image/svg+xml</dc:format>
-    <dc:creator>
-     <cc:Agent>
-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>
-     </cc:Agent>
-    </dc:creator>
-   </cc:Work>
-  </rdf:RDF>
- </metadata>
- <defs>
-  <style type=""text/css"">*{stroke-linejoin: round; stroke-linecap: butt}</style>
- </defs>
- <g id=""figure_1"">
-  <g id=""patch_1"">
-   <path d=""M 0 432 
-L 720 432 
-L 720 0 
-L 0 0 
-L 0 432 
-z
-"" style=""fill: none""/>
-  </g>
-  <g id=""axes_1"">
-   <g id=""patch_2"">
-    <path d=""M 90 384.48 
-L 648 384.48 
-L 648 51.84 
-L 90 51.84 
-L 90 384.48 
-z
-"" style=""fill: none""/>
-   </g>
-   <g id=""patch_3"">
-    <path d=""M 90 369.36 
-L 621.428571 369.36 
-L 621.428571 305.696842 
-L 90 305.696842 
-z
-"" clip-path=""url(#p42f2634aae)"" style=""fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""patch_4"">
-    <path d=""M 90 289.781053 
-L 202.147218 289.781053 
-L 202.147218 226.117895 
-L 90 226.117895 
-z
-"" clip-path=""url(#p42f2634aae)"" style=""fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""patch_5"">
-    <path d=""M 90 210.202105 
-L 108.547009 210.202105 
-L 108.547009 146.538947 
-L 90 146.538947 
-z
-"" clip-path=""url(#p42f2634aae)"" style=""fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""patch_6"">
-    <path d=""M 90 130.623158 
-L 106.978894 130.623158 
-L 106.978894 66.96 
-L 90 66.96 
-z
-"" clip-path=""url(#p42f2634aae)"" style=""fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""matplotlib.axis_1"">
-    <g id=""xtick_1"">
-     <g id=""line2d_1"">
-      <defs>
-       <path id=""m7f9e7b8ac6"" d=""M 0 0 
-L 0 3.5 
-"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </defs>
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""90"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_1"">
-      <!-- 0 -->
-      <g transform=""translate(86.81875 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-30"" d=""M 2034 4250 
-Q 1547 4250 1301 3770 
-Q 1056 3291 1056 2328 
-Q 1056 1369 1301 889 
-Q 1547 409 2034 409 
-Q 2525 409 2770 889 
-Q 3016 1369 3016 2328 
-Q 3016 3291 2770 3770 
-Q 2525 4250 2034 4250 
-z
-M 2034 4750 
-Q 2819 4750 3233 4129 
-Q 3647 3509 3647 2328 
-Q 3647 1150 3233 529 
-Q 2819 -91 2034 -91 
-Q 1250 -91 836 529 
-Q 422 1150 422 2328 
-Q 422 3509 836 4129 
-Q 1250 4750 2034 4750 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-30""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_2"">
-     <g id=""line2d_2"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""154.887493"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_2"">
-      <!-- 20 -->
-      <g transform=""translate(148.524993 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-32"" d=""M 1228 531 
-L 3431 531 
-L 3431 0 
-L 469 0 
-L 469 531 
-Q 828 903 1448 1529 
-Q 2069 2156 2228 2338 
-Q 2531 2678 2651 2914 
-Q 2772 3150 2772 3378 
-Q 2772 3750 2511 3984 
-Q 2250 4219 1831 4219 
-Q 1534 4219 1204 4116 
-Q 875 4013 500 3803 
-L 500 4441 
-Q 881 4594 1212 4672 
-Q 1544 4750 1819 4750 
-Q 2544 4750 2975 4387 
-Q 3406 4025 3406 3419 
-Q 3406 3131 3298 2873 
-Q 3191 2616 2906 2266 
-Q 2828 2175 2409 1742 
-Q 1991 1309 1228 531 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-32""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_3"">
-     <g id=""line2d_3"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""219.774987"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_3"">
-      <!-- 40 -->
-      <g transform=""translate(213.412487 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-34"" d=""M 2419 4116 
-L 825 1625 
-L 2419 1625 
-L 2419 4116 
-z
-M 2253 4666 
-L 3047 4666 
-L 3047 1625 
-L 3713 1625 
-L 3713 1100 
-L 3047 1100 
-L 3047 0 
-L 2419 0 
-L 2419 1100 
-L 313 1100 
-L 313 1709 
-L 2253 4666 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-34""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_4"">
-     <g id=""line2d_4"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""284.66248"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_4"">
-      <!-- 60 -->
-      <g transform=""translate(278.29998 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-36"" d=""M 2113 2584 
-Q 1688 2584 1439 2293 
-Q 1191 2003 1191 1497 
-Q 1191 994 1439 701 
-Q 1688 409 2113 409 
-Q 2538 409 2786 701 
-Q 3034 994 3034 1497 
-Q 3034 2003 2786 2293 
-Q 2538 2584 2113 2584 
-z
-M 3366 4563 
-L 3366 3988 
-Q 3128 4100 2886 4159 
-Q 2644 4219 2406 4219 
-Q 1781 4219 1451 3797 
-Q 1122 3375 1075 2522 
-Q 1259 2794 1537 2939 
-Q 1816 3084 2150 3084 
-Q 2853 3084 3261 2657 
-Q 3669 2231 3669 1497 
-Q 3669 778 3244 343 
-Q 2819 -91 2113 -91 
-Q 1303 -91 875 529 
-Q 447 1150 447 2328 
-Q 447 3434 972 4092 
-Q 1497 4750 2381 4750 
-Q 2619 4750 2861 4703 
-Q 3103 4656 3366 4563 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-36""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_5"">
-     <g id=""line2d_5"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""349.549974"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_5"">
-      <!-- 80 -->
-      <g transform=""translate(343.187474 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-38"" d=""M 2034 2216 
-Q 1584 2216 1326 1975 
-Q 1069 1734 1069 1313 
-Q 1069 891 1326 650 
-Q 1584 409 2034 409 
-Q 2484 409 2743 651 
-Q 3003 894 3003 1313 
-Q 3003 1734 2745 1975 
-Q 2488 2216 2034 2216 
-z
-M 1403 2484 
-Q 997 2584 770 2862 
-Q 544 3141 544 3541 
-Q 544 4100 942 4425 
-Q 1341 4750 2034 4750 
-Q 2731 4750 3128 4425 
-Q 3525 4100 3525 3541 
-Q 3525 3141 3298 2862 
-Q 3072 2584 2669 2484 
-Q 3125 2378 3379 2068 
-Q 3634 1759 3634 1313 
-Q 3634 634 3220 271 
-Q 2806 -91 2034 -91 
-Q 1263 -91 848 271 
-Q 434 634 434 1313 
-Q 434 1759 690 2068 
-Q 947 2378 1403 2484 
-z
-M 1172 3481 
-Q 1172 3119 1398 2916 
-Q 1625 2713 2034 2713 
-Q 2441 2713 2670 2916 
-Q 2900 3119 2900 3481 
-Q 2900 3844 2670 4047 
-Q 2441 4250 2034 4250 
-Q 1625 4250 1398 4047 
-Q 1172 3844 1172 3481 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-38""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_6"">
-     <g id=""line2d_6"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""414.437467"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_6"">
-      <!-- 100 -->
-      <g transform=""translate(404.893717 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-31"" d=""M 794 531 
-L 1825 531 
-L 1825 4091 
-L 703 3866 
-L 703 4441 
-L 1819 4666 
-L 2450 4666 
-L 2450 531 
-L 3481 531 
-L 3481 0 
-L 794 0 
-L 794 531 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-31""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_7"">
-     <g id=""line2d_7"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""479.324961"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_7"">
-      <!-- 120 -->
-      <g transform=""translate(469.781211 399.078438) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-31""/>
-       <use xlink:href=""#DejaVuSans-32"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_8"">
-     <g id=""line2d_8"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""544.212454"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_8"">
-      <!-- 140 -->
-      <g transform=""translate(534.668704 399.078438) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-31""/>
-       <use xlink:href=""#DejaVuSans-34"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_9"">
-     <g id=""line2d_9"">
-      <g>
-       <use xlink:href=""#m7f9e7b8ac6"" x=""609.099948"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_9"">
-      <!-- 160 -->
-      <g transform=""translate(599.556198 399.078438) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-31""/>
-       <use xlink:href=""#DejaVuSans-36"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""text_10"">
-     <!-- Time taken (lower is better). * Unsloth Open uses 1 GPU only. -->
-     <g transform=""translate(184.611563 414.27625) scale(0.12 -0.12)"">
-      <defs>
-       <path id=""DejaVuSans-54"" d=""M -19 4666 
-L 3928 4666 
-L 3928 4134 
-L 2272 4134 
-L 2272 0 
-L 1638 0 
-L 1638 4134 
-L -19 4134 
-L -19 4666 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-69"" d=""M 603 3500 
-L 1178 3500 
-L 1178 0 
-L 603 0 
-L 603 3500 
-z
-M 603 4863 
-L 1178 4863 
-L 1178 4134 
-L 603 4134 
-L 603 4863 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6d"" d=""M 3328 2828 
-Q 3544 3216 3844 3400 
-Q 4144 3584 4550 3584 
-Q 5097 3584 5394 3201 
-Q 5691 2819 5691 2113 
-L 5691 0 
-L 5113 0 
-L 5113 2094 
-Q 5113 2597 4934 2840 
-Q 4756 3084 4391 3084 
-Q 3944 3084 3684 2787 
-Q 3425 2491 3425 1978 
-L 3425 0 
-L 2847 0 
-L 2847 2094 
-Q 2847 2600 2669 2842 
-Q 2491 3084 2119 3084 
-Q 1678 3084 1418 2786 
-Q 1159 2488 1159 1978 
-L 1159 0 
-L 581 0 
-L 581 3500 
-L 1159 3500 
-L 1159 2956 
-Q 1356 3278 1631 3431 
-Q 1906 3584 2284 3584 
-Q 2666 3584 2933 3390 
-Q 3200 3197 3328 2828 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-65"" d=""M 3597 1894 
-L 3597 1613 
-L 953 1613 
-Q 991 1019 1311 708 
-Q 1631 397 2203 397 
-Q 2534 397 2845 478 
-Q 3156 559 3463 722 
-L 3463 178 
-Q 3153 47 2828 -22 
-Q 2503 -91 2169 -91 
-Q 1331 -91 842 396 
-Q 353 884 353 1716 
-Q 353 2575 817 3079 
-Q 1281 3584 2069 3584 
-Q 2775 3584 3186 3129 
-Q 3597 2675 3597 1894 
-z
-M 3022 2063 
-Q 3016 2534 2758 2815 
-Q 2500 3097 2075 3097 
-Q 1594 3097 1305 2825 
-Q 1016 2553 972 2059 
-L 3022 2063 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-20"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-74"" d=""M 1172 4494 
-L 1172 3500 
-L 2356 3500 
-L 2356 3053 
-L 1172 3053 
-L 1172 1153 
-Q 1172 725 1289 603 
-Q 1406 481 1766 481 
-L 2356 481 
-L 2356 0 
-L 1766 0 
-Q 1100 0 847 248 
-Q 594 497 594 1153 
-L 594 3053 
-L 172 3053 
-L 172 3500 
-L 594 3500 
-L 594 4494 
-L 1172 4494 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-61"" d=""M 2194 1759 
-Q 1497 1759 1228 1600 
-Q 959 1441 959 1056 
-Q 959 750 1161 570 
-Q 1363 391 1709 391 
-Q 2188 391 2477 730 
-Q 2766 1069 2766 1631 
-L 2766 1759 
-L 2194 1759 
-z
-M 3341 1997 
-L 3341 0 
-L 2766 0 
-L 2766 531 
-Q 2569 213 2275 61 
-Q 1981 -91 1556 -91 
-Q 1019 -91 701 211 
-Q 384 513 384 1019 
-Q 384 1609 779 1909 
-Q 1175 2209 1959 2209 
-L 2766 2209 
-L 2766 2266 
-Q 2766 2663 2505 2880 
-Q 2244 3097 1772 3097 
-Q 1472 3097 1187 3025 
-Q 903 2953 641 2809 
-L 641 3341 
-Q 956 3463 1253 3523 
-Q 1550 3584 1831 3584 
-Q 2591 3584 2966 3190 
-Q 3341 2797 3341 1997 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6b"" d=""M 581 4863 
-L 1159 4863 
-L 1159 1991 
-L 2875 3500 
-L 3609 3500 
-L 1753 1863 
-L 3688 0 
-L 2938 0 
-L 1159 1709 
-L 1159 0 
-L 581 0 
-L 581 4863 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6e"" d=""M 3513 2113 
-L 3513 0 
-L 2938 0 
-L 2938 2094 
-Q 2938 2591 2744 2837 
-Q 2550 3084 2163 3084 
-Q 1697 3084 1428 2787 
-Q 1159 2491 1159 1978 
-L 1159 0 
-L 581 0 
-L 581 3500 
-L 1159 3500 
-L 1159 2956 
-Q 1366 3272 1645 3428 
-Q 1925 3584 2291 3584 
-Q 2894 3584 3203 3211 
-Q 3513 2838 3513 2113 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-28"" d=""M 1984 4856 
-Q 1566 4138 1362 3434 
-Q 1159 2731 1159 2009 
-Q 1159 1288 1364 580 
-Q 1569 -128 1984 -844 
-L 1484 -844 
-Q 1016 -109 783 600 
-Q 550 1309 550 2009 
-Q 550 2706 781 3412 
-Q 1013 4119 1484 4856 
-L 1984 4856 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6c"" d=""M 603 4863 
-L 1178 4863 
-L 1178 0 
-L 603 0 
-L 603 4863 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6f"" d=""M 1959 3097 
-Q 1497 3097 1228 2736 
-Q 959 2375 959 1747 
-Q 959 1119 1226 758 
-Q 1494 397 1959 397 
-Q 2419 397 2687 759 
-Q 2956 1122 2956 1747 
-Q 2956 2369 2687 2733 
-Q 2419 3097 1959 3097 
-z
-M 1959 3584 
-Q 2709 3584 3137 3096 
-Q 3566 2609 3566 1747 
-Q 3566 888 3137 398 
-Q 2709 -91 1959 -91 
-Q 1206 -91 779 398 
-Q 353 888 353 1747 
-Q 353 2609 779 3096 
-Q 1206 3584 1959 3584 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-77"" d=""M 269 3500 
-L 844 3500 
-L 1563 769 
-L 2278 3500 
-L 2956 3500 
-L 3675 769 
-L 4391 3500 
-L 4966 3500 
-L 4050 0 
-L 3372 0 
-L 2619 2869 
-L 1863 0 
-L 1184 0 
-L 269 3500 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-72"" d=""M 2631 2963 
-Q 2534 3019 2420 3045 
-Q 2306 3072 2169 3072 
-Q 1681 3072 1420 2755 
-Q 1159 2438 1159 1844 
-L 1159 0 
-L 581 0 
-L 581 3500 
-L 1159 3500 
-L 1159 2956 
-Q 1341 3275 1631 3429 
-Q 1922 3584 2338 3584 
-Q 2397 3584 2469 3576 
-Q 2541 3569 2628 3553 
-L 2631 2963 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-73"" d=""M 2834 3397 
-L 2834 2853 
-Q 2591 2978 2328 3040 
-Q 2066 3103 1784 3103 
-Q 1356 3103 1142 2972 
-Q 928 2841 928 2578 
-Q 928 2378 1081 2264 
-Q 1234 2150 1697 2047 
-L 1894 2003 
-Q 2506 1872 2764 1633 
-Q 3022 1394 3022 966 
-Q 3022 478 2636 193 
-Q 2250 -91 1575 -91 
-Q 1294 -91 989 -36 
-Q 684 19 347 128 
-L 347 722 
-Q 666 556 975 473 
-Q 1284 391 1588 391 
-Q 1994 391 2212 530 
-Q 2431 669 2431 922 
-Q 2431 1156 2273 1281 
-Q 2116 1406 1581 1522 
-L 1381 1569 
-Q 847 1681 609 1914 
-Q 372 2147 372 2553 
-Q 372 3047 722 3315 
-Q 1072 3584 1716 3584 
-Q 2034 3584 2315 3537 
-Q 2597 3491 2834 3397 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-62"" d=""M 3116 1747 
-Q 3116 2381 2855 2742 
-Q 2594 3103 2138 3103 
-Q 1681 3103 1420 2742 
-Q 1159 2381 1159 1747 
-Q 1159 1113 1420 752 
-Q 1681 391 2138 391 
-Q 2594 391 2855 752 
-Q 3116 1113 3116 1747 
-z
-M 1159 2969 
-Q 1341 3281 1617 3432 
-Q 1894 3584 2278 3584 
-Q 2916 3584 3314 3078 
-Q 3713 2572 3713 1747 
-Q 3713 922 3314 415 
-Q 2916 -91 2278 -91 
-Q 1894 -91 1617 61 
-Q 1341 213 1159 525 
-L 1159 0 
-L 581 0 
-L 581 4863 
-L 1159 4863 
-L 1159 2969 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-29"" d=""M 513 4856 
-L 1013 4856 
-Q 1481 4119 1714 3412 
-Q 1947 2706 1947 2009 
-Q 1947 1309 1714 600 
-Q 1481 -109 1013 -844 
-L 513 -844 
-Q 928 -128 1133 580 
-Q 1338 1288 1338 2009 
-Q 1338 2731 1133 3434 
-Q 928 4138 513 4856 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-2e"" d=""M 684 794 
-L 1344 794 
-L 1344 0 
-L 684 0 
-L 684 794 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-2a"" d=""M 3009 3897 
-L 1888 3291 
-L 3009 2681 
-L 2828 2375 
-L 1778 3009 
-L 1778 1831 
-L 1422 1831 
-L 1422 3009 
-L 372 2375 
-L 191 2681 
-L 1313 3291 
-L 191 3897 
-L 372 4206 
-L 1422 3572 
-L 1422 4750 
-L 1778 4750 
-L 1778 3572 
-L 2828 4206 
-L 3009 3897 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-55"" d=""M 556 4666 
-L 1191 4666 
-L 1191 1831 
-Q 1191 1081 1462 751 
-Q 1734 422 2344 422 
-Q 2950 422 3222 751 
-Q 3494 1081 3494 1831 
-L 3494 4666 
-L 4128 4666 
-L 4128 1753 
-Q 4128 841 3676 375 
-Q 3225 -91 2344 -91 
-Q 1459 -91 1007 375 
-Q 556 841 556 1753 
-L 556 4666 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-68"" d=""M 3513 2113 
-L 3513 0 
-L 2938 0 
-L 2938 2094 
-Q 2938 2591 2744 2837 
-Q 2550 3084 2163 3084 
-Q 1697 3084 1428 2787 
-Q 1159 2491 1159 1978 
-L 1159 0 
-L 581 0 
-L 581 4863 
-L 1159 4863 
-L 1159 2956 
-Q 1366 3272 1645 3428 
-Q 1925 3584 2291 3584 
-Q 2894 3584 3203 3211 
-Q 3513 2838 3513 2113 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-4f"" d=""M 2522 4238 
-Q 1834 4238 1429 3725 
-Q 1025 3213 1025 2328 
-Q 1025 1447 1429 934 
-Q 1834 422 2522 422 
-Q 3209 422 3611 934 
-Q 4013 1447 4013 2328 
-Q 4013 3213 3611 3725 
-Q 3209 4238 2522 4238 
-z
-M 2522 4750 
-Q 3503 4750 4090 4092 
-Q 4678 3434 4678 2328 
-Q 4678 1225 4090 567 
-Q 3503 -91 2522 -91 
-Q 1538 -91 948 565 
-Q 359 1222 359 2328 
-Q 359 3434 948 4092 
-Q 1538 4750 2522 4750 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-70"" d=""M 1159 525 
-L 1159 -1331 
-L 581 -1331 
-L 581 3500 
-L 1159 3500 
-L 1159 2969 
-Q 1341 3281 1617 3432 
-Q 1894 3584 2278 3584 
-Q 2916 3584 3314 3078 
-Q 3713 2572 3713 1747 
-Q 3713 922 3314 415 
-Q 2916 -91 2278 -91 
-Q 1894 -91 1617 61 
-Q 1341 213 1159 525 
-z
-M 3116 1747 
-Q 3116 2381 2855 2742 
-Q 2594 3103 2138 3103 
-Q 1681 3103 1420 2742 
-Q 1159 2381 1159 1747 
-Q 1159 1113 1420 752 
-Q 1681 391 2138 391 
-Q 2594 391 2855 752 
-Q 3116 1113 3116 1747 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-75"" d=""M 544 1381 
-L 544 3500 
-L 1119 3500 
-L 1119 1403 
-Q 1119 906 1312 657 
-Q 1506 409 1894 409 
-Q 2359 409 2629 706 
-Q 2900 1003 2900 1516 
-L 2900 3500 
-L 3475 3500 
-L 3475 0 
-L 2900 0 
-L 2900 538 
-Q 2691 219 2414 64 
-Q 2138 -91 1772 -91 
-Q 1169 -91 856 284 
-Q 544 659 544 1381 
-z
-M 1991 3584 
-L 1991 3584 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-47"" d=""M 3809 666 
-L 3809 1919 
-L 2778 1919 
-L 2778 2438 
-L 4434 2438 
-L 4434 434 
-Q 4069 175 3628 42 
-Q 3188 -91 2688 -91 
-Q 1594 -91 976 548 
-Q 359 1188 359 2328 
-Q 359 3472 976 4111 
-Q 1594 4750 2688 4750 
-Q 3144 4750 3555 4637 
-Q 3966 4525 4313 4306 
-L 4313 3634 
-Q 3963 3931 3569 4081 
-Q 3175 4231 2741 4231 
-Q 1884 4231 1454 3753 
-Q 1025 3275 1025 2328 
-Q 1025 1384 1454 906 
-Q 1884 428 2741 428 
-Q 3075 428 3337 486 
-Q 3600 544 3809 666 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-50"" d=""M 1259 4147 
-L 1259 2394 
-L 2053 2394 
-Q 2494 2394 2734 2622 
-Q 2975 2850 2975 3272 
-Q 2975 3691 2734 3919 
-Q 2494 4147 2053 4147 
-L 1259 4147 
-z
-M 628 4666 
-L 2053 4666 
-Q 2838 4666 3239 4311 
-Q 3641 3956 3641 3272 
-Q 3641 2581 3239 2228 
-Q 2838 1875 2053 1875 
-L 1259 1875 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-79"" d=""M 2059 -325 
-Q 1816 -950 1584 -1140 
-Q 1353 -1331 966 -1331 
-L 506 -1331 
-L 506 -850 
-L 844 -850 
-Q 1081 -850 1212 -737 
-Q 1344 -625 1503 -206 
-L 1606 56 
-L 191 3500 
-L 800 3500 
-L 1894 763 
-L 2988 3500 
-L 3597 3500 
-L 2059 -325 
-z
-"" transform=""scale(0.015625)""/>
-      </defs>
-      <use xlink:href=""#DejaVuSans-54""/>
-      <use xlink:href=""#DejaVuSans-69"" x=""57.958984""/>
-      <use xlink:href=""#DejaVuSans-6d"" x=""85.742188""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""183.154297""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""244.677734""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""276.464844""/>
-      <use xlink:href=""#DejaVuSans-61"" x=""315.673828""/>
-      <use xlink:href=""#DejaVuSans-6b"" x=""376.953125""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""431.238281""/>
-      <use xlink:href=""#DejaVuSans-6e"" x=""492.761719""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""556.140625""/>
-      <use xlink:href=""#DejaVuSans-28"" x=""587.927734""/>
-      <use xlink:href=""#DejaVuSans-6c"" x=""626.941406""/>
-      <use xlink:href=""#DejaVuSans-6f"" x=""654.724609""/>
-      <use xlink:href=""#DejaVuSans-77"" x=""715.90625""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""797.693359""/>
-      <use xlink:href=""#DejaVuSans-72"" x=""859.216797""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""900.330078""/>
-      <use xlink:href=""#DejaVuSans-69"" x=""932.117188""/>
-      <use xlink:href=""#DejaVuSans-73"" x=""959.900391""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""1012""/>
-      <use xlink:href=""#DejaVuSans-62"" x=""1043.787109""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""1107.263672""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""1168.787109""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""1207.996094""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""1247.205078""/>
-      <use xlink:href=""#DejaVuSans-72"" x=""1308.728516""/>
-      <use xlink:href=""#DejaVuSans-29"" x=""1349.841797""/>
-      <use xlink:href=""#DejaVuSans-2e"" x=""1388.855469""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""1420.642578""/>
-      <use xlink:href=""#DejaVuSans-2a"" x=""1452.429688""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""1502.429688""/>
-      <use xlink:href=""#DejaVuSans-55"" x=""1534.216797""/>
-      <use xlink:href=""#DejaVuSans-6e"" x=""1607.410156""/>
-      <use xlink:href=""#DejaVuSans-73"" x=""1670.789062""/>
-      <use xlink:href=""#DejaVuSans-6c"" x=""1722.888672""/>
-      <use xlink:href=""#DejaVuSans-6f"" x=""1750.671875""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""1811.853516""/>
-      <use xlink:href=""#DejaVuSans-68"" x=""1851.0625""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""1914.441406""/>
-      <use xlink:href=""#DejaVuSans-4f"" x=""1946.228516""/>
-      <use xlink:href=""#DejaVuSans-70"" x=""2024.939453""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""2088.416016""/>
-      <use xlink:href=""#DejaVuSans-6e"" x=""2149.939453""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""2213.318359""/>
-      <use xlink:href=""#DejaVuSans-75"" x=""2245.105469""/>
-      <use xlink:href=""#DejaVuSans-73"" x=""2308.484375""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""2360.583984""/>
-      <use xlink:href=""#DejaVuSans-73"" x=""2422.107422""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""2474.207031""/>
-      <use xlink:href=""#DejaVuSans-31"" x=""2505.994141""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""2569.617188""/>
-      <use xlink:href=""#DejaVuSans-47"" x=""2601.404297""/>
-      <use xlink:href=""#DejaVuSans-50"" x=""2678.894531""/>
-      <use xlink:href=""#DejaVuSans-55"" x=""2739.197266""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""2812.390625""/>
-      <use xlink:href=""#DejaVuSans-6f"" x=""2844.177734""/>
-      <use xlink:href=""#DejaVuSans-6e"" x=""2905.359375""/>
-      <use xlink:href=""#DejaVuSans-6c"" x=""2968.738281""/>
-      <use xlink:href=""#DejaVuSans-79"" x=""2996.521484""/>
-      <use xlink:href=""#DejaVuSans-2e"" x=""3041.451172""/>
-     </g>
-    </g>
-   </g>
-   <g id=""matplotlib.axis_2"">
-    <g id=""ytick_1"">
-     <g id=""line2d_10"">
-      <defs>
-       <path id=""me3cc6245ad"" d=""M 0 0 
-L -3.5 0 
-"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </defs>
-      <g>
-       <use xlink:href=""#me3cc6245ad"" x=""90"" y=""337.528421"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_11"">
-      <!-- Huggingface -->
-      <g transform=""translate(19.68125 341.32764) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-48"" d=""M 628 4666 
-L 1259 4666 
-L 1259 2753 
-L 3553 2753 
-L 3553 4666 
-L 4184 4666 
-L 4184 0 
-L 3553 0 
-L 3553 2222 
-L 1259 2222 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-67"" d=""M 2906 1791 
-Q 2906 2416 2648 2759 
-Q 2391 3103 1925 3103 
-Q 1463 3103 1205 2759 
-Q 947 2416 947 1791 
-Q 947 1169 1205 825 
-Q 1463 481 1925 481 
-Q 2391 481 2648 825 
-Q 2906 1169 2906 1791 
-z
-M 3481 434 
-Q 3481 -459 3084 -895 
-Q 2688 -1331 1869 -1331 
-Q 1566 -1331 1297 -1286 
-Q 1028 -1241 775 -1147 
-L 775 -588 
-Q 1028 -725 1275 -790 
-Q 1522 -856 1778 -856 
-Q 2344 -856 2625 -561 
-Q 2906 -266 2906 331 
-L 2906 616 
-Q 2728 306 2450 153 
-Q 2172 0 1784 0 
-Q 1141 0 747 490 
-Q 353 981 353 1791 
-Q 353 2603 747 3093 
-Q 1141 3584 1784 3584 
-Q 2172 3584 2450 3431 
-Q 2728 3278 2906 2969 
-L 2906 3500 
-L 3481 3500 
-L 3481 434 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-66"" d=""M 2375 4863 
-L 2375 4384 
-L 1825 4384 
-Q 1516 4384 1395 4259 
-Q 1275 4134 1275 3809 
-L 1275 3500 
-L 2222 3500 
-L 2222 3053 
-L 1275 3053 
-L 1275 0 
-L 697 0 
-L 697 3053 
-L 147 3053 
-L 147 3500 
-L 697 3500 
-L 697 3744 
-Q 697 4328 969 4595 
-Q 1241 4863 1831 4863 
-L 2375 4863 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-63"" d=""M 3122 3366 
-L 3122 2828 
-Q 2878 2963 2633 3030 
-Q 2388 3097 2138 3097 
-Q 1578 3097 1268 2742 
-Q 959 2388 959 1747 
-Q 959 1106 1268 751 
-Q 1578 397 2138 397 
-Q 2388 397 2633 464 
-Q 2878 531 3122 666 
-L 3122 134 
-Q 2881 22 2623 -34 
-Q 2366 -91 2075 -91 
-Q 1284 -91 818 406 
-Q 353 903 353 1747 
-Q 353 2603 823 3093 
-Q 1294 3584 2113 3584 
-Q 2378 3584 2631 3529 
-Q 2884 3475 3122 3366 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-48""/>
-       <use xlink:href=""#DejaVuSans-75"" x=""75.195312""/>
-       <use xlink:href=""#DejaVuSans-67"" x=""138.574219""/>
-       <use xlink:href=""#DejaVuSans-67"" x=""202.050781""/>
-       <use xlink:href=""#DejaVuSans-69"" x=""265.527344""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""293.310547""/>
-       <use xlink:href=""#DejaVuSans-67"" x=""356.689453""/>
-       <use xlink:href=""#DejaVuSans-66"" x=""420.166016""/>
-       <use xlink:href=""#DejaVuSans-61"" x=""455.371094""/>
-       <use xlink:href=""#DejaVuSans-63"" x=""516.650391""/>
-       <use xlink:href=""#DejaVuSans-65"" x=""571.630859""/>
-      </g>
-     </g>
-    </g>
-    <g id=""ytick_2"">
-     <g id=""line2d_11"">
-      <g>
-       <use xlink:href=""#me3cc6245ad"" x=""90"" y=""257.949474"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_12"">
-      <!-- Unsloth Open* -->
-      <g transform=""translate(10.090625 261.748692) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-55""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""73.193359""/>
-       <use xlink:href=""#DejaVuSans-73"" x=""136.572266""/>
-       <use xlink:href=""#DejaVuSans-6c"" x=""188.671875""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""216.455078""/>
-       <use xlink:href=""#DejaVuSans-74"" x=""277.636719""/>
-       <use xlink:href=""#DejaVuSans-68"" x=""316.845703""/>
-       <use xlink:href=""#DejaVuSans-20"" x=""380.224609""/>
-       <use xlink:href=""#DejaVuSans-4f"" x=""412.011719""/>
-       <use xlink:href=""#DejaVuSans-70"" x=""490.722656""/>
-       <use xlink:href=""#DejaVuSans-65"" x=""554.199219""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""615.722656""/>
-       <use xlink:href=""#DejaVuSans-2a"" x=""679.101562""/>
-      </g>
-     </g>
-    </g>
-    <g id=""ytick_3"">
-     <g id=""line2d_12"">
-      <g>
-       <use xlink:href=""#me3cc6245ad"" x=""90"" y=""178.370526"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_13"">
-      <!-- Unsloth Pro -->
-      <g transform=""translate(25.942187 182.169745) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-55""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""73.193359""/>
-       <use xlink:href=""#DejaVuSans-73"" x=""136.572266""/>
-       <use xlink:href=""#DejaVuSans-6c"" x=""188.671875""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""216.455078""/>
-       <use xlink:href=""#DejaVuSans-74"" x=""277.636719""/>
-       <use xlink:href=""#DejaVuSans-68"" x=""316.845703""/>
-       <use xlink:href=""#DejaVuSans-20"" x=""380.224609""/>
-       <use xlink:href=""#DejaVuSans-50"" x=""412.011719""/>
-       <use xlink:href=""#DejaVuSans-72"" x=""470.564453""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""509.427734""/>
-      </g>
-     </g>
-    </g>
-    <g id=""ytick_4"">
-     <g id=""line2d_13"">
-      <g>
-       <use xlink:href=""#me3cc6245ad"" x=""90"" y=""98.791579"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_14"">
-      <!-- Unsloth Max -->
-      <g transform=""translate(21.126562 102.590798) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-4d"" d=""M 628 4666 
-L 1569 4666 
-L 2759 1491 
-L 3956 4666 
-L 4897 4666 
-L 4897 0 
-L 4281 0 
-L 4281 4097 
-L 3078 897 
-L 2444 897 
-L 1241 4097 
-L 1241 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-78"" d=""M 3513 3500 
-L 2247 1797 
-L 3578 0 
-L 2900 0 
-L 1881 1375 
-L 863 0 
-L 184 0 
-L 1544 1831 
-L 300 3500 
-L 978 3500 
-L 1906 2253 
-L 2834 3500 
-L 3513 3500 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-55""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""73.193359""/>
-       <use xlink:href=""#DejaVuSans-73"" x=""136.572266""/>
-       <use xlink:href=""#DejaVuSans-6c"" x=""188.671875""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""216.455078""/>
-       <use xlink:href=""#DejaVuSans-74"" x=""277.636719""/>
-       <use xlink:href=""#DejaVuSans-68"" x=""316.845703""/>
-       <use xlink:href=""#DejaVuSans-20"" x=""380.224609""/>
-       <use xlink:href=""#DejaVuSans-4d"" x=""412.011719""/>
-       <use xlink:href=""#DejaVuSans-61"" x=""498.291016""/>
-       <use xlink:href=""#DejaVuSans-78"" x=""559.570312""/>
-      </g>
-     </g>
-    </g>
-   </g>
-   <g id=""patch_7"">
-    <path d=""M 90 384.48 
-L 90 51.84 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""patch_8"">
-    <path d=""M 648 384.48 
-L 648 51.84 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""patch_9"">
-    <path d=""M 90 384.48 
-L 648 384.48 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""patch_10"">
-    <path d=""M 90 51.84 
-L 648 51.84 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""text_15"">
-    <!-- 163.80 hrs (1.0X) -->
-    <g transform=""translate(491.653585 340.839671) scale(0.12 -0.12)"">
-     <defs>
-      <path id=""DejaVuSans-33"" d=""M 2597 2516 
-Q 3050 2419 3304 2112 
-Q 3559 1806 3559 1356 
-Q 3559 666 3084 287 
-Q 2609 -91 1734 -91 
-Q 1441 -91 1130 -33 
-Q 819 25 488 141 
-L 488 750 
-Q 750 597 1062 519 
-Q 1375 441 1716 441 
-Q 2309 441 2620 675 
-Q 2931 909 2931 1356 
-Q 2931 1769 2642 2001 
-Q 2353 2234 1838 2234 
-L 1294 2234 
-L 1294 2753 
-L 1863 2753 
-Q 2328 2753 2575 2939 
-Q 2822 3125 2822 3475 
-Q 2822 3834 2567 4026 
-Q 2313 4219 1838 4219 
-Q 1578 4219 1281 4162 
-Q 984 4106 628 3988 
-L 628 4550 
-Q 988 4650 1302 4700 
-Q 1616 4750 1894 4750 
-Q 2613 4750 3031 4423 
-Q 3450 4097 3450 3541 
-Q 3450 3153 3228 2886 
-Q 3006 2619 2597 2516 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-58"" d=""M 403 4666 
-L 1081 4666 
-L 2241 2931 
-L 3406 4666 
-L 4084 4666 
-L 2584 2425 
-L 4184 0 
-L 3506 0 
-L 2194 1984 
-L 872 0 
-L 191 0 
-L 1856 2491 
-L 403 4666 
-z
-"" transform=""scale(0.015625)""/>
-     </defs>
-     <use xlink:href=""#DejaVuSans-31""/>
-     <use xlink:href=""#DejaVuSans-36"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-33"" x=""127.246094""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""190.869141""/>
-     <use xlink:href=""#DejaVuSans-38"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""286.279297""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""349.902344""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""381.689453""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""445.068359""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""486.181641""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""538.28125""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""570.068359""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""672.705078""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""768.115234""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""836.621094""/>
-    </g>
-   </g>
-   <g id=""text_16"">
-    <!-- 34.57 hrs (4.7X) -->
-    <g transform=""translate(205.391593 261.260724) scale(0.12 -0.12)"">
-     <defs>
-      <path id=""DejaVuSans-35"" d=""M 691 4666 
-L 3169 4666 
-L 3169 4134 
-L 1269 4134 
-L 1269 2991 
-Q 1406 3038 1543 3061 
-Q 1681 3084 1819 3084 
-Q 2600 3084 3056 2656 
-Q 3513 2228 3513 1497 
-Q 3513 744 3044 326 
-Q 2575 -91 1722 -91 
-Q 1428 -91 1123 -41 
-Q 819 9 494 109 
-L 494 744 
-Q 775 591 1075 516 
-Q 1375 441 1709 441 
-Q 2250 441 2565 725 
-Q 2881 1009 2881 1497 
-Q 2881 1984 2565 2268 
-Q 2250 2553 1709 2553 
-Q 1456 2553 1204 2497 
-Q 953 2441 691 2322 
-L 691 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-37"" d=""M 525 4666 
-L 3525 4666 
-L 3525 4397 
-L 1831 0 
-L 1172 0 
-L 2766 4134 
-L 525 4134 
-L 525 4666 
-z
-"" transform=""scale(0.015625)""/>
-     </defs>
-     <use xlink:href=""#DejaVuSans-33""/>
-     <use xlink:href=""#DejaVuSans-34"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""127.246094""/>
-     <use xlink:href=""#DejaVuSans-35"" x=""159.033203""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""286.279297""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""318.066406""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""381.445312""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""422.558594""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""474.658203""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""506.445312""/>
-     <use xlink:href=""#DejaVuSans-34"" x=""545.458984""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""640.869141""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""772.998047""/>
-    </g>
-   </g>
-   <g id=""text_17"">
-    <!-- 5.72 hrs (28.7X) -->
-    <g transform=""translate(111.791383 181.681776) scale(0.12 -0.12)"">
-     <use xlink:href=""#DejaVuSans-35""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""95.410156""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""159.033203""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""254.443359""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""317.822266""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""358.935547""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""411.035156""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""442.822266""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""481.835938""/>
-     <use xlink:href=""#DejaVuSans-38"" x=""545.458984""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""640.869141""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""772.998047""/>
-    </g>
-   </g>
-   <g id=""text_18"">
-    <!-- 5.23 hrs (31.3X) -->
-    <g transform=""translate(110.223269 102.102829) scale(0.12 -0.12)"">
-     <use xlink:href=""#DejaVuSans-35""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""95.410156""/>
-     <use xlink:href=""#DejaVuSans-33"" x=""159.033203""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""254.443359""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""317.822266""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""358.935547""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""411.035156""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""442.822266""/>
-     <use xlink:href=""#DejaVuSans-33"" x=""481.835938""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""545.458984""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-33"" x=""640.869141""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""772.998047""/>
-    </g>
-   </g>
-   <g id=""text_19"">
-    <!-- LAION Chip2 210K (1 epoch on 2 T4 GPUs DDP) -->
-    <g transform=""translate(178.88375 45.84) scale(0.16 -0.16)"">
-     <defs>
-      <path id=""DejaVuSans-4c"" d=""M 628 4666 
-L 1259 4666 
-L 1259 531 
-L 3531 531 
-L 3531 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-41"" d=""M 2188 4044 
-L 1331 1722 
-L 3047 1722 
-L 2188 4044 
-z
-M 1831 4666 
-L 2547 4666 
-L 4325 0 
-L 3669 0 
-L 3244 1197 
-L 1141 1197 
-L 716 0 
-L 50 0 
-L 1831 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-49"" d=""M 628 4666 
-L 1259 4666 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-4e"" d=""M 628 4666 
-L 1478 4666 
-L 3547 763 
-L 3547 4666 
-L 4159 4666 
-L 4159 0 
-L 3309 0 
-L 1241 3903 
-L 1241 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-43"" d=""M 4122 4306 
-L 4122 3641 
-Q 3803 3938 3442 4084 
-Q 3081 4231 2675 4231 
-Q 1875 4231 1450 3742 
-Q 1025 3253 1025 2328 
-Q 1025 1406 1450 917 
-Q 1875 428 2675 428 
-Q 3081 428 3442 575 
-Q 3803 722 4122 1019 
-L 4122 359 
-Q 3791 134 3420 21 
-Q 3050 -91 2638 -91 
-Q 1578 -91 968 557 
-Q 359 1206 359 2328 
-Q 359 3453 968 4101 
-Q 1578 4750 2638 4750 
-Q 3056 4750 3426 4639 
-Q 3797 4528 4122 4306 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-4b"" d=""M 628 4666 
-L 1259 4666 
-L 1259 2694 
-L 3353 4666 
-L 4166 4666 
-L 1850 2491 
-L 4331 0 
-L 3500 0 
-L 1259 2247 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-44"" d=""M 1259 4147 
-L 1259 519 
-L 2022 519 
-Q 2988 519 3436 956 
-Q 3884 1394 3884 2338 
-Q 3884 3275 3436 3711 
-Q 2988 4147 2022 4147 
-L 1259 4147 
-z
-M 628 4666 
-L 1925 4666 
-Q 3281 4666 3915 4102 
-Q 4550 3538 4550 2338 
-Q 4550 1131 3912 565 
-Q 3275 0 1925 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-     </defs>
-     <use xlink:href=""#DejaVuSans-4c""/>
-     <use xlink:href=""#DejaVuSans-41"" x=""57.962891""/>
-     <use xlink:href=""#DejaVuSans-49"" x=""126.371094""/>
-     <use xlink:href=""#DejaVuSans-4f"" x=""155.863281""/>
-     <use xlink:href=""#DejaVuSans-4e"" x=""234.574219""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""309.378906""/>
-     <use xlink:href=""#DejaVuSans-43"" x=""341.166016""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""410.990234""/>
-     <use xlink:href=""#DejaVuSans-69"" x=""474.369141""/>
-     <use xlink:href=""#DejaVuSans-70"" x=""502.152344""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""565.628906""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""629.251953""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""661.039062""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""724.662109""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""788.285156""/>
-     <use xlink:href=""#DejaVuSans-4b"" x=""851.908203""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""917.484375""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""949.271484""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""988.285156""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1051.908203""/>
-     <use xlink:href=""#DejaVuSans-65"" x=""1083.695312""/>
-     <use xlink:href=""#DejaVuSans-70"" x=""1145.21875""/>
-     <use xlink:href=""#DejaVuSans-6f"" x=""1208.695312""/>
-     <use xlink:href=""#DejaVuSans-63"" x=""1269.876953""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""1324.857422""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1388.236328""/>
-     <use xlink:href=""#DejaVuSans-6f"" x=""1420.023438""/>
-     <use xlink:href=""#DejaVuSans-6e"" x=""1481.205078""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1544.583984""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""1576.371094""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1639.994141""/>
-     <use xlink:href=""#DejaVuSans-54"" x=""1671.78125""/>
-     <use xlink:href=""#DejaVuSans-34"" x=""1732.865234""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1796.488281""/>
-     <use xlink:href=""#DejaVuSans-47"" x=""1828.275391""/>
-     <use xlink:href=""#DejaVuSans-50"" x=""1905.765625""/>
-     <use xlink:href=""#DejaVuSans-55"" x=""1966.068359""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""2039.261719""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""2091.361328""/>
-     <use xlink:href=""#DejaVuSans-44"" x=""2123.148438""/>
-     <use xlink:href=""#DejaVuSans-44"" x=""2200.150391""/>
-     <use xlink:href=""#DejaVuSans-50"" x=""2277.152344""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""2337.455078""/>
-    </g>
-   </g>
-  </g>
- </g>
- <defs>
-  <clipPath id=""p42f2634aae"">
-   <rect x=""90"" y=""51.84"" width=""558"" height=""332.64""/>
-  </clipPath>
- </defs>
-</svg>
diff --git a/images/SlimOrca 1GPU.svg b/images/SlimOrca 1GPU.svg
deleted file mode 100644
index a7861f7..0000000
--- a/images/SlimOrca 1GPU.svg	
+++ /dev/null
@@ -1,1424 +0,0 @@
-<?xml version=""1.0"" encoding=""utf-8"" standalone=""no""?>
-<!DOCTYPE svg PUBLIC ""-//W3C//DTD SVG 1.1//EN""
-  ""http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"">
-<svg xmlns:xlink=""http://www.w3.org/1999/xlink"" width=""720pt"" height=""432pt"" viewBox=""0 0 720 432"" xmlns=""http://www.w3.org/2000/svg"" version=""1.1"">
- <metadata>
-  <rdf:RDF xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:cc=""http://creativecommons.org/ns#"" xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"">
-   <cc:Work>
-    <dc:type rdf:resource=""http://purl.org/dc/dcmitype/StillImage""/>
-    <dc:date>2023-11-30T13:15:28.212061</dc:date>
-    <dc:format>image/svg+xml</dc:format>
-    <dc:creator>
-     <cc:Agent>
-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>
-     </cc:Agent>
-    </dc:creator>
-   </cc:Work>
-  </rdf:RDF>
- </metadata>
- <defs>
-  <style type=""text/css"">*{stroke-linejoin: round; stroke-linecap: butt}</style>
- </defs>
- <g id=""figure_1"">
-  <g id=""patch_1"">
-   <path d=""M 0 432 
-L 720 432 
-L 720 0 
-L 0 0 
-L 0 432 
-z
-"" style=""fill: none""/>
-  </g>
-  <g id=""axes_1"">
-   <g id=""patch_2"">
-    <path d=""M 90 384.48 
-L 648 384.48 
-L 648 51.84 
-L 90 51.84 
-L 90 384.48 
-z
-"" style=""fill: none""/>
-   </g>
-   <g id=""patch_3"">
-    <path d=""M 90 369.36 
-L 621.428571 369.36 
-L 621.428571 305.696842 
-L 90 305.696842 
-z
-"" clip-path=""url(#p682a0e8bed)"" style=""fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""patch_4"">
-    <path d=""M 90 289.781053 
-L 415.717933 289.781053 
-L 415.717933 226.117895 
-L 90 226.117895 
-z
-"" clip-path=""url(#p682a0e8bed)"" style=""fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""patch_5"">
-    <path d=""M 90 210.202105 
-L 287.004626 210.202105 
-L 287.004626 146.538947 
-L 90 146.538947 
-z
-"" clip-path=""url(#p682a0e8bed)"" style=""fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""patch_6"">
-    <path d=""M 90 130.623158 
-L 159.87423 130.623158 
-L 159.87423 66.96 
-L 90 66.96 
-z
-"" clip-path=""url(#p682a0e8bed)"" style=""fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter""/>
-   </g>
-   <g id=""matplotlib.axis_1"">
-    <g id=""xtick_1"">
-     <g id=""line2d_1"">
-      <defs>
-       <path id=""m7e152711a6"" d=""M 0 0 
-L 0 3.5 
-"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </defs>
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""90"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_1"">
-      <!-- 0 -->
-      <g transform=""translate(86.81875 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-30"" d=""M 2034 4250 
-Q 1547 4250 1301 3770 
-Q 1056 3291 1056 2328 
-Q 1056 1369 1301 889 
-Q 1547 409 2034 409 
-Q 2525 409 2770 889 
-Q 3016 1369 3016 2328 
-Q 3016 3291 2770 3770 
-Q 2525 4250 2034 4250 
-z
-M 2034 4750 
-Q 2819 4750 3233 4129 
-Q 3647 3509 3647 2328 
-Q 3647 1150 3233 529 
-Q 2819 -91 2034 -91 
-Q 1250 -91 836 529 
-Q 422 1150 422 2328 
-Q 422 3509 836 4129 
-Q 1250 4750 2034 4750 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-30""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_2"">
-     <g id=""line2d_2"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""157.839059"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_2"">
-      <!-- 50 -->
-      <g transform=""translate(151.476559 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-35"" d=""M 691 4666 
-L 3169 4666 
-L 3169 4134 
-L 1269 4134 
-L 1269 2991 
-Q 1406 3038 1543 3061 
-Q 1681 3084 1819 3084 
-Q 2600 3084 3056 2656 
-Q 3513 2228 3513 1497 
-Q 3513 744 3044 326 
-Q 2575 -91 1722 -91 
-Q 1428 -91 1123 -41 
-Q 819 9 494 109 
-L 494 744 
-Q 775 591 1075 516 
-Q 1375 441 1709 441 
-Q 2250 441 2565 725 
-Q 2881 1009 2881 1497 
-Q 2881 1984 2565 2268 
-Q 2250 2553 1709 2553 
-Q 1456 2553 1204 2497 
-Q 953 2441 691 2322 
-L 691 4666 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-35""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_3"">
-     <g id=""line2d_3"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""225.678117"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_3"">
-      <!-- 100 -->
-      <g transform=""translate(216.134367 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-31"" d=""M 794 531 
-L 1825 531 
-L 1825 4091 
-L 703 3866 
-L 703 4441 
-L 1819 4666 
-L 2450 4666 
-L 2450 531 
-L 3481 531 
-L 3481 0 
-L 794 0 
-L 794 531 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-31""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_4"">
-     <g id=""line2d_4"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""293.517176"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_4"">
-      <!-- 150 -->
-      <g transform=""translate(283.973426 399.078438) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-31""/>
-       <use xlink:href=""#DejaVuSans-35"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_5"">
-     <g id=""line2d_5"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""361.356234"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_5"">
-      <!-- 200 -->
-      <g transform=""translate(351.812484 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-32"" d=""M 1228 531 
-L 3431 531 
-L 3431 0 
-L 469 0 
-L 469 531 
-Q 828 903 1448 1529 
-Q 2069 2156 2228 2338 
-Q 2531 2678 2651 2914 
-Q 2772 3150 2772 3378 
-Q 2772 3750 2511 3984 
-Q 2250 4219 1831 4219 
-Q 1534 4219 1204 4116 
-Q 875 4013 500 3803 
-L 500 4441 
-Q 881 4594 1212 4672 
-Q 1544 4750 1819 4750 
-Q 2544 4750 2975 4387 
-Q 3406 4025 3406 3419 
-Q 3406 3131 3298 2873 
-Q 3191 2616 2906 2266 
-Q 2828 2175 2409 1742 
-Q 1991 1309 1228 531 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-32""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_6"">
-     <g id=""line2d_6"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""429.195293"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_6"">
-      <!-- 250 -->
-      <g transform=""translate(419.651543 399.078438) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-32""/>
-       <use xlink:href=""#DejaVuSans-35"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_7"">
-     <g id=""line2d_7"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""497.034351"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_7"">
-      <!-- 300 -->
-      <g transform=""translate(487.490601 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-33"" d=""M 2597 2516 
-Q 3050 2419 3304 2112 
-Q 3559 1806 3559 1356 
-Q 3559 666 3084 287 
-Q 2609 -91 1734 -91 
-Q 1441 -91 1130 -33 
-Q 819 25 488 141 
-L 488 750 
-Q 750 597 1062 519 
-Q 1375 441 1716 441 
-Q 2309 441 2620 675 
-Q 2931 909 2931 1356 
-Q 2931 1769 2642 2001 
-Q 2353 2234 1838 2234 
-L 1294 2234 
-L 1294 2753 
-L 1863 2753 
-Q 2328 2753 2575 2939 
-Q 2822 3125 2822 3475 
-Q 2822 3834 2567 4026 
-Q 2313 4219 1838 4219 
-Q 1578 4219 1281 4162 
-Q 984 4106 628 3988 
-L 628 4550 
-Q 988 4650 1302 4700 
-Q 1616 4750 1894 4750 
-Q 2613 4750 3031 4423 
-Q 3450 4097 3450 3541 
-Q 3450 3153 3228 2886 
-Q 3006 2619 2597 2516 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-33""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_8"">
-     <g id=""line2d_8"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""564.87341"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_8"">
-      <!-- 350 -->
-      <g transform=""translate(555.32966 399.078438) scale(0.1 -0.1)"">
-       <use xlink:href=""#DejaVuSans-33""/>
-       <use xlink:href=""#DejaVuSans-35"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""xtick_9"">
-     <g id=""line2d_9"">
-      <g>
-       <use xlink:href=""#m7e152711a6"" x=""632.712468"" y=""384.48"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_9"">
-      <!-- 400 -->
-      <g transform=""translate(623.168718 399.078438) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-34"" d=""M 2419 4116 
-L 825 1625 
-L 2419 1625 
-L 2419 4116 
-z
-M 2253 4666 
-L 3047 4666 
-L 3047 1625 
-L 3713 1625 
-L 3713 1100 
-L 3047 1100 
-L 3047 0 
-L 2419 0 
-L 2419 1100 
-L 313 1100 
-L 313 1709 
-L 2253 4666 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-34""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""63.623047""/>
-       <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-      </g>
-     </g>
-    </g>
-    <g id=""text_10"">
-     <!-- Time taken (lower is better). -->
-     <g transform=""translate(283.763438 414.27625) scale(0.12 -0.12)"">
-      <defs>
-       <path id=""DejaVuSans-54"" d=""M -19 4666 
-L 3928 4666 
-L 3928 4134 
-L 2272 4134 
-L 2272 0 
-L 1638 0 
-L 1638 4134 
-L -19 4134 
-L -19 4666 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-69"" d=""M 603 3500 
-L 1178 3500 
-L 1178 0 
-L 603 0 
-L 603 3500 
-z
-M 603 4863 
-L 1178 4863 
-L 1178 4134 
-L 603 4134 
-L 603 4863 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6d"" d=""M 3328 2828 
-Q 3544 3216 3844 3400 
-Q 4144 3584 4550 3584 
-Q 5097 3584 5394 3201 
-Q 5691 2819 5691 2113 
-L 5691 0 
-L 5113 0 
-L 5113 2094 
-Q 5113 2597 4934 2840 
-Q 4756 3084 4391 3084 
-Q 3944 3084 3684 2787 
-Q 3425 2491 3425 1978 
-L 3425 0 
-L 2847 0 
-L 2847 2094 
-Q 2847 2600 2669 2842 
-Q 2491 3084 2119 3084 
-Q 1678 3084 1418 2786 
-Q 1159 2488 1159 1978 
-L 1159 0 
-L 581 0 
-L 581 3500 
-L 1159 3500 
-L 1159 2956 
-Q 1356 3278 1631 3431 
-Q 1906 3584 2284 3584 
-Q 2666 3584 2933 3390 
-Q 3200 3197 3328 2828 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-65"" d=""M 3597 1894 
-L 3597 1613 
-L 953 1613 
-Q 991 1019 1311 708 
-Q 1631 397 2203 397 
-Q 2534 397 2845 478 
-Q 3156 559 3463 722 
-L 3463 178 
-Q 3153 47 2828 -22 
-Q 2503 -91 2169 -91 
-Q 1331 -91 842 396 
-Q 353 884 353 1716 
-Q 353 2575 817 3079 
-Q 1281 3584 2069 3584 
-Q 2775 3584 3186 3129 
-Q 3597 2675 3597 1894 
-z
-M 3022 2063 
-Q 3016 2534 2758 2815 
-Q 2500 3097 2075 3097 
-Q 1594 3097 1305 2825 
-Q 1016 2553 972 2059 
-L 3022 2063 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-20"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-74"" d=""M 1172 4494 
-L 1172 3500 
-L 2356 3500 
-L 2356 3053 
-L 1172 3053 
-L 1172 1153 
-Q 1172 725 1289 603 
-Q 1406 481 1766 481 
-L 2356 481 
-L 2356 0 
-L 1766 0 
-Q 1100 0 847 248 
-Q 594 497 594 1153 
-L 594 3053 
-L 172 3053 
-L 172 3500 
-L 594 3500 
-L 594 4494 
-L 1172 4494 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-61"" d=""M 2194 1759 
-Q 1497 1759 1228 1600 
-Q 959 1441 959 1056 
-Q 959 750 1161 570 
-Q 1363 391 1709 391 
-Q 2188 391 2477 730 
-Q 2766 1069 2766 1631 
-L 2766 1759 
-L 2194 1759 
-z
-M 3341 1997 
-L 3341 0 
-L 2766 0 
-L 2766 531 
-Q 2569 213 2275 61 
-Q 1981 -91 1556 -91 
-Q 1019 -91 701 211 
-Q 384 513 384 1019 
-Q 384 1609 779 1909 
-Q 1175 2209 1959 2209 
-L 2766 2209 
-L 2766 2266 
-Q 2766 2663 2505 2880 
-Q 2244 3097 1772 3097 
-Q 1472 3097 1187 3025 
-Q 903 2953 641 2809 
-L 641 3341 
-Q 956 3463 1253 3523 
-Q 1550 3584 1831 3584 
-Q 2591 3584 2966 3190 
-Q 3341 2797 3341 1997 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6b"" d=""M 581 4863 
-L 1159 4863 
-L 1159 1991 
-L 2875 3500 
-L 3609 3500 
-L 1753 1863 
-L 3688 0 
-L 2938 0 
-L 1159 1709 
-L 1159 0 
-L 581 0 
-L 581 4863 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6e"" d=""M 3513 2113 
-L 3513 0 
-L 2938 0 
-L 2938 2094 
-Q 2938 2591 2744 2837 
-Q 2550 3084 2163 3084 
-Q 1697 3084 1428 2787 
-Q 1159 2491 1159 1978 
-L 1159 0 
-L 581 0 
-L 581 3500 
-L 1159 3500 
-L 1159 2956 
-Q 1366 3272 1645 3428 
-Q 1925 3584 2291 3584 
-Q 2894 3584 3203 3211 
-Q 3513 2838 3513 2113 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-28"" d=""M 1984 4856 
-Q 1566 4138 1362 3434 
-Q 1159 2731 1159 2009 
-Q 1159 1288 1364 580 
-Q 1569 -128 1984 -844 
-L 1484 -844 
-Q 1016 -109 783 600 
-Q 550 1309 550 2009 
-Q 550 2706 781 3412 
-Q 1013 4119 1484 4856 
-L 1984 4856 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6c"" d=""M 603 4863 
-L 1178 4863 
-L 1178 0 
-L 603 0 
-L 603 4863 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-6f"" d=""M 1959 3097 
-Q 1497 3097 1228 2736 
-Q 959 2375 959 1747 
-Q 959 1119 1226 758 
-Q 1494 397 1959 397 
-Q 2419 397 2687 759 
-Q 2956 1122 2956 1747 
-Q 2956 2369 2687 2733 
-Q 2419 3097 1959 3097 
-z
-M 1959 3584 
-Q 2709 3584 3137 3096 
-Q 3566 2609 3566 1747 
-Q 3566 888 3137 398 
-Q 2709 -91 1959 -91 
-Q 1206 -91 779 398 
-Q 353 888 353 1747 
-Q 353 2609 779 3096 
-Q 1206 3584 1959 3584 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-77"" d=""M 269 3500 
-L 844 3500 
-L 1563 769 
-L 2278 3500 
-L 2956 3500 
-L 3675 769 
-L 4391 3500 
-L 4966 3500 
-L 4050 0 
-L 3372 0 
-L 2619 2869 
-L 1863 0 
-L 1184 0 
-L 269 3500 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-72"" d=""M 2631 2963 
-Q 2534 3019 2420 3045 
-Q 2306 3072 2169 3072 
-Q 1681 3072 1420 2755 
-Q 1159 2438 1159 1844 
-L 1159 0 
-L 581 0 
-L 581 3500 
-L 1159 3500 
-L 1159 2956 
-Q 1341 3275 1631 3429 
-Q 1922 3584 2338 3584 
-Q 2397 3584 2469 3576 
-Q 2541 3569 2628 3553 
-L 2631 2963 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-73"" d=""M 2834 3397 
-L 2834 2853 
-Q 2591 2978 2328 3040 
-Q 2066 3103 1784 3103 
-Q 1356 3103 1142 2972 
-Q 928 2841 928 2578 
-Q 928 2378 1081 2264 
-Q 1234 2150 1697 2047 
-L 1894 2003 
-Q 2506 1872 2764 1633 
-Q 3022 1394 3022 966 
-Q 3022 478 2636 193 
-Q 2250 -91 1575 -91 
-Q 1294 -91 989 -36 
-Q 684 19 347 128 
-L 347 722 
-Q 666 556 975 473 
-Q 1284 391 1588 391 
-Q 1994 391 2212 530 
-Q 2431 669 2431 922 
-Q 2431 1156 2273 1281 
-Q 2116 1406 1581 1522 
-L 1381 1569 
-Q 847 1681 609 1914 
-Q 372 2147 372 2553 
-Q 372 3047 722 3315 
-Q 1072 3584 1716 3584 
-Q 2034 3584 2315 3537 
-Q 2597 3491 2834 3397 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-62"" d=""M 3116 1747 
-Q 3116 2381 2855 2742 
-Q 2594 3103 2138 3103 
-Q 1681 3103 1420 2742 
-Q 1159 2381 1159 1747 
-Q 1159 1113 1420 752 
-Q 1681 391 2138 391 
-Q 2594 391 2855 752 
-Q 3116 1113 3116 1747 
-z
-M 1159 2969 
-Q 1341 3281 1617 3432 
-Q 1894 3584 2278 3584 
-Q 2916 3584 3314 3078 
-Q 3713 2572 3713 1747 
-Q 3713 922 3314 415 
-Q 2916 -91 2278 -91 
-Q 1894 -91 1617 61 
-Q 1341 213 1159 525 
-L 1159 0 
-L 581 0 
-L 581 4863 
-L 1159 4863 
-L 1159 2969 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-29"" d=""M 513 4856 
-L 1013 4856 
-Q 1481 4119 1714 3412 
-Q 1947 2706 1947 2009 
-Q 1947 1309 1714 600 
-Q 1481 -109 1013 -844 
-L 513 -844 
-Q 928 -128 1133 580 
-Q 1338 1288 1338 2009 
-Q 1338 2731 1133 3434 
-Q 928 4138 513 4856 
-z
-"" transform=""scale(0.015625)""/>
-       <path id=""DejaVuSans-2e"" d=""M 684 794 
-L 1344 794 
-L 1344 0 
-L 684 0 
-L 684 794 
-z
-"" transform=""scale(0.015625)""/>
-      </defs>
-      <use xlink:href=""#DejaVuSans-54""/>
-      <use xlink:href=""#DejaVuSans-69"" x=""57.958984""/>
-      <use xlink:href=""#DejaVuSans-6d"" x=""85.742188""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""183.154297""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""244.677734""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""276.464844""/>
-      <use xlink:href=""#DejaVuSans-61"" x=""315.673828""/>
-      <use xlink:href=""#DejaVuSans-6b"" x=""376.953125""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""431.238281""/>
-      <use xlink:href=""#DejaVuSans-6e"" x=""492.761719""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""556.140625""/>
-      <use xlink:href=""#DejaVuSans-28"" x=""587.927734""/>
-      <use xlink:href=""#DejaVuSans-6c"" x=""626.941406""/>
-      <use xlink:href=""#DejaVuSans-6f"" x=""654.724609""/>
-      <use xlink:href=""#DejaVuSans-77"" x=""715.90625""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""797.693359""/>
-      <use xlink:href=""#DejaVuSans-72"" x=""859.216797""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""900.330078""/>
-      <use xlink:href=""#DejaVuSans-69"" x=""932.117188""/>
-      <use xlink:href=""#DejaVuSans-73"" x=""959.900391""/>
-      <use xlink:href=""#DejaVuSans-20"" x=""1012""/>
-      <use xlink:href=""#DejaVuSans-62"" x=""1043.787109""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""1107.263672""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""1168.787109""/>
-      <use xlink:href=""#DejaVuSans-74"" x=""1207.996094""/>
-      <use xlink:href=""#DejaVuSans-65"" x=""1247.205078""/>
-      <use xlink:href=""#DejaVuSans-72"" x=""1308.728516""/>
-      <use xlink:href=""#DejaVuSans-29"" x=""1349.841797""/>
-      <use xlink:href=""#DejaVuSans-2e"" x=""1388.855469""/>
-     </g>
-    </g>
-   </g>
-   <g id=""matplotlib.axis_2"">
-    <g id=""ytick_1"">
-     <g id=""line2d_10"">
-      <defs>
-       <path id=""m45234ecef3"" d=""M 0 0 
-L -3.5 0 
-"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </defs>
-      <g>
-       <use xlink:href=""#m45234ecef3"" x=""90"" y=""337.528421"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_11"">
-      <!-- Huggingface -->
-      <g transform=""translate(19.68125 341.32764) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-48"" d=""M 628 4666 
-L 1259 4666 
-L 1259 2753 
-L 3553 2753 
-L 3553 4666 
-L 4184 4666 
-L 4184 0 
-L 3553 0 
-L 3553 2222 
-L 1259 2222 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-75"" d=""M 544 1381 
-L 544 3500 
-L 1119 3500 
-L 1119 1403 
-Q 1119 906 1312 657 
-Q 1506 409 1894 409 
-Q 2359 409 2629 706 
-Q 2900 1003 2900 1516 
-L 2900 3500 
-L 3475 3500 
-L 3475 0 
-L 2900 0 
-L 2900 538 
-Q 2691 219 2414 64 
-Q 2138 -91 1772 -91 
-Q 1169 -91 856 284 
-Q 544 659 544 1381 
-z
-M 1991 3584 
-L 1991 3584 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-67"" d=""M 2906 1791 
-Q 2906 2416 2648 2759 
-Q 2391 3103 1925 3103 
-Q 1463 3103 1205 2759 
-Q 947 2416 947 1791 
-Q 947 1169 1205 825 
-Q 1463 481 1925 481 
-Q 2391 481 2648 825 
-Q 2906 1169 2906 1791 
-z
-M 3481 434 
-Q 3481 -459 3084 -895 
-Q 2688 -1331 1869 -1331 
-Q 1566 -1331 1297 -1286 
-Q 1028 -1241 775 -1147 
-L 775 -588 
-Q 1028 -725 1275 -790 
-Q 1522 -856 1778 -856 
-Q 2344 -856 2625 -561 
-Q 2906 -266 2906 331 
-L 2906 616 
-Q 2728 306 2450 153 
-Q 2172 0 1784 0 
-Q 1141 0 747 490 
-Q 353 981 353 1791 
-Q 353 2603 747 3093 
-Q 1141 3584 1784 3584 
-Q 2172 3584 2450 3431 
-Q 2728 3278 2906 2969 
-L 2906 3500 
-L 3481 3500 
-L 3481 434 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-66"" d=""M 2375 4863 
-L 2375 4384 
-L 1825 4384 
-Q 1516 4384 1395 4259 
-Q 1275 4134 1275 3809 
-L 1275 3500 
-L 2222 3500 
-L 2222 3053 
-L 1275 3053 
-L 1275 0 
-L 697 0 
-L 697 3053 
-L 147 3053 
-L 147 3500 
-L 697 3500 
-L 697 3744 
-Q 697 4328 969 4595 
-Q 1241 4863 1831 4863 
-L 2375 4863 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-63"" d=""M 3122 3366 
-L 3122 2828 
-Q 2878 2963 2633 3030 
-Q 2388 3097 2138 3097 
-Q 1578 3097 1268 2742 
-Q 959 2388 959 1747 
-Q 959 1106 1268 751 
-Q 1578 397 2138 397 
-Q 2388 397 2633 464 
-Q 2878 531 3122 666 
-L 3122 134 
-Q 2881 22 2623 -34 
-Q 2366 -91 2075 -91 
-Q 1284 -91 818 406 
-Q 353 903 353 1747 
-Q 353 2603 823 3093 
-Q 1294 3584 2113 3584 
-Q 2378 3584 2631 3529 
-Q 2884 3475 3122 3366 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-48""/>
-       <use xlink:href=""#DejaVuSans-75"" x=""75.195312""/>
-       <use xlink:href=""#DejaVuSans-67"" x=""138.574219""/>
-       <use xlink:href=""#DejaVuSans-67"" x=""202.050781""/>
-       <use xlink:href=""#DejaVuSans-69"" x=""265.527344""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""293.310547""/>
-       <use xlink:href=""#DejaVuSans-67"" x=""356.689453""/>
-       <use xlink:href=""#DejaVuSans-66"" x=""420.166016""/>
-       <use xlink:href=""#DejaVuSans-61"" x=""455.371094""/>
-       <use xlink:href=""#DejaVuSans-63"" x=""516.650391""/>
-       <use xlink:href=""#DejaVuSans-65"" x=""571.630859""/>
-      </g>
-     </g>
-    </g>
-    <g id=""ytick_2"">
-     <g id=""line2d_11"">
-      <g>
-       <use xlink:href=""#m45234ecef3"" x=""90"" y=""257.949474"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_12"">
-      <!-- Unsloth Open -->
-      <g transform=""translate(15.090625 261.748692) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-55"" d=""M 556 4666 
-L 1191 4666 
-L 1191 1831 
-Q 1191 1081 1462 751 
-Q 1734 422 2344 422 
-Q 2950 422 3222 751 
-Q 3494 1081 3494 1831 
-L 3494 4666 
-L 4128 4666 
-L 4128 1753 
-Q 4128 841 3676 375 
-Q 3225 -91 2344 -91 
-Q 1459 -91 1007 375 
-Q 556 841 556 1753 
-L 556 4666 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-68"" d=""M 3513 2113 
-L 3513 0 
-L 2938 0 
-L 2938 2094 
-Q 2938 2591 2744 2837 
-Q 2550 3084 2163 3084 
-Q 1697 3084 1428 2787 
-Q 1159 2491 1159 1978 
-L 1159 0 
-L 581 0 
-L 581 4863 
-L 1159 4863 
-L 1159 2956 
-Q 1366 3272 1645 3428 
-Q 1925 3584 2291 3584 
-Q 2894 3584 3203 3211 
-Q 3513 2838 3513 2113 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-4f"" d=""M 2522 4238 
-Q 1834 4238 1429 3725 
-Q 1025 3213 1025 2328 
-Q 1025 1447 1429 934 
-Q 1834 422 2522 422 
-Q 3209 422 3611 934 
-Q 4013 1447 4013 2328 
-Q 4013 3213 3611 3725 
-Q 3209 4238 2522 4238 
-z
-M 2522 4750 
-Q 3503 4750 4090 4092 
-Q 4678 3434 4678 2328 
-Q 4678 1225 4090 567 
-Q 3503 -91 2522 -91 
-Q 1538 -91 948 565 
-Q 359 1222 359 2328 
-Q 359 3434 948 4092 
-Q 1538 4750 2522 4750 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-70"" d=""M 1159 525 
-L 1159 -1331 
-L 581 -1331 
-L 581 3500 
-L 1159 3500 
-L 1159 2969 
-Q 1341 3281 1617 3432 
-Q 1894 3584 2278 3584 
-Q 2916 3584 3314 3078 
-Q 3713 2572 3713 1747 
-Q 3713 922 3314 415 
-Q 2916 -91 2278 -91 
-Q 1894 -91 1617 61 
-Q 1341 213 1159 525 
-z
-M 3116 1747 
-Q 3116 2381 2855 2742 
-Q 2594 3103 2138 3103 
-Q 1681 3103 1420 2742 
-Q 1159 2381 1159 1747 
-Q 1159 1113 1420 752 
-Q 1681 391 2138 391 
-Q 2594 391 2855 752 
-Q 3116 1113 3116 1747 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-55""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""73.193359""/>
-       <use xlink:href=""#DejaVuSans-73"" x=""136.572266""/>
-       <use xlink:href=""#DejaVuSans-6c"" x=""188.671875""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""216.455078""/>
-       <use xlink:href=""#DejaVuSans-74"" x=""277.636719""/>
-       <use xlink:href=""#DejaVuSans-68"" x=""316.845703""/>
-       <use xlink:href=""#DejaVuSans-20"" x=""380.224609""/>
-       <use xlink:href=""#DejaVuSans-4f"" x=""412.011719""/>
-       <use xlink:href=""#DejaVuSans-70"" x=""490.722656""/>
-       <use xlink:href=""#DejaVuSans-65"" x=""554.199219""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""615.722656""/>
-      </g>
-     </g>
-    </g>
-    <g id=""ytick_3"">
-     <g id=""line2d_12"">
-      <g>
-       <use xlink:href=""#m45234ecef3"" x=""90"" y=""178.370526"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_13"">
-      <!-- Unsloth Pro -->
-      <g transform=""translate(25.942187 182.169745) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-50"" d=""M 1259 4147 
-L 1259 2394 
-L 2053 2394 
-Q 2494 2394 2734 2622 
-Q 2975 2850 2975 3272 
-Q 2975 3691 2734 3919 
-Q 2494 4147 2053 4147 
-L 1259 4147 
-z
-M 628 4666 
-L 2053 4666 
-Q 2838 4666 3239 4311 
-Q 3641 3956 3641 3272 
-Q 3641 2581 3239 2228 
-Q 2838 1875 2053 1875 
-L 1259 1875 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-55""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""73.193359""/>
-       <use xlink:href=""#DejaVuSans-73"" x=""136.572266""/>
-       <use xlink:href=""#DejaVuSans-6c"" x=""188.671875""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""216.455078""/>
-       <use xlink:href=""#DejaVuSans-74"" x=""277.636719""/>
-       <use xlink:href=""#DejaVuSans-68"" x=""316.845703""/>
-       <use xlink:href=""#DejaVuSans-20"" x=""380.224609""/>
-       <use xlink:href=""#DejaVuSans-50"" x=""412.011719""/>
-       <use xlink:href=""#DejaVuSans-72"" x=""470.564453""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""509.427734""/>
-      </g>
-     </g>
-    </g>
-    <g id=""ytick_4"">
-     <g id=""line2d_13"">
-      <g>
-       <use xlink:href=""#m45234ecef3"" x=""90"" y=""98.791579"" style=""stroke: #000000; stroke-width: 0.8""/>
-      </g>
-     </g>
-     <g id=""text_14"">
-      <!-- Unsloth Max -->
-      <g transform=""translate(21.126562 102.590798) scale(0.1 -0.1)"">
-       <defs>
-        <path id=""DejaVuSans-4d"" d=""M 628 4666 
-L 1569 4666 
-L 2759 1491 
-L 3956 4666 
-L 4897 4666 
-L 4897 0 
-L 4281 0 
-L 4281 4097 
-L 3078 897 
-L 2444 897 
-L 1241 4097 
-L 1241 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-        <path id=""DejaVuSans-78"" d=""M 3513 3500 
-L 2247 1797 
-L 3578 0 
-L 2900 0 
-L 1881 1375 
-L 863 0 
-L 184 0 
-L 1544 1831 
-L 300 3500 
-L 978 3500 
-L 1906 2253 
-L 2834 3500 
-L 3513 3500 
-z
-"" transform=""scale(0.015625)""/>
-       </defs>
-       <use xlink:href=""#DejaVuSans-55""/>
-       <use xlink:href=""#DejaVuSans-6e"" x=""73.193359""/>
-       <use xlink:href=""#DejaVuSans-73"" x=""136.572266""/>
-       <use xlink:href=""#DejaVuSans-6c"" x=""188.671875""/>
-       <use xlink:href=""#DejaVuSans-6f"" x=""216.455078""/>
-       <use xlink:href=""#DejaVuSans-74"" x=""277.636719""/>
-       <use xlink:href=""#DejaVuSans-68"" x=""316.845703""/>
-       <use xlink:href=""#DejaVuSans-20"" x=""380.224609""/>
-       <use xlink:href=""#DejaVuSans-4d"" x=""412.011719""/>
-       <use xlink:href=""#DejaVuSans-61"" x=""498.291016""/>
-       <use xlink:href=""#DejaVuSans-78"" x=""559.570312""/>
-      </g>
-     </g>
-    </g>
-   </g>
-   <g id=""patch_7"">
-    <path d=""M 90 384.48 
-L 90 51.84 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""patch_8"">
-    <path d=""M 648 384.48 
-L 648 51.84 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""patch_9"">
-    <path d=""M 90 384.48 
-L 648 384.48 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""patch_10"">
-    <path d=""M 90 51.84 
-L 648 51.84 
-"" style=""fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square""/>
-   </g>
-   <g id=""text_15"">
-    <!-- 391.68 hrs (1.0X) -->
-    <g transform=""translate(512.886078 340.839671) scale(0.12 -0.12)"">
-     <defs>
-      <path id=""DejaVuSans-39"" d=""M 703 97 
-L 703 672 
-Q 941 559 1184 500 
-Q 1428 441 1663 441 
-Q 2288 441 2617 861 
-Q 2947 1281 2994 2138 
-Q 2813 1869 2534 1725 
-Q 2256 1581 1919 1581 
-Q 1219 1581 811 2004 
-Q 403 2428 403 3163 
-Q 403 3881 828 4315 
-Q 1253 4750 1959 4750 
-Q 2769 4750 3195 4129 
-Q 3622 3509 3622 2328 
-Q 3622 1225 3098 567 
-Q 2575 -91 1691 -91 
-Q 1453 -91 1209 -44 
-Q 966 3 703 97 
-z
-M 1959 2075 
-Q 2384 2075 2632 2365 
-Q 2881 2656 2881 3163 
-Q 2881 3666 2632 3958 
-Q 2384 4250 1959 4250 
-Q 1534 4250 1286 3958 
-Q 1038 3666 1038 3163 
-Q 1038 2656 1286 2365 
-Q 1534 2075 1959 2075 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-36"" d=""M 2113 2584 
-Q 1688 2584 1439 2293 
-Q 1191 2003 1191 1497 
-Q 1191 994 1439 701 
-Q 1688 409 2113 409 
-Q 2538 409 2786 701 
-Q 3034 994 3034 1497 
-Q 3034 2003 2786 2293 
-Q 2538 2584 2113 2584 
-z
-M 3366 4563 
-L 3366 3988 
-Q 3128 4100 2886 4159 
-Q 2644 4219 2406 4219 
-Q 1781 4219 1451 3797 
-Q 1122 3375 1075 2522 
-Q 1259 2794 1537 2939 
-Q 1816 3084 2150 3084 
-Q 2853 3084 3261 2657 
-Q 3669 2231 3669 1497 
-Q 3669 778 3244 343 
-Q 2819 -91 2113 -91 
-Q 1303 -91 875 529 
-Q 447 1150 447 2328 
-Q 447 3434 972 4092 
-Q 1497 4750 2381 4750 
-Q 2619 4750 2861 4703 
-Q 3103 4656 3366 4563 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-38"" d=""M 2034 2216 
-Q 1584 2216 1326 1975 
-Q 1069 1734 1069 1313 
-Q 1069 891 1326 650 
-Q 1584 409 2034 409 
-Q 2484 409 2743 651 
-Q 3003 894 3003 1313 
-Q 3003 1734 2745 1975 
-Q 2488 2216 2034 2216 
-z
-M 1403 2484 
-Q 997 2584 770 2862 
-Q 544 3141 544 3541 
-Q 544 4100 942 4425 
-Q 1341 4750 2034 4750 
-Q 2731 4750 3128 4425 
-Q 3525 4100 3525 3541 
-Q 3525 3141 3298 2862 
-Q 3072 2584 2669 2484 
-Q 3125 2378 3379 2068 
-Q 3634 1759 3634 1313 
-Q 3634 634 3220 271 
-Q 2806 -91 2034 -91 
-Q 1263 -91 848 271 
-Q 434 634 434 1313 
-Q 434 1759 690 2068 
-Q 947 2378 1403 2484 
-z
-M 1172 3481 
-Q 1172 3119 1398 2916 
-Q 1625 2713 2034 2713 
-Q 2441 2713 2670 2916 
-Q 2900 3119 2900 3481 
-Q 2900 3844 2670 4047 
-Q 2441 4250 2034 4250 
-Q 1625 4250 1398 4047 
-Q 1172 3844 1172 3481 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-58"" d=""M 403 4666 
-L 1081 4666 
-L 2241 2931 
-L 3406 4666 
-L 4084 4666 
-L 2584 2425 
-L 4184 0 
-L 3506 0 
-L 2194 1984 
-L 872 0 
-L 191 0 
-L 1856 2491 
-L 403 4666 
-z
-"" transform=""scale(0.015625)""/>
-     </defs>
-     <use xlink:href=""#DejaVuSans-33""/>
-     <use xlink:href=""#DejaVuSans-39"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""127.246094""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""190.869141""/>
-     <use xlink:href=""#DejaVuSans-36"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-38"" x=""286.279297""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""349.902344""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""381.689453""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""445.068359""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""486.181641""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""538.28125""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""570.068359""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""672.705078""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""768.115234""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""836.621094""/>
-    </g>
-   </g>
-   <g id=""text_16"">
-    <!-- 240.07 hrs (1.6X) -->
-    <g transform=""translate(417.074714 261.260724) scale(0.12 -0.12)"">
-     <defs>
-      <path id=""DejaVuSans-37"" d=""M 525 4666 
-L 3525 4666 
-L 3525 4397 
-L 1831 0 
-L 1172 0 
-L 2766 4134 
-L 525 4134 
-L 525 4666 
-z
-"" transform=""scale(0.015625)""/>
-     </defs>
-     <use xlink:href=""#DejaVuSans-32""/>
-     <use xlink:href=""#DejaVuSans-34"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""127.246094""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""190.869141""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""286.279297""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""349.902344""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""381.689453""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""445.068359""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""486.181641""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""538.28125""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""570.068359""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""672.705078""/>
-     <use xlink:href=""#DejaVuSans-36"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""768.115234""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""836.621094""/>
-    </g>
-   </g>
-   <g id=""text_17"">
-    <!-- 145.20 hrs (2.7X) -->
-    <g transform=""translate(288.361407 181.681776) scale(0.12 -0.12)"">
-     <use xlink:href=""#DejaVuSans-31""/>
-     <use xlink:href=""#DejaVuSans-34"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-35"" x=""127.246094""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""190.869141""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""286.279297""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""349.902344""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""381.689453""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""445.068359""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""486.181641""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""538.28125""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""570.068359""/>
-     <use xlink:href=""#DejaVuSans-32"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""672.705078""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""768.115234""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""836.621094""/>
-    </g>
-   </g>
-   <g id=""text_18"">
-    <!-- 51.50 hrs (7.6X) -->
-    <g transform=""translate(161.231011 102.102829) scale(0.12 -0.12)"">
-     <use xlink:href=""#DejaVuSans-35""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""63.623047""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""127.246094""/>
-     <use xlink:href=""#DejaVuSans-35"" x=""159.033203""/>
-     <use xlink:href=""#DejaVuSans-30"" x=""222.65625""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""286.279297""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""318.066406""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""381.445312""/>
-     <use xlink:href=""#DejaVuSans-73"" x=""422.558594""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""474.658203""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""506.445312""/>
-     <use xlink:href=""#DejaVuSans-37"" x=""545.458984""/>
-     <use xlink:href=""#DejaVuSans-2e"" x=""609.082031""/>
-     <use xlink:href=""#DejaVuSans-36"" x=""640.869141""/>
-     <use xlink:href=""#DejaVuSans-58"" x=""704.492188""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""772.998047""/>
-    </g>
-   </g>
-   <g id=""text_19"">
-    <!-- SlimOrca 518K (1 epoch on 1 T4 GPU) -->
-    <g transform=""translate(217.05625 45.84) scale(0.16 -0.16)"">
-     <defs>
-      <path id=""DejaVuSans-53"" d=""M 3425 4513 
-L 3425 3897 
-Q 3066 4069 2747 4153 
-Q 2428 4238 2131 4238 
-Q 1616 4238 1336 4038 
-Q 1056 3838 1056 3469 
-Q 1056 3159 1242 3001 
-Q 1428 2844 1947 2747 
-L 2328 2669 
-Q 3034 2534 3370 2195 
-Q 3706 1856 3706 1288 
-Q 3706 609 3251 259 
-Q 2797 -91 1919 -91 
-Q 1588 -91 1214 -16 
-Q 841 59 441 206 
-L 441 856 
-Q 825 641 1194 531 
-Q 1563 422 1919 422 
-Q 2459 422 2753 634 
-Q 3047 847 3047 1241 
-Q 3047 1584 2836 1778 
-Q 2625 1972 2144 2069 
-L 1759 2144 
-Q 1053 2284 737 2584 
-Q 422 2884 422 3419 
-Q 422 4038 858 4394 
-Q 1294 4750 2059 4750 
-Q 2388 4750 2728 4690 
-Q 3069 4631 3425 4513 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-4b"" d=""M 628 4666 
-L 1259 4666 
-L 1259 2694 
-L 3353 4666 
-L 4166 4666 
-L 1850 2491 
-L 4331 0 
-L 3500 0 
-L 1259 2247 
-L 1259 0 
-L 628 0 
-L 628 4666 
-z
-"" transform=""scale(0.015625)""/>
-      <path id=""DejaVuSans-47"" d=""M 3809 666 
-L 3809 1919 
-L 2778 1919 
-L 2778 2438 
-L 4434 2438 
-L 4434 434 
-Q 4069 175 3628 42 
-Q 3188 -91 2688 -91 
-Q 1594 -91 976 548 
-Q 359 1188 359 2328 
-Q 359 3472 976 4111 
-Q 1594 4750 2688 4750 
-Q 3144 4750 3555 4637 
-Q 3966 4525 4313 4306 
-L 4313 3634 
-Q 3963 3931 3569 4081 
-Q 3175 4231 2741 4231 
-Q 1884 4231 1454 3753 
-Q 1025 3275 1025 2328 
-Q 1025 1384 1454 906 
-Q 1884 428 2741 428 
-Q 3075 428 3337 486 
-Q 3600 544 3809 666 
-z
-"" transform=""scale(0.015625)""/>
-     </defs>
-     <use xlink:href=""#DejaVuSans-53""/>
-     <use xlink:href=""#DejaVuSans-6c"" x=""63.476562""/>
-     <use xlink:href=""#DejaVuSans-69"" x=""91.259766""/>
-     <use xlink:href=""#DejaVuSans-6d"" x=""119.042969""/>
-     <use xlink:href=""#DejaVuSans-4f"" x=""216.455078""/>
-     <use xlink:href=""#DejaVuSans-72"" x=""295.166016""/>
-     <use xlink:href=""#DejaVuSans-63"" x=""334.029297""/>
-     <use xlink:href=""#DejaVuSans-61"" x=""389.009766""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""450.289062""/>
-     <use xlink:href=""#DejaVuSans-35"" x=""482.076172""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""545.699219""/>
-     <use xlink:href=""#DejaVuSans-38"" x=""609.322266""/>
-     <use xlink:href=""#DejaVuSans-4b"" x=""672.945312""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""738.521484""/>
-     <use xlink:href=""#DejaVuSans-28"" x=""770.308594""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""809.322266""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""872.945312""/>
-     <use xlink:href=""#DejaVuSans-65"" x=""904.732422""/>
-     <use xlink:href=""#DejaVuSans-70"" x=""966.255859""/>
-     <use xlink:href=""#DejaVuSans-6f"" x=""1029.732422""/>
-     <use xlink:href=""#DejaVuSans-63"" x=""1090.914062""/>
-     <use xlink:href=""#DejaVuSans-68"" x=""1145.894531""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1209.273438""/>
-     <use xlink:href=""#DejaVuSans-6f"" x=""1241.060547""/>
-     <use xlink:href=""#DejaVuSans-6e"" x=""1302.242188""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1365.621094""/>
-     <use xlink:href=""#DejaVuSans-31"" x=""1397.408203""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1461.03125""/>
-     <use xlink:href=""#DejaVuSans-54"" x=""1492.818359""/>
-     <use xlink:href=""#DejaVuSans-34"" x=""1553.902344""/>
-     <use xlink:href=""#DejaVuSans-20"" x=""1617.525391""/>
-     <use xlink:href=""#DejaVuSans-47"" x=""1649.3125""/>
-     <use xlink:href=""#DejaVuSans-50"" x=""1726.802734""/>
-     <use xlink:href=""#DejaVuSans-55"" x=""1787.105469""/>
-     <use xlink:href=""#DejaVuSans-29"" x=""1860.298828""/>
-    </g>
-   </g>
-  </g>
- </g>
- <defs>
-  <clipPath id=""p682a0e8bed"">
-   <rect x=""90"" y=""51.84"" width=""558"" height=""332.64""/>
-  </clipPath>
- </defs>
-</svg>
diff --git a/images/try live demo green.png b/images/try live demo green.png
new file mode 100644
index 0000000..540ff55
Binary files /dev/null and b/images/try live demo green.png differ
diff --git a/pyproject.toml b/pyproject.toml
index 070ae25..09dd118 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,7 +33,7 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
-	""transformers"",
+    ""transformers"",
     ""datasets"",
     ""sentencepiece"",
     ""accelerate"",
@@ -70,4 +70,4 @@ colab = [
 [project.urls]
 homepage = ""http://www.unsloth.ai""
 documentation = ""https://github.com/unslothai/unsloth""
-repository = ""https://github.com/unslothai/unsloth""
\ No newline at end of file
+repository = ""https://github.com/unslothai/unsloth""
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 100625f..e9f8b5e 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -11,7 +11,7 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-__version__ = ""2023.11""
+__version__ = ""2023.12""
 import os
 import warnings
 import importlib
@@ -35,7 +35,7 @@ if ""CUDA_VISIBLE_DEVICES"" in os.environ:
         )
         os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
 else:
-    warnings.warn(""Unsloth: 'CUDA_VISIBLE_DEVICES' is not set. We shall set it ourselves."")
+    # warnings.warn(""Unsloth: 'CUDA_VISIBLE_DEVICES' is not set. We shall set it ourselves."")
     os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
     os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
 pass
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 175caab..67ad306 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -43,7 +43,6 @@ def _cross_entropy_forward(logits_ptr, logits_row_stride,
     mask = col_offsets < n_cols
 
     # TODO: Fixup int32 locations to int64
-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72
     label_idx = tl.load(labels_ptr).to(tl.int32)
     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
     max_logits = tl.max(logits, 0)
@@ -88,7 +87,6 @@ def _cross_entropy_backward(logits_ptr, logits_row_stride,
     col_offsets = tl.arange(0, BLOCK_SIZE)
     mask = col_offsets < n_cols
     # TODO: Fixup int32 locations to int64
-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72
     label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)
 
     if label_idx != -100:
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index ee1b20f..99a7a50 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -35,7 +35,6 @@ def _rope_embedding(
     mask = col_offsets < half_head_dim
 
     # TODO: Fixup int32 locations to int64
-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72
     rot_position = row_position % seqlen
 
     Q   += row_position*  Q_row_stride + head_position*head_dim
@@ -48,8 +47,6 @@ def _rope_embedding(
 
     Q2   = tl.load(Q   + half_head_dim*1 + col_offsets, mask = mask, other = 0)
     # RoPE repeats sin and cos so 128 = [64, 64].
-    # sin2 = tl.load(sin + half_head_dim*1, mask = mask, other = 0)
-    # cos2 = tl.load(cos + half_head_dim*1, mask = mask, other = 0)
 
     if BACKWARD_PASS:
         """"""
@@ -62,11 +59,8 @@ def _rope_embedding(
             where R.T is again the same  [ 0, -I]
             but the minus is transposed. [ I,  0]
         """"""
-        # sin1, sin2 = -sin1, -sin2
         sin1 = -sin1
-
-    # tl.store(Q + half_head_dim*0, Q1*cos1 - Q2*sin1, mask = mask)
-    # tl.store(Q + half_head_dim*1, Q2*cos2 + Q1*sin2, mask = mask)
+    
     # RoPE repeats sin and cos so 128 = [64, 64].
     tl.store(Q + half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)
     tl.store(Q + half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index df4b85f..34906a5 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -13,12 +13,12 @@
 # limitations under the License.
 
 import triton
-MAX_FUSED_SIZE = 65535 # 2**16 - 1
+MAX_FUSED_SIZE = 65536 # 2**16 Solves https://github.com/unslothai/unsloth/issues/7
 next_power_of_2 = triton.next_power_of_2
 
 def calculate_settings(n):
     BLOCK_SIZE = next_power_of_2(n)
-    # CUDA only supports 65535 - 2^16-1 threads per block
+    # CUDA only supports 65536 - 2^16 threads per block
     if BLOCK_SIZE > MAX_FUSED_SIZE:
         raise RuntimeError(f""Cannot launch Triton kernel since n = {n} exceeds ""\
                            f""the maximum CUDA blocksize = {MAX_FUSED_SIZE}."")
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 8ab451f..bfc0e26 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -16,14 +16,15 @@ import torch
 from typing import Optional, Tuple, List, Union
 from torch.nn.functional import scaled_dot_product_attention
 from transformers.models.llama.modeling_llama import (
-    # apply_rotary_pos_emb,
-    # repeat_kv,
-    # _prepare_4d_causal_attention_mask,
+    _prepare_4d_causal_attention_mask,
     logger,
     BaseModelOutputWithPast,
     CausalLMOutputWithPast,
 )
 from ..kernels import *
+from ._utils import (
+    prepare_model_for_kbit_training,
+)
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
@@ -37,7 +38,6 @@ else:
     # Tri Dao's benchmark shows xformers is faster for now.
     HAS_FLASH_ATTENTION = False
 pass
-
 import xformers.ops.fmha as xformers
 xformers_attention = xformers.memory_efficient_attention
 
@@ -55,12 +55,9 @@ import bitsandbytes as bnb
 import numpy as np
 import types
 
-from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig
 from transformers import set_seed as transformers_set_seed
 from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
-from ._utils import (
-    prepare_model_for_kbit_training,
-)
 
 
 def original_apply_qkv(self, X):
@@ -92,10 +89,6 @@ def LlamaAttention_fast_forward(
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     
     bsz, q_len, _ = hidden_states.size()
-
-    # Q = self.q_proj(hidden_states)
-    # K = self.k_proj(hidden_states)
-    # V = self.v_proj(hidden_states)
     Q, K, V = self.apply_qkv(self, hidden_states)
 
     n_heads    = self.num_heads
@@ -112,8 +105,6 @@ def LlamaAttention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
-    # cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
-    # Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)
     if position_ids is None:
         cos = self.rotary_emb.cos_cached
         sin = self.rotary_emb.sin_cached
@@ -130,10 +121,9 @@ def LlamaAttention_fast_forward(
     past_key_value = (K, V) if use_cache else None
 
     # Attention module
-    # no_attention_mask = attention_mask is None
-    # Ignore attention_mask
-
-    if (not HAS_FLASH_ATTENTION): #and no_attention_mask:
+    # Xformers doesnt support backward pass for GQA (yet)
+    # TEMP fix
+    if (n_groups == 1) and (not HAS_FLASH_ATTENTION):
         # Xformers memory efficient attention
         # Also has Flash Attention v2 dispatching
         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)
@@ -143,18 +133,17 @@ def LlamaAttention_fast_forward(
 
         # Grouped query attention
         if n_groups != 1:
-            Q = Q.reshape(bsz, q_len, n_groups, n_kv_heads, head_dim)
-
-            K = K.reshape(bsz, q_len, n_groups,          1, head_dim)
-            V = V.reshape(bsz, q_len, n_groups,          1, head_dim)
-            K = K .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)
-            V = V .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)
+            Q = Q.reshape(bsz, q_len, n_kv_heads, n_groups, head_dim)
+            K = K.reshape(bsz, q_len, n_kv_heads,        1, head_dim)
+            V = V.reshape(bsz, q_len, n_kv_heads,        1, head_dim)
+            K = K .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
+            V = V .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)
         pass
 
         A = xformers_attention(Q, K, V, attn_bias = causal_mask)
         A = A.view(bsz, q_len, n_heads, head_dim)
 
-    elif HAS_FLASH_ATTENTION:# and no_attention_mask:
+    elif HAS_FLASH_ATTENTION:
         # Flash Attention
         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)
         Q = Q.transpose(1, 2)
@@ -163,37 +152,22 @@ def LlamaAttention_fast_forward(
 
         # Flash Attention v2 auto supports grouped query attention
         A = flash_attn_func(Q, K, V, causal = True)
-
     else:
-        # Uses Pytorch's scaled dot product attention
-        if attention_mask is not None:
-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
-                raise ValueError(
-                    f""Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}""
-                )
-        pass
-
         # Grouped query attention
-        # K = repeat_kv(K, n_groups)
-        # V = repeat_kv(V, n_groups)
         if n_groups != 1:
             K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
             V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
             K = K.reshape(bsz, n_heads, q_len, head_dim)
             V = V.reshape(bsz, n_heads, q_len, head_dim)
         pass
-
         # Needs (batch_size, n_heads, seq_len, head_dim)
         # is_casual and attention_mask must not be both set!
-        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = attention_mask is None)
+        A = scaled_dot_product_attention(Q, K, V, attn_mask = None, is_causal = True)
         # Go back to (batch_size, seq_len, n_heads, head_dim)
         A = A.transpose(1, 2)
     pass
     attn_output = A.reshape(bsz, q_len, self.hidden_size)
-
-    # attn_output = self.o_proj(attn_output)
     attn_output = self.apply_o(self, attn_output)
-
     attn_weights = None
     return attn_output, attn_weights, past_key_value
 pass
@@ -227,7 +201,6 @@ def LlamaDecoderLayer_fast_forward(
     """"""
     residual = hidden_states
 
-    # hidden_states = self.input_layernorm(hidden_states)
     hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
 
     # Self Attention
@@ -245,7 +218,6 @@ def LlamaDecoderLayer_fast_forward(
 
     # Fully Connected
     residual = hidden_states
-    # hidden_states = self.post_attention_layernorm(hidden_states)
     hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
     hidden_states = self.mlp(hidden_states)
     hidden_states = residual + hidden_states
@@ -308,7 +280,7 @@ def LlamaModel_fast_forward(
     if (past_key_values_length != 0):
         position_ids = torch.arange(
             past_key_values_length, seq_length + past_key_values_length,
-            dtype  = torch.int32,#dtype=torch.long,
+            dtype  = torch.int32,
             device = ""cuda"",
         )
         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
@@ -326,11 +298,7 @@ def LlamaModel_fast_forward(
         inputs_embeds = self.embed_tokens(input_ids)
 
     # Ignore attention_mask
-    if True:
-    # if attention_mask is None:
-        # attention_mask = torch.ones(
-        #     (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device
-        # )
+    if attention_mask is None:
         padding_mask = None
     else:
         if 0 in attention_mask:
@@ -339,7 +307,7 @@ def LlamaModel_fast_forward(
             padding_mask = None
 
         attention_mask = _prepare_4d_causal_attention_mask(
-            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
+            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length,
         )
     pass
 
@@ -403,7 +371,6 @@ def LlamaModel_fast_forward(
             all_self_attns += (layer_outputs[1],)
     pass
 
-    # hidden_states = self.norm(hidden_states)
     hidden_states = fast_rms_layernorm(self.norm, hidden_states)
 
     # add hidden states from the last decoder layer
@@ -466,19 +433,13 @@ def LlamaForCausalLM_fast_forward(
 
     loss = None
     if labels is not None:
-        # logits = logits.float()
-        # shift_logits = logits[..., :-1, :].contiguous()
-        # shift_labels = labels[..., 1:].contiguous()
-        # shift_labels = shift_labels.view(-1)
-        # shift_logits = shift_logits.view(-1, self.config.vocab_size)
         shift_logits = logits
+        if not hasattr(self, ""extra_ignored_labels""):
+            # Fixes https://github.com/unslothai/unsloth/issues/10
+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
+        pass
+        
         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
-
-        # loss_fct = torch.nn.CrossEntropyLoss(
-        #     ignore_index = self.ignore_index,
-        #     label_smoothing = self.label_smoothing,
-        # )
-        # loss = loss_fct(shift_logits, shift_labels)
         loss = fast_cross_entropy_loss(
             logits = shift_logits,
             labels = shift_labels,
@@ -547,13 +508,14 @@ class FastLlamaModel:
         load_in_4bit = True,
         token = None,
         device_map = ""sequential"",
+        rope_scaling = None,
     ):
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
 
         statistics = \
-            ""==((====))==  Unsloth: Fast Llama patching release 23.11\n""\
+            ""==((====))==  Unsloth: Fast Llama patching release 2023.12\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
            f""O^O/ \_/ \\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\n""\
            f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
@@ -570,9 +532,20 @@ class FastLlamaModel:
 
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
-        # [TODO]: Determine RoPE scaling
-        # https://github.com/huggingface/transformers/pull/24653
-        assert(max_seq_length <= 4096)
+        # RoPE scaling
+        model_max_seq_length = \
+            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings
+
+        if (rope_scaling is None) and (max_seq_length > model_max_seq_length):
+            rope_scaling = max_seq_length / model_max_seq_length
+            logger.warning_once(
+                f""Unsloth: {model_name} can only handle sequence lengths of of most ""\
+                f""{model_max_seq_length}.\nBut with kaiokendev's RoPE scaling of ""\
+                f""{round(rope_scaling, 3)}, it can be magically be extended to ""\
+                f""{max_seq_length}!""
+            )
+            rope_scaling = {""type"": ""linear"", ""factor"": rope_scaling,}
+        pass
 
         bnb_config = None
         if load_in_4bit:
@@ -589,6 +562,7 @@ class FastLlamaModel:
             torch_dtype = dtype,
             quantization_config = bnb_config,
             token = token,
+            rope_scaling = rope_scaling,
         )
         tokenizer = AutoTokenizer.from_pretrained(
             model_name,
@@ -596,9 +570,22 @@ class FastLlamaModel:
             padding_side = ""right"",
             token = token,
         )
-        tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token});
-        tokenizer.pad_token = tokenizer.unk_token
-        config = model.config.update({""pad_token_id"" : tokenizer.unk_token_id});
+
+        if not hasattr(tokenizer, ""pad_token""):
+            # Fixes https://github.com/unslothai/unsloth/issues/5
+            if hasattr(tokenizer, ""unk_token""):
+                tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token})
+                tokenizer.pad_token = tokenizer.unk_token
+            else:
+                logger.warning_one(
+                    f""{model_name} does not have a padding or unknown token!\n""\
+                    f""Will use the EOS token of id {tokenizer.eos_token_id} as padding.""
+                )
+                assert(hasattr(tokenizer, ""eos_token""))
+                tokenizer.add_special_tokens({""pad_token"" : tokenizer.eos_token})
+                tokenizer.pad_token = tokenizer.eos_token
+            config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
+        pass
 
         model = FastLlamaModel.post_patch(model)
 
@@ -607,6 +594,8 @@ class FastLlamaModel:
             layer.self_attn.apply_qkv = original_apply_qkv
             layer.self_attn.apply_o   = original_apply_o
         pass
+
+        model.max_seq_length = max_seq_length
         return model, tokenizer
     pass
 
@@ -668,6 +657,8 @@ class FastLlamaModel:
         random_state = 3407,
         max_seq_length = 2048,
     ):
+        assert(max_seq_length <= model.max_seq_length)
+
         if lora_dropout != 0:
             raise TypeError(""Unsloth: Fast Llama patching only works with dropout = 0."")
         if bias != ""none"":
@@ -727,8 +718,14 @@ class FastLlamaModel:
         pass
 
         # Patch cross entropy loss labels
-        model.model.extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda"")
-        
+        # Fixes https://github.com/unslothai/unsloth/issues/10
+        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda"")
+        model.model.extra_ignored_labels = extra_ignored_labels
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            internal_model.max_seq_length = max_seq_length
+            internal_model = internal_model.model
+        pass
         return model
     pass
 pass
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 5f5b4e1..596548d 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -416,6 +416,21 @@ gemma_chatml_eos_token = (
 CHAT_TEMPLATES[""gemma_chatml""] = (gemma_chatml_template, gemma_chatml_eos_token, True, gemma_chatml_ollama,)
 pass
 
+# =========================================== Gemma 2
+# Same as Gemma 1, but with sliding window attention!
+# https://ollama.com/library/gemma2/blobs/6522ca797f47
+gemma2_template = gemma_template
+gemma2_ollama = gemma_ollama + ""PARAMETER num_ctx 4096\n""
+gemma2_eos_token = ""<end_of_turn>""
+CHAT_TEMPLATES[""gemma2""] = (gemma2_template, gemma2_eos_token, True, gemma2_ollama,)
+
+# =========================================== Gemma 2 with ChatML instead
+gemma2_chatml_template = gemma_chatml_template
+gemma2_chatml_ollama = gemma_chatml_ollama + ""PARAMETER num_ctx 4096\n""
+gemma2_chatml_eos_token = gemma_chatml_eos_token
+CHAT_TEMPLATES[""gemma2_chatml""] = (gemma2_chatml_template, gemma2_chatml_eos_token, True, gemma2_chatml_ollama,)
+pass
+
 # =========================================== Llama-3
 # Weirdly \n\n is needed?
 llama3_template = \
@@ -1014,7 +1029,17 @@ def get_ollama_eos_tokens(tokenizer, extra_eos_tokens = []):
     pass
     final_eos_tokens += extra_eos_tokens
     final_eos_tokens += repeatted_tokens
-    return final_eos_tokens
+
+    # Remove new lines, spaces and HTML tags
+    filtered_eos_tokens = []
+    for token in final_eos_tokens:
+        if   token.count(""\n"") == len(token): continue
+        elif token.count("""") == len(token): continue
+        elif token.startswith(""<"") and len(token) <= 2: continue
+        elif token.startswith(""</"") and len(token) == 3: continue
+        filtered_eos_tokens.append(token)
+    pass
+    return filtered_eos_tokens
 pass
 
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 73aa0c6..fd1b87c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,9 +12,43 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+__version__ = ""2024.7""
+
+__all__ = [
+    ""prepare_model_for_kbit_training"",
+    ""xformers"",
+    ""xformers_attention"",
+    ""xformers_version"",
+    ""__version__"",
+    ""HAS_FLASH_ATTENTION"",
+    ""platform_system"",
+    ""patch_tokenizer"",
+    ""get_statistics"",
+    ""Unsloth_Offloaded_Gradient_Checkpointer"",
+    ""offload_to_disk"",
+    ""offload_input_embeddings"",
+    ""offload_output_embeddings"",
+    ""is_bfloat16_supported"",
+    ""unsloth_offloaded_gradient_checkpoint"",
+    ""torch_compile_options"",
+    ""patch_linear_scaling"",
+    ""create_boolean_mask"",
+]
+
 import torch
-from typing import Union, Optional, List, Any, Callable
+from typing import Union, Optional, List, Any, Callable, Tuple
 import warnings
+from platform import system as platform_system
+platform_system = platform_system()
+import math
+import numpy as np
+import os
+import psutil
+import inspect
+import re
+
+# =============================================
+# Disable some warnings which can get annoying
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""torch"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""huggingface_hub"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
@@ -26,20 +60,42 @@ warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""
 # Stop ""Special tokens have been added in the vocabulary, ...""
 import logging
 logging.getLogger(""transformers.tokenization_utils_base"").setLevel(logging.CRITICAL+1)
+# =============================================
+
+# =============================================
+# Edits all Config files to enable RoPE Scaling for all models
+from transformers import PretrainedConfig
+
+model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"",]
+
+for model_name in model_architectures:
+    config_filepath = f""transformers.models.{model_name}.configuration_{model_name}""
+    model_filepath = f""transformers.models.{model_name}.modeling_{model_name}""
+    config_filename = f""{model_name.title()}Config""
+    exec(f""from {config_filepath} import {config_filename}"", globals())
+
+    config = inspect.getsource(eval(config_filename))
+    if ""rope_scaling"" in config: continue
+    config = re.sub(
+        r""(\*\*kwargs)[\s]{0,}\,[\s]{0,}\)[\s]{0,}\:"",
+        r""rope_scaling=None,""\
+        r""\n        **kwargs):\n""\
+        r""\n        self.rope_scaling = rope_scaling\n"",
+        config,
+    )
+    exec(config, globals())
+
+    exec(f""import {config_filepath}"", globals())
+    exec(f""{config_filepath}.{config_filename} = {config_filename}"", globals())
+pass
+# =============================================
 
+# =============================================
+# Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
-from platform import system as platform_system
-platform_system = platform_system()
-import math
-import numpy as np
-import os
-import psutil
 
-__version__ = ""2024.7""
-
-# Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
 SUPPORTS_BFLOAT16 = False
 
@@ -69,25 +125,10 @@ pass
 import xformers.ops.fmha as xformers
 xformers_attention = xformers.memory_efficient_attention
 from xformers import __version__ as xformers_version
+# =============================================
 
-__all__ = [
-    ""prepare_model_for_kbit_training"",
-    ""xformers"",
-    ""xformers_attention"",
-    ""xformers_version"",
-    ""__version__"",
-    ""HAS_FLASH_ATTENTION"",
-    ""platform_system"",
-    ""patch_tokenizer"",
-    ""get_statistics"",
-    ""Unsloth_Offloaded_Gradient_Checkpointer"",
-    ""offload_to_disk"",
-    ""offload_input_embeddings"",
-    ""offload_output_embeddings"",
-    ""is_bfloat16_supported"",
-    ""unsloth_offloaded_gradient_checkpoint"",
-    ""torch_compile_options"",
-]
+# =============================================
+# Torch compile settings
 
 # Just remove max_autotune_gemm warning
 import functools
@@ -128,7 +169,7 @@ torch_compile_options = {
     ""trace.enabled""     : False, # Output Triton kernel outputs!
     ""triton.cudagraphs"" : False,
 }
-
+# =============================================
 
 def prepare_model_for_kbit_training(
     model                      : Any,
@@ -266,6 +307,7 @@ def patch_tokenizer(model, tokenizer):
 pass
 
 
+# =============================================
 # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??
 # For mixed precision, we need it to be in float32 not float16.
 from peft.tuners.lora.layer import LoraLayer
@@ -295,6 +337,7 @@ except:
         ""Luckily, your training run will still work in the meantime!""
     )
 pass
+# =============================================
 
 
 def get_statistics():
@@ -456,9 +499,8 @@ def unsloth_offloaded_gradient_checkpoint(function, *args, use_reentrant = None,
 pass
 
 
-""""""
-    Remove warnings about missing kwargs and patch stuff
-""""""
+# =============================================
+# Fixes Bitsandbytes to remove missing warnings
 from transformers.utils.quantization_config import BitsAndBytesConfig, QuantizationMethod
 from inspect import getsource
 from accelerate.utils.dataclasses import DistributedType
@@ -501,7 +543,7 @@ exec(BitsAndBytesConfig__init__, globals())
 
 import transformers.utils.quantization_config
 transformers.utils.quantization_config.BitsAndBytesConfig.__init__ = _BitsAndBytesConfig__init__
-
+# =============================================
 
 # Offloading to disk for modules (lm_head, embed_tokens)
 import pickle
@@ -549,3 +591,106 @@ pass
 def is_bfloat16_supported():
     return SUPPORTS_BFLOAT16
 pass
+
+
+# Patches models to add RoPE Scaling
+def patch_linear_scaling(
+    model_name = ""gemma2"",
+    rope_module = None,
+    scaled_rope_module = None,
+    attention_module = None,
+):
+    assert(rope_module is not None and scaled_rope_module is not None)
+    assert(attention_module is not None)
+
+    rope_name = rope_module.__name__
+    scaled_rope_name = scaled_rope_module.__name__
+    model_filepath = f""transformers.models.{model_name}.modeling_{model_name}""
+    exec_code = \
+        f""import torch.nn as nn\n""\
+        f""from typing import Union, Optional, List, Any, Callable, Tuple\n""\
+        f""from {model_filepath} import logger, ""\
+        f""{model_name.title()}Attention, {model_name.title()}Config""
+
+    function = inspect.getsource(attention_module.__init__)
+    where = function.find(""def"")
+    function = function.split(""\n"")
+    function = ""\n"".join(x[where:] for x in function)
+    init_name = f""{model_name.title()}Attention__init__""
+    function = function.replace(""def __init__"", f""def {init_name}"")
+    function = function.replace(
+        ""super().__init__()"",
+        f""super({model_name.title()}Attention, self).__init__()"",
+    )
+    fix_rope_function = """"""
+    if getattr(self.config, ""rope_scaling"", None) is None:
+        self.rotary_emb = {rope_function}(
+            self.head_dim,
+            max_position_embeddings=self.max_position_embeddings,
+            base=self.rope_theta,
+        )
+    else:
+        scaling_type = self.config.rope_scaling[""type""]
+        scaling_factor = self.config.rope_scaling[""factor""]
+        if scaling_type == ""linear"":
+            self.rotary_emb = {scaled_rope_function}(
+                self.head_dim,
+                max_position_embeddings=self.max_position_embeddings,
+                scaling_factor=scaling_factor,
+                base=self.rope_theta,
+            )
+        else:
+            raise ValueError(f""Unknown RoPE scaling type {{scaling_type}}"")
+    pass
+    """"""
+    fix_rope_function = fix_rope_function.format(
+        rope_function        = rope_module.__name__,
+        scaled_rope_function = scaled_rope_module.__name__,
+    )
+    rotary_emb = re.findall(
+        ""self.rotary_emb = .+?\)"", function,
+        flags = re.DOTALL | re.MULTILINE,
+    )
+    if len(rotary_emb) == 0: return
+    rotary_emb = rotary_emb[0]
+    function = function.replace(rotary_emb, fix_rope_function, 1)
+    function = exec_code + ""\n\n"" + function
+    return init_name, function
+pass
+
+
+def create_boolean_mask(n = 4096, sliding_window = 2048):
+    # Creates a boolean mask for attention
+    mask = torch.ones(n, n, dtype = torch.bool)
+    if sliding_window == 0:
+        return torch.triu(mask, diagonal = 1, out = mask)
+    pass
+    torch.triu(mask, diagonal = 0, out = mask)
+    torch.triu(mask.T, diagonal = -sliding_window, out = mask.T)
+    mask = mask.T
+    torch.logical_not(mask, out = mask)
+    return mask
+pass
+
+
+def test_mask_creation():
+    from transformers.modeling_attn_mask_utils import AttentionMaskConverter
+    for n in range(2, 23):
+        for s in range(1, 23):
+            correct_mask = AttentionMaskConverter(
+                is_causal = True,
+                sliding_window = s,
+            ).to_causal_4d(1, n, n, dtype = torch.float16,).squeeze(0).squeeze(0)
+            correct_mask = (correct_mask == correct_mask.min())
+            our_mask = create_boolean_mask(n = n, sliding_window = s)
+            assert(torch.all(correct_mask == our_mask))
+        pass
+        correct_mask = AttentionMaskConverter(
+            is_causal = True,
+            sliding_window = None,
+        ).to_causal_4d(1, n, n, dtype = torch.float16,).squeeze(0).squeeze(0)
+        correct_mask = (correct_mask == correct_mask.min())
+        our_mask = create_boolean_mask(n = n, sliding_window = 0)
+        assert(torch.all(correct_mask == our_mask))
+    pass
+pass
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 4c4515b..4d3db8d 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -236,10 +236,53 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
 pass
 
 
+class GemmaFixedLinearScalingRotaryEmbedding(GemmaFixedRotaryEmbedding):
+    """"""LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev""""""
+    # Fixes https://github.com/huggingface/transformers/pull/28837
+    # https://github.com/microsoft/DeepSpeed/issues/4932
+    # The precision of RoPE buffers is not correct, so we cast to int64.
+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):
+        self.scaling_factor = scaling_factor
+        super().__init__(dim, max_position_embeddings, base, device)
+    pass
+
+    def _set_cos_sin_cache(self, seq_len, device, dtype):
+# Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
+        # in FP32. They are applied (multiplied) in FP32 as well.
+        self.max_seq_len_cached = seq_len
+
+        # The difference is we do division explicity instead of t * (1/x) ie we do t/x.
+        freq_exponents = (2.0 / self.dim) * (
+            torch.arange(self.dim // 2, dtype = torch.int64, device = ""cpu"").float()
+        )
+        timescale = self.base**freq_exponents
+        positions = torch.arange(self.max_seq_len_cached, device = ""cpu"", dtype = torch.int64).float()
+        positions = positions /  self.scaling_factor
+        radians_new = positions[..., None] / timescale[None, None, :]
+        radians_new = radians_new.squeeze(0)
+
+        emb = torch.cat((radians_new, radians_new), dim = -1)
+        # We must do RoPE in float32!
+        cos = emb.cos().to(device = ""cuda:0"", non_blocking = True)#, dtype = dtype)
+        sin = emb.sin().to(device = ""cuda:0"", non_blocking = True)#, dtype = dtype)
+        self.register_buffer(""cos_cached"", cos, persistent = False)
+        self.register_buffer(""sin_cached"", sin, persistent = False)
+    pass
+pass
+
+
 class FastGemmaModel(FastLlamaModel):
 
     @staticmethod
     def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""gemma"",
+            rope_module        = GemmaFixedRotaryEmbedding,
+            scaled_rope_module = GemmaFixedLinearScalingRotaryEmbedding,
+            attention_module   = GemmaAttention,
+        )
+        exec(function, globals())
+        GemmaAttention.__init__      = eval(init_name)
         GemmaAttention      .forward = LlamaAttention_fast_forward
         GemmaSdpaAttention  .forward = LlamaAttention_fast_forward
         GemmaFlashAttention2.forward = LlamaAttention_fast_forward
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 0669e42..4a1420f 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -16,6 +16,7 @@ from .llama import *
 from ._utils import __version__
 from .gemma import (
     GemmaFixedRotaryEmbedding,
+    GemmaFixedLinearScalingRotaryEmbedding,
     fast_geglu_inference,
 )
 from transformers.models.gemma2.modeling_gemma2 import (
@@ -27,7 +28,6 @@ from transformers.models.gemma2.modeling_gemma2 import (
     apply_rotary_pos_emb,
     repeat_kv,
 )
-from transformers.models.gemma2.modeling_gemma2 import *
 from transformers.modeling_attn_mask_utils import (
     _prepare_4d_causal_attention_mask_for_sdpa,
 )
@@ -46,7 +46,7 @@ pass
 # [TODO] We must randomnly use torch.compile?
 # I checked the gradients and formulas and I'm sure it's correct.
 # I'm stumped :(
-@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
+@torch.compile(fullgraph = True, dynamic = True)#, options = torch_compile_options)
 def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
     old_dtype = X.dtype
     X = X.float()
@@ -77,6 +77,8 @@ def gemma2_attention(Q, K, V, causal_mask, self, bsz, q_len):
     A = torch.matmul(Q, K.transpose(2, 3))
     A = t * torch.tanh(A / t) # Logit softcapping
     A += causal_mask[:q_len, :q_len]
+    # Much slower in torch compile!
+    # A.masked_fill_(causal_mask[:q_len, :q_len], -float(""inf""))
     A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
     A = torch.matmul(A, V)
     A = A.transpose(1, 2).contiguous()
@@ -255,6 +257,8 @@ def Gemma2Attention_fast_forward_inference(
         self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+        # Only for Gemma2
+        self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
         self.scalar = 1.0 / math_sqrt(self.config.hidden_size // self.config.num_attention_heads)
         self.half_head_dim = head_dim // 2
@@ -341,7 +345,7 @@ def Gemma2Attention_fast_forward_inference(
     # pass
     A = A.transpose(1, 2)
     A = A.reshape(bsz, 1, attention_size)
-    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_O)
     return A, (Kn, Vn)
 pass
 
@@ -426,6 +430,14 @@ class FastGemma2Model(FastLlamaModel):
 
     @staticmethod
     def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""gemma2"",
+            rope_module        = GemmaFixedRotaryEmbedding,
+            scaled_rope_module = GemmaFixedLinearScalingRotaryEmbedding,
+            attention_module   = Gemma2Attention,
+        )
+        exec(function, globals())
+        Gemma2Attention.__init__      = eval(init_name)
         Gemma2Attention      .forward = Gemma2Attention_fast_forward
         Gemma2SdpaAttention  .forward = Gemma2Attention_fast_forward
         Gemma2FlashAttention2.forward = Gemma2Attention_fast_forward
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index e19b857..c7ae67e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -663,8 +663,11 @@ def LlamaModel_fast_forward(
 
     # Gemma2 has alternating SWA and global attn
     if IS_GEMMA2 and not hasattr(self, ""SWA_mask""):
-        from transformers.modeling_attn_mask_utils import AttentionMaskConverter
         n = self.config.max_position_embeddings
+        # masked_fill is making stuff slower!
+        # self. GA_mask = create_boolean_mask(n = n, sliding_window = 0)
+        # self.SWA_mask = create_boolean_mask(n = n, sliding_window = self.config.sliding_window)
+        from transformers.modeling_attn_mask_utils import AttentionMaskConverter
         self.SWA_mask = AttentionMaskConverter(
             is_causal = True,
             sliding_window = self.config.sliding_window,
@@ -1099,6 +1102,13 @@ class FastLlamaModel:
         trust_remote_code = False,
         **kwargs,
     ):
+        if trust_remote_code:
+            print(
+                ""Unsloth: WARNING `trust_remote_code` is True.\n""\
+                ""Are you certain you want to do remote code execution?""
+            )
+        pass
+
         if token is None and ""HF_TOKEN"" in os.environ:
             token = os.environ[""HF_TOKEN""]
 
@@ -1139,6 +1149,7 @@ class FastLlamaModel:
             with open(inspect.getfile(model_function), ""r"") as file:
                 has_rope_scaling = ""self.config.rope_scaling"" in file.read()
         except: pass
+        has_rope_scaling = True
 
         # If max_seq_length is not specified, use maximum fron config
         if max_seq_length is None:
@@ -1183,6 +1194,7 @@ class FastLlamaModel:
         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12
         # RoPE Scaling's max_position_embeddings must be updated
         max_position_embeddings = max(max_seq_length, model_max_seq_length)
+        kwargs.pop(""attn_implementation"", None); # No need since we auto call it
         model = AutoModelForCausalLM.from_pretrained(
             model_name,
             device_map              = device_map,
@@ -1191,6 +1203,7 @@ class FastLlamaModel:
             token                   = token,
             max_position_embeddings = max_position_embeddings,
             trust_remote_code       = trust_remote_code,
+            attn_implementation     = ""eager"",
             **kwargs,
         )
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index e0b51a1..28f664c 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -15,7 +15,10 @@
 from .llama import *
 import os
 from ._utils import __version__
-
+from .llama import (
+    LlamaRotaryEmbedding,
+    LlamaLinearScalingRotaryEmbedding,
+)
 from transformers.models.mistral.modeling_mistral import (
     MistralAttention,
     MistralDecoderLayer,
@@ -268,6 +271,14 @@ class FastMistralModel(FastLlamaModel):
 
     @staticmethod
     def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""mistral"",
+            rope_module        = LlamaRotaryEmbedding,
+            scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
+            attention_module   = MistralAttention,
+        )
+        exec(function, globals())
+        MistralAttention.__init__      = eval(init_name)
         MistralAttention      .forward = MistralAttention_fast_forward
         MistralSdpaAttention  .forward = MistralAttention_fast_forward
         MistralFlashAttention2.forward = MistralAttention_fast_forward
diff --git a/unsloth/models/qwen2.py b/unsloth/models/qwen2.py
index 5b9fff5..dcd05af 100644
--- a/unsloth/models/qwen2.py
+++ b/unsloth/models/qwen2.py
@@ -13,7 +13,10 @@
 # limitations under the License.
 
 from .llama import *
-
+from .llama import (
+    LlamaRotaryEmbedding,
+    LlamaLinearScalingRotaryEmbedding,
+)
 from transformers.models.qwen2.modeling_qwen2 import (
     Qwen2Attention,
     Qwen2DecoderLayer,
@@ -36,6 +39,14 @@ class FastQwen2Model(FastLlamaModel):
 
     @staticmethod
     def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""qwen2"",
+            rope_module        = LlamaRotaryEmbedding,
+            scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
+            attention_module   = Qwen2Attention,
+        )
+        exec(function, globals())
+        Qwen2Attention.__init__      = eval(init_name)
         Qwen2Attention      .forward = LlamaAttention_fast_forward
         Qwen2SdpaAttention  .forward = LlamaAttention_fast_forward
         Qwen2FlashAttention2.forward = LlamaAttention_fast_forward
diff --git a/unsloth/save.py b/unsloth/save.py
index 1ceea3c..293e430 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -49,6 +49,7 @@ LLAMA_WEIGHTS = (
 )
 LLAMA_LAYERNORMS = (
     ""input_layernorm"", ""post_attention_layernorm"",
+    ""pre_feedforward_layernorm"", ""post_feedforward_layernorm"",
 )
 
 # https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp#L19
@@ -557,7 +558,11 @@ def unsloth_save_model(
                 state_dict[name] = torch.load(filename, map_location = ""cpu"", mmap = True)
         pass
         for item in LLAMA_LAYERNORMS:
-            state_dict[f""model.layers.{j}.{item}.weight""] = eval(f""layer.{item}.weight.data"")
+            try:
+                # Skip for Gemma 2
+                state_dict[f""model.layers.{j}.{item}.weight""] = eval(f""layer.{item}.weight.data"")
+            except:
+                continue
         pass
     pass
 
"
"diff --git a/.github/ISSUE_TEMPLATE/other.md b/.github/ISSUE_TEMPLATE/other.md
deleted file mode 100644
index b78d201..0000000
--- a/.github/ISSUE_TEMPLATE/other.md
+++ /dev/null
@@ -1,10 +0,0 @@
----
-name: Other
-about: Everything else
-title: ''
-labels: ''
-assignees: ''
-
----
-
-Try asking https://discord.com/invite/unsloth for fast support!
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index b8da9c0..8280ed6 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -887,11 +887,15 @@ def install_llama_cpp_old(version = -10):
         os.path.exists(""llama.cpp/llama-quantize.exe"") or
         os.path.exists(""llama.cpp/llama-quantize"") or
         os.path.exists(""llama.cpp/quantize.exe"") or
-        os.path.exists(""llama.cpp/quantize"")
+        os.path.exists(""llama.cpp/quantize"") or
+        os.path.exists(""llama.cpp/build/bin/llama-quantize"") or
+        os.path.exists(""llama.cpp/build/bin/quantize"") or
+        os.path.exists()
     ):
         raise RuntimeError(
             ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
-            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
+            ""We've also double checked the building directory under 'llama.cpp/build/bin/'.\n""\
+            ""But we expect this file to exist! Check if the file exists under llama.cp and investigate the building process of llama.cpp (make/cmake)""
         )
     pass
 pass
@@ -1081,11 +1085,16 @@ def save_to_gguf(
             quantize_location = ""llama.cpp/llama-quantize.exe""
         elif os.path.exists(""llama.cpp/llama-quantize""):
             quantize_location = ""llama.cpp/llama-quantize""
+        elif os.path.exists(""llama.cpp/build/bin/llama-quantize""):
+            quantize_location = ""llama.cpp/build/bin/llama-quantize""
+        elif os.path.exists(""llama.cpp/build/bin/quantize""):
+            quantize_location = ""llama.cpp/build/bin/quantize""
         else:
-            raise RuntimeError(
-                ""Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\n""\
-                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
-            )
+        raise RuntimeError(
+            ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
+            ""We've also double checked the building directory under 'llama.cpp/build/bin/'.\n""\
+            ""But we expect this file to exist! Check if the file exists under llama.cp and investigate the building process of llama.cpp (make/cmake)""
+        )
         pass
 
         # See https://github.com/unslothai/unsloth/pull/730
"
"diff --git a/pyproject.toml b/pyproject.toml
index d17859c..3d381fa 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.5"",
+    ""unsloth_zoo>=2025.7.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.5"",
+    ""unsloth_zoo>=2025.7.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index e965b29..7fb0093 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -222,7 +222,7 @@ elif DEVICE_TYPE == ""xpu"":
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.7.1""):
+    if Version(unsloth_zoo_version) < Version(""2025.7.7""):
         print(
             ""Unsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\n""\
             ""Do this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index bfd7c8c..1774ed3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.4""
+__version__ = ""2025.7.5""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index e3f1e61..a0894ec 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -14,6 +14,7 @@
 
 from .llama import *
 from ._utils import __version__
+import math
 
 try:
     from transformers.models.gemma.modeling_gemma import (
@@ -256,7 +257,7 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b5244ed..e6c9280 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -14,6 +14,7 @@
 
 import torch
 import gc
+import math
 from typing import Optional, Tuple, List, Union
 from ._utils import *
 from ._utils import __version__
@@ -1036,7 +1037,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1109,7 +1110,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
         # in FP32. They are applied (multiplied) in FP32 as well.
         self.current_rope_size = seq_len
         
-        t = torch.arange(self.current_rope_size, device=""cpu"", dtype=torch.int64).float()
+        t = torch.arange(self.current_rope_size, device=self.inv_freq.device, dtype=torch.int64).float()
 
         freqs = torch.outer(t, self.inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
@@ -1158,7 +1159,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
"
"diff --git a/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
new file mode 100644
index 0000000..0bf548b
--- /dev/null
+++ b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
@@ -0,0 +1,255 @@
+# -*- coding: utf-8 -*-
+
+from unsloth import FastVisionModel
+
+import torch
+from qwen_vl_utils import process_vision_info
+import os
+from datasets import load_dataset
+from trl import SFTTrainer, SFTConfig
+
+import sys
+from pathlib import Path
+
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.ocr_eval import OCRModelEvaluator
+
+
+## Dataset Preparation
+from datasets import load_dataset
+
+dataset = load_dataset(""lbourdois/OCR-liboaccn-OPUS-MIT-5M-clean"", 'en', split=""train"")
+# To select the first 2000 examples
+train_dataset = dataset.select(range(2000))
+
+# To select the next 200 examples for evaluation
+eval_dataset = dataset.select(range(2000, 2200))
+
+# Convert dataset to OAI messages
+def format_data(sample):
+    return {""messages"": [
+                {
+                    ""role"": ""system"",
+                    ""content"": [{""type"": ""text"", ""text"": system_message}],
+                },
+                {
+                    ""role"": ""user"",
+                    ""content"": [
+                        {
+                            ""type"": ""text"",
+                            ""text"": sample[""question""],
+                        },{
+                            ""type"": ""image"",
+                            ""image"": sample[""image""],
+                        }
+                    ],
+                },
+                {
+                    ""role"": ""assistant"",
+                    ""content"": [{""type"": ""text"", ""text"": sample[""answer""]}],
+                },
+            ],
+        }
+
+system_message = ""You are an expert french ocr system.""
+# Convert dataset to OAI messages
+# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes
+train_dataset = [format_data(sample) for sample in train_dataset]
+eval_dataset = [format_data(sample) for sample in eval_dataset]
+
+## Setup OCR main evaluation function and helpers
+import os
+import torch
+from tqdm import tqdm
+import pandas as pd
+from jiwer import wer, cer
+from qwen_vl_utils import process_vision_info
+
+#
+ocr_evaluator = OCRModelEvaluator()
+model_comparison_results = {}
+
+## Finetuning Setup and Run
+# Load Base Model
+
+model, tokenizer = FastVisionModel.from_pretrained(
+    model_name = ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
+    max_seq_length = 2048, # Choose any for long context!
+    load_in_4bit = True,  # 4 bit quantization to reduce memory
+    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
+    full_finetuning = False, # [NEW!] We have full finetuning now!
+)
+
+# benchmark base model performance
+model_name = ""Unsloth Base model""
+FastVisionModel.for_inference(model)
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_base_model_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+## Lora Finetuning
+model = FastVisionModel.get_peft_model(
+    model,
+    finetune_vision_layers     = True, # Turn off for just text!
+    finetune_language_layers   = True,  # Should leave on!
+    finetune_attention_modules = True,  # Attention good for GRPO
+    finetune_mlp_modules       = True,  # SHould leave on always!
+
+    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    #target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                      #""gate_proj"", ""up_proj"", ""down_proj"",],
+    lora_alpha = 32,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+)
+
+from unsloth import is_bf16_supported
+from unsloth.trainer import UnslothVisionDataCollator
+FastVisionModel.for_training(model) # Enable for training!
+model.config.use_cache = False
+
+
+trainer = SFTTrainer(
+    model = model,
+    tokenizer = tokenizer,
+    data_collator = UnslothVisionDataCollator(model, tokenizer),
+    train_dataset = train_dataset,
+    args = SFTConfig(
+        #per_device_train_batch_size = 4,
+        #gradient_accumulation_steps = 8,
+        per_device_train_batch_size = 2,
+        gradient_accumulation_steps = 4,
+        gradient_checkpointing=True,
+        gradient_checkpointing_kwargs = {""use_reentrant"": False}, # use reentrant checkpointing
+        max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper
+        warmup_ratio=0.03,
+        #num_train_epochs = 2, # Set this instead of max_steps for full training runs
+        max_steps=60,
+        learning_rate = 2e-4,
+        fp16 = not is_bf16_supported(),
+        bf16 = is_bf16_supported(),
+        logging_steps = 5,
+        save_strategy=""epoch"",
+        optim = ""adamw_torch_fused"",
+        weight_decay = 0.01,
+        lr_scheduler_type = ""linear"",
+        seed = 3407,
+        output_dir = ""unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"",
+        report_to = ""none"",     # For Weights and Biases
+
+        # You MUST put the below items for vision finetuning:
+        remove_unused_columns = False,
+        dataset_text_field = """",
+        dataset_kwargs = {""skip_prepare_dataset"": True},
+        dataset_num_proc = 4,
+        max_seq_length = 2048,
+    ),
+)
+
+# run training
+trainer_stats = trainer.train()
+
+model.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"", tokenizer)
+tokenizer.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
+
+## Measure Adapter Performance
+
+# benchmark lora model performance
+model_name = ""Unsloth lora adapter model""
+FastVisionModel.for_inference(model)
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_lora_model_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+## Merge Model
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+base = find_lora_base_model(model)
+
+print((base.__class__.__name__))
+
+# merge default 16 bits
+model.save_pretrained_merged(save_directory=""qwen2.5-ocr-merged-finetune-merge-16bit"", tokenizer=tokenizer)
+
+
+## Benchmark merged model performance
+
+### 16 bits merged model
+
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=False)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-16bits""
+model.config.use_cache = True
+
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_16bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# load 16bits-merged model in 4 bits
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=True, load_in_8bit=False)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-4bits""
+model.config.use_cache = True
+
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_4bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# load model in 8 bits
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=True)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-8bits""
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_8bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# """"""### 4 bits merged model""""""
+#
+# # load 4bits-merged model in 4 bits
+# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=True, load_in_8bit=False)
+#
+# # benchmark 4bit loaded, 4bits merged model performance
+# model_name = ""Unsloth 4bits-merged model load-4bits""
+#
+# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_4bits_results"")
+# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+#
+# # load model in 8 bits
+# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=False, load_in_8bit=True)
+#
+# # benchmark 8bit loaded, 4bits merged model performance
+# model_name = ""Unsloth 4bits-merged model load-8bits""
+#
+# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_8bits_results"")
+# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# Model comparison report
+#print model comparison
+ocr_evaluator.print_model_comparison()
+
+
+
+# Final cleanup
+print(""\n Cleaning up temporary files..."")
+safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
+safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"")
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./qwen2.5-ocr-merged-finetune-merge-16bit"")
+
+print(""\n Pipeline completed successfully!"")
+print(""="" * 80)
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index f559c6c..28fa163 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -618,6 +618,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen2.5-VL-7B-Instruct"",
         ""unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"",
     ),
+    ""unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-VL-32B-Instruct"",
+        ""Qwen/Qwen2.5-VL-32B-Instruct"",
+        ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
+    ),
     ""unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-VL-72B-Instruct"",
         ""Qwen/Qwen2.5-VL-72B-Instruct"",
"
"diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 90208c5..f24108b 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -114,6 +114,22 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/OpenHermes-2.5-Mistral-7B"",
         ""teknium/OpenHermes-2.5-Mistral-7B"",
     ),
+    ""unsloth/codegemma-2b-bnb-4bit"" : (
+        ""unsloth/codegemma-2b"",
+        ""google/codegemma-2b"",
+    ),
+    ""unsloth/codegemma-7b-bnb-4bit"" : (
+        ""unsloth/codegemma-7b"",
+        ""google/codegemma-7b"",
+    ),
+    ""unsloth/codegemma-2b-it-bnb-4bit"" : (
+        ""unsloth/codegemma-2b-it"",
+        ""google/codegemma-2b-it"",
+    ),
+    ""unsloth/codegemma-7b-it-bnb-4bit"" : (
+        ""unsloth/codegemma-7b-it"",
+        ""google/codegemma-7b-it"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ce644f8..af61cc3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.9""
+__version__ = ""2024.12.10""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -1093,6 +1093,22 @@ def patch_gradient_accumulation_fix(Trainer):
         ""if self.model_accepts_loss_kwargs:"",
         ""if False:"",
     )
+
+    # Fix when num_items_in_batch is nothing
+    # https://github.com/huggingface/transformers/pull/35207
+    function = re.sub(
+        r""else:\n""\
+        r""([\s]{4,})self\.accelerator\.backward\(loss, \*\*kwargs\)\n""\
+        r""(.+?)if num_items_in_batch is None\:\n""\
+        r""(.+?)return loss\.detach\(\) \/ self\.args\.gradient_accumulation_steps"",
+
+        ""else:\n""\
+        ""\2if num_items_in_batch is None:\n""\
+        ""\3loss /= self.args.gradient_accumulation_steps\n""\
+        ""\1self.accelerator.backward(loss, **kwargs)"",
+        
+        function,
+    )
     
     exec(function, globals())
     Trainer.training_step = _unsloth_training_step
@@ -1130,6 +1146,8 @@ def unsloth_compile_transformers(
     fuse_lm_head            = True,
     gradient_checkpointing  = True,
     manual_replacements     = True,
+    fast_lora_forwards      = True,
+    fast_residual_stream    = True,
     epilogue_fusion         = True,
     max_autotune            = False,
     shape_padding           = True,
@@ -1174,6 +1192,8 @@ def unsloth_compile_transformers(
             fuse_lm_head           = fuse_lm_head,
             gradient_checkpointing = gradient_checkpointing,
             manual_replacements    = manual_replacements,
+            fast_lora_forwards     = fast_lora_forwards,
+            fast_residual_stream   = fast_residual_stream,
             epilogue_fusion        = epilogue_fusion,
             max_autotune           = max_autotune,
             shape_padding          = shape_padding,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 9c5ea5b..d1c8b1e 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -470,6 +470,8 @@ class FastVisionModel(FastBaseVisionModel):
                 fuse_lm_head            = True,
                 gradient_checkpointing  = True,
                 manual_replacements     = True,
+                fast_lora_forwards      = False,
+                fast_residual_stream    = False,
                 epilogue_fusion         = True,
                 max_autotune            = False,
                 shape_padding           = True,
"
"diff --git a/README.md b/README.md
index 555e080..6df6616 100644
--- a/README.md
+++ b/README.md
@@ -10,7 +10,7 @@
 <a href=""https://discord.gg/u54VK8m8tk""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
 <a href=""https://ko-fi.com/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png"" height=""48""></a>
 
-### Finetune Mistral, Gemma, Llama 2-5x faster with 80% less memory!
+### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!
 
 ![](https://i.ibb.co/sJ7RhGG/image-41.png)
 
@@ -22,12 +22,11 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 
 | Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
 |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
-| **Llama-3 8b**      | [ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
-| **Gemma 7b**      | [ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |
-| **Mistral 7b**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |
-| **TinyLlama**  | [ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 82% less |
-| **CodeLlama 34b** A100   | [ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 49% less |
-| **Mistral 7b** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\* | 73% less |
+| **Llama 3 (8B)**      | [ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
+| **Mistral (7B)**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |
+| **Gemma (7B)**      | [ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |
+| **Llama 3 (8B)** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook) | 5x faster\* | 73% less |
+| **ORPO**     | [ Start on Colab](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |
 | **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
 
 - Benchmarking compared to FA2 + Hugging Face combined.
@@ -36,7 +35,8 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 - \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.
 
 ##  Unsloth.ai News
--  NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (just change the model name in the notebook).
+-  NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).
+-  NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!
 -  NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you're using our notebooks. To enable, simply change 1 line:
 ```python
 model = FastLanguageModel.get_peft_model(
@@ -46,8 +46,6 @@ model = FastLanguageModel.get_peft_model(
 ```
 -  [CodeGemma](https://colab.research.google.com/drive/19lwcRk_ZQ_ZtX-qzFP3qZBBHZNcMD1hh?usp=sharing) now works along with [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) and [Gemma 2b](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)
 -  [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models
--  [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO
--  We did a [blog](https://huggingface.co/blog/unsloth-trl) with Hugging Face and are in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)
 
 ##  Links and Resources
 | Type                            | Links                               |
@@ -182,18 +180,20 @@ max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
 url = ""https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl""
 dataset = load_dataset(""json"", data_files = {""train"" : url}, split = ""train"")
 
-# 4bit pre quantized models we support - 4x faster downloading!
+# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
 fourbit_models = [
     ""unsloth/mistral-7b-bnb-4bit"",
+    ""unsloth/mistral-7b-instruct-v0.2-bnb-4bit"",
     ""unsloth/llama-2-7b-bnb-4bit"",
-    ""unsloth/llama-2-13b-bnb-4bit"",
-    ""unsloth/codellama-34b-bnb-4bit"",
-    ""unsloth/tinyllama-bnb-4bit"",
-] # Go to https://huggingface.co/unsloth for more 4-bit models!
+    ""unsloth/gemma-7b-bnb-4bit"",
+    ""unsloth/gemma-7b-it-bnb-4bit"", # Instruct version of Gemma 7b
+    ""unsloth/gemma-2b-bnb-4bit"",
+    ""unsloth/gemma-2b-it-bnb-4bit"", # Instruct version of Gemma 2b
+    ""unsloth/llama-3-8b-bnb-4bit"", # [NEW] 15 Trillion token Llama-3
+] # More models at https://huggingface.co/unsloth
 
-# Load Llama model
 model, tokenizer = FastLanguageModel.from_pretrained(
-    model_name = ""unsloth/mistral-7b-bnb-4bit"", # Supports Llama, Mistral - replace this!
+    model_name = ""unsloth/llama-3-8b-bnb-4bit"",
     max_seq_length = max_seq_length,
     dtype = None,
     load_in_4bit = True,
@@ -208,7 +208,8 @@ model = FastLanguageModel.get_peft_model(
     lora_alpha = 16,
     lora_dropout = 0, # Supports any, but = 0 is optimized
     bias = ""none"",    # Supports any, but = ""none"" is optimized
-    use_gradient_checkpointing = True,
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
     random_state = 3407,
     max_seq_length = max_seq_length,
     use_rslora = False,  # We support rank stabilized LoRA
@@ -272,7 +273,8 @@ model = FastLanguageModel.get_peft_model(
     lora_alpha = 64,
     lora_dropout = 0, # Supports any, but = 0 is optimized
     bias = ""none"",    # Supports any, but = ""none"" is optimized
-    use_gradient_checkpointing = True,
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
     random_state = 3407,
     max_seq_length = max_seq_length,
 )
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index d31b6cf..4e7a71a 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -281,6 +281,17 @@ def get_chat_template(
         IS_GEMMA = True
     pass
 
+    # We add a check for Llama-3
+    # if chat_template == ""llama-3"":
+    #     tokenizer._using_llama3_template = True
+    # else:
+    #     llama3_tokens = set([""<|end_header_id|>"", ""<|eot_id|>"", ""<|start_header_id|>""])
+    #     check_llama3_tokens = llama3_tokens & set(str(x) for x in tokenizer.added_tokens_decoder.values())
+    #     if len(check_llama3_tokens) == len(llama3_tokens):
+    #         tokenizer._using_llama3_template = True
+    #     pass
+    # pass
+
     # We first check if the tokenizer is a fast one. If not, we cannot convert this!
     is_fast_tokenizer = getattr(tokenizer, ""is_fast"", False)
     old_padding_side = tokenizer.padding_side
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 45c7501..a7cacea 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1284,6 +1284,15 @@ class FastLlamaModel:
         # Add save modules
         patch_saving_functions(model)
 
+        # Save tokenizer for inference purposes
+        tokenizer.padding_side = ""left"" # Force inference
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            internal_model._saved_temp_tokenizer = tokenizer
+            internal_model = internal_model.model
+        pass
+        internal_model._saved_temp_tokenizer = tokenizer
+        
         return model, tokenizer
     pass
 
@@ -1534,8 +1543,12 @@ class FastLlamaModel:
         if not SUPPORTS_LOFTQ:  del arguments[""loftq_config""]
         if not SUPPORTS_RSLORA: del arguments[""use_rslora""]
 
+        _saved_temp_tokenizer = model._saved_temp_tokenizer
+
         lora_config = LoraConfig(**arguments)
         model = _get_peft_model(model, lora_config)
+        
+        model._saved_temp_tokenizer = _saved_temp_tokenizer
 
         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)
 
@@ -1554,6 +1567,18 @@ class FastLlamaModel:
             model.model.lm_head.modules_to_save.default.requires_grad_(True)
         pass
 
+        # Patch tokenizer to pad to the right
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_saved_temp_tokenizer""):
+                internal_model._saved_temp_tokenizer.padding_side = ""right""
+            pass
+            internal_model = internal_model.model
+        pass
+        if hasattr(internal_model, ""_saved_temp_tokenizer""):
+            internal_model._saved_temp_tokenizer.padding_side = ""right""
+        pass
+
         return model
     pass
 
@@ -1751,6 +1776,18 @@ class FastLlamaModel:
         # Wrap model.generate
         model._unwrapped_old_generate = model.generate
         model.generate = _wrap_fast_inference(model.generate, device_type, dtype)
+
+        # Patch tokenizer to pad to the left
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_saved_temp_tokenizer""):
+                internal_model._saved_temp_tokenizer.padding_side = ""left""
+            pass
+            internal_model = internal_model.model
+        pass
+        if hasattr(internal_model, ""_saved_temp_tokenizer""):
+            internal_model._saved_temp_tokenizer.padding_side = ""left""
+        pass
     pass
 
 
@@ -1777,8 +1814,18 @@ class FastLlamaModel:
             model.generate = model._unwrapped_old_generate
             del model._unwrapped_old_generate
         pass
+
+        # Patch tokenizer to pad to the right
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_saved_temp_tokenizer""):
+                internal_model._saved_temp_tokenizer.padding_side = ""right""
+            pass
+            internal_model = internal_model.model
+        pass
+        if hasattr(internal_model, ""_saved_temp_tokenizer""):
+            internal_model._saved_temp_tokenizer.padding_side = ""right""
+        pass
     pass
 pass
 
-
-
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index fa864a9..a107200 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -76,6 +76,7 @@ class FastLanguageModel(FastLlamaModel):
         fix_tokenizer  = True,
         trust_remote_code = False,
         use_gradient_checkpointing = True,
+        resize_model_vocab = None,
         *args, **kwargs,
     ):
         if token is None and ""HF_TOKEN"" in os.environ:
@@ -149,6 +150,9 @@ class FastLanguageModel(FastLlamaModel):
             trust_remote_code = trust_remote_code,
             *args, **kwargs,
         )
+        
+        if resize_model_vocab is not None:
+            model.resize_token_embeddings(resize_model_vocab)
 
         # In case the model supports tagging, add the unsloth tag.
         if hasattr(model, ""add_model_tags""):
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 5610893..80d0ffd 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -559,6 +559,15 @@ class FastMistralModel(FastLlamaModel):
 
         # Add save modules
         patch_saving_functions(model)
+
+        # Save tokenizer for inference purposes
+        tokenizer.padding_side = ""left"" # Force inference
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            internal_model._saved_temp_tokenizer = tokenizer
+            internal_model = internal_model.model
+        pass
+        internal_model._saved_temp_tokenizer = tokenizer
         
         return model, tokenizer
     pass
diff --git a/unsloth/save.py b/unsloth/save.py
index 6e9d82c..a2c55bb 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -689,7 +689,7 @@ pass
 
 
 def install_llama_cpp_clone_non_blocking():
-    full_command = [""git"", ""clone"", ""https://github.com/ggerganov/llama.cpp""]
+    full_command = [""git"", ""clone"", ""--recursive"", ""https://github.com/ggerganov/llama.cpp""]
     run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
     return run_installer
 pass
@@ -742,7 +742,7 @@ def install_llama_cpp_old(version = -10):
     # Clone a specific commit
     # Also don't use the GPU!
     commands = [
-        ""git clone https://github.com/ggerganov/llama.cpp"",
+        ""git clone --recursive https://github.com/ggerganov/llama.cpp"",
         f""cd llama.cpp && git reset --hard {version} && git clean -df"",
         ""make clean -C llama.cpp"",
         f""make all -j{psutil.cpu_count()*2} -C llama.cpp"",
@@ -767,7 +767,7 @@ def install_llama_cpp_blocking(use_cuda = True):
     use_cuda = ""LLAMA_CUDA=1"" if use_cuda else """"
 
     commands = [
-        ""git clone https://github.com/ggerganov/llama.cpp"",
+        ""git clone --recursive https://github.com/ggerganov/llama.cpp"",
         ""make clean -C llama.cpp"",
         f""{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp"",
         ""pip install gguf protobuf"",
@@ -922,16 +922,9 @@ def save_to_gguf(
           f""The output location will be {final_location}\n""\
           ""This will take 3 minutes..."")
 
-    # We first check if tokenizer.model exists in the model_directory
-    if os.path.exists(f""{model_directory}/tokenizer.model""):
-        vocab_type = ""hfft""
-    else:
-        vocab_type = ""bpe""
-    pass
-
     if use_fast_convert:
         command = f""python llama.cpp/convert.py {model_directory} ""\
-            f""--outfile {final_location} --vocab-type {vocab_type} ""\
+            f""--outfile {final_location} --vocab-type spm,hfft,bpe ""\
             f""--outtype {first_conversion} --concurrency {n_cpus}""
     else:
         # Need to fix convert-hf-to-gguf.py for some models!
@@ -966,7 +959,7 @@ def save_to_gguf(
                 ""You might have to compile llama.cpp yourself, then run this again.\n""\
                 ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
                 ""You must run this in the same folder as you're saving your model.\n""\
-                ""git clone https://github.com/ggerganov/llama.cpp\n""\
+                ""git clone --recursive https://github.com/ggerganov/llama.cpp\n""\
                 ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
                 ""Once that's done, redo the quantization.""
             )
@@ -1006,7 +999,7 @@ def save_to_gguf(
                     ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
                     ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
                     ""You must run this in the same folder as you're saving your model.\n""\
-                    ""git clone https://github.com/ggerganov/llama.cpp\n""\
+                    ""git clone --recursive https://github.com/ggerganov/llama.cpp\n""\
                     ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
                     ""Once that's done, redo the quantization.""
                 )
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index f1a9daa..5dc5856 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -524,7 +524,7 @@ def add_new_tokens(
     tokenizer,
     new_tokens = [],
     method = ""mean"",
-    interpolation = 0.05,
+    interpolation = 0.5,
 ):
     """"""
     Smartly resizes the tokenizer and adds new tokens to the model.
"
"diff --git a/pyproject.toml b/pyproject.toml
index d9df119..88c757b 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -131,6 +131,12 @@ cu124onlytorch240 = [
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
+cu118onlytorch250 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+]
 cu121onlytorch250 = [
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
@@ -147,6 +153,12 @@ cu124onlytorch250 = [
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
+cu118onlytorch251 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+]
 cu121onlytorch251 = [
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
@@ -163,6 +175,28 @@ cu124onlytorch251 = [
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
+cu118onlytorch260 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+]
+cu124onlytorch260 = [
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+]
+cu126onlytorch260 = [
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+]
 cu118 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
@@ -223,21 +257,31 @@ cu121-torch240 = [
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch240]"",
 ]
-cu121-torch250 = [
+cu124-torch240 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
-    ""unsloth[cu121onlytorch250]"",
+    ""unsloth[cu124onlytorch240]"",
 ]
-cu124-torch240 = [
+cu118-torch250 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
-    ""unsloth[cu124onlytorch240]"",
+    ""unsloth[cu118onlytorch250]"",
+]
+cu121-torch250 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu121onlytorch250]"",
 ]
 cu124-torch250 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu124onlytorch250]"",
 ]
+cu118-torch251 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu118onlytorch251]"",
+]
 cu121-torch251 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
@@ -248,6 +292,21 @@ cu124-torch251 = [
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu124onlytorch251]"",
 ]
+cu118-torch260 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.1"",
+    ""unsloth[cu118onlytorch260]"",
+]
+cu124-torch260 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.1"",
+    ""unsloth[cu124onlytorch260]"",
+]
+cu126-torch260 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.1"",
+    ""unsloth[cu126onlytorch260]"",
+]
 kaggle = [
     ""unsloth[huggingface]"",
 ]
@@ -381,16 +440,22 @@ cu121-ampere-torch240 = [
     ""unsloth[cu121onlytorch240]"",
     ""unsloth[flashattention]"",
 ]
-cu121-ampere-torch250 = [
+cu124-ampere-torch240 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
-    ""unsloth[cu121onlytorch250]"",
+    ""unsloth[cu124onlytorch240]"",
     ""unsloth[flashattention]"",
 ]
-cu124-ampere-torch240 = [
+cu118-ampere-torch250 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
-    ""unsloth[cu124onlytorch240]"",
+    ""unsloth[cu118onlytorch250]"",
+    ""unsloth[flashattention]"",
+]
+cu121-ampere-torch250 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu121onlytorch250]"",
     ""unsloth[flashattention]"",
 ]
 cu124-ampere-torch250 = [
@@ -399,6 +464,12 @@ cu124-ampere-torch250 = [
     ""unsloth[cu124onlytorch250]"",
     ""unsloth[flashattention]"",
 ]
+cu118-ampere-torch251 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu118onlytorch251]"",
+    ""unsloth[flashattention]"",
+]
 cu121-ampere-torch251 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
@@ -411,6 +482,24 @@ cu124-ampere-torch251 = [
     ""unsloth[cu124onlytorch251]"",
     ""unsloth[flashattention]"",
 ]
+cu118-ampere-torch260 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.1"",
+    ""unsloth[cu118onlytorch260]"",
+    ""unsloth[flashattention]"",
+]
+cu124-ampere-torch260 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.1"",
+    ""unsloth[cu124onlytorch260]"",
+    ""unsloth[flashattention]"",
+]
+cu126-ampere-torch260 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.1"",
+    ""unsloth[cu126onlytorch260]"",
+    ""unsloth[flashattention]"",
+]
 
 [project.urls]
 homepage = ""http://www.unsloth.ai""
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 1f82dd8..c89fd0f 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -196,7 +196,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.1.4""):
+    if Version(unsloth_zoo_version) < Version(""2025.2.1""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/_auto_install.py b/unsloth/_auto_install.py
index c3b94c6..8bb5485 100644
--- a/unsloth/_auto_install.py
+++ b/unsloth/_auto_install.py
@@ -18,14 +18,16 @@ from packaging.version import Version as V
 v = V(torch.__version__)
 cuda = str(torch.version.cuda)
 is_ampere = torch.cuda.get_device_capability()[0] >= 8
-if cuda != ""12.1"" and cuda != ""11.8"" and cuda != ""12.4"": raise RuntimeError(f""CUDA = {cuda} not supported!"")
+if cuda != ""12.1"" and cuda != ""11.8"" and cuda != ""12.4"" and cuda != ""12.6"": raise RuntimeError(f""CUDA = {cuda} not supported!"")
 if   v <= V('2.1.0'): raise RuntimeError(f""Torch = {v} too old!"")
 elif v <= V('2.1.1'): x = 'cu{}{}-torch211'
 elif v <= V('2.1.2'): x = 'cu{}{}-torch212'
 elif v  < V('2.3.0'): x = 'cu{}{}-torch220'
 elif v  < V('2.4.0'): x = 'cu{}{}-torch230'
 elif v  < V('2.5.0'): x = 'cu{}{}-torch240'
-elif v  < V('2.6.0'): x = 'cu{}{}-torch250'
+elif v  < V('2.5.1'): x = 'cu{}{}-torch250'
+elif v <= V('2.5.1'): x = 'cu{}{}-torch251'
+elif v  < V('2.7.0'): x = 'cu{}{}-torch260'
 else: raise RuntimeError(f""Torch = {v} too new!"")
 x = x.format(cuda.replace(""."", """"), ""-ampere"" if is_ampere else """")
 print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git""')
\ No newline at end of file
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index d8dc385..c401393 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -759,6 +759,10 @@ DEFAULT_SYSTEM_MESSAGE[""llama-3.1""] = """" # Llama3.1 default system message is em
 
 CHAT_TEMPLATES[""llama-31""]  = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
 DEFAULT_SYSTEM_MESSAGE[""llama-31""] = """" # Llama3.1 default system message is empty + the dates
+
+for version in (""llama-3.2"", ""llama-3.3"", ""llama-32"", ""llama-33""):
+    CHAT_TEMPLATES[version] = CHAT_TEMPLATES[""llama-3.1""]
+    DEFAULT_SYSTEM_MESSAGE[version] = """"
 pass
 
 
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index de54396..f052914 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -15,6 +15,7 @@
 import triton
 MAX_FUSED_SIZE : int = 65536
 next_power_of_2 = triton.next_power_of_2
+import functools
 
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
 import torch
@@ -66,6 +67,8 @@ global CUDA_STREAM
 CUDA_STREAM = None
 get_ptr = bnb.functional.get_ptr
 import ctypes
+ctypes_c_int   = ctypes.c_int
+ctypes_c_int32 = ctypes.c_int32
 cdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32
 cdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_nf4
 cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
@@ -98,25 +101,31 @@ pass
 
 def get_lora_parameters_bias(proj):
     # For DPO or disabled adapters
-    base_layer = (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
+    base_layer = getattr(proj, ""base_layer"", proj) # (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
     W = base_layer.weight
     bias = base_layer.bias
 
-    if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
+    # if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
+    if getattr(proj, ""disable_adapters"", True) or proj.merged:
         return W, QUANT_STATE(W), None, None, None, bias
     pass
 
     active_adapter = proj.active_adapters[0] if \
-        hasattr(proj, ""active_adapters"") else proj.active_adapter
+        getattr(proj, ""active_adapters"", ) else proj.active_adapter
     A = proj.lora_A [active_adapter].weight
     B = proj.lora_B [active_adapter].weight
     s = proj.scaling[active_adapter]
     return W, QUANT_STATE(W), A, B, s, bias
 pass
 
+global WEIGHT_BUFFER
+WEIGHT_BUFFER = None
+global ABSMAX_BUFFER
+ABSMAX_BUFFER = None
 
 if HAS_CUDA_STREAM:
-    def fast_dequantize(W, quant_state = None, out = None):
+    @torch.inference_mode
+    def fast_dequantize(W, quant_state = None, out = None, use_global_buffer = False):
         if quant_state is None: return W
         if type(quant_state) is not list:
             # New quant_state as a class
@@ -139,36 +148,54 @@ if HAS_CUDA_STREAM:
         global CUDA_STREAM
         if CUDA_STREAM is None: CUDA_STREAM = torch.cuda.current_stream(""cuda:0"")
 
+        n_elements_absmax = absmax.numel()
+
         # Create weight matrix
-        if out is None:
-            out = torch.empty(shape, dtype = dtype, device = ""cuda:0"")
+        if use_global_buffer:
+
+            # Use same buffers for faster inference
+            size = shape[0]*shape[1]
+            global WEIGHT_BUFFER
+            global ABSMAX_BUFFER
+            if WEIGHT_BUFFER is None:
+                WEIGHT_BUFFER = torch.empty(size, dtype = dtype, device = ""cuda:0"", requires_grad = False)
+                ABSMAX_BUFFER = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"", requires_grad = False)
+
+            if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)
+            if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)
+
+            out = WEIGHT_BUFFER[:size].view(shape)
+            out_absmax = ABSMAX_BUFFER[:n_elements_absmax]
         else:
-            assert(out.shape == shape)
-            assert(out.dtype == dtype)
+            if out is None:
+                out = torch.empty(shape, dtype = dtype, device = ""cuda:0"", requires_grad = False)
+            else:
+                assert(out.shape == shape)
+                assert(out.dtype == dtype)
+            out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"", requires_grad = False)
+        pass
 
         # NF4 dequantization of statistics
-        n_elements_absmax = absmax.numel()
-        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"")
-
-        # Do dequantization
         ptr_out_absmax = get_ptr(out_absmax)
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
-            ctypes.c_int(blocksize2), ctypes.c_int(n_elements_absmax), CUDA_STREAM,
+            ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax), CUDA_STREAM,
         )
         out_absmax += offset
 
+        # Dequantize W
         fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
              cdequantize_blockwise_bf16_nf4
         fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
-           ctypes.c_int(blocksize), ctypes.c_int(out.numel()), CUDA_STREAM,)
+           ctypes_c_int(blocksize), ctypes_c_int(out.numel()), CUDA_STREAM,)
 
         # Careful returning transposed data
         is_transposed = (True if W.shape[0] == 1 else False)
         return out.t() if is_transposed else out
     pass
 else:
-    def fast_dequantize(W, quant_state = None, out = None):
+    @torch.inference_mode
+    def fast_dequantize(W, quant_state = None, out = None, use_global_buffer = False):
         if quant_state is None: return W
         if type(quant_state) is not list:
             # New quant_state as a class
@@ -189,29 +216,45 @@ else:
             absmax2, code2, blocksize2, _, _, _, _ = state2
         pass
 
+        n_elements_absmax = absmax.numel()
+
         # Create weight matrix
-        if out is None:
-            out = torch.empty(shape, dtype = dtype, device = ""cuda:0"")
-        else:
-            assert(out.shape == shape)
-            assert(out.dtype == dtype)
+        if use_global_buffer:
 
-        # NF4 dequantization of statistics
-        n_elements_absmax = absmax.numel()
-        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"")
+            # Use same buffers for faster inference
+            size = shape[0]*shape[1]
+            global WEIGHT_BUFFER
+            global ABSMAX_BUFFER
+            if WEIGHT_BUFFER is None:
+                WEIGHT_BUFFER = torch.empty(size, dtype = dtype, device = ""cuda:0"", requires_grad = False)
+                ABSMAX_BUFFER = torch.empty(n_elements_absmax, dtype = dtype, device = ""cuda:0"", requires_grad = False)
+
+            if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)
+            if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)
+
+            out = WEIGHT_BUFFER[:size].view(shape)
+            out_absmax = ABSMAX_BUFFER[:n_elements_absmax]
+        else:
+            if out is None:
+                out = torch.empty(shape, dtype = dtype, device = ""cuda:0"", requires_grad = False)
+            else:
+                assert(out.shape == shape)
+                assert(out.dtype == dtype)
+            out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"", requires_grad = False)
+        pass
 
         # Do dequantization
         ptr_out_absmax = get_ptr(out_absmax)
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
-            ctypes.c_int(blocksize2), ctypes.c_int(n_elements_absmax),
+            ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax),
         )
         out_absmax += offset
 
         fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
              cdequantize_blockwise_bf16_nf4
         fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
-           ctypes.c_int(blocksize), ctypes.c_int(out.numel()),)
+           ctypes_c_int(blocksize), ctypes_c_int(out.numel()),)
 
         # Careful returning transposed data
         is_transposed = (True if W.shape[0] == 1 else False)
@@ -263,17 +306,17 @@ if HAS_CUDA_STREAM:
         lda = shape[0]
         ldc = shape[0]
         ldb = (hd+1)//2
-        m = ctypes.c_int32(m)
-        n = ctypes.c_int32(n)
-        k = ctypes.c_int32(k)
-        lda = ctypes.c_int32(lda)
-        ldb = ctypes.c_int32(ldb)
-        ldc = ctypes.c_int32(ldc)
+        m = ctypes_c_int32(m)
+        n = ctypes_c_int32(n)
+        k = ctypes_c_int32(k)
+        lda = ctypes_c_int32(lda)
+        ldb = ctypes_c_int32(ldb)
+        ldc = ctypes_c_int32(ldc)
 
         df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda:0"")
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
-            ctypes.c_int(blocksize2), ctypes.c_int(df.numel()), CUDA_STREAM,
+            ctypes_c_int(blocksize2), ctypes_c_int(df.numel()), CUDA_STREAM,
         )
         df += offset
         absmax = df
@@ -281,7 +324,7 @@ if HAS_CUDA_STREAM:
         fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
             cgemm_4bit_inference_naive_bf16
 
-        blocksize = ctypes.c_int32(blocksize)
+        blocksize = ctypes_c_int32(blocksize)
         fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
            lda, ldb, ldc, blocksize, CUDA_STREAM,)
 
@@ -327,17 +370,17 @@ else:
         lda = shape[0]
         ldc = shape[0]
         ldb = (hd+1)//2
-        m = ctypes.c_int32(m)
-        n = ctypes.c_int32(n)
-        k = ctypes.c_int32(k)
-        lda = ctypes.c_int32(lda)
-        ldb = ctypes.c_int32(ldb)
-        ldc = ctypes.c_int32(ldc)
+        m = ctypes_c_int32(m)
+        n = ctypes_c_int32(n)
+        k = ctypes_c_int32(k)
+        lda = ctypes_c_int32(lda)
+        ldb = ctypes_c_int32(ldb)
+        ldc = ctypes_c_int32(ldc)
 
         df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda:0"")
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
-            ctypes.c_int(blocksize2), ctypes.c_int(df.numel()),
+            ctypes_c_int(blocksize2), ctypes_c_int(df.numel()),
         )
         df += offset
         absmax = df
@@ -345,7 +388,7 @@ else:
         fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
             cgemm_4bit_inference_naive_bf16
 
-        blocksize = ctypes.c_int32(blocksize)
+        blocksize = ctypes_c_int32(blocksize)
         fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
            lda, ldb, ldc, blocksize,)
 
@@ -354,6 +397,9 @@ else:
 pass
 
 
+torch_mm = torch.mm
+torch_mv = torch.mv
+torch_matmul = torch.matmul
 def fast_linear_forward(proj, X, temp_lora = None, out = None):
 
     W, W_quant, lora_A, lora_B, lora_S, bias = get_lora_parameters_bias(proj)
@@ -361,12 +407,12 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
     if q_len != 1: return matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)
 
     if W_quant is None:
-        out = torch.matmul(X, W.t(), out = out)
+        out = torch_matmul(X, W.t(), out = out)
     elif bsz == 1 and q_len == 1:
         out = fast_gemv(X, W, W_quant, out = out)
     else:
-        W = fast_dequantize(W.t(), W_quant)
-        out = torch.matmul(X, W, out = out)
+        W = fast_dequantize(W.t(), W_quant, use_global_buffer = True)
+        out = torch_matmul(X, W, out = out)
     pass
 
     # Add in LoRA weights
@@ -381,11 +427,11 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
         
         if bsz == 1:
             out = out.view(out_dim)
-            temp_lora = torch.mv(lora_A._fast_lora, X.ravel(), out = temp_lora)
+            temp_lora = torch_mv(lora_A._fast_lora, X.ravel(), out = temp_lora)
             out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)
         else:
             out = out.view(bsz, out_dim)
-            temp_lora = torch.mm(X.view(bsz, in_dim), lora_A._fast_lora.t(), out = temp_lora)
+            temp_lora = torch_mm(X.view(bsz, in_dim), lora_A._fast_lora.t(), out = temp_lora)
             out.addmm_(temp_lora, lora_B._fast_lora.t(), alpha = lora_S)
         pass
         out = out.view(bsz, 1, out_dim)
@@ -399,7 +445,7 @@ pass
 
 def matmul_lora(X, W, W_quant, A, B, s, out = None):
     dtype = X.dtype
-    W = fast_dequantize(W.t(), W_quant)
+    W = fast_dequantize(W.t(), W_quant, use_global_buffer = True)
 
     if X.dim() == 3:
         batch, seq_len, d = X.shape
@@ -409,7 +455,7 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):
         reshape = False
     pass
 
-    out = torch.matmul(X, W, out = out)
+    out = torch_matmul(X, W, out = out)
     if W_quant is not None: del W
 
     if A is not None:
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index c52d14f..b15e04a 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -20,3 +20,4 @@ from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
 from .dpo     import PatchDPOTrainer, PatchKTOTrainer
 from ._utils  import is_bfloat16_supported
+from .rl      import PatchFastRL
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index b0d51a8..017b5b5 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.1.8""
+__version__ = ""2025.2.1""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index 5dc71f9..9c12abb 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -17,115 +17,8 @@ __all__ = [
     ""PatchKTOTrainer"",
 ]
 
-try:
-    from transformers.utils.notebook import (
-        IntervalStrategy,
-        NotebookTrainingTracker,
-        NotebookProgressCallback,
-    )
-    HAS_NOTEBOOK = True
-except:
-    HAS_NOTEBOOK = False
-pass
-import torch
-from ._utils import torch_compile_options
-import inspect
-import torch.nn as nn
-from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union
+from .rl import PatchFastRL
 
+def PatchDPOTrainer(): PatchFastRL(""DPO"")
 
-DPOTrainer_metrics = [
-    ""rewards/chosen"",
-    ""rewards/rejected"",
-    ""rewards/accuracies"",
-    ""rewards/margins"",
-    ""logps/rejected"",
-    ""logps/chosen"",
-    ""logits/rejected"",
-    ""logits/chosen"",
-]
-set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)
-
-
-def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):
-    self.first_column = ""Epoch"" if args.eval_strategy == IntervalStrategy.EPOCH else ""Step""
-    self.training_loss = 0
-    self.last_log = 0
-    column_names = [self.first_column] + [""Training Loss""]
-    if args.eval_strategy != IntervalStrategy.NO:
-        column_names.append(""Validation Loss"")
-    column_names += [x.replace(""/"", "" / "") for x in DPOTrainer_metrics]
-    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)
-pass
-
-
-def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwargs):
-    # Only for when there is no evaluation
-    if args.eval_strategy == IntervalStrategy.NO and ""loss"" in logs:
-        values = {""Training Loss"": logs[""loss""]}
-        for metric in DPOTrainer_metrics:
-            values[metric.replace(""/"", "" / "")] = logs[metric]
-        pass
-        # First column is necessarily Step since we're not in epoch eval strategy
-        values[""Step""] = state.global_step
-        self.training_tracker.write_line(values)
-    pass
-pass
-
-
-def NotebookTrainingTracker_write_line(self, values):
-    """"""
-    Write the values in the inner table.
-
-    Args:
-        values (`Dict[str, float]`): The values to display.
-    """"""
-    if self.inner_table is None:
-        self.inner_table = [list(values.keys()), list(values.values())]
-    else:
-        columns = self.inner_table[0]
-        new_values = {}
-        for key, value in values.items():
-            lowered = key.lower()
-            if lowered in set_DPOTrainer_metrics:
-                new_values[lowered.replace(""/"", "" / "")] = value
-            else:
-                new_values[key] = value
-        pass
-        values = new_values
-
-        self.inner_table[0] = columns
-        if len(self.inner_table) > 1:
-            last_values = self.inner_table[-1]
-            first_column = self.inner_table[0][0]
-            if last_values[0] != values[first_column]:
-                # write new line
-                self.inner_table.append([values[c] if c in values else ""No Log"" for c in columns])
-            else:
-                # update last line
-                new_values = values
-                for c in columns:
-                    if c not in new_values.keys():
-                        new_values[c] = last_values[columns.index(c)]
-                self.inner_table[-1] = [new_values[c] for c in columns]
-        else:
-            # Edit for evaluation purposes
-            self.inner_table.append([values[c] if c in values else 0 for c in columns])
-        pass
-    pass
-pass
-
-
-def PatchDPOTrainer():
-    if HAS_NOTEBOOK:
-        from transformers.trainer import is_in_notebook
-        if is_in_notebook():
-            # Patch DPO notebook printing
-            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
-            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
-            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
-            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
-        pass
-    pass
-pass
-PatchKTOTrainer = PatchDPOTrainer
+def PatchKTOTrainer(): PatchFastRL(""KTO"")
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index c654343..bc29c46 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -210,7 +210,15 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
         config = None, # [TODO] Hack to pass in config - need to remove later
     ):
         super().__init__()
-        if config is not None: return # [TODO] Hack to pass in config - need to remove later
+        if config is not None:
+            # [TODO] Hack to pass in config - need to remove later
+            base = config.rope_theta
+            partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
+            dim = getattr(config, ""head_dim"", None)
+            if dim is None: dim = int((config.hidden_size // config.num_attention_heads))
+            device = ""cuda""
+            max_position_embeddings = config.max_position_embeddings
+        pass
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index da3295a..a337472 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -20,7 +20,7 @@ from ._utils import *
 from ._utils import __version__
 from torch.nn.functional import scaled_dot_product_attention
 from transformers import __version__ as transformers_version
-from unsloth_zoo.utils import Version
+from unsloth_zoo.utils import Version, _get_dtype
 transformers_version = Version(transformers_version)
 # Transformers moved rotary embeddings out of all attention layers
 IS_ATTENTION_REFACTOR = transformers_version > Version(""4.47.1"")
@@ -70,7 +70,8 @@ except:
     from huggingface_hub.utils._token import get_token
 pass
 from triton import __version__ as triton_version
-BlockDiagonalCausalMask = xformers.attn_bias.BlockDiagonalCausalMask if xformers is not None else None
+HAS_XFORMERS = xformers is not None
+BlockDiagonalCausalMask = xformers.attn_bias.BlockDiagonalCausalMask if HAS_XFORMERS else None
 
 
 def original_apply_qkv(self, X):
@@ -89,6 +90,8 @@ pass
 from math import sqrt as math_sqrt
 KV_CACHE_INCREMENT = 256 # KV Cache update size
 torch_nn_functional_softmax = torch.nn.functional.softmax
+# SDPA has GQA internally
+SDPA_HAS_GQA = ""enable_gqa"" in scaled_dot_product_attention.__doc__
 
 # Fix new HF's inference code
 def _fast_prepare_inputs_for_generation(self, input_ids, **kwargs,):
@@ -243,7 +246,7 @@ def LlamaAttention_fast_forward_inference(
 
     # Grouped query attention
     _, _, cached_len, _ = Knn.shape
-    if n_groups != 1:
+    if bsz == 1 or not SDPA_HAS_GQA and n_groups != 1:
         Knn = Knn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
         Vnn = Vnn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
         Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)
@@ -262,7 +265,10 @@ def LlamaAttention_fast_forward_inference(
         A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
         A = torch_matmul(A, Vnn, out = Qn)
     else:
-        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+        if SDPA_HAS_GQA:
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False, enable_gqa = True)
+        else:
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
     pass
     A = A.transpose(1, 2)
     A = A.reshape(bsz, 1, attention_size)
@@ -272,15 +278,15 @@ pass
 
 
 torch_nn_functional_silu = torch.nn.functional.silu
-def fast_swiglu_inference(self, X):
+def fast_swiglu_inference(self, X, temp_gate = None, temp_up = None):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     # mlp_size = self.config.intermediate_size
     # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
 
-    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
-    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
+    gate = fast_linear_forward(self.gate_proj, X, out = temp_gate)
+    up   = fast_linear_forward(self.  up_proj, X, out = temp_up)
     gate = torch_nn_functional_silu(gate, inplace = True)
     gate *= up
 
@@ -289,14 +295,23 @@ def fast_swiglu_inference(self, X):
     return down
 pass
 
-
-def fast_rms_layernorm_inference(self, X):
+torch_square = torch.square
+torch_mean   = torch.mean
+def fast_rms_layernorm_inference(self, X, XX = None, XX2 = None, variance = None):
     old_dtype = X.dtype
-    XX = X.to(torch.float32)
-    variance = XX.square().mean(-1, keepdim = True)
+    if XX is None:
+        XX = X.to(torch.float32)
+        variance = XX.square().mean(-1, keepdim = True)
+    else:
+        XX.copy_(X)
+        torch_mean(torch_square(XX, out = XX2), -1, keepdim = True, out = variance)
+    pass
     variance += self.variance_epsilon
     XX *= variance.rsqrt_()
-    X = XX.to(old_dtype) # Must preserve due to residual
+
+    if XX is None: X = XX.to(old_dtype)
+    else: X.copy_(XX)
+
     X *= self.weight
     return X
 pass
@@ -403,7 +418,7 @@ def LlamaAttention_fast_forward(
     past_key_value = (K, V) if use_cache else None
 
     # Attention module
-    if (not HAS_FLASH_ATTENTION and attention_mask is None):
+    if (not HAS_FLASH_ATTENTION and HAS_XFORMERS and attention_mask is None):
         # Xformers memory efficient attention
         # Also has Flash Attention v2 dispatching
         Q = Q.transpose(1, 2)
@@ -902,15 +917,29 @@ def LlamaModel_fast_forward_inference(
     attention_mask = None,
 ):
     input_ids = input_ids[:,:self.max_seq_length]
-    hidden_states = self.model.embed_tokens(input_ids)
-    hidden_states = hidden_states.to(self.config.torch_dtype)
-    bsz, q_len, hd = hidden_states.shape
+    bsz, q_len = input_ids.shape
+    hd = self.config.hidden_size
+    mlp_size = self.config.intermediate_size
+
+    X = self.model.embed_tokens(input_ids)
+    X = X.to(self.config.torch_dtype)
+    bsz, q_len, hd = X.shape
+    assert(q_len == 1)
+    
+    # Get saved buffers to reduce memory movement
+    residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+    _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+    XX, XX2 = _XX[0], _XX[1]
+    variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = ""cuda:0"")
+    temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
+    temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
+
     seq_len = past_key_values[0][0].shape[-2]
     if bsz != 1:
         attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
             attention_mask,
             (bsz, q_len),
-            hidden_states,
+            X,
             seq_len,
             sliding_window = getattr(self.config, ""sliding_window"", None),
         )
@@ -919,30 +948,54 @@ def LlamaModel_fast_forward_inference(
     pass
 
     next_decoder_cache = []
+
     for idx, decoder_layer in enumerate(self.model.layers):
-        residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)
-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+        residual.copy_(X) # residual = X
+        X = fast_rms_layernorm_inference(
+            decoder_layer.input_layernorm,
+            X,
+            XX = XX,
+            XX2 = XX2,
+            variance = variance,
+        )
+        X, present_key_value = LlamaAttention_fast_forward_inference(
             decoder_layer.self_attn,
-            hidden_states = hidden_states,
+            hidden_states = X,
             past_key_value = past_key_values[idx],
             position_ids = position_ids,
             attention_mask = attention_mask,
             do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
         )
-        hidden_states += residual
-
-        residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
-        hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)
-        hidden_states += residual
+        X += residual
+
+        residual.copy_(X) # residual = X
+        X = fast_rms_layernorm_inference(
+            decoder_layer.post_attention_layernorm,
+            X,
+            XX = XX,
+            XX2 = XX2,
+            variance = variance,
+        )
+        X = fast_swiglu_inference(
+            decoder_layer.mlp,
+            X,
+            temp_gate = temp_gate,
+            temp_up = temp_up,
+        )
+        X += residual
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)
+    X = fast_rms_layernorm_inference(
+        self.model.norm,
+        X,
+        XX = XX,
+        XX2 = XX2,
+        variance = variance,
+    )
 
     return BaseModelOutputWithPast(
-        last_hidden_state = hidden_states,
+        last_hidden_state = X,
         past_key_values = next_decoder_cache,
         hidden_states = [],
         attentions = [],
@@ -977,7 +1030,7 @@ def CausalLM_fast_forward(fast_forward_inference):
                 attention_mask = attention_mask,
             )
         else:
-            causal_mask = xformers.attn_bias.LowerTriangularMask()
+            causal_mask = xformers.attn_bias.LowerTriangularMask() if HAS_XFORMERS else None
 
             output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
             output_hidden_states = (
@@ -1159,7 +1212,8 @@ class LlamaRotaryEmbedding(torch.nn.Module):
             # [TODO] Hack to pass in config - need to remove later
             base = config.rope_theta
             partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
-            dim = int((config.hidden_size // config.num_attention_heads))
+            dim = getattr(config, ""head_dim"", None)
+            if dim is None: dim = int((config.hidden_size // config.num_attention_heads))
             device = ""cuda""
             max_position_embeddings = config.max_position_embeddings
         pass
@@ -1580,9 +1634,18 @@ class FastLlamaModel:
         model_patcher     = None,
         tokenizer_name    = None,
         trust_remote_code = False,
+
+        fast_inference    = False, # uses vLLM
+        gpu_memory_utilization = 0.5,
+        float8_kv_cache   = False,
+        random_state      = 3407,
+        max_lora_rank     = 16,
+        disable_log_stats = False,
         **kwargs,
     ):
         if trust_remote_code:
+            if fast_inference:
+                raise NotImplementedError(""Unsloth: Fast inference does not support `trust_remote_code` yet."")
             print(
                 ""Unsloth: WARNING `trust_remote_code` is True.\n""\
                 ""Are you certain you want to do remote code execution?""
@@ -1596,9 +1659,9 @@ class FastLlamaModel:
 
         statistics = \
            f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
-           f""O^O/ \_/ \\    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
-           f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
+           f""   {chr(92)}{chr(92)}   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
+           f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
 
@@ -1626,7 +1689,11 @@ class FastLlamaModel:
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
         # RoPE Scaling
-        model_config = AutoConfig.from_pretrained(model_name, token = token)
+        model_config = AutoConfig.from_pretrained(
+            model_name,
+            token = token,
+            attn_implementation = ""sdpa"",
+        )
         model_max_seq_length = model_config.max_position_embeddings
 
         # Check if RoPE Scaling is even allowed
@@ -1647,6 +1714,9 @@ class FastLlamaModel:
 
             rope_scaling = max_seq_length / model_max_seq_length
 
+            if fast_inference:
+                raise NotImplementedError(""Unsloth: Fast inference does not yet work with RoPE Scaling."")
+
             logger.warning_once(
                 f""Unsloth: {model_name} can only handle sequence lengths of at most ""\
                 f""{model_max_seq_length}.\nBut with kaiokendev's RoPE scaling of ""\
@@ -1688,17 +1758,54 @@ class FastLlamaModel:
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
         
-        model = AutoModelForCausalLM.from_pretrained(
-            model_name,
-            device_map              = device_map,
-            torch_dtype             = dtype,
-            # quantization_config     = bnb_config,
-            token                   = token,
-            max_position_embeddings = max_position_embeddings,
-            trust_remote_code       = trust_remote_code,
-            attn_implementation     = ""eager"",
-            **kwargs,
-        )
+        if not fast_inference:
+            model = AutoModelForCausalLM.from_pretrained(
+                model_name,
+                device_map              = device_map,
+                torch_dtype             = dtype,
+                # quantization_config     = bnb_config,
+                token                   = token,
+                max_position_embeddings = max_position_embeddings,
+                trust_remote_code       = trust_remote_code,
+                attn_implementation     = ""eager"",
+                **kwargs,
+            )
+        else:
+            from unsloth_zoo.vllm_utils import (
+                load_vllm,
+                get_vllm_state_dict,
+                convert_vllm_to_huggingface,
+                generate_batches,
+            )
+            allowed_args = inspect.getfullargspec(load_vllm).args
+            load_vllm_kwargs = dict(
+                model_name             = model_name,
+                config                 = model_config,
+                gpu_memory_utilization = gpu_memory_utilization,
+                max_seq_length         = max_seq_length,
+                dtype                  = dtype,
+                float8_kv_cache        = float8_kv_cache,
+                enable_lora            = True,
+                max_lora_rank          = max_lora_rank,
+                disable_log_stats      = disable_log_stats,
+            )
+            for allowed_arg in allowed_args:
+                if allowed_arg not in load_vllm_kwargs and allowed_arg in kwargs:
+                    load_vllm_kwargs[allowed_arg] = kwargs[allowed_arg]
+            pass
+
+            # Load vLLM first
+            llm = load_vllm(**load_vllm_kwargs)
+
+            # Convert to HF format
+            _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)
+            model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype)
+            model.vllm_engine = llm
+            model.fast_generate = model.vllm_engine.generate
+
+            from functools import partial
+            model.fast_generate_batches = partial(generate_batches, model.vllm_engine)
+        pass
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
         # We currently only support NVIDIA GPUs - AMD / Intel is a work in progress!
@@ -2194,6 +2301,20 @@ class FastLlamaModel:
             modules_to_save = list(set(modules_to_save))
         pass
 
+        vllm_engine = None
+        if hasattr(model, ""vllm_engine""):
+            # Fast inference!
+            vllm_engine = model.vllm_engine
+            vllm_fast_generate = model.fast_generate
+            vllm_fast_generate_batches = model.fast_generate_batches
+
+            if modules_to_save is not None:
+                raise NotImplementedError(""Unsloth: Currently fast inference does not work with training embeddings or lm_head."")
+
+            if bias != ""none"":
+                raise NotImplementedError(""Unsloth: Currently fast inference does not work with using biases for LoRA."")
+        pass
+
         # Get LoRA
         arguments = dict(
             r                   = r,
@@ -2300,6 +2421,19 @@ class FastLlamaModel:
             torch.cuda.empty_cache()
         pass
 
+        # Patch for fast inference
+        if vllm_engine is not None:
+            model.vllm_engine = vllm_engine
+            model.fast_generate = vllm_fast_generate
+            model.fast_generate_batches = vllm_fast_generate_batches
+
+            # Also saving and loading LoRA
+            from functools import partial
+            from unsloth_zoo.vllm_utils import save_lora, load_lora
+            model.save_lora = partial(save_lora, model)
+            model.load_lora = partial(load_lora, model)
+        pass
+
         return model
     pass
 
@@ -2509,18 +2643,24 @@ class FastLlamaModel:
         #     return
         # pass
 
-        internal_model = model
-        internal_model.gradient_checkpointing = False
-        internal_model.training = False
-
-        while hasattr(internal_model, ""model""):
-            internal_model = internal_model.model
-            internal_model.gradient_checkpointing = False
-            internal_model.training = False
-        pass
-        if hasattr(internal_model, ""training""):
-            internal_model.training = False
-        pass
+        m = model
+        while hasattr(m, ""model""):
+            if hasattr(m, ""gradient_checkpointing""):
+                m.gradient_checkpointing = False
+            if hasattr(m, ""training""):
+                m.training = False
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""):
+                m._saved_temp_tokenizer.padding_side = ""left""
+            m = m.model
+        pass
+        if hasattr(m, ""gradient_checkpointing""):
+            m.gradient_checkpointing = False
+        if hasattr(m, ""training""):
+            m.training = False
+        # Pad tokenizer to the left
+        if hasattr(m, ""_saved_temp_tokenizer""):
+            m._saved_temp_tokenizer.padding_side = ""left""
 
         # Also check if lm_head / embeddings are trained
         internal_model = model
@@ -2529,30 +2669,13 @@ class FastLlamaModel:
         pass
         lm_head = internal_model.lm_head.weight
         device_type = lm_head.device.type
-        dtype = model.config.torch_dtype
-        
-        if type(dtype) is str:
-            if   dtype ==  ""float16"": dtype = torch.float16
-            elif dtype == ""bfloat16"": dtype = torch.bfloat16
-        pass
+        dtype = _get_dtype(model.config.torch_dtype)
 
         # Wrap model.generate
         if model.generate.__name__ != ""_fast_generate"":
             model._unwrapped_old_generate = model.generate
             model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)
         pass
-        
-        # Patch tokenizer to pad to the left
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""left""
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""left""
-        pass
 
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -2570,9 +2693,6 @@ class FastLlamaModel:
 
     @staticmethod
     def for_training(model, use_gradient_checkpointing = True):
-        internal_model = model
-        internal_model.gradient_checkpointing = use_gradient_checkpointing
-        internal_model.training = True
 
         # Delete all fast inference loras
         for param in model.parameters():
@@ -2580,14 +2700,24 @@ class FastLlamaModel:
                 del param._fast_lora
         pass
 
-        while hasattr(internal_model, ""model""):
-            internal_model = internal_model.model
-            internal_model.gradient_checkpointing = use_gradient_checkpointing
-            internal_model.training = True
-        pass
-        if hasattr(internal_model, ""training""):
-            internal_model.training = True
-        pass
+        m = model
+        while hasattr(m, ""model""):
+            if hasattr(m, ""gradient_checkpointing""):
+                m.gradient_checkpointing = use_gradient_checkpointing
+            if hasattr(m, ""training""):
+                m.training = True
+            # Pad tokenizer to the right
+            if hasattr(m, ""_saved_temp_tokenizer""):
+                m._saved_temp_tokenizer.padding_side = ""right""
+            m = m.model
+        pass
+        if hasattr(m, ""gradient_checkpointing""):
+            m.gradient_checkpointing = use_gradient_checkpointing
+        if hasattr(m, ""training""):
+            m.training = True
+        # Pad tokenizer to the right
+        if hasattr(m, ""_saved_temp_tokenizer""):
+            m._saved_temp_tokenizer.padding_side = ""right""
 
         # Also revert model.generate
         if hasattr(model, ""_unwrapped_old_generate""):
@@ -2595,18 +2725,6 @@ class FastLlamaModel:
             del model._unwrapped_old_generate
         pass
 
-        # Patch tokenizer to pad to the right
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""right""
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""right""
-        pass
-
         # Also re-enable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
             embeddings = model.get_input_embeddings()
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index e9caad0..39b367e 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -30,11 +30,11 @@ except:
     from huggingface_hub.utils._token import get_token
 pass
 from huggingface_hub import HfFileSystem
+import importlib.util
 
 # [TODO] Move USE_MODELSCOPE to utils
 USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
 if USE_MODELSCOPE:
-    import importlib
     if importlib.util.find_spec(""modelscope"") is None:
         raise ImportError(f'You are using the modelscope hub, please install modelscope by `pip install modelscope -U`')
     pass
@@ -73,9 +73,25 @@ class FastLanguageModel(FastLlamaModel):
         resize_model_vocab         = None,
         revision                   = None,
         use_exact_model_name       = False,
+
+        fast_inference             = False, # uses vLLM
+        gpu_memory_utilization     = 0.5,
+        float8_kv_cache            = False,
+        random_state               = 3407,
+        max_lora_rank              = 64,
+        disable_log_stats          = True,
         *args, **kwargs,
     ):
         if token is None: token = get_token()
+
+        if fast_inference:
+            if importlib.util.find_spec(""vllm"") is None:
+                raise ImportError(
+                    ""Unsloth: Please install vLLM before enabling `fast_inference`!\n""\
+                    ""You can do this in a terminal via `pip install vllm`""
+                )
+            pass
+        pass
         
         old_model_name = model_name
         if not use_exact_model_name:
@@ -255,6 +271,24 @@ class FastLanguageModel(FastLlamaModel):
             tokenizer_name = None
         pass
 
+        if fast_inference:
+            from unsloth_zoo.vllm_utils import (
+                patch_vllm, 
+                vllm_dynamic_quant_supported,
+            )
+            patch_vllm()
+            if model_name.endswith(""unsloth-bnb-4bit""):
+                if not vllm_dynamic_quant_supported(model_name, model_config):
+                    # Instead use -bnb-4bit variant
+                    print(
+                        f""Unsloth: Switching from Unsloth dynamic quant to normal quant since\n""\
+                        f""we do not yet support fast inference for {model_name}""
+                    )
+                    model_name = model_name[:-len(""unsloth-bnb-4bit"")] + ""bnb-4bit""
+                pass
+            pass
+        pass
+
         model, tokenizer = dispatch_model.from_pretrained(
             model_name        = model_name,
             max_seq_length    = max_seq_length,
@@ -268,6 +302,13 @@ class FastLanguageModel(FastLlamaModel):
             tokenizer_name    = tokenizer_name,
             trust_remote_code = trust_remote_code,
             revision          = revision if not is_peft else None,
+
+            fast_inference    = fast_inference,
+            gpu_memory_utilization = gpu_memory_utilization,
+            float8_kv_cache   = float8_kv_cache,
+            random_state      = random_state,
+            max_lora_rank     = max_lora_rank,
+            disable_log_stats = disable_log_stats,
             *args, **kwargs,
         )
         
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index bc01c28..c81290b 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -304,25 +304,30 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Mistral-Small-Instruct-2409"",
         ""mistralai/Mistral-Small-Instruct-2409"",
     ),
-    ""unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-0.5B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-0.5B-Instruct"",
         ""Qwen/Qwen2.5-0.5B-Instruct"",
+        ""unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-1.5B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-1.5B-Instruct"",
         ""Qwen/Qwen2.5-1.5B-Instruct"",
+        ""unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-3B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-3B-Instruct"",
         ""Qwen/Qwen2.5-3B-Instruct"",
+        ""unsloth/Qwen2.5-3B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-7B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-7B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-7B-Instruct"",
         ""Qwen/Qwen2.5-7B-Instruct"",
+        ""unsloth/Qwen2.5-7B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-14B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-14B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-14B-Instruct"",
         ""Qwen/Qwen2.5-14B-Instruct"",
+        ""unsloth/Qwen2.5-14B-Instruct-bnb-4bit"",
     ),
     ""unsloth/Qwen2.5-32B-Instruct-bnb-4bit"" : (
         ""unsloth/Qwen2.5-32B-Instruct"",
@@ -332,25 +337,30 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Qwen2.5-72B-Instruct"",
         ""Qwen/Qwen2.5-72B-Instruct"",
     ),
-    ""unsloth/Qwen2.5-0.5B-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-0.5B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-0.5B"",
         ""Qwen/Qwen2.5-0.5B"",
+        ""unsloth/Qwen2.5-0.5B-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-1.5B-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-1.5B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-1.5B"",
         ""Qwen/Qwen2.5-1.5B"",
+        ""unsloth/Qwen2.5-1.5B-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-3B-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-3B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-3B"",
         ""Qwen/Qwen2.5-3B"",
+        ""unsloth/Qwen2.5-3B-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-7B-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-7B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-7B"",
         ""Qwen/Qwen2.5-7B"",
+        ""unsloth/Qwen2.5-7B-bnb-4bit"",
     ),
-    ""unsloth/Qwen2.5-14B-bnb-4bit"" : (
+    ""unsloth/Qwen2.5-14B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-14B"",
         ""Qwen/Qwen2.5-14B"",
+        ""unsloth/Qwen2.5-14B-bnb-4bit"",
     ),
     ""unsloth/Qwen2.5-32B-bnb-4bit"" : (
         ""unsloth/Qwen2.5-32B"",
@@ -555,12 +565,12 @@ __INT_TO_FLOAT_MAPPER = \
         ""deepseek-ai/DeepSeek-R1-Distill-Llama-70B"",
     ),
     ""unsloth/Mistral-Small-24B-Base-2501-unsloth-bnb-4bit"" : (
-        ""unsloth/Mistral-Small-24B-Base"",
+        ""unsloth/Mistral-Small-24B-Base-2501"",
         ""mistralai/Mistral-Small-24B-Base-2501"",
         ""unsloth/Mistral-Small-24B-Base-2501-bnb-4bit"",
     ),
     ""unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit"" : (
-        ""unsloth/Mistral-Small-24B-Instruct"",
+        ""unsloth/Mistral-Small-24B-Instruct-2501"",
         ""mistralai/Mistral-Small-24B-Instruct-2501"",
         ""unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit"",
     ),
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
new file mode 100644
index 0000000..22e1e0f
--- /dev/null
+++ b/unsloth/models/rl.py
@@ -0,0 +1,423 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+__all__ = [
+    ""PatchFastRL"",
+]
+
+METRICS_MOVE_TO_END = [
+    ""nll"",
+    ""aux"",
+    ""beta"",
+    ""alpha"",
+]
+import torch
+try:
+    from transformers.utils.notebook import (
+        IntervalStrategy,
+        NotebookTrainingTracker,
+        NotebookProgressCallback,
+    )
+    HAS_NOTEBOOK = True
+except:
+    HAS_NOTEBOOK = False
+pass
+from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union
+import inspect
+import os
+import re
+import functools
+from unsloth_zoo.compiler import create_new_function
+
+
+def PatchRL(FastLanguageModel):
+
+    from trl.models.utils import unwrap_model_for_generation
+    from contextlib import contextmanager
+
+    @contextmanager
+    def unsloth_unwrap_model_for_generation(model, accelerator):
+        with unwrap_model_for_generation(model, accelerator) as unwrapped_model:
+            # Put the model in inference mode.
+            FastLanguageModel.for_inference(unwrapped_model)
+
+            # We must use .clone for Unsloth since we force inference_mode
+            # Rather we should have used no_grad
+            original_generate = unwrapped_model.generate
+            def generate_with_clone(*args, **kwargs):
+                out = original_generate(*args, **kwargs)
+                if isinstance(out, torch.Tensor):
+                    return out.clone()
+                return out
+            pass
+            unwrapped_model.generate = generate_with_clone
+
+            try:
+                yield unwrapped_model
+            finally:
+                # Restore generate and return
+                unwrapped_model.generate = original_generate
+                FastLanguageModel.for_training(model)
+            pass
+        pass
+    pass
+
+    import trl.trainer
+    trainers = dir(trl.trainer)
+    trainers = [x for x in trainers if x.endswith(""_trainer"")]
+    unwrap = ""unwrap_model_for_generation""
+    for trainer in trainers:
+        if hasattr(eval(f""trl.trainer.{trainer}""), unwrap):
+            exec(f""trl.trainer.{trainer}.{unwrap} = unsloth_{unwrap}"")
+    pass
+pass
+
+
+def NotebookProgressCallback_on_train_begin(Trainer_metrics):
+    def _NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):
+        self.first_column = ""Epoch"" if args.eval_strategy == IntervalStrategy.EPOCH else ""Step""
+        self.training_loss = 0
+        self.last_log = 0
+        column_names = [self.first_column] + [""Training Loss""]
+        if args.eval_strategy != IntervalStrategy.NO:
+            column_names.append(""Validation Loss"")
+        column_names += [x.replace(""/"", "" / "") for x in Trainer_metrics]
+        self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)
+    pass
+    return _NotebookProgressCallback_on_train_begin
+pass
+
+
+def NotebookProgressCallback_on_log(Trainer_metrics):
+    def _NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwargs):
+        # Only for when there is no evaluation
+        if args.eval_strategy == IntervalStrategy.NO and ""loss"" in logs:
+            values = {""Training Loss"": logs[""loss""]}
+            for metric in Trainer_metrics:
+                # Sometimes metric is not inside logs
+                try: values[metric.replace(""/"", "" / "")] = logs[metric]
+                except: pass
+            pass
+            # First column is necessarily Step since we're not in epoch eval strategy
+            values[""Step""] = state.global_step
+            self.training_tracker.write_line(values)
+        pass
+    pass
+    return _NotebookProgressCallback_on_log
+pass
+
+
+def NotebookTrainingTracker_write_line(Trainer_metrics):
+    set_Trainer_metrics = set(Trainer_metrics)
+    def _NotebookTrainingTracker_write_line(self, values):
+        """"""
+        Write the values in the inner table.
+
+        Args:
+            values (`Dict[str, float]`): The values to display.
+        """"""
+        if self.inner_table is None:
+            self.inner_table = [list(values.keys()), list(values.values())]
+        else:
+            columns = self.inner_table[0]
+            new_values = {}
+            for key, value in values.items():
+                lowered = key.lower()
+                if lowered in set_Trainer_metrics:
+                    new_values[lowered.replace(""/"", "" / "")] = value
+                else:
+                    new_values[key] = value
+            pass
+            values = new_values
+
+            self.inner_table[0] = columns
+            if len(self.inner_table) > 1:
+                last_values = self.inner_table[-1]
+                first_column = self.inner_table[0][0]
+                if last_values[0] != values[first_column]:
+                    # write new line
+                    self.inner_table.append([values[c] if c in values else ""No Log"" for c in columns])
+                else:
+                    # update last line
+                    new_values = values
+                    for c in columns:
+                        if c not in new_values.keys():
+                            new_values[c] = last_values[columns.index(c)]
+                    self.inner_table[-1] = [new_values[c] for c in columns]
+            else:
+                # Edit for evaluation purposes
+                self.inner_table.append([values[c] if c in values else 0 for c in columns])
+            pass
+        pass
+    pass
+    return _NotebookTrainingTracker_write_line
+pass
+
+
+def _PatchRLStatistics(metrics, algorithm):
+    if HAS_NOTEBOOK:
+        if len(metrics) == 0:
+            raise RuntimeError(f""Unsloth: RL statistics for {algorithm} failed with no metrics seen?"")
+        from transformers.trainer import is_in_notebook
+        if is_in_notebook():
+            # Patch DPO notebook printing
+            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line(metrics)
+            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
+            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin(metrics)
+            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log(metrics)
+        pass
+    pass
+pass
+
+
+@functools.cache
+def get_trl_metrics():
+    # Gets metrics so we can output them in notebooks
+
+    import trl.trainer
+    trainers = dir(trl.trainer)
+    trainers = [x for x in trainers if x.endswith(""_trainer"")]
+    filepath = inspect.getfile(trl.trainer)
+    filepath = os.path.split(filepath)[0]
+
+    all_metrics = dict()
+    for trainer in trainers:
+        filename = os.path.join(filepath, f""{trainer}.py"")
+        if not os.path.exists(filename): continue
+        with open(filename, ""r"") as file: file = file.read()
+
+        # Get metrics['kl'] or stats['kl']
+        metrics = re.findall(r""metrics\[[\""\']([^\""\']{1,})[\""\']\]"", file)
+        stats = re.findall(r""stats\[[\""\']([^\""\']{1,})[\""\']\]"", file)
+        metrics = metrics + stats
+
+        # Get optional f-strings
+        metrics_f = re.findall(r""metrics\[f[\""\']\{[^\}]{1,}\}([^\""\']{1,})[\""\']\]"", file)
+        stats_f = re.findall(r""stats\[f[\""\']\{[^\}]{1,}\}([^\""\']{1,})[\""\']\]"", file)
+        metrics_f = metrics_f + stats_f
+        # Filter out prefixes if seen
+        # metrics[f""{prefix}rewards/chosen""]
+        left_prefix = 'prefix = ""eval_"" if train_eval == ""eval"" else """"' in file
+        if left_prefix: metrics += metrics_f
+
+        # Move all eval_ things to the end and reward to the front
+        beginning = []
+        middle = []
+        end = []
+        for x in metrics:
+            lowered = x.lower()
+            if ""reward"" in lowered:
+                beginning.append(x)
+            elif x.lower().startswith(""eval""):
+                end.append(x)
+            else:
+                # Check if we want to move to the end
+                moved = False
+                for move_end in METRICS_MOVE_TO_END:
+                    if move_end in lowered:
+                        end.append(x)
+                        moved = True
+                        break
+                if not moved:
+                    middle.append(x)
+            pass
+        pass
+        metrics = beginning + middle + end
+
+        all_metrics[trainer[:trainer.find(""_"")].upper()] = metrics
+    pass
+    return all_metrics
+pass
+
+
+def PatchRLStatistics(algorithm = ""GRPO""):
+    # Get notebook statistics columns to show up
+    algorithm = algorithm.upper()
+    all_metrics = get_trl_metrics()
+    if algorithm not in all_metrics:
+        print(
+            f""Unsloth for {algorithm.upper()} is not yet implemented! Just ignore this function.\n""\
+            f""We support: `{list(all_metrics.keys())}`""
+        )
+    pass
+    _PatchRLStatistics(all_metrics[algorithm], algorithm)
+pass
+
+
+def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
+    # Patch for vLLM and Unsloth PEFT
+    import trl
+    import trl.trainer
+
+    trainer = eval(f""trl.trainer.{trainer_file}"")
+    name = [x for x in dir(trainer) if x.endswith(""Trainer"") and x != ""Trainer"" and trainer_file.split(""_"")[0] in x.lower()]
+    assert(len(name) == 1)
+    RLTrainer_name = name[0]
+    RLTrainer = eval(f""trl.trainer.{trainer_file}.{RLTrainer_name}"")
+
+    try:
+        __init__ = inspect.getsource(RLTrainer.__init__)
+    except:
+        # Already patched most likely!
+        return
+    old__init__ = __init__
+    all_imports = dir(trainer)
+    assert(""Union"" in all_imports)
+    imports = [x for x in all_imports if not x.startswith(""_"")]
+    imports += [""Trainer""]
+
+    spaces = __init__.find(""def"")
+    __init__ = __init__.split(""\n"")
+    __init__ = ""\n"".join(x[spaces:] for x in __init__)
+
+    # Replace vLLM sections since we already have it done!
+    vllm_part = re.findall(
+        r""(\n[\s]{4}""\
+        r""if (self|args)\.use_vllm\:.+?""\
+        r""\n[\s]{4,}""\
+        ""else:\n)"",
+        __init__,
+        flags = re.MULTILINE | re.DOTALL,
+    )
+    if (len(vllm_part) != 1): return
+
+    vllm_part, args = vllm_part[0][0], vllm_part[0][1]
+    # Strip all comments
+    new_vllm_part = re.sub(r""\#[^\n]{1,}\n"", """", vllm_part)
+
+    # Get SamplingParams
+    sampling_params = re.findall(
+        r""\n[\s]{4,}(self\.[^\s]{1,}[\s]{0,}\=[\s]{0,}""\
+        r""SamplingParams\(.+?\))"",
+        new_vllm_part,
+        flags = re.MULTILINE | re.DOTALL,
+    )
+    if len(sampling_params) != 1: return
+
+    sampling_params = sampling_params[0]
+    # Replace with our vLLM engine
+    sampling_params = \
+        "" ""*8 + ""self.llm = model.vllm_engine; self._last_loaded_step = 0; "" + \
+        sampling_params # Add spaces
+    new_vllm_part = f""\n    if {args}.use_vllm:\n{sampling_params}\n    else:\n""
+    __init__ = __init__.replace(vllm_part, new_vllm_part)
+
+    # Remove peft_config
+    __init__ = __init__.replace(""elif peft_config is None:"", ""elif False:"")
+    __init__ = __init__.replace(""elif peft_config is not None:"", ""elif False:"")
+    __init__ = __init__.replace(""if peft_config is None:"", ""if False:"")
+    __init__ = __init__.replace(""if peft_config is not None:"", ""if False:"")
+    __init__ = __init__.replace(""get_peft_model(model, peft_config)"", ""model"")
+
+    # Add spaces back into __init__
+    __init__ = __init__.split(""\n"")
+    __init__ = ""\n"".join(' '*spaces + x for x in __init__)
+
+    # Search for vLLM calling in all child functions
+    functions = dir(RLTrainer)
+    RLTrainer_source = inspect.getsource(RLTrainer)
+    functions = [x for x in functions if f""def {x}"" in RLTrainer_source]
+
+    changed = {""__init__"" : (old__init__, __init__,)}
+    for function in functions:
+        if not hasattr(RLTrainer, function): continue
+        fx = getattr(RLTrainer, function)
+        try:
+            source = inspect.getsource(fx)
+        except:
+            continue
+        original_source = source
+
+        # llm_model = self.llm.llm_engine.model_executor.driver_worker.model_runner.model
+        source = re.sub(
+            r""(\n[\s]{4,}).+?model_executor\.driver_worker.+?\n"",
+            r""\n\1pass\n"",
+            source,
+        )
+
+        # llm_model.load_weights(model.state_dict().items())
+        source = re.sub(
+            r""(\n[\s]{4,}).+?load_weights\(.+?\n"",
+            r""\n\1pass\n"",
+            source,
+        )
+
+        # .state_dict()
+        source = re.sub(
+            r""\.state_dict\(\)"",
+            r"""",
+            source,
+        )
+        
+        # Replace self.llm.generate and self.llm.chat
+        lora_name = trainer_file + ""_lora_model""
+        source = re.sub(
+            r""(self\.llm\.(?:generate|chat)\([^\)]{1,})\)"",
+            r""\1, lora_request = model.load_lora('"" + lora_name + r""', load_tensors = True))"",
+            source
+        )
+
+        # Skip if no changes done
+        if source == original_source: continue
+
+        # Find all imports
+        imports += [x for x in all_imports if not x.startswith(""_"") and x in source]
+
+        changed[function] = (original_source, source,)
+    pass
+
+    # Import all functions
+    imports = list(set(imports))
+
+    # Patch all functions
+    for function in changed:
+        old, new = changed[function]
+        RLTrainer_source = RLTrainer_source.replace(old, new)
+    pass
+    RLTrainer_source = RLTrainer_source.replace(
+        f""class {RLTrainer_name}"", f""class Unsloth{RLTrainer_name}"", 1
+    )
+
+    # Create new class in compiled cache and import it
+    module = create_new_function(
+        RLTrainer_name,
+        RLTrainer_source,
+        f""trl.trainer.{trainer_file}"",
+        imports,
+    )
+
+    # Patch over modules
+    exec(f""trl.{RLTrainer_name} = module.Unsloth{RLTrainer_name}"", locals(), globals())
+    exec(f""trl.trainer.{RLTrainer_name} = module.Unsloth{RLTrainer_name}"", locals(), globals())
+    exec(f""trl.trainer.{trainer_file}.{RLTrainer_name} = module.Unsloth{RLTrainer_name}"", locals(), globals())
+    return module
+pass
+
+
+def patch_trl_rl_trainers():
+    # Patch all TRL modules if they have vLLM or PEFT
+    import trl.trainer
+    all_trainers = dir(trl.trainer)
+    all_trainers = [x for x in all_trainers if x.islower() and x.endswith(""_trainer"")]
+    for trainer in all_trainers:
+        _patch_trl_rl_trainers(trainer)
+    return
+pass
+
+
+def PatchFastRL(algorithm = ""GRPO"", FastLanguageModel = None):
+    if FastLanguageModel is not None: PatchRL(FastLanguageModel)
+    patch_trl_rl_trainers()
+    PatchRLStatistics(algorithm)
+pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 69f36f0..3a29352 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.11.9""
+__version__ = ""2024.11.10""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -54,6 +54,7 @@ __all__ = [
     ""unpatch_gradient_checkpointing"",
 
     ""HAS_CUT_CROSS_ENTROPY"",
+    ""EMPTY_LOGITS"",
     ""fused_linear_cross_entropy"",
     ""patch_unsloth_smart_gradient_checkpointing"",
     ""unpatch_unsloth_smart_gradient_checkpointing"",
@@ -1128,14 +1129,27 @@ def unsloth_compile_transformers(
     debug                   = False,
     import_from_cache       = False,
     disable                 = False,
+    return_logits           = False,
 ):
+    if Version(torch_version) < Version(""2.4.0""):
+        print(
+            ""=""*30 + \
+            ""Unsloth: Unfortunately Unsloth vision and other newer optimized models need Torch 2.4 or later.\n""\
+            f""You have Torch version {torch_version}. Please upgrade your Torch version by visiting https://pytorch.org/\n""\
+            ""For now your models will not get optimized, but will still work for now!""
+        )
+        return
+    pass
+
     if disable: return
+
     model_types = get_transformers_model_type(
         model_name        = model_name,
         token             = token,
         revision          = revision,
         trust_remote_code = trust_remote_code,
     )
+
     for model_type in model_types:
         _unsloth_compile_transformers(
             model_type,
@@ -1158,7 +1172,36 @@ def unsloth_compile_transformers(
             debug                  = debug,
             import_from_cache      = import_from_cache,
             disable                = disable,
+            return_logits          = return_logits,
         )
     pass
     return model_types
 pass
+
+# We need an empty logits flag to warn people logits will not be returned anymore unless asked ie
+# os.environ['UNSLOTH_RETURN_LOGITS'] = '1'
+LOGITS_ERROR_STRING = \
+    ""Unsloth: Logits are empty from 2024.11 onwards. To get raw logits again, please ""\
+    'set the environment variable `UNSLOTH_RETURN_LOGITS` to `""1"" BEFORE starting to train ie before `trainer.train()`. For example:\n\n'\
+    ""import os\n""\
+    ""os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n""\
+    ""... trainer.train() ...""
+
+def raise_logits_error(*args, **kwargs): raise NotImplementedError(LOGITS_ERROR_STRING)
+def return_none(*args, **kwargs): return None
+class EmptyLogits:
+    def __init__(self): return
+    def raise_getattr_error(self, attr): return return_none if attr == ""to"" else raise_logits_error
+    __getitem__ = raise_logits_error
+    __getattr__ = raise_getattr_error
+    def __repr__(self): return LOGITS_ERROR_STRING
+    def __str__ (self): return LOGITS_ERROR_STRING
+pass
+EMPTY_LOGITS = EmptyLogits()
+functions = dir(torch.Tensor)
+for j, function in enumerate(functions):
+    if function.startswith(""__"") and function.endswith(""__""):
+        exec(f""def raise_{j}(*args, **kwargs): print('{function}')"", globals(), locals())
+        try: exec(f""EMPTY_LOGITS.{function} = raise_{j}"", globals(), locals())
+        except: continue
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 0256fc1..bb5c841 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -65,7 +65,7 @@ except:
     # Old HF Hub versions <= 0.0.25
     from huggingface_hub.utils._token import get_token
 pass
-
+from triton import __version__ as triton_version
 
 def original_apply_qkv(self, X):
     Q = self.q_proj(X)
@@ -980,7 +980,8 @@ def CausalLM_fast_forward(fast_forward_inference):
         elif num_logits_to_keep != 0:
             logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
         else:
-            if HAS_CUT_CROSS_ENTROPY and labels is not None:
+            RETURN_LOGITS = os.environ.get(""UNSLOTH_RETURN_LOGITS"", ""0"") == ""1""
+            if not RETURN_LOGITS and HAS_CUT_CROSS_ENTROPY and labels is not None:
                 n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
                 loss = fused_linear_cross_entropy(
                     hidden_states      = hidden_states,
@@ -993,13 +994,14 @@ def CausalLM_fast_forward(fast_forward_inference):
                     output = (logits,) + outputs[1:]
                     return (loss,) + output if loss is not None else output
 
-                return CausalLMOutputWithPast(
+                output = CausalLMOutputWithPast(
                     loss=loss,
-                    logits=None,
+                    logits=EMPTY_LOGITS,
                     past_key_values=outputs.past_key_values,
                     hidden_states=outputs.hidden_states,
                     attentions=outputs.attentions,
                 )
+                return output
             pass
             logits = self.lm_head(hidden_states.to(lm_head.dtype))
         pass
@@ -1547,9 +1549,9 @@ class FastLlamaModel:
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers = {transformers_version}.\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
-           f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers:{transformers_version}.\n""\
+           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""O^O/ \_/ \\    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 232fe6a..f8ed3a8 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -347,6 +347,7 @@ class FastVisionModel(FastBaseVisionModel):
         use_gradient_checkpointing = ""unsloth"",
         resize_model_vocab         = None, # [TODO] No effect
         revision                   = None,
+        return_logits              = False, # Return logits
         *args, **kwargs,
     ):
         if token is None: token = get_token()
@@ -359,37 +360,11 @@ class FastVisionModel(FastBaseVisionModel):
         old_model_name = model_name
         model_name = get_model_name(model_name, load_in_4bit)
 
-        with contextlib.redirect_stdout(open(os.devnull, ""w"")):
-            patch_loss_functions(torch_compile = False)
-            model_types = unsloth_compile_transformers(
-                model_name              = model_name,
-                sdpa_dynamic_mask       = True,
-                sdpa_bool_masks         = True,
-                sdpa_gqa_replace        = True,
-                sdpa_dynamic_compile    = True,
-                compile_attention       = True,
-                disable_causal_masks    = True,
-                compile_torch_modules   = True,
-                compile_custom_modules  = True,
-                compile_function_calls  = True,
-                fuse_lm_head            = True,
-                gradient_checkpointing  = True,
-                manual_replacements     = True,
-                epilogue_fusion         = True,
-                max_autotune            = False,
-                shape_padding           = True,
-                cudagraphs              = False,
-                debug                   = False,
-                import_from_cache       = False,
-                disable                 = False,
-            )
-        pass
-
         # First check if it's a normal model via AutoConfig
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         was_disabled = are_progress_bars_disabled()
         disable_progress_bars()
-
+        
         autoconfig_error = None
         peft_error = None
         try:
@@ -473,6 +448,33 @@ class FastVisionModel(FastBaseVisionModel):
 
         if not was_disabled: enable_progress_bars()
 
+        with contextlib.redirect_stdout(open(os.devnull, ""w"")):
+            patch_loss_functions(torch_compile = False)
+            model_types = unsloth_compile_transformers(
+                model_name              = model_name,
+                sdpa_dynamic_mask       = True,
+                sdpa_bool_masks         = True,
+                sdpa_gqa_replace        = True,
+                sdpa_dynamic_compile    = True,
+                compile_attention       = True,
+                disable_causal_masks    = True,
+                compile_torch_modules   = True,
+                compile_custom_modules  = True,
+                compile_function_calls  = True,
+                fuse_lm_head            = True,
+                gradient_checkpointing  = True,
+                manual_replacements     = True,
+                epilogue_fusion         = True,
+                max_autotune            = False,
+                shape_padding           = True,
+                cudagraphs              = False,
+                debug                   = False,
+                import_from_cache       = False,
+                disable                 = False,
+                return_logits           = return_logits,
+            )
+        pass
+
         # Check if this is local model since the tokenizer gets overwritten
         if  os.path.exists(os.path.join(old_model_name, ""tokenizer_config.json"")) and \
             os.path.exists(os.path.join(old_model_name, ""tokenizer.json"")) and \
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index fc1dc8c..b2f73aa 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -492,6 +492,14 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/llava-v1.6-mistral-7b-hf"",
         ""llava-hf/llava-v1.6-mistral-7b-hf"",
     ),
+    ""unsloth/Llama-3.1-Tulu-3-8B-bnb-4bit"" : (
+        ""unsloth/Llama-3.1-Tulu-3-8B"",
+        ""allenai/Llama-3.1-Tulu-3-8B"",
+    ),
+    ""unsloth/Llama-3.1-Tulu-3-70B-bnb-4bit"" : (
+        ""unsloth/Llama-3.1-Tulu-3-70B"",
+        ""allenai/Llama-3.1-Tulu-3-70B"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 69fb3fd..80c1f82 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -31,6 +31,7 @@ from unsloth_zoo.peft_utils import (
     get_peft_regex,
     merge_and_overwrite_lora,
 )
+from triton import __version__ as triton_version
 
 __all__ = [
     ""FastBaseVisionModel"",
@@ -95,9 +96,9 @@ class FastBaseVisionModel:
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} vision patching. Transformers = {transformers_version}.\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
-           f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} vision patching. Transformers: {transformers_version}.\n""\
+           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""O^O/ \_/ \\    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 5f9b41d..590465a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -369,6 +369,7 @@ def LlamaModel_fast_forward(
         raise ValueError(""Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds"")
 
     seq_length_with_past = seq_length
+    assert(seq_length <= self.max_seq_length)
     past_key_values_length = 0
 
     if past_key_values is not None:
@@ -661,6 +662,9 @@ class FastLlamaModel:
                 bnb_4bit_compute_dtype    = dtype,
             )
 
+        # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12
+        # RoPE Scaling's max_position_embeddings must be updated
+        max_position_embeddings = max(max_seq_length, model_max_seq_length)
         model = AutoModelForCausalLM.from_pretrained(
             model_name,
             device_map = device_map,
@@ -668,6 +672,7 @@ class FastLlamaModel:
             quantization_config = bnb_config,
             token = token,
             rope_scaling = rope_scaling,
+            max_position_embeddings = max_position_embeddings,
         )
         tokenizer = AutoTokenizer.from_pretrained(
             model_name,
@@ -685,7 +690,7 @@ class FastLlamaModel:
             layer.self_attn.apply_o   = original_apply_o
         pass
 
-        model.max_seq_length = max_seq_length
+        model.max_seq_length = max_position_embeddings
         return model, tokenizer
     pass
 
@@ -746,7 +751,7 @@ class FastLlamaModel:
         layers_to_transform = None,
         use_gradient_checkpointing = True,
         random_state = 3407,
-        max_seq_length = 2048,
+        max_seq_length = 2048, # not used anymore
         **kwargs,
     ):
         assert(max_seq_length <= model.max_seq_length)
@@ -824,6 +829,7 @@ class FastLlamaModel:
 
         # Patch cross entropy loss labels
         # Fixes https://github.com/unslothai/unsloth/issues/10
+        max_seq_length = model.max_seq_length
         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda"")
         model.model.extra_ignored_labels = extra_ignored_labels
         internal_model = model
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index f3973c9..865493e 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -125,7 +125,7 @@ def MistralAttention_fast_forward(
         V = V.transpose(1, 2)
 
         # Flash Attention v2 auto supports grouped query attention
-        sliding_window = self.config.sliding_window
+        sliding_window = getattr(self.config, ""sliding_window"")
         sliding_window = q_len if sliding_window is None else sliding_window
         window = (-1, -1) if (q_len <= sliding_window) else (sliding_window, sliding_window)
         A = flash_attn_func(Q, K, V, causal = True, window_size = window)
@@ -169,7 +169,7 @@ def MistralForCausalLM_fast_forward(
 
     if causal_mask is None:
         bsz, q_len = input_ids.shape
-        sliding_window = self.config.sliding_window
+        sliding_window = getattr(self.config, ""sliding_window"")
         if sliding_window is None or sliding_window <= 0:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
         elif q_len <= sliding_window:
@@ -312,7 +312,7 @@ class FastMistralModel(FastLlamaModel):
             layer.self_attn.apply_o   = original_apply_o
         pass
 
-        model.max_seq_length = max_seq_length
+        model.max_seq_length = max(max_seq_length, model.config.max_position_embeddings)
         return model, tokenizer
     pass
 pass
"
"diff --git a/pyproject.toml b/pyproject.toml
index 7e8956c..385366a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,8 +33,8 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
-    ""transformers>=4.38.0"",
-    ""datasets"",
+    ""transformers>=4.38.2"",
+    ""datasets>=2.16.0"",
     ""sentencepiece"",
     ""accelerate>=0.26.1"",
     ""trl>=0.7.9"",
@@ -64,6 +64,16 @@ cu121onlytorch211 = [
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
+cu118onlytorch212 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+]
+cu121onlytorch212 = [
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+]
 cu118onlytorch220 = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 9c231e6..8ff255e 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -16,11 +16,17 @@ from .cross_entropy_loss import fast_cross_entropy_loss
 from .rms_layernorm import fast_rms_layernorm
 from .rope_embedding import fast_rope_embedding, inplace_rope_embedding
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
-from .geglu import geglu_forward_kernel, geglu_backward_kernel
+from .geglu import (
+	geglu_exact_forward_kernel,
+	geglu_exact_backward_kernel,
+	geglu_approx_forward_kernel,
+	geglu_approx_backward_kernel,
+)
 from .fast_lora import (
 	get_lora_parameters,
 	apply_lora_mlp_swiglu,
-	apply_lora_mlp_geglu,
+	apply_lora_mlp_geglu_exact,
+	apply_lora_mlp_geglu_approx,
 	apply_lora_qkv,
 	apply_lora_o,
 )
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index 3ed0d3c..6568bba 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -183,8 +183,8 @@ def apply_lora_mlp_swiglu(self, X):
 pass
 
 
-from .geglu import geglu_forward_kernel, geglu_backward_kernel
-def apply_lora_mlp_geglu(self, X):
+from .geglu import geglu_exact_forward_kernel, geglu_exact_backward_kernel
+def apply_lora_mlp_geglu_exact(self, X):
     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
@@ -192,7 +192,21 @@ def apply_lora_mlp_geglu(self, X):
                          gateW, gateW_quant, gateA, gateB, gateS,
                          upW,     upW_quant, upA,   upB,   upS,
                          downW, downW_quant, downA, downB, downS,
-                         geglu_forward_kernel, geglu_backward_kernel,)
+                         geglu_exact_forward_kernel, geglu_exact_backward_kernel,)
+    return out
+pass
+
+
+from .geglu import geglu_approx_forward_kernel, geglu_approx_backward_kernel
+def apply_lora_mlp_geglu_approx(self, X):
+    gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
+    upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
+    downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
+    out = LoRA_MLP.apply(X,
+                         gateW, gateW_quant, gateA, gateB, gateS,
+                         upW,     upW_quant, upA,   upB,   upS,
+                         downW, downW_quant, downA, downB, downS,
+                         geglu_approx_forward_kernel, geglu_approx_backward_kernel,)
     return out
 pass
 
diff --git a/unsloth/kernels/geglu.py b/unsloth/kernels/geglu.py
index 7001b8f..df80fcb 100644
--- a/unsloth/kernels/geglu.py
+++ b/unsloth/kernels/geglu.py
@@ -19,7 +19,7 @@ from .utils import calculate_settings
 
 
 @triton.jit
-def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
+def _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
     block_idx = tl.program_id(0)
     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
     mask = offsets < n_elements
@@ -38,18 +38,18 @@ def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
 pass
 
 
-def geglu_forward_kernel(gate, up):
+def geglu_exact_forward_kernel(gate, up):
     batch, seq_len, hd = gate.shape
     n_elements = gate.numel()
     out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda"")
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
+    _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
     return out
 pass
 
 
 @triton.jit
-def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
+def _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
     """"""
     f = 1/2 * e * (1 + erf(1/sqrt(2) * e))
     h = f * up
@@ -95,10 +95,109 @@ def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
 pass
 
 
-def geglu_backward_kernel(DW, e, g):
+def geglu_exact_backward_kernel(DW, e, g):
     batch_seq_len, hd = e.shape
     n_elements = e.numel()
     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
-    _backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
+    _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
+    return DW, e, g
+pass
+
+
+@triton.jit
+def _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
+    block_idx = tl.program_id(0)
+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
+    mask = offsets < n_elements
+
+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))
+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))
+    # h = f * up
+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)
+    
+    e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)
+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)
+
+    f_row = 0.5 * e_row * (
+        tl.math.tanh(s * e_row * (1.0 + 0.044715 * e_row * e_row)) \
+        + 1.0
+    )
+    f_row = f_row.to(g_row.dtype) # Exact copy from HF
+    h_row = f_row * g_row
+
+    # Store h
+    tl.store(h + offsets, h_row, mask = mask)
+pass
+
+
+def geglu_approx_forward_kernel(gate, up):
+    batch, seq_len, hd = gate.shape
+    n_elements = gate.numel()
+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda"")
+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
+    _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
+    return out
+pass
+
+
+@triton.jit
+def _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
+    """"""
+    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))
+    h = f * up
+
+    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))
+    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +
+            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] * \
+                           ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )
+
+    Notice sech^2(x) = 1 - tanh^2(x)
+    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )
+
+    See https://www.desmos.com/calculator/nqprfoni6x
+    """"""
+    block_idx = tl.program_id(0)
+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
+    mask = offsets < n_elements
+
+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)
+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)
+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)
+
+    # See https://www.desmos.com/calculator/nqprfoni6x
+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)
+    a = s * e_row # a = sqrt(2 / pi) * x
+    b = a * 0.044715 * e_row * e_row # b = a * 0.044715 * x^2
+    T = 1.0 + tl.math.tanh(a + b)
+    T2 = 0.5 * T
+    # Q = 0.5 * -T * (T - 2.0) * (a + 3.0 * b)
+    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b) 
+    df_de = T2 + Q2 # 1/2 * (T + Q)
+
+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))
+    f_row = T2 * e_row
+    f_row = f_row.to(DW_row.dtype)
+    # h = f * g
+    h_row  =  f_row * g_row
+    # df = DW * f
+    df_row = DW_row * f_row
+    # dg = DW * g
+    dg_row = DW_row * g_row
+
+    de_row = dg_row.to(tl.float32) * df_de
+    de_row = de_row.to(DW_row.dtype)
+
+    # Store derivatives in buffers
+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g
+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f
+    tl.store(g  + offsets, de_row, mask = mask) # de
+pass
+
+
+def geglu_approx_backward_kernel(DW, e, g):
+    batch_seq_len, hd = e.shape
+    n_elements = e.numel()
+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
+    _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
     return DW, e, g
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index e92be5e..3e3b8ff 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -25,7 +25,7 @@ from platform import system as platform_system
 platform_system = platform_system()
 import math
 
-__version__ = ""2024.2""
+__version__ = ""2024.3""
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index 3ae4d63..b7c7305 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -12,11 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from transformers.utils.notebook import (
-    IntervalStrategy,
-    NotebookTrainingTracker,
-    NotebookProgressCallback,
-)
+try:
+    from transformers.utils.notebook import (
+        IntervalStrategy,
+        NotebookTrainingTracker,
+        NotebookProgressCallback,
+    )
+    HAS_NOTEBOOK = True
+except:
+    HAS_NOTEBOOK = False
+pass
 
 DPOTrainer_metrics = [
     ""rewards/chosen"",
@@ -101,13 +106,15 @@ pass
 
 
 def PatchDPOTrainer():
-    from transformers.trainer import is_in_notebook
-    if is_in_notebook():
-        # Patch DPO notebook printing
-        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
-        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
-        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
-        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
+    if HAS_NOTEBOOK:
+        from transformers.trainer import is_in_notebook
+        if is_in_notebook():
+            # Patch DPO notebook printing
+            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
+            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
+            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
+            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
+        pass
     pass
 pass
 
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 4aa634a..97da833 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -48,7 +48,7 @@ def fast_geglu_inference(self, X):
 
     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
-    gate = torch.nn.functional.gelu(gate)
+    gate = torch.nn.functional.gelu(gate, approximate = ""tanh"")
     gate *= up
 
     # X = self.down_proj(gate)
@@ -70,7 +70,7 @@ def GemmaDecoderLayer_fast_forward(
     padding_mask:         Optional[torch.LongTensor] = None,
     *args, **kwargs,
 ):
-    if False:#past_key_value is not None:
+    if past_key_value is not None:
         do_prefill = not hasattr(self.self_attn, ""paged_attention"")
 
         # Self Attention
@@ -267,6 +267,9 @@ class FastGemmaModel(FastLlamaModel):
         # Patch RMS Layernorm
         for name, module in model.named_modules():
             if isinstance(module, GemmaRMSNorm):
+                # Must be in float32
+                # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36
+                module = module.to(torch.float32)
                 module.weight += 1.0 # return output * (1 + self.weight)
                 if not hasattr(module, ""variance_epsilon""):
                     module.variance_epsilon = module.eps # Gemma doesn't use variance_epsilon
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 54e016d..2055264 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -511,7 +511,12 @@ def LlamaModel_fast_forward(
     # Mormalized from Gemma
     if self.config.model_type == ""gemma"":
         inputs_requires_grad = inputs_embeds.requires_grad
-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)
+        if not inputs_embeds.is_leaf:
+            inputs_embeds = inputs_embeds.detach()
+            inputs_requires_grad = True
+        elif inputs_requires_grad:
+            inputs_embeds.requires_grad_(False)
+        pass
         inputs_embeds *= math_sqrt(self.config.hidden_size)
         if inputs_requires_grad: inputs_embeds.requires_grad_(True)
     pass
@@ -522,7 +527,12 @@ def LlamaModel_fast_forward(
         # Careful for inference the attention_mask is size (1, kv_seq_len)
         # Whilst the input_embeds is size (1, 1, 4096)
         inputs_requires_grad = inputs_embeds.requires_grad
-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)
+        if not inputs_embeds.is_leaf:
+            inputs_embeds = inputs_embeds.detach()
+            inputs_requires_grad = True
+        elif inputs_requires_grad:
+            inputs_embeds.requires_grad_(False)
+        pass
         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)
         if inputs_requires_grad: inputs_embeds.requires_grad_(True)
     pass
@@ -1335,7 +1345,7 @@ class FastLlamaModel:
 
         if   model_type == ""llama"":   apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""mistral"": apply_lora_mlp = apply_lora_mlp_swiglu
-        elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu
+        elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu_approx
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
         pass
"
"diff --git a/pyproject.toml b/pyproject.toml
index 5a9d922..6bf4038 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -7,7 +7,7 @@ name = ""unsloth""
 dynamic = [""version""]
 description = ""2-5X faster LLM finetuning""
 readme = ""README.md""
-requires-python = "">=3.9,<=3.12""
+requires-python = "">=3.9,<3.13""
 license = {file = ""LICENSE""}
 keywords = [""ai"", ""llm"",]
 authors = [
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 66926bc..7d6bbfb 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.4""
+__version__ = ""2025.3.5""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6a111c9..6139115 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1387,11 +1387,15 @@ class FastLlamaModel:
         # RoPE Scaling's max_position_embeddings must be updated
         max_position_embeddings = max(max_seq_length, model_max_seq_length)
         kwargs.pop(""attn_implementation"", None); # No need since we auto call it
+
+        # Cannot be None, since HF now checks for the config
+        if load_in_4bit: kwargs[""quantization_config""] = bnb_config
+
         model = AutoModelForCausalLM.from_pretrained(
             model_name,
             device_map              = device_map,
             torch_dtype             = dtype,
-            quantization_config     = bnb_config,
+            # quantization_config     = bnb_config,
             token                   = token,
             max_position_embeddings = max_position_embeddings,
             trust_remote_code       = trust_remote_code,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index cce22ae..ad1098e 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -42,10 +42,11 @@ def __get_model_name(
     INT_TO_FLOAT_MAPPER = None,
     FLOAT_TO_INT_MAPPER = None,
 ):
-
     model_name = str(model_name)
-    if not SUPPORTS_FOURBIT and model_name.lower() in INT_TO_FLOAT_MAPPER:
-        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
+    lower_model_name = model_name.lower()
+
+    if not SUPPORTS_FOURBIT and lower_model_name in INT_TO_FLOAT_MAPPER:
+        model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
         logger.warning_once(
             f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
             f""4bit loading.\nThe minimum required version is 4.37.\n""\
@@ -55,16 +56,18 @@ def __get_model_name(
         )
         return model_name
     
-    elif not load_in_4bit and model_name.lower() in INT_TO_FLOAT_MAPPER:
-        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
+    elif not load_in_4bit and lower_model_name in INT_TO_FLOAT_MAPPER:
+        new_model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
         #     f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
         # )
         return new_model_name
-
-    elif load_in_4bit and SUPPORTS_FOURBIT and model_name.lower() in FLOAT_TO_INT_MAPPER:
-        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]
+    elif not load_in_4bit and lower_model_name in FLOAT_TO_INT_MAPPER:
+        new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
+        return new_model_name
+    elif load_in_4bit and SUPPORTS_FOURBIT and lower_model_name in FLOAT_TO_INT_MAPPER:
+        new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
         #     f""We shall load `{new_model_name}` for 4x faster loading.""
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6139115..6a23335 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1390,7 +1390,7 @@ class FastLlamaModel:
 
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
-
+        
         model = AutoModelForCausalLM.from_pretrained(
             model_name,
             device_map              = device_map,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index ad1098e..e260017 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -19,7 +19,7 @@ from .qwen2 import FastQwen2Model
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
 from peft import PeftConfig, PeftModel
-from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER
+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER, MAP_TO_UNSLOTH_16bit
 import os
 
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
@@ -39,13 +39,15 @@ pass
 def __get_model_name(
     model_name,
     load_in_4bit = True,
-    INT_TO_FLOAT_MAPPER = None,
-    FLOAT_TO_INT_MAPPER = None,
+    INT_TO_FLOAT_MAPPER  = None,
+    FLOAT_TO_INT_MAPPER  = None,
+    MAP_TO_UNSLOTH_16bit = None,
 ):
     model_name = str(model_name)
     lower_model_name = model_name.lower()
 
     if not SUPPORTS_FOURBIT and lower_model_name in INT_TO_FLOAT_MAPPER:
+
         model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
         logger.warning_once(
             f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
@@ -57,16 +59,21 @@ def __get_model_name(
         return model_name
     
     elif not load_in_4bit and lower_model_name in INT_TO_FLOAT_MAPPER:
+
         new_model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
         #     f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
         # )
         return new_model_name
-    elif not load_in_4bit and lower_model_name in FLOAT_TO_INT_MAPPER:
-        new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
+
+    elif not load_in_4bit and lower_model_name in MAP_TO_UNSLOTH_16bit:
+
+        new_model_name = MAP_TO_UNSLOTH_16bit[lower_model_name]
         return new_model_name
+
     elif load_in_4bit and SUPPORTS_FOURBIT and lower_model_name in FLOAT_TO_INT_MAPPER:
+
         new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
@@ -86,12 +93,14 @@ def _get_new_mapper():
         with requests.get(new_mapper, timeout = 3) as new_mapper: new_mapper = new_mapper.text
         new_mapper = new_mapper[new_mapper.find(""__INT_TO_FLOAT_MAPPER""):]
         new_mapper = new_mapper\
-            .replace(""INT_TO_FLOAT_MAPPER"", ""NEW_INT_TO_FLOAT_MAPPER"")\
-            .replace(""FLOAT_TO_INT_MAPPER"", ""NEW_FLOAT_TO_INT_MAPPER"")
+            .replace(""INT_TO_FLOAT_MAPPER"",  ""NEW_INT_TO_FLOAT_MAPPER"")\
+            .replace(""FLOAT_TO_INT_MAPPER"",  ""NEW_FLOAT_TO_INT_MAPPER"")\
+            .replace(""MAP_TO_UNSLOTH_16bit"", ""NEW_MAP_TO_UNSLOTH_16bit"")
+
         exec(new_mapper, globals())
-        return NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER
+        return NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit
     except:
-        return {}, {}
+        return {}, {}, {}
     pass
 pass
 
@@ -100,17 +109,19 @@ def get_model_name(model_name, load_in_4bit = True):
     new_model_name = __get_model_name(
         model_name = model_name,
         load_in_4bit = load_in_4bit,
-        INT_TO_FLOAT_MAPPER = INT_TO_FLOAT_MAPPER,
-        FLOAT_TO_INT_MAPPER = FLOAT_TO_INT_MAPPER,
+        INT_TO_FLOAT_MAPPER  = INT_TO_FLOAT_MAPPER,
+        FLOAT_TO_INT_MAPPER  = FLOAT_TO_INT_MAPPER,
+        MAP_TO_UNSLOTH_16bit = MAP_TO_UNSLOTH_16bit,
     )
     if new_model_name is None and model_name.count(""/"") == 1 and model_name[0].isalnum():
         # Try checking if a new Unsloth version allows it!
-        NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER = _get_new_mapper()
+        NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit = _get_new_mapper()
         upgraded_model_name = __get_model_name(
             model_name = model_name,
             load_in_4bit = load_in_4bit,
-            INT_TO_FLOAT_MAPPER = NEW_INT_TO_FLOAT_MAPPER,
-            FLOAT_TO_INT_MAPPER = NEW_FLOAT_TO_INT_MAPPER,
+            INT_TO_FLOAT_MAPPER  = NEW_INT_TO_FLOAT_MAPPER,
+            FLOAT_TO_INT_MAPPER  = NEW_FLOAT_TO_INT_MAPPER,
+            MAP_TO_UNSLOTH_16bit = NEW_MAP_TO_UNSLOTH_16bit,
         )
         if upgraded_model_name is not None:
             raise NotImplementedError(
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 57ba676..b8259a0 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -251,8 +251,9 @@ __INT_TO_FLOAT_MAPPER = \
     ),
 }
 
-INT_TO_FLOAT_MAPPER = {}
-FLOAT_TO_INT_MAPPER = {}
+INT_TO_FLOAT_MAPPER  = {}
+FLOAT_TO_INT_MAPPER  = {}
+MAP_TO_UNSLOTH_16bit = {}
 
 for key, values in __INT_TO_FLOAT_MAPPER.items():
     INT_TO_FLOAT_MAPPER[key] = values[0]
@@ -261,6 +262,14 @@ for key, values in __INT_TO_FLOAT_MAPPER.items():
         FLOAT_TO_INT_MAPPER[value] = key
     pass
 
+    # Map to Unsloth version for 16bit versions
+    if len(values) == 2:
+        if values[0].startswith(""unsloth""):
+            MAP_TO_UNSLOTH_16bit[values[1]] = values[0]
+            MAP_TO_UNSLOTH_16bit[values[1].lower()] = values[0]
+        pass
+    pass
+
     # Get lowercased
     lowered_key = key.lower()
     INT_TO_FLOAT_MAPPER[lowered_key] = values[0].lower()
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a7cacea..136ceb2 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1503,10 +1503,16 @@ class FastLlamaModel:
             pass
         pass
 
+        # Check for Llama-3
+        # if hasattr(model._saved_temp_tokenizer, ""_using_llama3_template""):
+        #     if not train_embed_tokens and not train_lm_head:
+        #         raise RuntimeError("""")
+
         # First fix untrained tokens
-        if train_embed_tokens or train_lm_head:
-            fix_untrained_tokens(model, eps = 1e-16)
-        pass
+        # Wrong - can cause reserved tokens to pop out!!
+        # if train_embed_tokens or train_lm_head:
+        #     fix_untrained_tokens(model, eps = 1e-16)
+        # pass
 
         # Check modules_to_save
         if modules_to_save is not None:
@@ -1547,7 +1553,7 @@ class FastLlamaModel:
 
         lora_config = LoraConfig(**arguments)
         model = _get_peft_model(model, lora_config)
-        
+
         model._saved_temp_tokenizer = _saved_temp_tokenizer
 
         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)
diff --git a/unsloth/save.py b/unsloth/save.py
index 0e131fe..e50f0d3 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -118,14 +118,14 @@ def _merge_lora(layer, name):
             W = fast_dequantize(W, quant_state)
         else:
             dtype = W.dtype
-        # W = W.to(torch.float32).t()
-        W = W.t()
+        W = W.to(torch.float32).t()
+        # W = W.t()
 
         if A is not None:
             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))
             # W += sAB
-            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)
-            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)
+            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)
+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)
             # if not torch.isfinite(W).all():
             maximum_element = torch.max(W.min().abs(), W.max())
             if not torch.isfinite(maximum_element).item():
@@ -696,12 +696,18 @@ pass
 
 
 def install_llama_cpp_make_non_blocking():
-    env = { **os.environ, ""LLAMA_CUDA"": ""1"", }
+    # https://github.com/ggerganov/llama.cpp/issues/7062
+    # Weirdly GPU conversion for GGUF breaks??
+    # env = { **os.environ, ""LLAMA_CUDA"": ""1"", }
     n_jobs = max(int(psutil.cpu_count()*1.5), 1)
     # Force make clean
     os.system(""make clean -C llama.cpp"")
     full_command = [""make"", ""all"", ""-j""+str(n_jobs), ""-C"", ""llama.cpp""]
-    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
+
+    # https://github.com/ggerganov/llama.cpp/issues/7062
+    # Weirdly GPU conversion for GGUF breaks??
+    # run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
     return run_installer
 pass
 
@@ -764,12 +770,17 @@ pass
 
 
 def install_llama_cpp_blocking(use_cuda = True):
-    use_cuda = ""LLAMA_CUDA=1"" if use_cuda else """"
+    # https://github.com/ggerganov/llama.cpp/issues/7062
+    # Weirdly GPU conversion for GGUF breaks??
+    # use_cuda = ""LLAMA_CUDA=1"" if use_cuda else """"
 
     commands = [
         ""git clone --recursive https://github.com/ggerganov/llama.cpp"",
         ""make clean -C llama.cpp"",
-        f""{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp"",
+        # https://github.com/ggerganov/llama.cpp/issues/7062
+        # Weirdly GPU conversion for GGUF breaks??
+        # f""{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp"",
+        f""make all -j{psutil.cpu_count()*2} -C llama.cpp"",
         ""pip install gguf protobuf"",
     ]
     if os.path.exists(""llama.cpp""): return
@@ -833,6 +844,12 @@ def save_to_gguf(
     first_conversion     : str = ""f16"",
     _run_installer = None, # Non blocking install of llama.cpp
 ):
+    logger.warning(
+        ""WARNING: llama.cpp GGUF conversion is currently unstable, since llama.cpp is\n""\
+        ""undergoing some major bug fixes as at 5th of May 2024. This is not an Unsloth issue.\n""\
+        ""Please be patient - GGUF saving should still work, but might not work as well.""
+    )
+
     from transformers.models.llama.modeling_llama import logger
 
     if quantization_method.startswith(""iq2""):
@@ -967,7 +984,7 @@ def save_to_gguf(
                 ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
                 ""You must run this in the same folder as you're saving your model.\n""\
                 ""git clone --recursive https://github.com/ggerganov/llama.cpp\n""\
-                ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
+                ""cd llama.cpp && make clean && make all -j\n""\
                 ""Once that's done, redo the quantization.""
             )
         pass
@@ -1007,7 +1024,7 @@ def save_to_gguf(
                     ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
                     ""You must run this in the same folder as you're saving your model.\n""\
                     ""git clone --recursive https://github.com/ggerganov/llama.cpp\n""\
-                    ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
+                    ""cd llama.cpp && make clean && make all -j\n""\
                     ""Once that's done, redo the quantization.""
                 )
             pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 5dc5856..0d6dadf 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -25,7 +25,6 @@ __all__ = [
     ""load_correct_tokenizer"",
     ""fix_sentencepiece_tokenizer"",
     ""check_tokenizer"",
-    ""fix_untrained_tokens"",
     ""add_new_tokens"",
 ]
 
@@ -518,6 +517,44 @@ def fix_untrained_tokens(model, eps = 1e-16):
 pass
 
 
+@torch.inference_mode
+def mean_of_trained_tokens(model, eps = 1e-16):
+    """"""
+    Llama-3 for eg has untrained vectors in the base model.
+    These include <|eot_id|>, <|start_header_id|>, <|end_header_id|>
+    We reset them to the mean of the rest of the tokens
+    """"""
+    embedding_matrix = model.get_input_embeddings ().weight.data.clone()
+    lm_head_matrix   = model.get_output_embeddings().weight.data.clone()
+
+    # Get untrained tokens
+    indicator_untrained = torch.amax(embedding_matrix, axis = 1) <= eps
+    where_untrained = torch.where(indicator_untrained)[0]
+    n_untrained = where_untrained.shape[0]
+    n_trained = embedding_matrix.shape[0] - n_untrained
+    if n_untrained != 0:
+        print(
+            f""Unsloth: Not an error, but your model has {n_untrained} untrained tokens.\n""\
+            ""We shall set them to the mean of the other trained tokens.""
+        )
+    pass
+
+    # First set untrained to all 0s - sometimes it's not! 1e-23 for bfloat16
+    embedding_matrix[where_untrained] = 0
+    lm_head_matrix  [where_untrained] = 0
+
+    # Find sum
+    sum_embedding  = torch.sum(embedding_matrix, dtype = torch.float32, axis = 0)
+    sum_lm_head    = torch.sum(lm_head_matrix,   dtype = torch.float32, axis = 0)
+
+    # Find correct average by dividing by sum of trained tokens
+    mean_embedding = (sum_embedding / n_trained).to(embedding_matrix.dtype)
+    mean_lm_head   = (sum_lm_head   / n_trained).to(lm_head_matrix  .dtype)
+
+    return mean_embedding, mean_lm_head
+pass
+
+
 @torch.inference_mode
 def add_new_tokens(
     model,
@@ -547,7 +584,10 @@ def add_new_tokens(
     pass
 
     # Get mean of trained tokens
-    mean_embedding, mean_lm_head = fix_untrained_tokens(model)
+    # mean_embedding, mean_lm_head = fix_untrained_tokens(model)
+
+    # Weirdly be careful reserved tokens can pop out
+    mean_embedding, mean_lm_head = mean_of_trained_tokens(model)
     mean_embedding = mean_embedding.to(torch.float32)
     mean_lm_head   = mean_lm_head  .to(torch.float32)
 
@@ -595,3 +635,43 @@ def add_new_tokens(
     
     return
 pass
+
+
+from inspect import getsource
+import trl.trainer.sft_trainer
+from trl.trainer.sft_trainer import *
+
+def fix_sft_trainer_tokenizer():
+    """"""
+        Fixes double adding BOS tokens like in llama-3
+    """"""
+    for function_name, replacer in (
+        (""_prepare_non_packed_dataloader"", ""def tokenize(element):"",),
+        # (""_prepare_packed_dataloader"", ""if dataset_text_field is not None"",),
+    ):
+        function = getsource(eval(f""trl.trainer.sft_trainer.SFTTrainer.{function_name}""))
+        where = function.find(""def"")
+        function = function.split(""\n"")
+        function = ""\n"".join(x[where:] for x in function)
+
+        check_text = \
+        ""\n""\
+        ""test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\n""\
+        ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
+        ""chat_template = '' if chat_template is None else chat_template\n""\
+        ""has_bos_token_already = test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template\n""\
+        ""add_special_tokens = False if has_bos_token_already else add_special_tokens\n\n""
+
+        check_text = check_text.split(""\n"")
+        check_text = ""\n"".join("" ""*where + x for x in check_text)
+
+        function = function.replace(replacer, check_text + replacer)
+        exec(function, globals())
+
+        # Replace TRL's SFTTrainer
+        exec(f""trl.trainer.sft_trainer.SFTTrainer.{function_name} = {function_name}"", globals())
+    pass
+pass
+
+# Fixes double adding BOS tokens like in llama-3
+fix_sft_trainer_tokenizer()
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 94cf1b7..903093e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.11.1""
+__version__ = ""2024.11.3""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -69,7 +69,6 @@ from unsloth_zoo.patching_utils import (
     patch_compiling_bitsandbytes,
     patch_layernorm,
     patch_torch_compile,
-    patch_regional_compilation,
     patch_model_and_tokenizer,
 )
 from unsloth_zoo.gradient_checkpointing import (
@@ -88,8 +87,9 @@ from unsloth_zoo.gradient_checkpointing import (
 # Disable some warnings which can get annoying
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""torch"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""huggingface_hub"")
-warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""trl"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""huggingface_hub"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""trl"")
+warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""trl"")
 warnings.filterwarnings(action = ""ignore"", category = FutureWarning,  module = ""xformers"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""transformers"")
@@ -374,8 +374,9 @@ pass
 
 # =============================================
 # Torch compile settings
-UNSLOTH_COMPILE_DEBUG   = ""UNSLOTH_COMPILE_DEBUG""   in os.environ
-UNSLOTH_COMPILE_MAXIMUM = ""UNSLOTH_COMPILE_MAXIMUM"" in os.environ
+UNSLOTH_COMPILE_DEBUG         = os.environ.get(""UNSLOTH_COMPILE_DEBUG"",         ""0"") == ""1""
+UNSLOTH_COMPILE_MAXIMUM       = os.environ.get(""UNSLOTH_COMPILE_MAXIMUM"",       ""0"") == ""1""
+UNSLOTH_COMPILE_IGNORE_ERRORS = os.environ.get(""UNSLOTH_COMPILE_IGNORE_ERRORS"", ""1"") == ""1""
 # Just remove max_autotune_gemm warning
 import functools
 @functools.lru_cache(None)
@@ -387,7 +388,11 @@ def is_big_gpu(index):
     return True
 import torch._inductor.utils
 torch._inductor.utils.is_big_gpu = is_big_gpu
-patch_torch_compile(debug = UNSLOTH_COMPILE_DEBUG, O3 = UNSLOTH_COMPILE_MAXIMUM)
+patch_torch_compile(
+    debug = UNSLOTH_COMPILE_DEBUG,
+    O3 = UNSLOTH_COMPILE_MAXIMUM,
+    ignore_errors = UNSLOTH_COMPILE_IGNORE_ERRORS,
+)
 
 torch_compile_options = {
     ""epilogue_fusion""   : True,
@@ -408,6 +413,26 @@ accelerate.utils.TorchDynamoPlugin.to_kwargs             = torch_compile_kwargs
 accelerate.accelerator.TorchDynamoPlugin.to_kwargs       = torch_compile_kwargs
 del accelerate
 
+def patch_regional_compilation():
+    # Regional torch 2.5 Recompilation - weirdly very slow??
+    if torch.nn.ModuleList.__name__ == ""UnslothModuleList"": return
+    # Only works for torch 2.5
+    if Version(torch.__version__) < Version(""2.5.0""): return
+
+    old_module_list = torch.nn.ModuleList
+    os.environ[""UNSLOTH_PATCHED""] = ""1""
+
+    def UnslothModuleList(*args, **kwargs):
+        if len(args) == 1 and len(kwargs) == 0 and type(args[0]) is list:
+            args = [old_module_list([torch.compile(x, dynamic = True, options = torch_compile_options, fullgraph = False) for x in args[0]])]
+        return old_module_list(*args, **kwargs)
+    pass
+    UnslothModuleList.__doc__ = old_module_list.__doc__
+
+    torch.nn.ModuleList = UnslothModuleList
+    return
+pass
+
 # =============================================
 
 def prepare_model_for_kbit_training(
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index da10f7e..d8dc385 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -890,6 +890,46 @@ CHAT_TEMPLATES[""qwen2.5""]  = (qwen25_template, qwen25_template_eos_token, False,
 DEFAULT_SYSTEM_MESSAGE[""qwen2.5""] = qwen25_default_system_message # No system message in Qwen 2.5
 pass
 
+# =========================================== Phi-4
+# ""{{ bos_token }}""\ # Phi-4 removes BOS?
+phi4_template = \
+    ""{% for message in messages %}""\
+        ""{% if (message['role'] == 'system') %}""\
+            ""{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}""\
+        ""{% elif (message['role'] == 'user') %}""\
+            ""{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}""\
+        ""{% elif (message['role'] == 'assistant') %}""\
+            ""{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '<|im_start|>assistant<|im_sep|>' }}""\
+    ""{% endif %}""
+pass
+
+_phi4_ollama_template = \
+    ""{{ if .System }}<|im_start|><|system|><|im_sep|>{{ .System }}<|im_end|>{{ end }}""\
+    ""{{ if .Prompt }}<|im_start|><|user|><|im_sep|>{{ .Prompt }}<|im_end|>{{ end }}""\
+    ""<|im_start|><|assistant|><|im_sep|>{{ .Response }}<|im_end|>""
+
+# Ollama from https://www.ollama.com/library/phi4 is different
+phi4_ollama = \
+f'''
+FROM {{__FILE_LOCATION__}}
+TEMPLATE """"""{_phi4_ollama_template}""""""
+PARAMETER stop ""<|im_end|>""
+PARAMETER stop ""<|im_start|>""
+PARAMETER stop ""<|im_sep|>""
+PARAMETER temperature 1.5
+PARAMETER min_p 0.1
+'''
+
+phi4_template_eos_token = ""<|im_end|>""
+CHAT_TEMPLATES[""phi-4""] = (phi4_template, phi4_template_eos_token, False, phi4_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""phi-4""] = None # No system message in Phi-4
+pass
+
+
 def _change_system_message(template: str, type_chat_template: str, system_message: str = None):
     system_message_pattern = r""\{system_message\}""
     
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 86adc0e..a93f18c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.1.1""
+__version__ = ""2025.1.2""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 41f7444..b7b24b5 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -520,6 +520,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Llama-3.3-70B-Instruct"",
         ""meta-llama/Llama-3.3-70B-Instruct"",
     ),
+    ""unsloth/phi-4-unsloth-bnb-4bit"" : (
+        ""unsloth/phi-4"",
+        ""microsoft/phi-4"",
+        ""unsloth/phi-4-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
"
"diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 914d403..b385dba 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -43,6 +43,7 @@ torch_compile_options = {
     ""triton.cudagraphs"" : False,
 }
 
+from trl import __version__ as trl_version
 
 def vLLMSamplingParams(**kwargs):
     from vllm import SamplingParams
@@ -545,7 +546,12 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
 
         selective_log_softmax_code = selective_log_softmax_code,
     )
-
+    
+    if RLTrainer_name == ""SFTTrainer"":
+        original_text = 'self._signature_columns = [""input_ids"", ""attention_mask"", ""completion_mask""]'
+        new_text = 'self._signature_columns = [""input_ids"", ""attention_mask"", ""completion_mask"",""labels""]'
+        RLTrainer_source = RLTrainer_source.replace(original_text, new_text)
+        
     # Remove multiple doc strings
     if __RLConfig_doc__ != """" and RLTrainer_source.count(__RLTrainer_doc__) == 2:
         RLTrainer_source = RLTrainer_source.replace(__RLTrainer_doc__, """", 1)
@@ -597,9 +603,15 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
         if len(replacer) != 0:
             replacer = replacer[0]
             vllm_setter = ""\n"" + "" ""*8 + \
-            ""if hasattr(model, 'vllm_engine') and ""\
-            ""hasattr(args, 'use_vllm') and (getattr(args, 'use_vllm', False) == False): ""\
-            ""args.use_vllm = True\n""
+            ""if hasattr(model, 'vllm_engine') and hasattr(args, 'use_vllm'):\n"" + \
+            "" "" * 12 + ""if (getattr(args, 'use_vllm', False) == False):\n"" + \
+            "" "" * 16 + ""args.use_vllm = True\n""
+
+            if ""grpo"" in trainer_file and trl_version >= ""0.18"":
+                # If model has vllm_engine, then use vllm in colocate mode. Donot wait for server
+                vllm_setter += \
+                "" "" * 12 + ""args.vllm_mode='colocate'\n""
+
             init = init.replace(replacer, replacer + vllm_setter)
         pass
     pass
@@ -615,7 +627,8 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
     if len(vllm_part) == 1:
         vllm_part, args = vllm_part[0][0], vllm_part[0][1]
         # Strip all comments
-        new_vllm_part = re.sub(r""\#[^\n]{1,}\n"", """", vllm_part)
+        new_vllm_part = re.sub(r""^\s*\#[^\n]*\n?"", """", vllm_part, flags=re.MULTILINE) # to also remove whole comment line instead of just starting at #
+        new_vllm_part = re.sub(r""\s*\#.*$"", """", new_vllm_part, flags=re.MULTILINE) # remove comments that occur after code
 
         # Get SamplingParams
         sampling_params = re.findall(
@@ -624,9 +637,9 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             new_vllm_part,
             flags = re.MULTILINE | re.DOTALL,
         )
+        
         if len(sampling_params) == 1:
             sampling_params = sampling_params[0]
-
             # Fix guided_decoding
             sampling_params = sampling_params.replace(
                 ""guided_decoding=guided_decoding,"",
@@ -638,11 +651,18 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             sampling_params = \
                 "" ""*12 + ""self.llm = model.vllm_engine; self._last_loaded_step = 0; "" + \
                 sampling_params # Add spaces
+            
+            # count the indentation of last line of sampling_params.
+            last_line = sampling_params.split(""\n"")[-1]
+            last_prev_line = sampling_params.split(""\n"")[-2]
+            last_prev_indentation = len(last_prev_line) - len(last_prev_line.lstrip())
+            last_indentation = len(last_line) - len(last_line.lstrip())
+
 
             # Add extra arguments to SamplingParams
             extra = ""**getattr(getattr(args, 'vllm_sampling_params', vLLMSamplingParams()), '_set_kwargs', {})""
             # Backwards replace
-            to_replace = "","" + extra + "","" + "")""
+            to_replace = "",\n"" + "" ""*last_prev_indentation + extra + "",\n"" + "" ""*last_indentation + "")""
             sampling_params = to_replace.join(sampling_params.rsplit("")"", 1))
             # Strip multiple commas
             sampling_params = re.sub(r""[\,][\s]{0,}\,"", "","", sampling_params)
@@ -650,9 +670,21 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             new_vllm_part = \
                 f""\n{' '*8}if {args}.use_vllm:\n{sampling_params}""\
                 f""\n{' '*8}else:\n""
-
-            init = init.replace(vllm_part, new_vllm_part)
         pass
+
+        if trl_version >= ""0.18"":
+            # Replace LLM init with already existing vLLM engine for colocate mode
+            vllm_llm_init_pattern = r""self\.llm\s*=\s*LLM\([^)]*\)*\)""
+            vllm_llm_replacement = ""self.llm = model.vllm_engine\n""
+            new_vllm_part = re.sub(
+                vllm_llm_init_pattern,
+                vllm_llm_replacement,
+                new_vllm_part,
+                flags=re.DOTALL  # Ensure . matches newlines [[5]]
+            )
+
+        init = init.replace(vllm_part, new_vllm_part)
+
     pass
 
     # Search for vLLM calling in all child functions
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 376d1e9..af401a2 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -20,6 +20,7 @@ __all__ = [
     ""RL_METRICS_CHANGES"",
 ]
 
+import os
 import re
 import torch
 import inspect
@@ -207,24 +208,34 @@ RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__move_model_to_vllm)
 def grpo_trainer__get_per_token_logps(function_name, function):
     if  function_name != ""_get_per_token_logps"": return function
 
-    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
-        if os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '0':
+    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep, calc_logprob_flag = None):
+        if os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '0' and  not calc_logprob_flag:
             return None # Unsloth efficient GRPO
         # Otherwise, calculate normally:
         if not hasattr(self, '_autocast_dtype'):
             self._autocast_dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16
             if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '1': self._autocast_dtype = torch.float16
+
+        os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""1""
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
             # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
-            logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
-            logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-
-            input_ids = input_ids[:, -logits_to_keep:]
+            hidden_states = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
+            #logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
+            return hidden_states
+            # input_ids = input_ids[:, -logits_to_keep:]
             # For transformers<=4.48, logits_to_keep argument isn't supported, so here we drop logits ourselves.
             # See https://github.com/huggingface/trl/issues/2770
-            logits = logits[:, -logits_to_keep:]
-            return logits
-            # return selective_log_softmax(logits, input_ids)  #  compute logprobs for the input tokens
+            # logits = logits[:, -logits_to_keep:]
+            # return logits
+            # logps = selective_log_softmax(logits, input_ids)
+
+            # row_indices, col_indices = torch.where(logps < -20)
+
+            # # Method 1: Check if tensors have elements
+            # if len(row_indices) > 0 and len(col_indices) > 0:
+            #     breakpoint()  # Breakpoint triggered here
+            #     print(""Found high values!"")
+            # return  logps #  compute logprobs for the input tokens
         pass
     pass
 
@@ -264,7 +275,13 @@ def grpo_trainer_compute_loss(function_name, function):
         per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
 
         # Compute the KL divergence between the model and the reference model
-        ref_per_token_logps = inputs[""ref_per_token_logps""]
+        # _prepare_inputs doesn't return reference log probs anymore. We need to calculate it ourselves.
+        # https://github.com/huggingface/trl/blob/05bc43e960396581e458195b8388efe6b82cae1f/trl/trainer/grpo_trainer.py#L1328
+        if self.beta != 0.0:
+            with torch.inference_mode(), model.disable_adapter():
+                ref_per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
+        else:
+            ref_per_token_logps = None
         # per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
 
         # x - x.detach() allows for preserving gradients from x
@@ -272,16 +289,35 @@ def grpo_trainer_compute_loss(function_name, function):
         # per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
         # per_token_loss = -(per_token_loss - self.beta * per_token_kl)
         # loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
+        if ""old_per_token_logps"" in inputs.keys():
+            old_hidden_states = inputs[""old_per_token_logps""]
+        else: 
+            old_hidden_states = None
         input_ids = input_ids[:, -logits_to_keep:]
         if per_token_logps is not None:
             loss, completion_length, mean_kl = grpo_compute_loss_slow(
-                ref_per_token_logps, per_token_logps, input_ids, completion_mask, self.beta, advantages,
+                ref_per_token_logps, per_token_logps, old_hidden_states, input_ids, completion_mask, self.beta, advantages, 
+                loss_type = self.args.loss_type,
+                epsilon_low = self.epsilon_low, epsilon_high = self.epsilon_high,
+                max_completion_length = self.args.max_completion_length,
+                delta = self.args.delta,
             )
         else:
-            loss, completion_length, mean_kl = grpo_accumulated_loss(
-                self, _input_ids, logits_to_keep, completion_mask, advantages,
-                n_chunks = self.args.unsloth_num_chunks,
-            )
+            if hasattr(self.args, ""loss_type""):
+                loss, completion_length, mean_kl = grpo_accumulated_loss(
+                    self, _input_ids, logits_to_keep, completion_mask, advantages, old_hidden_states,
+                    n_chunks = self.args.unsloth_num_chunks,
+                    loss_type = self.args.loss_type,
+                    epsilon_low = self.epsilon_low, epsilon_high = self.epsilon_high,
+                    max_completion_length = self.args.max_completion_length,
+                    delta = self.args.delta,
+                )
+            else:
+                # to ensure backwards compatibility with trl 0.15.2 and maybe even 0.17
+                loss, completion_length, mean_kl = grpo_accumulated_loss(
+                    self, _input_ids, logits_to_keep, completion_mask, advantages, old_hidden_states,
+                    n_chunks = self.args.unsloth_num_chunks,
+                )    
 
         # Log the metrics
         # completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index 012be4b..75fdd41 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -194,8 +194,15 @@ def _backwards_compatible_trainer(trainer_class, config_class):
             config_dict.update(additional_config_kwargs)
 
             # Create Config with all the collected parameters
-            config = config_class(**config_dict)
-            
+            # Reinitialising config class with parameters (that were none initially but populated on first init)
+            # causes the 2nd init to fail as there are mutual exclusive checks on pairs of parameters.
+            # Refer: https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_config.py#L499-L502 for example
+            # So we only create config class if the previous init was not TrainingArguments
+            if not isinstance(training_args, TrainingArguments):
+                config = config_class(**config_dict)
+            else:
+                config = training_args
+
             # Reconstruct kwargs for Trainer
             kwargs = trainer_kwargs
             kwargs[""args""] = config
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 876ccb2..0b8092e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1702,6 +1702,11 @@ class FastLlamaModel:
         lm_head = internal_model.lm_head.weight
         device_type = lm_head.device.type
         dtype = model.config.torch_dtype
+        
+        if type(dtype) is str:
+            if   dtype ==  ""float16"": dtype = torch.float16
+            elif dtype == ""bfloat16"": dtype = torch.bfloat16
+        pass
 
         # Wrap model.generate
         model._unwrapped_old_generate = model.generate
diff --git a/unsloth/save.py b/unsloth/save.py
index d1cd7d6..d001032 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -183,7 +183,7 @@ def unsloth_save_model(
 ):
     if token is None and ""HF_TOKEN"" in os.environ:
         token = os.environ[""HF_TOKEN""]
-    
+
     if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
         token = os.environ[""HUGGINGFACE_TOKEN""]
 
@@ -489,7 +489,12 @@ def unsloth_save_model(
     from collections import OrderedDict
     state_dict = OrderedDict()
 
-    torch_dtype = model.config.torch_dtype
+    torch_dtype = internal_model.config.torch_dtype
+    if type(torch_dtype) is str:
+        if   torch_dtype ==  ""float16"": torch_dtype = torch.float16
+        elif torch_dtype == ""bfloat16"": torch_dtype = torch.bfloat16
+    pass
+
     # Check modules to save float32 dtype
     state_dict[""model.embed_tokens.weight""] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 324e011..9c862a2 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -36,7 +36,7 @@ huggingface = [
     ""tyro"",
     ""transformers>=4.38.2"",
     ""datasets>=2.16.0"",
-    ""sentencepiece"",
+    ""sentencepiece>=0.2.0"",
     ""tqdm"",
     ""psutil"",
     ""wheel>=0.42.0"",
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 0259ab7..c0cce75 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -145,7 +145,12 @@ def GemmaModel_fast_forward_inference(
     bsz, q_len, hd = hidden_states.shape
     seq_len = past_key_values[0][0].shape[-2]
     if bsz != 1:
-        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)
+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+        )
     pass
 
     next_decoder_cache = []
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 6be2caf..05432fc 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -20,6 +20,7 @@ __all__ = [
 
     ""to_sharegpt"",
     ""standardize_sharegpt"",
+    ""standardize_data_formats"",
     ""apply_chat_template"",
     ""train_on_responses_only"",
 
@@ -37,7 +38,9 @@ from .models._utils import patch_tokenizer
 import re
 from unsloth_zoo.dataset_utils import (
     train_on_responses_only,
+    standardize_data_formats,
 )
+standardize_sharegpt = standardize_data_formats
 CHAT_TEMPLATES = {}
 DEFAULT_SYSTEM_MESSAGE = {}
 
@@ -1474,90 +1477,6 @@ def to_sharegpt(
 pass
 
 
-def standardize_sharegpt(
-    dataset,
-    aliases_for_system    = [""system"",],
-    aliases_for_user      = [""user"", ""human"", ""input"",],
-    aliases_for_assistant = [""gpt"", ""assistant"", ""output"",],
-):
-    """"""
-    Standardizes ShareGPT and other formats to user/assistant Hugging Face format.
-    
-    Get aliases for the system, user and assistant roles.
-    These shall map to ""system"", ""user"" and ""assistant"" respectively.
-    
-    aliases_for_system    = [""system"",],
-    aliases_for_user      = [""user"", ""human"", ""input"",],
-    aliases_for_assistant = [""gpt"", ""assistant"", ""output"",],
-    """"""
-    import collections
-    import itertools
-
-    convos = dataset[:10][""conversations""]
-    uniques = collections.defaultdict(list)
-    for convo in convos:
-        for message in convo:
-            for key, value in message.items():
-                uniques[key].append(value)
-    pass
-
-    # Must be only 2 entries
-    assert(len(uniques.keys()) == 2)
-
-    keys = list(uniques.keys())
-    length_first  = len(set(uniques[keys[0]]))
-    length_second = len(set(uniques[keys[1]]))
-
-    if length_first < length_second:
-        # Role is assigned to the first element
-        role_key    = keys[0]
-        content_key = keys[1]
-    else:
-        role_key    = keys[1]
-        content_key = keys[0]
-    pass
-
-    # Check roles are in aliases
-    all_aliases = set(aliases_for_system + aliases_for_user + aliases_for_assistant)
-    roles = set(uniques[role_key])
-    leftover_aliases = (all_aliases | roles) - all_aliases
-    if len(leftover_aliases) != 0:
-        raise TypeError(
-            f""Unsloth: {list(leftover_aliases)} are not in aliases. Please update aliases.""
-        )
-    pass
-
-    # Mapping for aliases
-    aliases_mapping = {}
-    for x in aliases_for_system:    aliases_mapping[x] = ""system""
-    for x in aliases_for_user:      aliases_mapping[x] = ""user""
-    for x in aliases_for_assistant: aliases_mapping[x] = ""assistant""
-
-    def _standardize_dataset(examples):
-        convos = examples[""conversations""]
-        all_convos = []
-        for convo in convos:
-            new_convo = [
-                { ""role"" : aliases_mapping[message[role_key]], ""content"" : message[content_key], }
-                for message in convo
-            ]
-            all_convos.append(new_convo)
-        pass
-        return { ""conversations"" : all_convos, }
-    pass
-
-    from multiprocessing import cpu_count
-    num_proc = cpu_count()
-
-    return dataset.map(
-        _standardize_dataset,
-        batched = True,
-        desc = ""Unsloth: Standardizing formats"",
-        num_proc = num_proc,
-    )
-pass
-
-
 def get_ollama_eos_tokens(tokenizer, extra_eos_tokens = []):
     added_tokens_decoder = tokenizer.added_tokens_decoder.values()
     added_tokens_decoder = [str(x) for x in added_tokens_decoder]
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 7ae6e92..bb2c756 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -38,6 +38,7 @@ from ..kernels import *
 from ..tokenizer_utils import *
 if HAS_FLASH_ATTENTION:
     from flash_attn import flash_attn_func
+from .vision import FastBaseModel
 
 # Final patching code
 from transformers.models.llama.modeling_llama import (
@@ -1648,6 +1649,7 @@ class FastLlamaModel:
         disable_log_stats = False,
         **kwargs,
     ):
+        os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""0""
         if trust_remote_code:
             if fast_inference:
                 raise NotImplementedError(""Unsloth: Fast inference does not support `trust_remote_code` yet."")
@@ -2016,6 +2018,31 @@ class FastLlamaModel:
         temporary_location  = ""_unsloth_temporary_saved_buffers"",
         **kwargs,
     ):
+        if os.environ.get(""UNSLOTH_USE_NEW_MODEL"", ""0"") == ""1"":
+            return FastBaseModel.get_model(
+                model                      = model,
+                r                          = r,
+                target_modules             = target_modules,
+                lora_alpha                 = lora_alpha,
+                lora_dropout               = lora_dropout,
+                bias                       = bias,
+                finetune_vision_layers     = False,
+                finetune_language_layers   = True,
+                finetune_attention_modules = True,
+                finetune_mlp_modules       = True,
+                layers_to_transform        = layers_to_transform,
+                layers_pattern             = layers_pattern,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+                random_state               = random_state,
+                max_seq_length             = max_seq_length,
+                use_rslora                 = use_rslora,
+                modules_to_save            = modules_to_save,
+                init_lora_weights          = init_lora_weights,
+                loftq_config               = loftq_config,
+                temporary_location         = temporary_location,
+                **kwargs,
+            )
+        pass
         if os.environ.get(""UNSLOTH_ENABLE_FULL_FINETUNING"", ""0"") == ""1"":
             print(""Unsloth: Full finetuning is enabled, so .get_peft_model has no effect"")
             return model
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index c595bcd..020bd4e 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -70,7 +70,7 @@ class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
         model_name                 = ""unsloth/Llama-3.2-1B-Instruct"",
-        max_seq_length             = None,
+        max_seq_length             = 2048,
         dtype                      = None,
         load_in_4bit               = True,
         load_in_8bit               = False,
@@ -96,7 +96,7 @@ class FastLanguageModel(FastLlamaModel):
         if load_in_8bit or full_finetuning:
             return FastModel.from_pretrained(
                 model_name                 = model_name,
-                max_seq_length             = max_seq_length, # [TODO] No effect
+                max_seq_length             = max_seq_length,
                 dtype                      = dtype,
                 load_in_4bit               = load_in_4bit,
                 load_in_8bit               = load_in_8bit,
@@ -295,7 +295,7 @@ class FastLanguageModel(FastLlamaModel):
         else:
             return FastModel.from_pretrained(
                 model_name                 = model_name,
-                max_seq_length             = max_seq_length, # [TODO] No effect
+                max_seq_length             = max_seq_length,
                 dtype                      = dtype,
                 load_in_4bit               = load_in_4bit,
                 load_in_8bit               = load_in_8bit,
@@ -442,7 +442,7 @@ class FastModel(FastBaseModel):
     @staticmethod
     def from_pretrained(
         model_name                 = ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
-        max_seq_length             = None, # [TODO] No effect
+        max_seq_length             = 2048,
         dtype                      = None,
         load_in_4bit               = True,
         load_in_8bit               = False,
@@ -668,7 +668,7 @@ class FastModel(FastBaseModel):
             use_gradient_checkpointing = use_gradient_checkpointing,
             *args, **kwargs,
         )
-        
+
         if resize_model_vocab is not None:
             model.resize_token_embeddings(resize_model_vocab)
         pass
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 0f6eda6..a65b538 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -25,7 +25,6 @@ try:
 except:
     from transformers import AutoModelForVision2Seq
 pass
-from .llama import *
 from ..kernels import (
     post_patch_loss_function,
 )
@@ -100,7 +99,7 @@ class FastBaseModel:
     @staticmethod
     def from_pretrained(
         model_name        = ""unsloth/Llama-3.2-1B-Instruct"",
-        max_seq_length    = None,
+        max_seq_length    = 2048,
         dtype             = None,
         load_in_4bit      = True,
         load_in_8bit      = False,
@@ -114,6 +113,7 @@ class FastBaseModel:
         use_gradient_checkpointing = ""unsloth"",
         **kwargs,
     ):
+        os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""1""
         if trust_remote_code:
             print(
                 ""Unsloth: WARNING `trust_remote_code` is True.\n""\
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
new file mode 100644
index 0000000..bea0606
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -0,0 +1,42 @@
+---
+name: Bug report
+about: Create a report to help us improve
+title: ""[BUG]""
+labels: bug
+assignees: ''
+
+---
+
+**Describe the bug**
+A clear and concise description of what the bug is.
+## Issue Type
+
+## Steps to Reproduce
+
+1. **Environment Setup:**
+   - OS: [e.g., Ubuntu 20.04]
+   - Python Version: [e.g., 3.8.10]
+   - Frameworks/Libraries: past output of `pip freeze` here
+   - Colab / Script
+
+2. **Dataset Details:**
+   - Dataset Name: 
+   - Data Preprocessing Steps: [e.g., Tokenization with specific parameters]
+
+3. **Model Details:**
+   - Model ID:
+   - Model Configuration: [e.g., lora params, quantization, etc.]
+
+4. **Training Configuration:**
+   - Trainer Args: `SFTConfig`, `GRPOConfig`
+
+5. **Reproduction Steps:**
+   - Minimal script to reproduce error
+
+6. **Expected Behavior:**
+
+7. **Actual Behavior:**
+   - [e.g., Description of the error, unexpected results, or performance issues encountered]
+   - [e.g., Error messages or logs]
+
+8. **Additional notes:**
diff --git a/.github/ISSUE_TEMPLATE/custom.md b/.github/ISSUE_TEMPLATE/custom.md
new file mode 100644
index 0000000..48d5f81
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/custom.md
@@ -0,0 +1,10 @@
+---
+name: Custom issue template
+about: Describe this issue template's purpose here.
+title: ''
+labels: ''
+assignees: ''
+
+---
+
+
diff --git a/.github/ISSUE_TEMPLATE/documentation.md b/.github/ISSUE_TEMPLATE/documentation.md
new file mode 100644
index 0000000..03891ed
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/documentation.md
@@ -0,0 +1,20 @@
+---
+name: Documentation
+about: Documentation request
+title: ""[DOCUMENTATION]""
+labels: documentation
+assignees: ''
+
+---
+
+**Is your feature request related to a problem? Please describe.**
+A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
+
+**Describe the solution you'd like**
+A clear and concise description of what you want to happen.
+
+**Describe alternatives you've considered**
+A clear and concise description of any alternative solutions or features you've considered.
+
+**Additional context**
+Add any other context or screenshots about the feature request here.
diff --git a/.github/ISSUE_TEMPLATE/feature_request.md b/.github/ISSUE_TEMPLATE/feature_request.md
new file mode 100644
index 0000000..f3615ed
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/feature_request.md
@@ -0,0 +1,20 @@
+---
+name: Feature request
+about: Suggest an idea for this project
+title: ""[FEAT]""
+labels: enhancement
+assignees: ''
+
+---
+
+**Is your feature request related to a problem? Please describe.**
+A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
+
+**Describe the solution you'd like**
+A clear and concise description of what you want to happen.
+
+**Describe alternatives you've considered**
+A clear and concise description of any alternative solutions or features you've considered.
+
+**Additional context**
+Add any other context or screenshots about the feature request here.
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index cf4f07a..1d9e2f9 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -1,7 +1,7 @@
 ---
 name: Bug report
 about: Create a report to help us improve
-title: ""[BUG]""
+title: ""[Bug]""
 labels: bug
 assignees: ''
 
@@ -38,4 +38,4 @@ A clear and concise description of what the bug is.  Please fill out the followi
    - [e.g., Error messages or logs]
 
 8. **Additional notes:**
-   - Any additional information that might help us reproduce the bug.
\ No newline at end of file
+   - Any additional information that might help us reproduce the bug.
"
"diff --git a/pyproject.toml b/pyproject.toml
index d438c83..4cadd3a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.5.5"",
+    ""unsloth_zoo>=2025.5.6"",
     ""packaging"",
     ""tyro"",
     ""transformers==4.51.3,!=4.47.0"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.5.5"",
+    ""unsloth_zoo>=2025.5.6"",
     ""packaging"",
     ""tyro"",
     ""transformers==4.51.3,!=4.47.0"",
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 5c8cc87..cfb3ece 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1036,9 +1036,21 @@ qwen3_template = \
     {%- endif %}
 {%- endif %}
 {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
-{%- for message in messages[::-1] %}
+{%- for forward_message in messages %}
     {%- set index = (messages|length - 1) - loop.index0 %}
-    {%- if ns.multi_step_tool and message.role == ""user"" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
+    {%- set message = messages[index] %}
+    {%- set current_content = message.content if message.content is not none else '' %}
+    {%- set tool_start = '<tool_response>' %}
+    {%- set tool_start_length = tool_start|length %}
+    {%- set start_of_message = current_content[:tool_start_length] %}
+    {%- set tool_end = '</tool_response>' %}
+    {%- set tool_end_length = tool_end|length %}
+    {%- set start_pos = (current_content|length) - tool_end_length %}
+    {%- if start_pos < 0 %}
+        {%- set start_pos = 0 %}
+    {%- endif %}
+    {%- set end_of_message = current_content[start_pos:] %}
+    {%- if ns.multi_step_tool and message.role == ""user"" and not(start_of_message == tool_start and end_of_message == tool_end) %}
         {%- set ns.multi_step_tool = false %}
         {%- set ns.last_query_index = index %}
     {%- endif %}
@@ -1053,8 +1065,9 @@ qwen3_template = \
             {%- set reasoning_content = message.reasoning_content %}
         {%- else %}
             {%- if '</think>' in message.content %}
-                {%- set content = message.content.split('</think>')[-1].lstrip('\n') %}
-                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\n').split('<think>')[-1].lstrip('\n') %}
+                {%- set content = (message.content.split('</think>')|last).lstrip('\n') %}
+                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\n') %}
+                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\n') %}
             {%- endif %}
         {%- endif %}
         {%- if loop.index0 > ns.last_query_index %}
@@ -1110,7 +1123,7 @@ qwen3_template = \
 qwen3_ollama = \
 '''
 FROM {__FILE_LOCATION__}
-TEMPLATE """"""{{ if .Messages }}
+TEMPLATE """"""{{- if .Messages }}
 {{- if or .System .Tools }}<|im_start|>system
 {{- if .System }}
 {{ .System }}
@@ -1161,8 +1174,12 @@ For each function call, return a json object with function name and arguments wi
 {{ end }}<|im_start|>assistant
 {{ end }}{{ .Response }}{{ if .Response }}<|im_end|>{{ end }}""""""
 PARAMETER stop ""<|im_end|>""
-PARAMETER temperature 1.5
-PARAMETER min_p 0.1
+PARAMETER stop ""<|im_start|>""
+PARAMETER temperature 0.6
+PARAMETER min_p 0.0
+PARAMETER top_k 20
+PARAMETER top_p 0.95
+PARAMETER repeat_penalty 1
 '''
 
 qwen3_template_eos_token = ""<|im_end|>""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 6065af1..5101a71 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.5.3""
+__version__ = ""2025.5.4""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index a1dbc82..a233b26 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -541,10 +541,12 @@ class FastModel(FastBaseModel):
             if transformers_version < Version(""4.50.0.dev0""):
                 raise RuntimeError(""Unsloth: Granite Vision only works on transformers >= 4.50.0."" + NIGHTLY)
         elif ""csm-1b"" in model_name.lower():
-            os.environ[""UNSLOTH_COMPILE_DISABLE""] = ""1""
-            os.environ[""UNSLOTH_DISABLE_FAST_GENERATION""] = ""1""
+            os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Sesame fails
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
         elif ""olmo-2"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""whisper"" in model_name.lower():
+            os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Whisper fails
         pass
 
         if USE_MODELSCOPE and not os.path.exists(model_name):
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index d723fc4..4bbd829 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -817,6 +817,26 @@ __INT_TO_FLOAT_MAPPER = \
         ""microsoft/Phi-4-mini-reasoning"",
         ""unsloth/phi-4-mini-reasoning-bnb-4bit"",
     ),
+    ""unsloth/csm-1b"" : (
+        ""unsloth/csm-1b"",
+        ""sesame/csm-1b"",
+    ),
+    ""unsloth/whisper-large-v3"" : (
+        ""unsloth/whisper-large-v3"",
+        ""openai/whisper-large-v3"",
+    ),
+    ""unsloth/whisper-large-v3-turbo"" : (
+        ""unsloth/whisper-large-v3-turbo"",
+        ""openai/whisper-large-v3-turbo"",
+    ),
+    ""unsloth/whisper-small"" : (
+        ""unsloth/whisper-small"",
+        ""openai/whisper-small"",
+    ),
+    ""unsloth/CrisperWhisper"" : (
+        ""unsloth/CrisperWhisper"",
+        ""nyrahealth/CrisperWhisper"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 67566fc..b335fd0 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -188,7 +188,10 @@ def unsloth_base_fast_generate(
     # Use hybrid if sliding window seen, otherwise try static
     cache_implementation = getattr(self.config, ""cache_implementation"", None)
     if getattr(self, ""_supports_static_cache"", True):
-        cache_implementation = ""static""
+        if os.environ.get(""UNSLOTH_DISABLE_STATIC_GENERATION"", ""0"") == ""0"":
+            cache_implementation = ""static""
+        else:
+            cache_implementation = None
     else:
         cache_implementation = None
     if cache_implementation is not None:
@@ -199,10 +202,10 @@ def unsloth_base_fast_generate(
             cache_implementation = ""hybrid""
     if ""generation_config"" in kwargs:
         kwargs[""generation_config""].cache_implementation = cache_implementation
-        kwargs[""generation_config""].compile_config = _compile_config
+        kwargs[""generation_config""].compile_config = _compile_config if cache_implementation is not None else None
     else:
         kwargs[""cache_implementation""] = cache_implementation
-        kwargs[""compile_config""] = _compile_config
+        kwargs[""compile_config""] = _compile_config if cache_implementation is not None else None
     pass
 
     try:
@@ -310,6 +313,19 @@ class FastBaseModel:
             bnb_compute_dtype = torch.float16
             do_forced_float32 = True
         pass
+
+        # Check for custom data-types
+        custom_datatype = None
+        correct_dtype = None
+        if os.environ.get(""UNSLOTH_FORCE_CUSTOM_DTYPE"", """") != """":
+            custom_datatype = os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""]
+            assert custom_datatype.count("";"") == 1
+            bnb_compute_dtype, custom_datatype = custom_datatype.split("";"", 1)
+            dtype = torch.float32
+            bnb_compute_dtype = eval(bnb_compute_dtype)
+            correct_dtype = bnb_compute_dtype
+        pass
+
         # Stop SDPA for some archs like Pixtral / Mistral3
         if not (""attn_implementation"" in kwargs):
             kwargs[""attn_implementation""] = ""sdpa""
@@ -374,12 +390,18 @@ class FastBaseModel:
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
 
+        # Edit data-types
+        if custom_datatype is not None:
+            for name, module in model.named_modules():
+                exec(custom_datatype)
+        pass
+
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         is_vlm = (auto_model is AutoModelForVision2Seq)
         is_whisper = (whisper_language is not None and whisper_task is not None)
         auto_processor = AutoProcessor if (is_vlm or is_whisper) else AutoTokenizer
-        if whisper_language and whisper_task:
+        if (whisper_language and whisper_task) or auto_model.__name__.endswith(""ForConditionalGeneration""):
            tokenizer = auto_processor.from_pretrained(
                 tokenizer_name,
                 padding_side = ""right"",
@@ -415,6 +437,7 @@ class FastBaseModel:
             downcast_rope = False,
             fix_embeddings = False,
             do_forced_float32 = do_forced_float32,
+            correct_dtype = correct_dtype,
         )
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = post_patch_loss_function(model)
"
"diff --git a/unsloth/models/falcon_h1.py b/unsloth/models/falcon_h1.py
index df4622d..8978e4d 100644
--- a/unsloth/models/falcon_h1.py
+++ b/unsloth/models/falcon_h1.py
@@ -30,12 +30,13 @@ try:
         FalconHybridMambaAttentionDynamicCache,
     )
 except:
+    from transformers import __version__ as transformers_version
     transformers_version = Version(transformers_version)
-    if not transformers_version >= Version(""4.50.3""): #TODO: Update when transformers is updated
+    if not transformers_version >= Version(""4.53.0""): #TODO: Update when transformers is updated
         raise ImportError(
             f""Unsloth: Your transformers version of {transformers_version} does not support FalconH1.\n""\
-            f""The minimum required version is 4.50.3.\n""\
-            f'Try `pip install --upgrade ""transformers>=4.50.3""`\n'\
+            f""The minimum required version is 4.53.0.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.53.0""`\n'\
             f""to obtain the latest transformers build, then restart this session.""\
         )
     pass
@@ -48,7 +49,10 @@ try:
         FalconH1Attention,
     )
 except:
-    FalconH1Attention   = FalconH1Attention
+    # if we are on a old version of transformers technically it should fail in the try except above
+    # but if somehow we make it here, we need to raise an error since FalconH1Attention is not available
+    # or renamed
+    raise ImportError(""Unsloth: Could not import FalconH1Attention from transformers.models.falcon_h1.modeling_falcon_h1."")
 pass
 
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index f7e08ed..70dd896 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -26,7 +26,6 @@ from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
 from .qwen3   import FastQwen3Model
 from .qwen3_moe import FastQwen3MoeModel
-from .falcon_h1 import FastFalconH1Model
 from .cohere  import FastCohereModel
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
@@ -65,6 +64,9 @@ if SUPPORTS_GEMMA:
 if SUPPORTS_GEMMA2:
     from .gemma2 import FastGemma2Model
 pass
+if SUPPORTS_FALCON_H1:
+    from .falcon_h1 import FastFalconH1Model
+pass
 import torch
 from ._utils import (
     patch_compiling_bitsandbytes,
"
"diff --git a/pyproject.toml b/pyproject.toml
index 1d20691..01636e7 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.4"",
+    ""unsloth_zoo>=2025.3.5"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.4"",
+    ""unsloth_zoo>=2025.3.5"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 4336ec4..9ed356d 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.4""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.5""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c01e0cc..7ac35d7 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.6""
+__version__ = ""2025.3.7""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -1052,8 +1052,7 @@ def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
 
     # Get gradient accumulation steps if possible
     if num_items_in_batch is None and \
-        getattr(self, ""args"", {}).get(""gradient_accumulation_steps"", 1) != 1:
-
+        getattr(getattr(self, ""args"", self), ""gradient_accumulation_steps"", 1) != 1:
         name = (model.base_model.model if hasattr(model, ""base_model"") else model).__class__.__name__
         logger.warning_once(
             f""Unsloth: Not an error, but {name} does not accept `num_items_in_batch`.\n""\
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3dacf5c..a490fb8 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1843,7 +1843,7 @@ class FastLlamaModel:
             else:
                 inner_training_loop = Trainer._original_training_loop
         except:
-            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
+            raise RuntimeError('Unsloth: Unsuccessfully patched inner_training_loop')
         pass
         
         import transformers.trainer
@@ -1869,7 +1869,7 @@ class FastLlamaModel:
         f""{chr(92)}        /    Data Parallel GPUs = {args.world_size} | Total batch size ({self._train_batch_size} x {args.gradient_accumulation_steps} x {args.world_size}) = {total_train_batch_size:,}\\n""\\
         f' ""-____-""     Trainable parameters = {get_model_param_count(model, trainable_only=True):,}/{get_model_param_count(model):,} ({get_model_param_count(model, trainable_only=True)/get_model_param_count(model)*100:.2f}% trained)'
         logger.warning(debug_info)
-        import subprocess, re, gc
+        import gc
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()""""""
@@ -1897,9 +1897,6 @@ class FastLlamaModel:
             ""_inner_training_loop"",
             ""_fast_inner_training_loop"", 1,
         )
-        exec(inner_training_loop, globals())
-
-        Trainer._inner_training_loop = _fast_inner_training_loop
         inner_training_loop = inner_training_loop.replace(
             ""is_torch_tpu_available()"",
             ""False"",
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index d0b7d4d..93415f5 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -621,6 +621,7 @@ class FastModel(FastBaseModel):
                     ""if hasattr(module, x): ""\
                     ""setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
                     ""x = 'down_proj_bias'\n""\
+                    ""setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
                     "";""
             else:
                 # Set down projection compute dtype to be float32 for float16 machines
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 5524d8f..a5de457 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -457,8 +457,10 @@ class FastBaseModel:
 
         # Edit data-types
         if custom_datatype is not None:
-            for jj, (name, module) in enumerate(model.named_modules()):
-                exec(custom_datatype)
+            with torch.no_grad():
+                for jj, (name, module) in enumerate(model.named_modules()):
+                    exec(custom_datatype)
+                pass
             pass
         pass
         # Clear deleted GPU items
"
"diff --git a/tests/__init__.py b/tests/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/test_model_registry.py b/tests/test_model_registry.py
new file mode 100644
index 0000000..f59f4f0
--- /dev/null
+++ b/tests/test_model_registry.py
@@ -0,0 +1,91 @@
+""""""
+
+Test model registration methods
+Checks that model registration methods work for respective models as well as all models
+The check is performed
+- by registering the models
+- checking that the instantiated models can be found on huggingface hub by querying for the model id
+
+""""""
+
+from dataclasses import dataclass
+
+import pytest
+from huggingface_hub import ModelInfo as HfModelInfo
+
+from unsloth.registry import register_models, search_models
+from unsloth.registry._deepseek import register_deepseek_models
+from unsloth.registry._gemma import register_gemma_models
+from unsloth.registry._llama import register_llama_models
+from unsloth.registry._mistral import register_mistral_models
+from unsloth.registry._phi import register_phi_models
+from unsloth.registry._qwen import register_qwen_models
+from unsloth.registry.registry import MODEL_REGISTRY, QUANT_TAG_MAP, QuantType
+from unsloth.utils.hf_hub import get_model_info
+
+MODEL_NAMES = [
+    ""llama"",
+    ""qwen"",
+    ""mistral"",
+    ""phi"",
+    ""gemma"",
+    ""deepseek"",
+]
+MODEL_REGISTRATION_METHODS = [
+    register_llama_models,
+    register_qwen_models,
+    register_mistral_models,
+    register_phi_models,
+    register_gemma_models,
+    register_deepseek_models,
+]
+
+
+@dataclass
+class ModelTestParam:
+    name: str
+    register_models: callable
+
+
+def _test_model_uploaded(model_ids: list[str]):
+    missing_models = []
+    for _id in model_ids:
+        model_info: HfModelInfo = get_model_info(_id)
+        if not model_info:
+            missing_models.append(_id)
+
+    return missing_models
+
+
+TestParams = [
+    ModelTestParam(name, models)
+    for name, models in zip(MODEL_NAMES, MODEL_REGISTRATION_METHODS)
+]
+
+
+# Test that model registration methods register respective models
+@pytest.mark.parametrize(""model_test_param"", TestParams, ids=lambda param: param.name)
+def test_model_registration(model_test_param: ModelTestParam):
+    MODEL_REGISTRY.clear()
+    registration_method = model_test_param.register_models
+    registration_method()
+    registered_models = MODEL_REGISTRY.keys()
+    missing_models = _test_model_uploaded(registered_models)
+    assert not missing_models, (
+        f""{model_test_param.name} missing following models: {missing_models}""
+    )
+
+
+def test_all_model_registration():
+    register_models()
+    registered_models = MODEL_REGISTRY.keys()
+    missing_models = _test_model_uploaded(registered_models)
+    assert not missing_models, f""Missing following models: {missing_models}""
+
+def test_quant_type():
+    # Test that the quant_type is correctly set for model paths
+    # NOTE: for models registered under org=""unsloth"" with QuantType.NONE aliases QuantType.UNSLOTH
+    dynamic_quant_models = search_models(quant_types=[QuantType.UNSLOTH])
+    assert all(m.quant_type == QuantType.UNSLOTH for m in dynamic_quant_models)
+    quant_tag = QUANT_TAG_MAP[QuantType.UNSLOTH]
+    assert all(quant_tag in m.model_path for m in dynamic_quant_models)
\ No newline at end of file
diff --git a/unsloth/_auto_install.py b/unsloth/_auto_install.py
index 8bb5485..308bf07 100644
--- a/unsloth/_auto_install.py
+++ b/unsloth/_auto_install.py
@@ -18,7 +18,7 @@ from packaging.version import Version as V
 v = V(torch.__version__)
 cuda = str(torch.version.cuda)
 is_ampere = torch.cuda.get_device_capability()[0] >= 8
-if cuda != ""12.1"" and cuda != ""11.8"" and cuda != ""12.4"" and cuda != ""12.6"": raise RuntimeError(f""CUDA = {cuda} not supported!"")
+if cuda != ""12.1"" and cuda != ""11.8"" and cuda != ""12.4"" and cuda != ""12.6"" and cuda != ""12.8"": raise RuntimeError(f""CUDA = {cuda} not supported!"")
 if   v <= V('2.1.0'): raise RuntimeError(f""Torch = {v} too old!"")
 elif v <= V('2.1.1'): x = 'cu{}{}-torch211'
 elif v <= V('2.1.2'): x = 'cu{}{}-torch212'
@@ -28,6 +28,7 @@ elif v  < V('2.5.0'): x = 'cu{}{}-torch240'
 elif v  < V('2.5.1'): x = 'cu{}{}-torch250'
 elif v <= V('2.5.1'): x = 'cu{}{}-torch251'
 elif v  < V('2.7.0'): x = 'cu{}{}-torch260'
+elif v  < V('2.8.0'): x = 'cu{}{}-torch270'
 else: raise RuntimeError(f""Torch = {v} too new!"")
 x = x.format(cuda.replace(""."", """"), ""-ampere"" if is_ampere else """")
 print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git""')
\ No newline at end of file
diff --git a/unsloth/dataprep/synthetic.py b/unsloth/dataprep/synthetic.py
index 8fcbc1b..2f4a85f 100644
--- a/unsloth/dataprep/synthetic.py
+++ b/unsloth/dataprep/synthetic.py
@@ -13,10 +13,7 @@
 # limitations under the License.
 
 __all__ = [
-    ""check_vllm_status"",
-    ""async_load_vllm"",
-    ""destroy_vllm"",
-    ""configure_synthetic_data_kit"",
+    ""SyntheticDataKit"",
 ]
 import subprocess
 import time
@@ -26,236 +23,240 @@ import torch
 import gc
 import time
 from unsloth_zoo.vllm_utils import load_vllm
-from transformers import AutoConfig
+from transformers import AutoConfig, AutoTokenizer
+import signal
 
-def check_vllm_status():
-    try:
-        response = requests.get(""http://localhost:8000/metrics"")
-        if response.status_code == 200:
-            return True
-    except requests.exceptions.ConnectionError:
-        return False
-    pass
-pass
-
-
-def async_load_vllm(
-    model_name = ""unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"",
-    max_seq_length = 2048,
-    gpu_memory_utilization = 0.85,
-    float8_kv_cache = False,
-    conservativeness = 1.0,
-    token = None,
-):
-    config = AutoConfig.from_pretrained(
-        model_name,
-        token = token,
-    )
-    engine_args = load_vllm(
-        model_name             = model_name,
-        config                 = config,
-        gpu_memory_utilization = gpu_memory_utilization,
-        max_seq_length         = max_seq_length,
-        disable_log_stats      = True,
-        float8_kv_cache        = float8_kv_cache,
-        conservativeness       = conservativeness,
-        return_args            = True,
-        enable_lora            = False,
-    )
-    if ""device"" in engine_args: del engine_args[""device""]
-    if ""model""  in engine_args: del engine_args[""model""]
-
-    subprocess_commands = [
-        ""vllm"", ""serve"", str(model_name),
-    ]
-    for key, value in engine_args.items():
-        flag  = ""--"" + key.replace(""_"", ""-"")
-        which = str(value).lower().replace(""torch."", """")
-        subprocess_commands += [flag, which,]
-    pass
-    print(subprocess_commands)
-    vllm_process = subprocess.Popen(
-        subprocess_commands,
-        stdout = subprocess.PIPE,
-        stderr = subprocess.PIPE,
-        start_new_session = True,
-    )
-    ready_message_part = b""Starting vLLM API server on""
-    ready = False
-    while vllm_process.poll() is None:
-        output = vllm_process.stdout.readline()
-        if not output:
-            print(""Stdout stream ended before readiness message detected."")
-            break
-        output_str = output.decode('utf-8', errors='ignore').strip()
-        print(f""vLLM STDOUT: {output_str}"")
-        if ready_message_part in output:
-            print(f""\n--- vLLM Server Ready (Detected: '{ready_message_part.decode()}') ---"")
-            ready = True
-            break
-        pass
-    pass
-    if vllm_process is None:
-        raise RuntimeError(""Unsloth: vllm_process failed to load!"")
-    trial = 0
-    while not check_vllm_status():
-        if trial >= 100:
-            raise RuntimeError(""Unsloth: vllm_process failed to load!"")
-        trial += 1
-        time.sleep(1)
-    return vllm_process
-pass
-
-
-def destroy_vllm(vllm_process):
-    print(""Attempting to terminate the VLLM server gracefully..."")
-    try:
-        vllm_process.terminate()
-        vllm_process.wait(timeout=10)
-        print(""Server terminated gracefully."")
-    except subprocess.TimeoutExpired:
-        print(""Server did not terminate gracefully after 10 seconds. Forcing kill..."")
-        vllm_process.kill()
-        vllm_process.wait()
-        print(""Server killed forcefully."")
-    except Exception as e:
-         print(f""An error occurred while trying to stop the process: {e}"")
-         try:
-             if vllm_process.poll() is None:
-                 print(""Attempting forceful kill due to error..."")
-                 vllm_process.kill()
-                 vllm_process.wait()
-                 print(""Server killed forcefully after error."")
-         except Exception as kill_e:
-             print(f""Error during forceful kill: {kill_e}"")
-    for _ in range(10):
-        torch.cuda.empty_cache()
-        gc.collect()
-pass
+from .synthetic_configs import (
+    synthetic_qa_config,
+)
 
+class SyntheticDataKit:
+    def __init__(
+        self,
+        model_name = ""unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"",
+        max_seq_length = 2048,
+        gpu_memory_utilization = 0.98,
+        float8_kv_cache = False,
+        conservativeness = 1.0,
+        token = None,
+        **kwargs,
+    ):
+        assert(type(model_name) is str)
+        assert(type(max_seq_length) is int)
+        assert(type(gpu_memory_utilization) is float)
+        assert(type(float8_kv_cache) is bool)
+        assert(type(conservativeness) is float)
+        assert(token is None or type(token) is str)
 
-synthetic_config_string = """"""\
-# Master configuration file for Synthetic Data Kit
+        self.model_name = model_name
+        self.max_seq_length = max_seq_length
 
-# Global paths configuration
-paths:
-  # Input data locations
-  input:
-    pdf: ""data/pdf""
-    html: ""data/html""
-    youtube: ""data/youtube""
-    docx: ""data/docx""
-    ppt: ""data/ppt""
-    txt: ""data/txt""
+        self.config = AutoConfig.from_pretrained(
+            model_name,
+            token = token,
+        )
+        self.tokenizer = AutoTokenizer.from_pretrained(
+            model_name,
+            token = token,
+        )
+        engine_args = load_vllm(
+            model_name             = model_name,
+            config                 = self.config,
+            gpu_memory_utilization = gpu_memory_utilization,
+            max_seq_length         = max_seq_length,
+            disable_log_stats      = True,
+            float8_kv_cache        = float8_kv_cache,
+            conservativeness       = conservativeness,
+            return_args            = True,
+            enable_lora            = False,
+            **kwargs,
+        )
 
-  # Output locations
-  output:
-    parsed: ""data/output""      # Where parsed text files are saved
-    generated: ""data/generated"" # Where generated content is saved
-    cleaned: ""data/cleaned""     # Where cleaned content is saved
-    final: ""data/final""         # Where final formatted content is saved
+        if ""device"" in engine_args: del engine_args[""device""]
+        if ""model""  in engine_args: del engine_args[""model""]
+        if ""compilation_config"" in engine_args: del engine_args[""compilation_config""]
 
-# VLLM server configuration
-vllm:
-  api_base: ""http://localhost:8000/v1"" # Base URL for VLLM API
-  port: 8000                           # Port for VLLM server
-  model: ""{model_name}""                # Default model to use
-  max_retries: 3                       # Number of retries for API calls
-  retry_delay: 1.0                     # Initial delay between retries (seconds)
-
-# Ingest configuration
-ingest:
-  default_format: ""txt""  # Default output format for parsed files
-  youtube_captions: ""auto""  # Options: ""auto"", ""manual"" - caption preference
+        subprocess_commands = [
+            ""vllm"", ""serve"", str(model_name),
+        ]
+        for key, value in engine_args.items():
+            flag  = key.replace(""_"", ""-"")
+            which = str(value).lower().replace(""torch."", """")
+            if which == ""true"":
+                # Ignore --enforce-eager True
+                subprocess_commands += [""--"" + flag,]
+            elif which == ""false"":
+                # Ignore flag
+                pass
+            else:
+                subprocess_commands += [""--"" + flag, which,]
+        pass
+        vllm_process = subprocess.Popen(
+            subprocess_commands,
+            stdout = subprocess.PIPE,
+            stderr = subprocess.PIPE,
+            start_new_session = True,
+        )
+        self.vllm_process = vllm_process
 
-# LLM generation parameters
-generation:
-  temperature: {temperature}     # Higher = more creative, lower = more deterministic
-  top_p: {top_p}                 # Nucleus sampling parameter
-  chunk_size: {chunk_size}       # Size of text chunks for processing
-  overlap: {overlap}             # Overlap between chunks to maintain context
-  max_tokens: {max_tokens}       # Maximum tokens in LLM responses
-  num_pairs: {default_num_pairs} # Default number of QA pairs to generate
+        ready_message_part = b""Starting vLLM API server on""
+        ready = False
+        while vllm_process.poll() is None:
+            output = vllm_process.stdout.readline()
+            if not output:
+                print(""Stdout stream ended before readiness message detected."")
+                break
+            output_str = output.decode('utf-8', errors='ignore').strip()
+            print(f""vLLM STDOUT: {output_str}"")
+            if ready_message_part in output:
+                print(f""\n--- vLLM Server Ready (Detected: '{ready_message_part.decode()}') ---"")
+                ready = True
+                break
+            pass
+        pass
+        if vllm_process is None:
+            raise RuntimeError(""Unsloth: vllm_process failed to load!"")
+        trial = 0
+        while not self.check_vllm_status():
+            if trial >= 100:
+                raise RuntimeError(""Unsloth: vllm_process failed to load!"")
+            trial += 1
+            time.sleep(1)
+        return
+    pass
 
-# Content cleanup parameters
-cleanup:
-  threshold: {cleanup_threshold}       # Default quality threshold (1-10)
-  batch_size: {cleanup_batch_size}     # Number of items per batch for rating
-  temperature: {cleanup_temperature}   # Temperature for rating (lower = more consistent)
+    @staticmethod
+    def from_pretrained(
+        model_name = ""unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"",
+        max_seq_length = 2048,
+        gpu_memory_utilization = 0.9,
+        float8_kv_cache = False,
+        conservativeness = 1.0,
+        token = None,
+        **kwargs,
+    ):
+        return SyntheticDataKit(
+            model_name = model_name,
+            max_seq_length = max_seq_length,
+            gpu_memory_utilization = gpu_memory_utilization,
+            float8_kv_cache = float8_kv_cache,
+            conservativeness = conservativeness,
+            token = token,
+            **kwargs,
+        )
+    pass
 
-# Format conversion parameters
-format:
-  default: ""jsonl""   # Default output format
-  include_metadata: true  # Include metadata in output files
-  pretty_json: true  # Use indentation in JSON output
+    @staticmethod
+    def check_vllm_status():
+        try:
+            response = requests.get(""http://localhost:8000/metrics"")
+            if response.status_code == 200:
+                return True
+        except requests.exceptions.ConnectionError:
+            return False
+        pass
+    pass
 
-# Prompts for different tasks
-prompts:
-  # Summary generation prompt
-  summary: |
-    Summarize this document in 3-5 sentences, focusing on the main topic and key concepts.
+    def cleanup(self):
+        if not hasattr(self, ""vllm_process""): return
 
-  # QA pair generation prompt
-  qa_generation: |
-    Create {num_pairs} question-answer pairs from this text for LLM training.
+        vllm_process = self.vllm_process
+        print(""Attempting to terminate the VLLM server gracefully..."")
+        try:
+            vllm_process.terminate()
+            vllm_process.wait(timeout=10)
+            print(""Server terminated gracefully."")
+        except subprocess.TimeoutExpired:
+            print(""Server did not terminate gracefully after 10 seconds. Forcing kill..."")
+            vllm_process.kill()
+            vllm_process.wait()
+            print(""Server killed forcefully."")
+        except Exception as e:
+             print(f""An error occurred while trying to stop the process: {e}"")
+             try:
+                 if vllm_process.poll() is None:
+                     print(""Attempting forceful kill due to error..."")
+                     vllm_process.kill()
+                     vllm_process.wait()
+                     print(""Server killed forcefully after error."")
+             except Exception as kill_e:
+                 print(f""Error during forceful kill: {kill_e}"")
+        for _ in range(10):
+            torch.cuda.empty_cache()
+            gc.collect()
+    pass
 
-    Rules:
-    1. Questions must be about important facts in the text
-    2. Answers must be directly supported by the text
-    3. Return JSON format only:
+    def __enter__(self): return self
+    def __exit__(self, *exc): self.cleanup()
+    def __del__(self): self.cleanup()
 
-    [
-      {{
-        ""question"": ""Question 1?"",
-        ""answer"": ""Answer 1.""
-      }},
-      {{
-        ""question"": ""Question 2?"",
-        ""answer"": ""Answer 2.""
-      }}
-    ]
+    def truncate(self, filename = None):
+        # Truncates by summary and max generation
+        assert(filename is not None)
+        assert(os.path.exists(filename))
+        assert(hasattr(self, ""tokenizer""))
 
-    Text:
-    {text}
+        with open(filename, ""r"") as f: text = f.read()
 
-  # QA pair rating prompt
-  qa_rating: |
-    Rate each of these question-answer pairs for quality and return exactly this JSON format:
+        max_tokens = self.max_seq_length - self.max_generation_tokens*2 - 2
+        input_ids = self.tokenizer(text).input_ids
+        length = len(text)
+        original_length = len(text)
+        original_n_tokens = len(input_ids)
 
-    [
-      {{""question"": ""same question text"", ""answer"": ""same answer text"", ""rating"": n}}
-    ]
+        if len(input_ids) > max_tokens:
+            # Will fix later, but for now we simply naively truncate by ratios
+            length = original_length
+            while True:
+                input_ids = self.tokenizer(text[:length]).input_ids
+                if len(input_ids) < max_tokens or length == 0: break
+                length = length * (max_tokens/len(input_ids))
+                length = max(int(length), 0)
+            pass
+            print(f""Unsloth: Will truncate your data which has {original_n_tokens} tokens to {len(input_ids)} tokens."")
 
-    Where n is a number from 1-10.
+            with open(filename, ""w"") as f: f.write(text[:length])
+        pass
+        return filename, length
+    pass
 
-    DO NOT include any text outside of the JSON array, just return valid JSON:
+    def prepare_qa_generation(
+        self,
+        output_folder = ""data"",
+        max_generation_tokens = 512,
+        temperature = 0.7,
+        top_p = 0.95,
+        overlap = 64,
+        default_num_pairs = 25,
+        cleanup_threshold = 1.0,
+        cleanup_batch_size = 4,
+        cleanup_temperature = 0.3,
+    ):
+        assert(hasattr(self, ""model_name""))
+        assert(hasattr(self, ""max_seq_length""))
+        assert(max_generation_tokens < self.max_seq_length)
 
-    {pairs}""""""
+        locations = ""pdf,html,youtube,docx,ppt,txt,output,generated,cleaned,final""
+        locations = locations.split("","")
+        for path in locations:
+            os.makedirs(os.path.join(output_folder, path), exist_ok = True)
+        pass
 
+        self.max_generation_tokens = max_generation_tokens
 
-def configure_synthetic_data_kit(
-    model_name = ""unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit"",
-    temperature = 0.7,
-    top_p = 0.95,
-    chunk_size = 4000,
-    overlap = 200,
-    max_tokens = 512,
-    default_num_pairs = 25,
-    cleanup_threshold = 1.0,
-    cleanup_batch_size = 4,
-    cleanup_temperature = 0.3,
-):
-    config = synthetic_config_string\
-        .replace(""{model_name}"", str(model_name))\
-        .replace(""{temperature}"", str(temperature))\
-        .replace(""{top_p}"", str(top_p))\
-        .replace(""{chunk_size}"", str(chunk_size))\
-        .replace(""{overlap}"", str(overlap))\
-        .replace(""{max_tokens}"", str(max_tokens))\
-        .replace(""{default_num_pairs}"", str(default_num_pairs))\
-        .replace(""{cleanup_threshold}"", str(cleanup_threshold))\
-        .replace(""{cleanup_batch_size}"", str(cleanup_batch_size))\
-        .replace(""{cleanup_temperature}"", str(cleanup_temperature))
+        config = synthetic_qa_config\
+            .replace(""{data_output_location}"", str(output_folder))\
+            .replace(""{model_name}"", str(self.model_name))\
+            .replace(""{temperature}"", str(temperature))\
+            .replace(""{top_p}"", str(top_p))\
+            .replace(""{chunk_size}"", str(self.max_seq_length - max_generation_tokens*2 - 2))\
+            .replace(""{overlap}"", str(overlap))\
+            .replace(""{max_tokens}"", str(max_generation_tokens))\
+            .replace(""{default_num_pairs}"", str(default_num_pairs))\
+            .replace(""{cleanup_threshold}"", str(cleanup_threshold))\
+            .replace(""{cleanup_batch_size}"", str(cleanup_batch_size))\
+            .replace(""{cleanup_temperature}"", str(cleanup_temperature))
 
-    return config
+        with open(""synthetic_data_kit_config.yaml"", ""w"") as f: f.write(config)
+    pass
 pass
diff --git a/unsloth/dataprep/synthetic_configs.py b/unsloth/dataprep/synthetic_configs.py
new file mode 100644
index 0000000..f428177
--- /dev/null
+++ b/unsloth/dataprep/synthetic_configs.py
@@ -0,0 +1,111 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+synthetic_qa_config = """"""\
+# Master configuration file for Synthetic Data Kit
+
+# Global paths configuration
+paths:
+  # Input data locations
+  input:
+    pdf: ""{data_output_location}/pdf""
+    html: ""{data_output_location}/html""
+    youtube: ""{data_output_location}/youtube""
+    docx: ""{data_output_location}/docx""
+    ppt: ""{data_output_location}/ppt""
+    txt: ""{data_output_location}/txt""
+
+  # Output locations
+  output:
+    parsed: ""{data_output_location}/output""      # Where parsed text files are saved
+    generated: ""{data_output_location}/generated"" # Where generated content is saved
+    cleaned: ""{data_output_location}/cleaned""     # Where cleaned content is saved
+    final: ""{data_output_location}/final""         # Where final formatted content is saved
+
+# VLLM server configuration
+vllm:
+  api_base: ""http://localhost:8000/v1"" # Base URL for VLLM API
+  port: 8000                           # Port for VLLM server
+  model: ""{model_name}""                # Default model to use
+  max_retries: 3                       # Number of retries for API calls
+  retry_delay: 1.0                     # Initial delay between retries (seconds)
+
+# Ingest configuration
+ingest:
+  default_format: ""txt""  # Default output format for parsed files
+  youtube_captions: ""auto""  # Options: ""auto"", ""manual"" - caption preference
+
+# LLM generation parameters
+generation:
+  temperature: {temperature}     # Higher = more creative, lower = more deterministic
+  top_p: {top_p}                 # Nucleus sampling parameter
+  chunk_size: {chunk_size}       # Size of text chunks for processing
+  overlap: {overlap}             # Overlap between chunks to maintain context
+  max_tokens: {max_tokens}       # Maximum tokens in LLM responses
+  num_pairs: {default_num_pairs} # Default number of QA pairs to generate
+
+# Content cleanup parameters
+cleanup:
+  threshold: {cleanup_threshold}       # Default quality threshold (1-10)
+  batch_size: {cleanup_batch_size}     # Number of items per batch for rating
+  temperature: {cleanup_temperature}   # Temperature for rating (lower = more consistent)
+
+# Format conversion parameters
+format:
+  default: ""jsonl""   # Default output format
+  include_metadata: true  # Include metadata in output files
+  pretty_json: true  # Use indentation in JSON output
+
+# Prompts for different tasks
+prompts:
+  # Summary generation prompt
+  summary: |
+    Summarize this document in 3-5 sentences, focusing on the main topic and key concepts.
+
+  # QA pair generation prompt
+  qa_generation: |
+    Create {num_pairs} question-answer pairs from this text for LLM training.
+
+    Rules:
+    1. Questions must be about important facts in the text
+    2. Answers must be directly supported by the text
+    3. Return JSON format only:
+
+    [
+      {{
+        ""question"": ""Question 1?"",
+        ""answer"": ""Answer 1.""
+      }},
+      {{
+        ""question"": ""Question 2?"",
+        ""answer"": ""Answer 2.""
+      }}
+    ]
+
+    Text:
+    {text}
+
+  # QA pair rating prompt
+  qa_rating: |
+    Rate each of these question-answer pairs for quality and return exactly this JSON format:
+
+    [
+      {{""question"": ""same question text"", ""answer"": ""same answer text"", ""rating"": n}}
+    ]
+
+    Where n is a number from 1-10.
+
+    DO NOT include any text outside of the JSON array, just return valid JSON:
+
+    {pairs}""""""
\ No newline at end of file
diff --git a/unsloth/models/llama4.py b/unsloth/models/llama4.py
new file mode 100644
index 0000000..9818b3d
--- /dev/null
+++ b/unsloth/models/llama4.py
@@ -0,0 +1,16 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from unsloth_studio.models import patch_llama4
+patch_llama4()
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 3d75c35..7e90447 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -302,7 +302,7 @@ class FastLanguageModel(FastLlamaModel):
             dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
-        elif model_type == ""qwen3"" or model_type == ""qwen3_moe"":
+        elif model_type == ""qwen3"":# or model_type == ""qwen3_moe"":
             if not SUPPORTS_QWEN3 or not SUPPORTS_QWEN3_MOE:
                 raise ImportError(
                     f""Unsloth: Your transformers version of {transformers_version} does not support Qwen3.\n""\
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index bf7a1a1..206d4e5 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -797,15 +797,6 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen3-14B-Base"",
         ""unsloth/Qwen3-14B-Base-bnb-4bit"",
     ),
-    ""unsloth/Qwen3-32B-Base-unsloth-bnb-4bit"" : (
-        ""unsloth/Qwen3-32B-Base"",
-        ""Qwen/Qwen3-32B-Base"",
-        ""unsloth/Qwen3-32B-Base-bnb-4bit"",
-    ),
-    ""unsloth/Qwen3-30B-A3B-Base-bnb-4bit"" : (
-        ""unsloth/Qwen3-30B-A3B-Base"",
-        ""Qwen/Qwen3-30B-A3B-Base"",
-    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/registry/REGISTRY.md b/unsloth/registry/REGISTRY.md
new file mode 100644
index 0000000..a0b3d96
--- /dev/null
+++ b/unsloth/registry/REGISTRY.md
@@ -0,0 +1,110 @@
+## Model Registry
+
+### Structure
+```
+unsloth
+    -registry
+        __init__.py
+        registry.py
+        _llama.py
+        _mistral.py
+        _phi.py
+        ...
+```
+
+Each model is registered in a separate file within the `registry` module (e.g. `registry/_llama.py`).
+
+Within each model registration file, a high-level `ModelMeta` is created for each model version, with the following structure:
+```python
+@dataclass
+class ModelMeta:
+    org: str
+    base_name: str
+    model_version: str
+    model_info_cls: type[ModelInfo]
+    model_sizes: list[str] = field(default_factory=list)
+    instruct_tags: list[str] = field(default_factory=list)
+    quant_types: list[QuantType] | dict[str, list[QuantType]] = field(default_factory=list)
+    is_multimodal: bool = False
+```
+
+Each model then instantiates a global `ModelMeta` for its specific model version, defining how the model path (e.g. `unsloth/Llama-3.1-8B-Instruct`) is constructed since each model type has a different naming convention.
+```python
+LlamaMeta_3_1 = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[None, ""Instruct""],
+    model_version=""3.1"",
+    model_sizes=[""8""],
+    model_info_cls=LlamaModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+```
+
+`LlamaModelInfo` is a subclass of `ModelInfo` that defines the model path for each model size and quant type.
+```python
+class LlamaModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{version}-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+```
+
+Once these constructs are defined, the model is registered by writing a register_xx_models function.
+```python
+def register_llama_3_1_models(include_original_model: bool = False):
+    global _IS_LLAMA_3_1_REGISTERED
+    if _IS_LLAMA_3_1_REGISTERED:
+        return
+    _register_models(LlamaMeta_3_1, include_original_model=include_original_model)
+    _IS_LLAMA_3_1_REGISTERED = True
+```
+
+`_register_models` is a helper function that registers the model with the registry.  The global `_IS_XX_REGISTERED` is used to prevent duplicate registration.
+
+Once a model is registered, registry.registry.MODEL_REGISTRY is updated with the model info and can be searched with `registry.search_models`.
+
+### Tests
+
+The `tests/test_model_registry.py` file contains tests for the model registry.
+
+Also, each model registration file is an executable module that checks that all registered models are available on `huggingface_hub`.
+```python
+python unsloth.registry._llama.py
+```
+
+Prints the following (abridged) output:
+```bash
+ unsloth/Llama-3.1-8B
+ unsloth/Llama-3.1-8B-bnb-4bit
+ unsloth/Llama-3.1-8B-unsloth-bnb-4bit
+ meta-llama/Llama-3.1-8B
+ unsloth/Llama-3.1-8B-Instruct
+ unsloth/Llama-3.1-8B-Instruct-bnb-4bit
+ unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit
+ meta-llama/Llama-3.1-8B-Instruct
+ unsloth/Llama-3.2-1B
+ unsloth/Llama-3.2-1B-bnb-4bit
+ unsloth/Llama-3.2-1B-unsloth-bnb-4bit
+ meta-llama/Llama-3.2-1B
+...
+```
+
+### TODO
+- Model Collections
+    - [x] Gemma3
+    - [ ] Llama3.1
+    - [x] Llama3.2
+    - [x] MistralSmall
+    - [x] Qwen2.5
+    - [x] Qwen2.5-VL
+    - [ ] Qwen2.5 Coder
+    - [x] QwenQwQ-32B
+    - [x] Deepseek v3
+    - [x] Deepseek R1
+    - [x] Phi-4
+    - [ ] Unsloth 4-bit Dynamic Quants
+    - [ ] Vision/multimodal models
+- Sync model uploads with registry
+- Add utility methods for tracking model stats
\ No newline at end of file
diff --git a/unsloth/registry/__init__.py b/unsloth/registry/__init__.py
new file mode 100644
index 0000000..5874743
--- /dev/null
+++ b/unsloth/registry/__init__.py
@@ -0,0 +1,51 @@
+from ._deepseek import register_deepseek_models as _register_deepseek_models
+from ._gemma import register_gemma_models as _register_gemma_models
+from ._llama import register_llama_models as _register_llama_models
+from ._mistral import register_mistral_models as _register_mistral_models
+from ._phi import register_phi_models as _register_phi_models
+from ._qwen import register_qwen_models as _register_qwen_models
+from .registry import MODEL_REGISTRY, ModelInfo, QuantType
+
+_ARE_MODELS_REGISTERED = False
+
+def register_models():
+    global _ARE_MODELS_REGISTERED
+
+    if _ARE_MODELS_REGISTERED:
+        return
+    _register_deepseek_models()
+    _register_gemma_models()
+    _register_llama_models()
+    _register_mistral_models()
+    _register_phi_models()
+    _register_qwen_models()
+
+    _ARE_MODELS_REGISTERED = True
+
+def search_models(org: str = None, base_name: str = None, version: str = None, size: str = None, quant_types: list[QuantType] = None, search_pattern: str = None) -> list[ModelInfo]:
+    """"""
+    Get model info from the registry.
+
+    See registry.ModelInfo for more fields.
+
+    If search_pattern is provided, the full model path will be matched against the pattern, where the model path is the model_id on huggingface hub.
+
+    """"""
+    if not _ARE_MODELS_REGISTERED:
+        register_models()
+    
+    model_infos = MODEL_REGISTRY.values()
+    if org:
+        model_infos = [model_info for model_info in model_infos if model_info.org == org]
+    if base_name:
+        model_infos = [model_info for model_info in model_infos if model_info.base_name == base_name]
+    if version:
+        model_infos = [model_info for model_info in model_infos if model_info.version == version]
+    if size:
+        model_infos = [model_info for model_info in model_infos if model_info.size == size]
+    if quant_types:
+        model_infos = [model_info for model_info in model_infos if any(model_info.quant_type == quant_type for quant_type in quant_types)]
+    if search_pattern:
+        model_infos = [model_info for model_info in model_infos if search_pattern in model_info.model_path]
+    
+    return model_infos
\ No newline at end of file
diff --git a/unsloth/registry/_deepseek.py b/unsloth/registry/_deepseek.py
new file mode 100644
index 0000000..153a0e5
--- /dev/null
+++ b/unsloth/registry/_deepseek.py
@@ -0,0 +1,179 @@
+from unsloth.registry.registry import ModelInfo, ModelMeta, QuantType, _register_models
+
+_IS_DEEPSEEK_V3_REGISTERED = False
+_IS_DEEPSEEK_V3_0324_REGISTERED = False
+_IS_DEEPSEEK_R1_REGISTERED = False
+_IS_DEEPSEEK_R1_ZERO_REGISTERED = False
+_IS_DEEPSEEK_R1_DISTILL_LLAMA_REGISTERED = False
+_IS_DEEPSEEK_R1_DISTILL_QWEN_REGISTERED = False
+
+class DeepseekV3ModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-V{version}""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+class DeepseekR1ModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{version}"" if version else base_name
+        if size:
+            key = f""{key}-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+    
+# Deepseek V3 Model Meta
+DeepseekV3Meta = ModelMeta(
+    org=""deepseek-ai"",
+    base_name=""DeepSeek"",
+    instruct_tags=[None],
+    model_version=""3"",
+    model_sizes=[""""],
+    model_info_cls=DeepseekV3ModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BF16],
+)
+
+DeepseekV3_0324Meta = ModelMeta(
+    org=""deepseek-ai"",
+    base_name=""DeepSeek"",
+    instruct_tags=[None],
+    model_version=""3-0324"",
+    model_sizes=[""""],
+    model_info_cls=DeepseekV3ModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.GGUF],
+)
+
+DeepseekR1Meta = ModelMeta(
+    org=""deepseek-ai"",
+    base_name=""DeepSeek-R1"",
+    instruct_tags=[None],
+    model_version="""",
+    model_sizes=[""""],
+    model_info_cls=DeepseekR1ModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BF16, QuantType.GGUF],
+)
+
+DeepseekR1ZeroMeta = ModelMeta(
+    org=""deepseek-ai"",
+    base_name=""DeepSeek-R1"",
+    instruct_tags=[None],
+    model_version=""Zero"",
+    model_sizes=[""""],
+    model_info_cls=DeepseekR1ModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.GGUF],
+)
+
+DeepseekR1DistillLlamaMeta = ModelMeta(
+    org=""deepseek-ai"",
+    base_name=""DeepSeek-R1-Distill"",
+    instruct_tags=[None],
+    model_version=""Llama"",
+    model_sizes=[""8"", ""70""],
+    model_info_cls=DeepseekR1ModelInfo,
+    is_multimodal=False,
+    quant_types={""8"": [QuantType.UNSLOTH, QuantType.GGUF], ""70"": [QuantType.GGUF]},
+)
+
+# Deepseek R1 Distill Qwen Model Meta
+DeepseekR1DistillQwenMeta = ModelMeta(
+    org=""deepseek-ai"",
+    base_name=""DeepSeek-R1-Distill"",
+    instruct_tags=[None],
+    model_version=""Qwen"",
+    model_sizes=[""1.5"", ""7"", ""14"", ""32""],
+    model_info_cls=DeepseekR1ModelInfo,
+    is_multimodal=False,
+    quant_types={
+        ""1.5"": [QuantType.UNSLOTH, QuantType.BNB, QuantType.GGUF],
+        ""7"": [QuantType.UNSLOTH, QuantType.BNB],
+        ""14"": [QuantType.UNSLOTH, QuantType.BNB, QuantType.GGUF],
+        ""32"": [QuantType.GGUF, QuantType.BNB],
+    },
+)
+        
+def register_deepseek_v3_models(include_original_model: bool = False):
+    global _IS_DEEPSEEK_V3_REGISTERED
+    if _IS_DEEPSEEK_V3_REGISTERED:
+        return
+    _register_models(DeepseekV3Meta, include_original_model=include_original_model)
+    _IS_DEEPSEEK_V3_REGISTERED = True
+
+def register_deepseek_v3_0324_models(include_original_model: bool = False):
+    global _IS_DEEPSEEK_V3_0324_REGISTERED
+    if _IS_DEEPSEEK_V3_0324_REGISTERED:
+        return
+    _register_models(DeepseekV3_0324Meta, include_original_model=include_original_model)
+    _IS_DEEPSEEK_V3_0324_REGISTERED = True
+
+def register_deepseek_r1_models(include_original_model: bool = False):
+    global _IS_DEEPSEEK_R1_REGISTERED
+    if _IS_DEEPSEEK_R1_REGISTERED:
+        return
+    _register_models(DeepseekR1Meta, include_original_model=include_original_model)
+    _IS_DEEPSEEK_R1_REGISTERED = True
+
+def register_deepseek_r1_zero_models(include_original_model: bool = False):
+    global _IS_DEEPSEEK_R1_ZERO_REGISTERED
+    if _IS_DEEPSEEK_R1_ZERO_REGISTERED:
+        return
+    _register_models(DeepseekR1ZeroMeta, include_original_model=include_original_model)
+    _IS_DEEPSEEK_R1_ZERO_REGISTERED = True
+
+def register_deepseek_r1_distill_llama_models(include_original_model: bool = False):
+    global _IS_DEEPSEEK_R1_DISTILL_LLAMA_REGISTERED
+    if _IS_DEEPSEEK_R1_DISTILL_LLAMA_REGISTERED:
+        return
+    _register_models(DeepseekR1DistillLlamaMeta, include_original_model=include_original_model)
+    _IS_DEEPSEEK_R1_DISTILL_LLAMA_REGISTERED = True
+
+def register_deepseek_r1_distill_qwen_models(include_original_model: bool = False):
+    global _IS_DEEPSEEK_R1_DISTILL_QWEN_REGISTERED
+    if _IS_DEEPSEEK_R1_DISTILL_QWEN_REGISTERED:
+        return
+    _register_models(DeepseekR1DistillQwenMeta, include_original_model=include_original_model)
+    _IS_DEEPSEEK_R1_DISTILL_QWEN_REGISTERED = True
+
+def register_deepseek_models(include_original_model: bool = False):
+    register_deepseek_v3_models(include_original_model=include_original_model)
+    register_deepseek_v3_0324_models(include_original_model=include_original_model)
+    register_deepseek_r1_models(include_original_model=include_original_model)
+    register_deepseek_r1_zero_models(include_original_model=include_original_model)
+    register_deepseek_r1_distill_llama_models(include_original_model=include_original_model)
+    register_deepseek_r1_distill_qwen_models(include_original_model=include_original_model)
+
+def _list_deepseek_r1_distill_models():
+    from unsloth.utils.hf_hub import ModelInfo as HfModelInfo
+    from unsloth.utils.hf_hub import list_models
+    models: list[HfModelInfo] = list_models(author=""unsloth"", search=""Distill"", limit=1000)
+    distill_models = []
+    for model in models:
+        model_id = model.id
+        model_name = model_id.split(""/"")[-1]
+        # parse out only the version
+        version = model_name.removeprefix(""DeepSeek-R1-Distill-"")
+        distill_models.append(version)
+
+    return distill_models
+
+
+register_deepseek_models(include_original_model=True)
+
+if __name__ == ""__main__"":
+    from unsloth.registry.registry import MODEL_REGISTRY, _check_model_info
+    MODEL_REGISTRY.clear()
+    
+    register_deepseek_models(include_original_model=True)
+    
+    for model_id, model_info in MODEL_REGISTRY.items():
+        model_info = _check_model_info(model_id)
+        if model_info is None:
+            print(f""\u2718 {model_id}"")
+        else:
+            print(f""\u2713 {model_id}"")
+    # distill_models = _list_deepseek_r1_distill_models()
+    # for model in sorted(distill_models):
+    #     if ""qwen"" in model.lower():
+    #         print(model)
\ No newline at end of file
diff --git a/unsloth/registry/_gemma.py b/unsloth/registry/_gemma.py
new file mode 100644
index 0000000..9490c84
--- /dev/null
+++ b/unsloth/registry/_gemma.py
@@ -0,0 +1,66 @@
+from unsloth.registry.registry import ModelInfo, ModelMeta, QuantType, _register_models
+
+_IS_GEMMA_3_BASE_REGISTERED = False
+_IS_GEMMA_3_INSTRUCT_REGISTERED = False
+
+class GemmaModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{version}-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+# Gemma3 Base Model Meta
+GemmaMeta3Base = ModelMeta(
+    org=""google"",
+    base_name=""gemma"",
+    instruct_tags=[""pt""],  # pt = base
+    model_version=""3"",
+    model_sizes=[""1"", ""4"", ""12"", ""27""],
+    model_info_cls=GemmaModelInfo,
+    is_multimodal=True,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+
+# Gemma3 Instruct Model Meta
+GemmaMeta3Instruct = ModelMeta(
+    org=""google"",
+    base_name=""gemma"",
+    instruct_tags=[""it""],  # it = instruction tuned
+    model_version=""3"",
+    model_sizes=[""1"", ""4"", ""12"", ""27""],
+    model_info_cls=GemmaModelInfo,
+    is_multimodal=True,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH, QuantType.GGUF],
+)
+
+def register_gemma_3_base_models(include_original_model: bool = False):
+    global _IS_GEMMA_3_BASE_REGISTERED
+    if _IS_GEMMA_3_BASE_REGISTERED:
+        return
+    _register_models(GemmaMeta3Base, include_original_model=include_original_model)
+    _IS_GEMMA_3_BASE_REGISTERED = True
+
+def register_gemma_3_instruct_models(include_original_model: bool = False):
+    global _IS_GEMMA_3_INSTRUCT_REGISTERED
+    if _IS_GEMMA_3_INSTRUCT_REGISTERED:
+        return
+    _register_models(GemmaMeta3Instruct, include_original_model=include_original_model)
+    _IS_GEMMA_3_INSTRUCT_REGISTERED = True
+
+def register_gemma_models(include_original_model: bool = False):
+    register_gemma_3_base_models(include_original_model=include_original_model)
+    register_gemma_3_instruct_models(include_original_model=include_original_model)
+
+
+if __name__ == ""__main__"":
+    from unsloth.registry.registry import MODEL_REGISTRY, _check_model_info
+    MODEL_REGISTRY.clear()
+    
+    register_gemma_models(include_original_model=True)
+    
+    for model_id, model_info in MODEL_REGISTRY.items():
+        model_info = _check_model_info(model_id)
+        if model_info is None:
+            print(f""\u2718 {model_id}"")
+        else:
+            print(f""\u2713 {model_id}"")
diff --git a/unsloth/registry/_llama.py b/unsloth/registry/_llama.py
new file mode 100644
index 0000000..f1b9dbd
--- /dev/null
+++ b/unsloth/registry/_llama.py
@@ -0,0 +1,113 @@
+from unsloth.registry.registry import ModelInfo, ModelMeta, QuantType, _register_models
+
+_IS_LLAMA_3_1_REGISTERED = False
+_IS_LLAMA_3_2_REGISTERED = False
+_IS_LLAMA_3_2_VISION_REGISTERED = False
+
+
+class LlamaModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{version}-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+
+class LlamaVisionModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{version}-{size}B-Vision""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+
+# Llama 3.1
+LlamaMeta_3_1 = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[None, ""Instruct""],
+    model_version=""3.1"",
+    model_sizes=[""8""],
+    model_info_cls=LlamaModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+
+# Llama 3.2 Base Models
+LlamaMeta_3_2_Base = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[None],
+    model_version=""3.2"",
+    model_sizes=[""1"", ""3""],
+    model_info_cls=LlamaModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+
+# Llama 3.2 Instruction Tuned Models
+LlamaMeta_3_2_Instruct = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[""Instruct""],
+    model_version=""3.2"",
+    model_sizes=[""1"", ""3""],
+    model_info_cls=LlamaModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH, QuantType.GGUF],
+)
+
+# Llama 3.2 Vision
+LlamaMeta_3_2_Vision = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[None, ""Instruct""],
+    model_version=""3.2"",
+    model_sizes=[""11"", ""90""],
+    model_info_cls=LlamaVisionModelInfo,
+    is_multimodal=True,
+    quant_types={
+        ""11"": [QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+        ""90"": [QuantType.NONE],
+    },
+)
+
+
+def register_llama_3_1_models(include_original_model: bool = False):
+    global _IS_LLAMA_3_1_REGISTERED
+    if _IS_LLAMA_3_1_REGISTERED:
+        return
+    _register_models(LlamaMeta_3_1, include_original_model=include_original_model)
+    _IS_LLAMA_3_1_REGISTERED = True
+
+def register_llama_3_2_models(include_original_model: bool = False):
+    global _IS_LLAMA_3_2_REGISTERED
+    if _IS_LLAMA_3_2_REGISTERED:
+        return
+    _register_models(LlamaMeta_3_2_Base, include_original_model=include_original_model)
+    _register_models(LlamaMeta_3_2_Instruct, include_original_model=include_original_model)
+    _IS_LLAMA_3_2_REGISTERED = True
+
+def register_llama_3_2_vision_models(include_original_model: bool = False):
+    global _IS_LLAMA_3_2_VISION_REGISTERED
+    if _IS_LLAMA_3_2_VISION_REGISTERED:
+        return
+    _register_models(LlamaMeta_3_2_Vision, include_original_model=include_original_model)
+    _IS_LLAMA_3_2_VISION_REGISTERED = True
+
+
+def register_llama_models(include_original_model: bool = False):
+    register_llama_3_1_models(include_original_model=include_original_model)
+    register_llama_3_2_models(include_original_model=include_original_model)
+    register_llama_3_2_vision_models(include_original_model=include_original_model)
+
+if __name__ == ""__main__"":
+    from unsloth.registry.registry import MODEL_REGISTRY, _check_model_info
+    MODEL_REGISTRY.clear()
+
+    register_llama_models(include_original_model=True)
+
+    for model_id, model_info in MODEL_REGISTRY.items():
+        model_info = _check_model_info(model_id)
+        if model_info is None:
+            print(f""\u2718 {model_id}"")
+        else:
+            print(f""\u2713 {model_id}"")
diff --git a/unsloth/registry/_mistral.py b/unsloth/registry/_mistral.py
new file mode 100644
index 0000000..44cd1e7
--- /dev/null
+++ b/unsloth/registry/_mistral.py
@@ -0,0 +1,70 @@
+import copy
+
+from unsloth.registry.registry import ModelInfo, ModelMeta, QuantType, _register_models
+
+_IS_MISTRAL_SMALL_REGISTERED = False
+
+_MISTRAL_SMALL_03_25_VERSION = ""2503""
+_MISTRAL_SMALL_01_25_VERSION = ""2501""
+_MISTRAL_SMALL_09_24_VERSION = ""2409"" # Not uploaded to unsloth
+
+class MistralSmallModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        if version == _MISTRAL_SMALL_03_25_VERSION:
+            key = f""{base_name}-3.1-{size}B-{instruct_tag}""
+        else:
+            key = f""{base_name}-{size}B-{instruct_tag}""
+        key += f""-{version}""
+        key = cls.append_quant_type(key, quant_type)
+        
+        return key
+
+
+MistralSmall_2503_Base_Meta = ModelMeta(
+    org=""mistralai"",
+    base_name=""Mistral-Small"",
+    instruct_tags=[""Base""],
+    model_version=_MISTRAL_SMALL_03_25_VERSION,
+    model_sizes=[""24""],
+    model_info_cls=MistralSmallModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.UNSLOTH, QuantType.BNB],
+)
+
+MistralSmall_2503_Instruct_Meta = copy.deepcopy(MistralSmall_2503_Base_Meta)
+MistralSmall_2503_Instruct_Meta.instruct_tags = [""Instruct""]
+MistralSmall_2503_Instruct_Meta.quant_types = [QuantType.NONE, QuantType.UNSLOTH, QuantType.BNB, QuantType.GGUF]
+
+MistralSmall_2501_Base_Meta = copy.deepcopy(MistralSmall_2503_Base_Meta)
+MistralSmall_2501_Base_Meta.model_version = _MISTRAL_SMALL_01_25_VERSION
+
+MistralSmall_2501_Instruct_Meta = copy.deepcopy(MistralSmall_2503_Instruct_Meta)
+MistralSmall_2501_Instruct_Meta.model_version = _MISTRAL_SMALL_01_25_VERSION
+
+def register_mistral_small_models(include_original_model: bool = False):
+    global _IS_MISTRAL_SMALL_REGISTERED
+    if _IS_MISTRAL_SMALL_REGISTERED:
+        return
+    _register_models(MistralSmall_2503_Base_Meta, include_original_model=include_original_model)
+    _register_models(MistralSmall_2503_Instruct_Meta, include_original_model=include_original_model)
+    _register_models(MistralSmall_2501_Base_Meta, include_original_model=include_original_model)
+    _register_models(MistralSmall_2501_Instruct_Meta, include_original_model=include_original_model)
+
+    _IS_MISTRAL_SMALL_REGISTERED = True
+
+def register_mistral_models(include_original_model: bool = False):
+    register_mistral_small_models(include_original_model=include_original_model)
+
+if __name__ == ""__main__"":
+    from unsloth.registry.registry import MODEL_REGISTRY, _check_model_info
+    MODEL_REGISTRY.clear()
+    
+    register_mistral_models(include_original_model=True)
+    
+    for model_id, model_info in MODEL_REGISTRY.items():
+        model_info = _check_model_info(model_id)
+        if model_info is None:
+            print(f""\u2718 {model_id}"")
+        else:
+            print(f""\u2713 {model_id}"")    
\ No newline at end of file
diff --git a/unsloth/registry/_phi.py b/unsloth/registry/_phi.py
new file mode 100644
index 0000000..d06ec8d
--- /dev/null
+++ b/unsloth/registry/_phi.py
@@ -0,0 +1,65 @@
+from unsloth.registry.registry import ModelInfo, ModelMeta, QuantType, _register_models
+
+_IS_PHI_4_REGISTERED = False
+_IS_PHI_4_INSTRUCT_REGISTERED = False
+
+class PhiModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{version}""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+# Phi Model Meta
+PhiMeta4 = ModelMeta(
+    org=""microsoft"",
+    base_name=""phi"",
+    instruct_tags=[None],
+    model_version=""4"",
+    model_sizes=[""1""],  # Assuming only one size
+    model_info_cls=PhiModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+
+# Phi Instruct Model Meta
+PhiInstructMeta4 = ModelMeta(
+    org=""microsoft"",
+    base_name=""phi"",
+    instruct_tags=[""mini-instruct""],
+    model_version=""4"",
+    model_sizes=[""1""],  # Assuming only one size
+    model_info_cls=PhiModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH, QuantType.GGUF],
+)
+
+def register_phi_4_models(include_original_model: bool = False):
+    global _IS_PHI_4_REGISTERED
+    if _IS_PHI_4_REGISTERED:
+        return
+    _register_models(PhiMeta4, include_original_model=include_original_model)
+    _IS_PHI_4_REGISTERED = True
+
+def register_phi_4_instruct_models(include_original_model: bool = False):
+    global _IS_PHI_4_INSTRUCT_REGISTERED
+    if _IS_PHI_4_INSTRUCT_REGISTERED:
+        return
+    _register_models(PhiInstructMeta4, include_original_model=include_original_model)
+    _IS_PHI_4_INSTRUCT_REGISTERED = True
+
+def register_phi_models(include_original_model: bool = False):
+    register_phi_4_models(include_original_model=include_original_model)
+    register_phi_4_instruct_models(include_original_model=include_original_model)
+
+if __name__ == ""__main__"":
+    from unsloth.registry.registry import MODEL_REGISTRY, _check_model_info
+    MODEL_REGISTRY.clear()
+    
+    register_phi_models(include_original_model=True)
+    
+    for model_id, model_info in MODEL_REGISTRY.items():
+        model_info = _check_model_info(model_id)
+        if model_info is None:
+            print(f""\u2718 {model_id}"")
+        else:
+            print(f""\u2713 {model_id}"") 
\ No newline at end of file
diff --git a/unsloth/registry/_qwen.py b/unsloth/registry/_qwen.py
new file mode 100644
index 0000000..4417515
--- /dev/null
+++ b/unsloth/registry/_qwen.py
@@ -0,0 +1,117 @@
+from unsloth.registry.registry import ModelInfo, ModelMeta, QuantType, _register_models
+
+_IS_QWEN_2_5_REGISTERED = False
+_IS_QWEN_2_5_VL_REGISTERED = False
+_IS_QWEN_QWQ_REGISTERED = False
+class QwenModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}{version}-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+
+class QwenVLModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}{version}-VL-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+
+class QwenQwQModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{size}B""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+    
+class QwenQVQPreviewModelInfo(ModelInfo):
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+        key = f""{base_name}-{size}B-Preview""
+        return super().construct_model_name(base_name, version, size, quant_type, instruct_tag, key)
+    
+# Qwen2.5 Model Meta
+Qwen_2_5_Meta = ModelMeta(
+    org=""Qwen"",
+    base_name=""Qwen"",
+    instruct_tags=[None, ""Instruct""],
+    model_version=""2.5"",
+    model_sizes=[""3"", ""7""],
+    model_info_cls=QwenModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+
+# Qwen2.5 VL Model Meta
+Qwen_2_5_VLMeta = ModelMeta(
+    org=""Qwen"",
+    base_name=""Qwen"",
+    instruct_tags=[""Instruct""],  # No base, only instruction tuned
+    model_version=""2.5"",
+    model_sizes=[""3"", ""7"", ""32"", ""72""],
+    model_info_cls=QwenVLModelInfo,
+    is_multimodal=True,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH],
+)
+
+# Qwen QwQ Model Meta
+QwenQwQMeta = ModelMeta(
+    org=""Qwen"",
+    base_name=""QwQ"",
+    instruct_tags=[None],
+    model_version="""",
+    model_sizes=[""32""],
+    model_info_cls=QwenQwQModelInfo,
+    is_multimodal=False,
+    quant_types=[QuantType.NONE, QuantType.BNB, QuantType.UNSLOTH, QuantType.GGUF],
+)
+
+# Qwen QVQ Preview Model Meta
+QwenQVQPreviewMeta = ModelMeta(
+    org=""Qwen"",
+    base_name=""QVQ"",
+    instruct_tags=[None],
+    model_version="""",
+    model_sizes=[""72""],
+    model_info_cls=QwenQVQPreviewModelInfo,
+    is_multimodal=True,
+    quant_types=[QuantType.NONE, QuantType.BNB],
+)
+
+def register_qwen_2_5_models(include_original_model: bool = False):
+    global _IS_QWEN_2_5_REGISTERED
+    if _IS_QWEN_2_5_REGISTERED:
+        return
+    _register_models(Qwen_2_5_Meta, include_original_model=include_original_model)
+    _IS_QWEN_2_5_REGISTERED = True
+
+def register_qwen_2_5_vl_models(include_original_model: bool = False):
+    global _IS_QWEN_2_5_VL_REGISTERED
+    if _IS_QWEN_2_5_VL_REGISTERED:
+        return
+    _register_models(Qwen_2_5_VLMeta, include_original_model=include_original_model)
+    _IS_QWEN_2_5_VL_REGISTERED = True
+
+def register_qwen_qwq_models(include_original_model: bool = False):
+    global _IS_QWEN_QWQ_REGISTERED
+    if _IS_QWEN_QWQ_REGISTERED:
+        return
+    _register_models(QwenQwQMeta, include_original_model=include_original_model)
+    _register_models(QwenQVQPreviewMeta, include_original_model=include_original_model)
+    _IS_QWEN_QWQ_REGISTERED = True
+
+def register_qwen_models(include_original_model: bool = False):
+    register_qwen_2_5_models(include_original_model=include_original_model)
+    register_qwen_2_5_vl_models(include_original_model=include_original_model)
+    register_qwen_qwq_models(include_original_model=include_original_model)
+
+if __name__ == ""__main__"":
+    from unsloth.registry.registry import MODEL_REGISTRY, _check_model_info
+    MODEL_REGISTRY.clear()
+    
+    register_qwen_models(include_original_model=True)
+    
+    for model_id, model_info in MODEL_REGISTRY.items():
+        model_info = _check_model_info(model_id)
+        if model_info is None:
+            print(f""\u2718 {model_id}"")
+        else:
+            print(f""\u2713 {model_id}"")
diff --git a/unsloth/registry/registry.py b/unsloth/registry/registry.py
new file mode 100644
index 0000000..590beeb
--- /dev/null
+++ b/unsloth/registry/registry.py
@@ -0,0 +1,185 @@
+import warnings
+from dataclasses import dataclass, field
+from enum import Enum
+
+
+class QuantType(Enum):
+    BNB = ""bnb""
+    UNSLOTH = ""unsloth"" # dynamic 4-bit quantization
+    GGUF = ""GGUF""
+    NONE = ""none""
+    BF16 = ""bf16"" # only for Deepseek V3
+
+# Tags for Hugging Face model paths
+BNB_QUANTIZED_TAG = ""bnb-4bit""
+UNSLOTH_DYNAMIC_QUANT_TAG = ""unsloth"" + ""-"" + BNB_QUANTIZED_TAG
+GGUF_TAG = ""GGUF""
+BF16_TAG = ""bf16""
+
+QUANT_TAG_MAP = {
+    QuantType.BNB: BNB_QUANTIZED_TAG,
+    QuantType.UNSLOTH: UNSLOTH_DYNAMIC_QUANT_TAG,
+    QuantType.GGUF: GGUF_TAG,
+    QuantType.NONE: None,
+    QuantType.BF16: BF16_TAG,
+} 
+
+# NOTE: models registered with org=""unsloth"" and QUANT_TYPE.NONE are aliases of QUANT_TYPE.UNSLOTH
+@dataclass
+class ModelInfo:
+    org: str
+    base_name: str
+    version: str
+    size: int
+    name: str = None  # full model name, constructed from base_name, version, and size unless provided
+    is_multimodal: bool = False
+    instruct_tag: str = None
+    quant_type: QuantType = None
+    description: str = None
+
+    def __post_init__(self):
+        self.name = self.name or self.construct_model_name(
+            self.base_name,
+            self.version,
+            self.size,
+            self.quant_type,
+            self.instruct_tag,
+        )
+
+    @staticmethod
+    def append_instruct_tag(key: str, instruct_tag: str = None):
+        if instruct_tag:
+            key = ""-"".join([key, instruct_tag])
+        return key
+
+    @staticmethod
+    def append_quant_type(
+        key: str, quant_type: QuantType = None
+    ):
+        if quant_type != QuantType.NONE:
+            key = ""-"".join([key, QUANT_TAG_MAP[quant_type]])
+        return key
+
+    @classmethod
+    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag, key=""""):
+        key = cls.append_instruct_tag(key, instruct_tag)
+        key = cls.append_quant_type(key, quant_type)
+        return key
+
+    @property
+    def model_path(
+        self,
+    ) -> str:
+        return f""{self.org}/{self.name}""
+
+
+@dataclass
+class ModelMeta:
+    org: str
+    base_name: str
+    model_version: str
+    model_info_cls: type[ModelInfo]
+    model_sizes: list[str] = field(default_factory=list)
+    instruct_tags: list[str] = field(default_factory=list)
+    quant_types: list[QuantType] | dict[str, list[QuantType]] = field(default_factory=list)
+    is_multimodal: bool = False
+
+
+MODEL_REGISTRY: dict[str, ModelInfo] = {}
+
+
+def register_model(
+    model_info_cls: ModelInfo,
+    org: str,
+    base_name: str,
+    version: str,
+    size: int,
+    instruct_tag: str = None,
+    quant_type: QuantType = None,
+    is_multimodal: bool = False,
+    name: str = None,
+):
+    name = name or model_info_cls.construct_model_name(
+        base_name=base_name,
+        version=version,
+        size=size,
+        quant_type=quant_type,
+        instruct_tag=instruct_tag,
+    )
+    key = f""{org}/{name}""
+
+    if key in MODEL_REGISTRY:
+        raise ValueError(f""Model {key} already registered, current keys: {MODEL_REGISTRY.keys()}"")
+
+    MODEL_REGISTRY[key] = model_info_cls(
+        org=org,
+        base_name=base_name,
+        version=version,
+        size=size,
+        is_multimodal=is_multimodal,
+        instruct_tag=instruct_tag,
+        quant_type=quant_type,
+        name=name,
+    )
+
+
+def _check_model_info(model_id: str, properties: list[str] = [""lastModified""]):
+    from huggingface_hub import HfApi
+    from huggingface_hub import ModelInfo as HfModelInfo
+    from huggingface_hub.utils import RepositoryNotFoundError
+
+    api = HfApi()
+
+    try:
+        model_info: HfModelInfo = api.model_info(model_id, expand=properties)
+    except Exception as e:
+        if isinstance(e, RepositoryNotFoundError):
+            warnings.warn(f""{model_id} not found on Hugging Face"")
+            model_info = None
+        else:
+            raise e
+    return model_info
+
+
+def _register_models(model_meta: ModelMeta, include_original_model: bool = False):
+    org = model_meta.org
+    base_name = model_meta.base_name
+    instruct_tags = model_meta.instruct_tags
+    model_version = model_meta.model_version
+    model_sizes = model_meta.model_sizes
+    is_multimodal = model_meta.is_multimodal
+    quant_types = model_meta.quant_types
+    model_info_cls = model_meta.model_info_cls
+
+    for size in model_sizes:
+        for instruct_tag in instruct_tags:
+            # Handle quant types per model size
+            if isinstance(quant_types, dict):
+                _quant_types = quant_types[size]
+            else:
+                _quant_types = quant_types
+            for quant_type in _quant_types:
+                # NOTE: models registered with org=""unsloth"" and QUANT_TYPE.NONE are aliases of QUANT_TYPE.UNSLOTH
+                _org = ""unsloth"" # unsloth models -- these are all quantized versions of the original model
+                register_model(
+                    model_info_cls=model_info_cls,
+                    org=_org,
+                    base_name=base_name,
+                    version=model_version,
+                    size=size,
+                    instruct_tag=instruct_tag,
+                    quant_type=quant_type,
+                    is_multimodal=is_multimodal,
+                )
+            # include original model from releasing organization
+            if include_original_model:
+                register_model(
+                    model_info_cls=model_info_cls,
+                    org=org,
+                    base_name=base_name,
+                    version=model_version,
+                    size=size,
+                    instruct_tag=instruct_tag,
+                    quant_type=QuantType.NONE,
+                    is_multimodal=is_multimodal,
+                )
diff --git a/unsloth/utils/__init__.py b/unsloth/utils/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/unsloth/utils/hf_hub.py b/unsloth/utils/hf_hub.py
new file mode 100644
index 0000000..30255b8
--- /dev/null
+++ b/unsloth/utils/hf_hub.py
@@ -0,0 +1,78 @@
+from huggingface_hub import HfApi, ModelInfo
+
+_HFAPI: HfApi = None
+
+POPULARITY_PROPERTIES = [
+    ""downloads"",
+    ""downloadsAllTime"",
+    ""trendingScore"",
+    ""likes"",
+]
+THOUSAND = 1000
+MILLION = 1000000
+BILLION = 1000000000
+
+
+def formatted_int(value: int) -> str:
+    if value < THOUSAND:
+        return str(value)
+    elif value < MILLION:
+        return f""{float(value) / 1000:,.1f}K""
+    elif value < BILLION:
+        return f""{float(value) // 1000000:,.1f}M""
+
+
+def get_model_info(
+    model_id: str, properties: list[str] = [""safetensors"", ""lastModified""]
+) -> ModelInfo:
+    """"""
+    Get the model info for a specific model.
+
+    properties: list[str] = See https://huggingface.co/docs/huggingface_hub/api-ref/hf_hub/hf_api/model_info
+    Default properties: [""safetensors"", ""lastModified""], only retrieves minimal information.
+    Set to None to retrieve the full model information.
+    """"""
+    global _HFAPI
+    if _HFAPI is None:
+        _HFAPI = HfApi()
+    try:
+        model_info: ModelInfo = _HFAPI.model_info(model_id, expand=properties)
+    except Exception as e:
+        print(f""Error getting model info for {model_id}: {e}"")
+        model_info = None
+    return model_info
+
+
+def list_models(
+    properties: list[str] = None,
+    full: bool = False,
+    sort: str = ""downloads"",
+    author: str = ""unsloth"",
+    search: str = None,
+    limit: int = 10,
+) -> list[ModelInfo]:
+    """"""
+    Retrieve model information from the Hugging Face Hub.
+
+    properties: list[str] = See https://huggingface.co/docs/huggingface_hub/api-ref/hf_hub/hf_api/list_models
+    full: bool = Whether to retrieve the full model information, if True properties will be ignored.
+    sort: str = The sort order.
+    author: str = The author of the model.
+    search: str = The search query for filtering models.
+
+    """"""
+    global _HFAPI
+    if _HFAPI is None:
+        _HFAPI = HfApi()
+    if full:
+        properties = None
+
+    models: list[ModelInfo] = _HFAPI.list_models(
+        author=author,
+        search=search,
+        sort=sort,
+        limit=limit,
+        expand=properties,
+        full=full,
+    )
+    return models
"
"diff --git a/pyproject.toml b/pyproject.toml
index 523794e..f9a33a8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,10 +37,10 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.5.8"",
+    ""unsloth_zoo>=2025.5.10"",
     ""packaging"",
     ""tyro"",
-    ""transformers==4.51.3,!=4.47.0"",
+    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
     ""datasets>=3.4.1"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -48,7 +48,7 @@ huggingface = [
     ""wheel>=0.42.0"",
     ""numpy"",
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0,<=0.15.2"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
@@ -381,10 +381,10 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.5.8"",
+    ""unsloth_zoo>=2025.5.9"",
     ""packaging"",
     ""tyro"",
-    ""transformers==4.51.3,!=4.47.0"",
+    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
     ""datasets>=3.4.1"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -399,7 +399,7 @@ colab-new = [
 ]
 colab-no-deps = [
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0,<=0.15.2"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
     ""peft>=0.7.1"",
     ""xformers"",
     ""bitsandbytes>=0.45.5"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 964e874..9325428 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.5.7""
+__version__ = ""2025.5.8""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index b385dba..e5cb226 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -395,7 +395,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     if trainer_file in RL_METRICS_CHANGES:
         process_extra_args = RL_METRICS_CHANGES[trainer_file]
         for process_extra_arg in process_extra_args:
-            other_metrics_processor += process_extra_arg(call_args, extra_args)
+            other_metrics_processor += process_extra_arg(old_RLTrainer_source, old_RLConfig_source)
     pass
 
     # Add statistics as well!
@@ -481,6 +481,39 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         extra_args += num_proc_check
     pass
 
+    # Check for loss_type = dr_grpo and scale_rewards for GRPO
+    if ""loss_type"" in call_args and ""scale_rewards"" in call_args:
+        check_dr_grpo = \
+        ""if loss_type.lower() == 'dr_grpo':\n""\
+        ""    loss_type = 'dr_grpo'\n""\
+        ""elif loss_type.lower() == 'dapo':\n""\
+        ""    loss_type = 'dapo'\n""\
+        ""if loss_type.lower() == 'dr_grpo':\n""\
+        ""    if scale_rewards == None:\n""\
+        ""        scale_rewards = True\n""\
+        ""    elif scale_rewards == True:\n""\
+        ""        print('The Dr GRPO paper recommends setting `scale_rewards` to False! Will override. Set it to `None` to force False.')\n""\
+        ""        scale_rewards = False\n""\
+        ""elif loss_type.lower() == 'dapo':\n""\
+        ""    print('The DAPO paper recommends `mask_truncated_completions = True`')\n""\
+        ""    print('The DAPO paper recommends `epsilon_high = 0.28`')\n""\
+        ""    mask_truncated_completions = True\n""\
+        ""    epsilon_high = 0.28\n""\
+        ""\n""
+        extra_args += check_dr_grpo
+    pass
+
+    # Check GRPO num_generations mismatch
+    if ""per_device_train_batch_size"" in call_args and ""num_generations"" in call_args: 
+        check_num_generations = \
+        ""if (per_device_train_batch_size // num_generations) * num_generations != per_device_train_batch_size:\n""\
+        ""    print('Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\\n""\
+                   ""We will change the batch size of ' + str(per_device_train_batch_size) + ' to the `num_generations` of ' + str(num_generations))\n""\
+        ""    per_device_train_batch_size = num_generations\n""\
+        ""\n""
+        extra_args += check_num_generations
+    pass
+
     # Edit config with anything extra
     if trainer_file in RL_CONFIG_CHANGES:
         process_extra_args = RL_CONFIG_CHANGES[trainer_file]
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 2ff0e25..171e75d 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -363,13 +363,27 @@ RL_CONFIG_CHANGES[""grpo_trainer""].append(grpo_trainer_fix_batch_size)
 def grpo_trainer_metrics(RLTrainer_source, RLConfig_source):
     if ""reward_funcs"" not in RLTrainer_source: return """"
 
+    # For new TRL we have /mean and /std
+    use_mean = ""rewards/{reward_func_name}/mean"" in RLTrainer_source
+    use_std  = ""rewards/{reward_func_name}/std""  in RLTrainer_source
+    if not use_mean:
+        use_normal = ""rewards/{reward_func_name}"" in RLTrainer_source
+    else:
+        use_normal = False
+    pass
+
     log_metrics = \
     ""if not isinstance(reward_funcs, list): _reward_funcs = [reward_funcs]\n""\
     ""else: _reward_funcs = reward_funcs\n""\
     ""for reward_func in _reward_funcs:\n""\
     ""    try:\n""\
     ""        reward_func_name = reward_func.__name__\n""\
-    ""        other_metrics.append(f'rewards/{reward_func_name}')\n""\
+   f""        if {use_mean}:\n""\
+    ""            other_metrics.append(f'rewards/{reward_func_name}/mean')\n""\
+   f""        if {use_std}:\n""\
+    ""            other_metrics.append(f'rewards/{reward_func_name}/std')\n""\
+   f""        if {use_normal}:\n""\
+    ""            other_metrics.append(f'rewards/{reward_func_name}')\n""\
     ""    except: pass\n""
     return log_metrics
 pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 63f48af..747858d 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.5.5""
+__version__ = ""2025.5.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 9c5a7b6..8a49026 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -461,6 +461,12 @@ except:
     from transformers import AutoModelForVision2Seq
 pass
 
+DISABLE_COMPILE_MODEL_NAMES = [
+    ""aya-vision"",
+    ""modernbert"",
+    ""granite-vision"",
+]
+
 
 class FastModel(FastBaseModel):
     @staticmethod
@@ -521,33 +527,35 @@ class FastModel(FastBaseModel):
             model_name = get_model_name(model_name, load_in_4bit)
 
         # Check versions
+        lowered_model_name = model_name.lower()
         LATEST  = '\nPlease use transformers via `pip install --no-deps git+https://github.com/huggingface/transformers.git`'
         NIGHTLY = '\nPlease use nightly transformers via pip install --upgrade ""transformers>=4.49.0""`'
-        if ""pixtral"" in model_name.lower() and transformers_version < Version(""4.49.0""):
+        if ""pixtral"" in lowered_model_name and transformers_version < Version(""4.49.0""):
             raise RuntimeError(""Unsloth: Pixtral only works on transformers >= 4.49.0."" + LATEST)
-        elif ""qwen2.5"" in model_name.lower() and transformers_version < Version(""4.49.0""):
+        elif ""qwen2.5"" in lowered_model_name and transformers_version < Version(""4.49.0""):
             raise RuntimeError(""Unsloth: Qwen 2.5 only works on transformers >= 4.49.0."" + LATEST)
-        elif ""aya-vision"" in model_name.lower():
-            # Disable compiling for now - errors out!
-            os.environ[""UNSLOTH_COMPILE_DISABLE""] = ""1""
-            if transformers_version < Version(""4.50.0.dev0""):
-                raise RuntimeError(""Unsloth: Aya Vision only works on transformers >= 4.50.0."" + NIGHTLY)
-        elif ""gemma-3"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+        elif ""gemma-3"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Gemma 3 only works on transformers >= 4.50.0."" + NIGHTLY)
-        elif ""c4ai-command-a-03-2025"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+        elif ""c4ai-command-a-03-2025"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Cohere's Command model only works on transformers >= 4.50.0."" + NIGHTLY)
-        elif ""granite-vision"" in model_name.lower():
-            # Disable compiling for now - errors out!
-            os.environ[""UNSLOTH_COMPILE_DISABLE""] = ""1""
-            if transformers_version < Version(""4.50.0.dev0""):
-                raise RuntimeError(""Unsloth: Granite Vision only works on transformers >= 4.50.0."" + NIGHTLY)
-        elif ""csm-1b"" in model_name.lower():
+        elif ""csm-1b"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Sesame fails
             os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
-        elif ""olmo-2"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+        elif ""olmo-2"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
-        elif ""whisper"" in model_name.lower():
-            os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Whisper fails
+        else:
+            for check_model_name in DISABLE_COMPILE_MODEL_NAMES:
+                if check_model_name in lowered_model_name:
+                    os.environ[""UNSLOTH_COMPILE_DISABLE""] = ""1""
+                    os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
+                    if transformers_version < Version(""4.50.0.dev0""):
+                        raise RuntimeError(f""Unsloth: {check_model_name} only works on transformers >= 4.50.0."" + NIGHTLY)
+                    break
+        pass
+
+        if auto_model is not None:
+            # All other models need to disable static cache
+            os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
         pass
 
         if USE_MODELSCOPE and not os.path.exists(model_name):
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 4bbd829..e50a5a8 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -728,16 +728,6 @@ __INT_TO_FLOAT_MAPPER = \
         ""mistralai/Mistral-Small-3.1-24B-Base-2503"",
         ""unsloth/Mistral-Small-3.1-24B-Base-2503-bnb-4bit"",
     ),
-    ""unsloth/orpheus-3b-0.1-pretrained-unsloth-bnb-4bit"" : (
-        ""unsloth/orpheus-3b-0.1-pretrained"",
-        ""canopylabs/orpheus-3b-0.1-pretrained"",
-        ""unsloth/orpheus-3b-0.1-pretrained-bnb-4bit"",
-    ),
-    ""unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit"" : (
-        ""unsloth/orpheus-3b-0.1-ft"",
-        ""canopylabs/orpheus-3b-0.1-ft"",
-        ""unsloth/orpheus-3b-0.1-ft-bnb-4bit"",
-    ),
     ""unsloth/Qwen3-0.6B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen3-0.6B"",
         ""Qwen/Qwen3-0.6B"",
@@ -817,6 +807,16 @@ __INT_TO_FLOAT_MAPPER = \
         ""microsoft/Phi-4-mini-reasoning"",
         ""unsloth/phi-4-mini-reasoning-bnb-4bit"",
     ),
+    ""unsloth/orpheus-3b-0.1-pretrained-unsloth-bnb-4bit"" : (
+        ""unsloth/orpheus-3b-0.1-pretrained"",
+        ""canopylabs/orpheus-3b-0.1-pretrained"",
+        ""unsloth/orpheus-3b-0.1-pretrained-bnb-4bit"",
+    ),
+    ""unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit"" : (
+        ""unsloth/orpheus-3b-0.1-ft"",
+        ""canopylabs/orpheus-3b-0.1-ft"",
+        ""unsloth/orpheus-3b-0.1-ft-bnb-4bit"",
+    ),
     ""unsloth/csm-1b"" : (
         ""unsloth/csm-1b"",
         ""sesame/csm-1b"",
@@ -837,6 +837,18 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/CrisperWhisper"",
         ""nyrahealth/CrisperWhisper"",
     ),
+    ""unsloth/Llasa-1B"" : (
+        ""unsloth/Llasa-1B"",
+        ""HKUSTAudio/Llasa-1B"",
+    ),
+    ""unsloth/Spark-TTS-0.5B"" : (
+        ""unsloth/Spark-TTS-0.5B"",
+        ""SparkAudio/Spark-TTS-0.5B"",
+    ),
+    ""unsloth/Llama-OuteTTS-1.0-1B"" : (
+        ""unsloth/Llama-OuteTTS-1.0-1B"",
+        ""OuteAI/Llama-OuteTTS-1.0-1B"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 2bff87d..4466128 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -95,8 +95,18 @@ def unsloth_base_fast_generate(
         input_ids = kwargs[""input_ids""]
     elif ""input"" in kwargs:
         input_ids = kwargs[""input_ids""]
+    elif ""input_features"" in kwargs:
+        input_ids = kwargs[""input_features""]
+    elif ""input_embeds"" in kwargs:
+        input_ids = kwargs[""input_embeds""]
+    elif ""inputs"" in kwargs:
+        input_ids = kwargs[""inputs""]
     else:
-        raise TypeError(""Unsloth: You need to pass in input_ids to .generate!"")
+        key = next(iter(kwargs.keys()))
+        if type(kwargs[""key""]) is not torch.Tensor:
+            raise TypeError(""Unsloth: You need to pass in input_ids to .generate!"")
+        input_ids = kwargs[key]
+    pass
     assert(type(input_ids) is torch.Tensor)
     bsz = input_ids.shape[0]
 
@@ -203,10 +213,11 @@ def unsloth_base_fast_generate(
 
     if ""generation_config"" in kwargs:
         kwargs[""generation_config""].cache_implementation = cache_implementation
-        kwargs[""generation_config""].compile_config = _compile_config if cache_implementation is not None else None
+        if cache_implementation is not None:
+            kwargs[""generation_config""].compile_config = _compile_config
     else:
         kwargs[""cache_implementation""] = cache_implementation
-        if cache_implementation:
+        if cache_implementation is not None:
             kwargs[""compile_config""] = _compile_config
     pass
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 46c7fc4..74e1ccc 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.6.7"",
+    ""unsloth_zoo>=2025.6.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.6.7"",
+    ""unsloth_zoo>=2025.6.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 39ff1fd..53c5497 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -440,12 +440,12 @@ class FastBaseModel:
             for jj, (name, module) in enumerate(model.named_modules()):
                 exec(custom_datatype)
             pass
-            # Clear deleted GPU items
-            for _ in range(3):
-                gc.collect()
-                if DEVICE_TYPE == ""cuda"":  torch.cuda.empty_cache()
-                elif DEVICE_TYPE == ""xpu"": torch.xpu.empty_cache()
-            pass
+        pass
+        # Clear deleted GPU items
+        for _ in range(3):
+            gc.collect()
+            if DEVICE_TYPE == ""cuda"":  torch.cuda.empty_cache()
+            elif DEVICE_TYPE == ""xpu"": torch.xpu.empty_cache()
         pass
 
         # Counteract saved tokenizers
@@ -562,7 +562,7 @@ class FastBaseModel:
         finetune_mlp_modules       = True,
         layers_to_transform        = None,
         layers_pattern             = None,
-        use_gradient_checkpointing = True,
+        use_gradient_checkpointing = ""unsloth"",
         random_state               = 3407,
         max_seq_length             = 2048, # not used anymore
         use_rslora                 = False,
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3d969d7..022be10 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1868,7 +1868,25 @@ class FastLlamaModel:
             internal_model.max_seq_length = max_seq_length
             internal_model = internal_model.model
         pass
-        internal_model.max_seq_length = max_seq_length
+        internal_model.max_seq_length = max_seq_length        
+
+        # Patch tokenizer to pad to the right
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_saved_temp_tokenizer""):
+                internal_model._saved_temp_tokenizer.padding_side = ""right""
+            pass
+            internal_model = internal_model.model
+        pass
+        if hasattr(internal_model, ""_saved_temp_tokenizer""):
+            internal_model._saved_temp_tokenizer.padding_side = ""right""
+        pass
+
+        # Clear deleted GPU items
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        pass
         return model
     pass
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index d7c0f07..d87af0a 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -113,7 +113,8 @@ class FastLanguageModel(FastLlamaModel):
         elif not is_model and not is_peft:
             raise RuntimeError(
                 f""Unsloth: `{model_name}` is not a base model or a PEFT model.\n""\
-                ""We could not locate a `config.json` or `adapter_config.json` file""
+                ""We could not locate a `config.json` or `adapter_config.json` file.\n""\
+                ""Are you certain the model name is correct? Does it actually exist?""
             )
         pass
 
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
deleted file mode 100644
index 4c6ab77..0000000
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ /dev/null
@@ -1,41 +0,0 @@
----
-name:  Bug report
-about: Create a report to help us improve
-title: ""[Bug]""
-labels: bug
-assignees: ''
-
----
-
-**Describe the bug**
-A clear and concise description of what the bug is.  Please fill out the following sections and provide a minimal reproduction script so that we can provide a solution as quickly as possible!
-
-1. **Environment Setup:**
-   - OS: [e.g., Ubuntu 20.04]
-   - Python Version: [e.g., 3.8.10]
-   - Frameworks/Libraries: please paste output of `pip freeze` here
-   - `colab` / script - was this run in `colab` or as a script?
-
-2. **Dataset Details:**
-   - Dataset Name: 
-   - Data Preprocessing Steps: [e.g., tokenization, formatting funcs, data collators, etc.]
-
-3. **Model Details:**
-   - Model ID:
-   - Model Configuration: [e.g., lora params, quantization, etc.]
-
-4. **Training Configuration:**
-   - Trainer Args: `SFTConfig`, `GRPOConfig`
-
-5. **Reproduction Steps:**
-   - Minimal script to reproduce error
-   - If using a `colab`, please provide the link to the notebook and describe any changes made.
-
-6. **Expected Behavior:**
-   
-7. **Actual Behavior:**
-   - [e.g., Description of the error, unexpected results, or performance issues encountered]
-   - [e.g., Error messages or logs]
-
-8. **Additional notes:**
-   - Any additional information that might help us reproduce the bug.
diff --git a/.github/ISSUE_TEMPLATE/documentation.md b/.github/ISSUE_TEMPLATE/documentation.md
deleted file mode 100644
index 19af724..0000000
--- a/.github/ISSUE_TEMPLATE/documentation.md
+++ /dev/null
@@ -1,34 +0,0 @@
----
-name:  Documentation
-about: Report incorrect or needed docs for https://docs.unsloth.ai/
-title: ""[Docs]""
-labels: documentation
-assignees: ''
-
----
-
-- [ ] Report incorrect documentation
-- [ ] Report needed documentation
-
-## Report incorrect documentation
-
-**Location of incorrect documentation -- provide links and line numbers if possible.**
-
-**Describe the problems or issues found in the documentation**
-
-**Steps taken to verify documentation is incorrect**
-
-**Suggested fix**
-
----
-
-## Report needed documentation
-
-**What's missing?**
-
-
-**Describe the documentation you'd like -- how can we make using `unsloth` easier?**
-
-
-**Help us understand how we can make finding the needed info easier!**
-List any steps you have taken, e.g. searching the repo, reading the docs, etc.  
diff --git a/.github/ISSUE_TEMPLATE/feature_request.md b/.github/ISSUE_TEMPLATE/feature_request.md
index 7f7d93c..9da73b2 100644
--- a/.github/ISSUE_TEMPLATE/feature_request.md
+++ b/.github/ISSUE_TEMPLATE/feature_request.md
@@ -1,14 +1,34 @@
 ---
-name:  Feature request
-about: ""Suggest an idea: new model, algorithm or feature etc.""
+name: ""\U0001F680 Feature request""
+about: New features, model support, ideas
 title: ""[Feature]""
-labels: ""feature request""
+labels: feature request
 assignees: ''
 
 ---
 
-**What features would you like to see? Is it related to a problem or a new feature you'd like to see? Please describe.**
-What we can do to improve `unsloth`?
-
-**Additional context**
-Feel free to add any other context, links, or screenshots here.
+1. For new models, have you tried:
+```python
+from unsloth import FastModel
+model, tokenizer = FastModel.from_pretrained(
+    ""microsoft/Phi-4-multimodal-instruct"",
+    trust_remote_code = True,
+)
+```
+If that doesn't work, try using the exact `AutoModel` class:
+```python
+from transformers import WhisperForConditionalGeneration
+model, tokenizer = FastModel.from_pretrained(
+    model_name = ""unsloth/whisper-large-v3"",
+    auto_model = WhisperForConditionalGeneration,
+)
+```
+For Sequence Classification / other `AutoModel` classes:
+```python
+from transformers import AutoModelForSequenceClassification
+model, tokenizer = FastModel.from_pretrained(
+    model_name = ""unsloth/whisper-large-v3"",
+    auto_model = AutoModelForSequenceClassification,
+)
+```
+2. Otherwise, ask away!
diff --git a/.github/ISSUE_TEMPLATE/question.md b/.github/ISSUE_TEMPLATE/question.md
deleted file mode 100644
index 90aa128..0000000
--- a/.github/ISSUE_TEMPLATE/question.md
+++ /dev/null
@@ -1,10 +0,0 @@
----
-name:  Submit question
-about: Ask a general question about unsloth
-title: ""[Question]""
-labels: ""question""
-assignees: ''
-
----
-
-**What is your question?**
diff --git ""a/.github/ISSUE_TEMPLATE/\342\235\223-other.md"" ""b/.github/ISSUE_TEMPLATE/\342\235\223-other.md""
new file mode 100644
index 0000000..dbec522
--- /dev/null
+++ ""b/.github/ISSUE_TEMPLATE/\342\235\223-other.md""
@@ -0,0 +1,10 @@
+---
+name: "" Other""
+about: Other
+title: ""[Question]""
+labels: ''
+assignees: ''
+
+---
+
+Be specific. If you need urgent help, head to https://discord.com/invite/unsloth for help. Have you tried https://docs.unsloth.ai/basics/errors-troubleshooting or https://github.com/unslothai/unsloth/wiki ?
diff --git ""a/.github/ISSUE_TEMPLATE/\360\237\220\233-bug.md"" ""b/.github/ISSUE_TEMPLATE/\360\237\220\233-bug.md""
new file mode 100644
index 0000000..6e59ccb
--- /dev/null
+++ ""b/.github/ISSUE_TEMPLATE/\360\237\220\233-bug.md""
@@ -0,0 +1,21 @@
+---
+name: ""\U0001F41B Bug""
+about: For bugs
+title: ""[Bug]""
+labels: bug
+assignees: ''
+
+---
+
+1. **Environment**
+   - `Colab`, `Kaggle`, local / cloud machine
+   - Number GPUs used, GPU type, VRAM amount
+   - Copy paste Unsloth printout with sloth emoji
+   - Which notebook are you using?
+
+2. **Code to reproduce**
+   - Which trainer - `SFTTrainer, GRPOTrainer` etc
+   - Exact code to repro. **Remove Hugging Face token!**
+   - Expected behavior
+
+For quick replies, head to https://discord.com/invite/unsloth and ask away! have you tried https://github.com/unslothai/unsloth/wiki or https://docs.unsloth.ai/basics/errors-troubleshooting?
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 242e306..531adad 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -1175,7 +1175,7 @@ LOGITS_ERROR_STRING = \
     ""os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n""\
     ""... trainer.train() ...""
 
-def raise_logits_error(*args, **kwargs): raise NotImplementedError(LOGITS_ERROR_STRING)
+def raise_logits_error(*args, **kwargs): print(LOGITS_ERROR_STRING)
 class EmptyLogits:
     def __init__(self): return
     __getitem__ = raise_logits_error
@@ -1185,7 +1185,7 @@ class EmptyLogits:
 pass
 EMPTY_LOGITS = EmptyLogits()
 functions = dir(torch.Tensor)
-# for function in functions:
-#     try: exec(f""EMPTY_LOGITS.{function} = raise_logits_error"", globals(), locals())
-#     except: continue
-# pass
+for function in functions:
+    try: exec(f""EMPTY_LOGITS.{function} = raise_logits_error"", globals(), locals())
+    except: continue
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index e2ce925..c024282 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1001,8 +1001,6 @@ def CausalLM_fast_forward(fast_forward_inference):
                     hidden_states=outputs.hidden_states,
                     attentions=outputs.attentions,
                 )
-                print(output)
-                print(output)
                 return output
             pass
             logits = self.lm_head(hidden_states.to(lm_head.dtype))
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 90b5917..ab53811 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.16""
+__version__ = ""2025.3.17""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index cd59e03..670e082 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -646,7 +646,8 @@ class FastModel(FastBaseModel):
         # Set forced float32 env flag
         os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""0""
         do_forced_float32 = False
-        model_type_arch = model_types[1]
+        for model_type_arch in model_types:
+            if model_type_arch != ""siglip"": break
         global FORCE_FLOAT32
         for disable_name in FORCE_FLOAT32:
             if (disable_name.lower() == model_type_arch.lower() or \
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index 53b274a..1448a74 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -13,12 +13,12 @@ A clear and concise description of what the bug is.
 1. **Environment Setup:**
    - OS: [e.g., Ubuntu 20.04]
    - Python Version: [e.g., 3.8.10]
-   - Frameworks/Libraries: past output of `pip freeze` here
-   - Colab / Script - was this run in colab or as a script?
+   - Frameworks/Libraries: please paste output of `pip freeze` here
+   - `colab` / script - was this run in colab or as a script?
 
 2. **Dataset Details:**
    - Dataset Name: 
-   - Data Preprocessing Steps: [e.g., Tokenization with specific parameters]
+   - Data Preprocessing Steps: [e.g., tokenization, formatting funcs, data collators, etc.]
 
 3. **Model Details:**
    - Model ID:
@@ -29,11 +29,13 @@ A clear and concise description of what the bug is.
 
 5. **Reproduction Steps:**
    - Minimal script to reproduce error
+   - If using a `colab`, please provide the link to the notebook and describe any changes made.
 
 6. **Expected Behavior:**
-
+   
 7. **Actual Behavior:**
    - [e.g., Description of the error, unexpected results, or performance issues encountered]
    - [e.g., Error messages or logs]
 
 8. **Additional notes:**
+   - Any additional information that might help us reproduce the bug.
\ No newline at end of file
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 1b122fc..49b8ba3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -374,7 +374,8 @@ pass
 
 # Unsloth only works on NVIDIA GPUs for now
 device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-device = f""cuda:{device_ids[:device_ids.find(',')]}""
+device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
+device = f""cuda:{device if device.isdigit() else '0'}""
 
 class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     """"""
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 9850283..0cc047d 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -38,11 +38,9 @@ except:
     GemmaFlashAttention2 = GemmaAttention
 pass
 
-# Unsloth currently only works on one GPU
 import os
 device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-device = f""cuda:{device_ids[:device_ids.find(',')]}""
-# Please obtain a commercial license
+device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
 
 torch_nn_functional_gelu = torch.nn.functional.gelu
 def fast_geglu_inference(self, X):
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9327b1b..f2f79de 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -76,7 +76,8 @@ pass
 
 import os # Unsloth only works on NVIDIA GPUs for now
 device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-device = f""cuda:{device_ids[:device_ids.find(',')]}""
+device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
+device = f""cuda:{device if device.isdigit() else '0'}""
 
 from math import sqrt as math_sqrt
 KV_CACHE_INCREMENT = 256 # KV Cache update size
@@ -846,7 +847,8 @@ def CausalLM_fast_forward(fast_forward_inference):
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
                 device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-                device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
+                device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
+                device = f""cuda:{device if device.isdigit() else '0'}""
                 # Fixes https://github.com/unslothai/unsloth/issues/10
                 self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = device)
             pass
@@ -1828,7 +1830,8 @@ class FastLlamaModel:
         # Fixes https://github.com/unslothai/unsloth/issues/10
         max_seq_length = model.max_seq_length
         device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-        device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
+        device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
+        device = f""cuda:{device if device.isdigit() else '0'}""
         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = device)
         model.model.extra_ignored_labels = extra_ignored_labels
         internal_model = model
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index e147f21..832189b 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -240,7 +240,8 @@ def MistralForCausalLM_fast_forward(
         shift_logits = logits
         if not hasattr(self, ""extra_ignored_labels""):
             device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
-            device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
+            device = device_ids[:device_ids.find(',')] # Unsloth only works on NVIDIA GPUs for now
+            device = f""cuda:{device if device.isdigit() else '0'}""
             # Fixes https://github.com/unslothai/unsloth/issues/10
             self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = device)
         pass
diff --git a/unsloth/save.py b/unsloth/save.py
index cae59ca..940feb4 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -418,6 +418,11 @@ def unsloth_save_model(
         print(""Unsloth: Saving model..."", end = """")
         if save_method != ""lora"": print("" This might take 10 minutes for Llama-7b..."", end = """")
 
+        # [TODO] Is this correct?
+        if save_method == ""lora"":
+            save_pretrained_settings[""selected_adapters""] = None
+        pass
+
         model.save_pretrained(**save_pretrained_settings)
 
         if push_to_hub and hasattr(model, ""config""):
@@ -649,8 +654,9 @@ def unsloth_save_model(
     model.config = new_config
 
     # Save!
-    
-    save_pretrained_settings[""selected_adapters""] = None
+    # [TODO] --> is this correct?
+    # save_pretrained_settings[""selected_adapters""] = None
+
     # Check if pushing to an organization
     if save_pretrained_settings[""push_to_hub""] and (username != actual_username):
         print(f""Unsloth: Saving to organization with address {new_save_directory}"")
@@ -834,7 +840,7 @@ def save_to_gguf(
     model_dtype          : str,
     is_sentencepiece     : bool = False,
     model_directory      : str = ""unsloth_finetuned_model"",
-    quantization_method  : str = ""fast_quantized"",
+    quantization_method  = ""fast_quantized"", # Can be a list of options! [""q4_k_m"", ""q8_0"", ""q5_k_m""]
     first_conversion     : str = None,
     _run_installer = None, # Non blocking install of llama.cpp
 ):
@@ -846,6 +852,10 @@ def save_to_gguf(
     assert(model_dtype == ""float16"" or model_dtype == ""bfloat16"")
     model_dtype = ""f16"" if model_dtype == ""float16"" else ""bf16""
 
+    # Convert quantization_method to list
+    quantization_method = \
+        quantization_method if type(quantization_method) is list else list(quantization_method)
+
     # Check if bfloat16 is supported
     if model_dtype == ""bf16"" and not torch.cuda.is_bf16_supported():
         logger.warning(
@@ -860,8 +870,11 @@ def save_to_gguf(
         first_conversion = model_dtype
     pass
 
-    if quantization_method.startswith(""iq2""):
-        raise RuntimeError(""Unsloth: Currently iq2 type quantizations aren't supported yet - sorry!"")
+    # Check I quants
+    for quant_method in quantization_method: 
+        if quant_method.startswith(""iq2""):
+            raise RuntimeError(""Unsloth: Currently iq2 type quantizations aren't supported yet - sorry!"")
+    pass
 
     # Careful convert.py is only for Llama / Mistral based archs
     use_fast_convert = False
@@ -871,25 +884,32 @@ def save_to_gguf(
     pass
     logger.warning_once(f""Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}."")
 
-    if   quantization_method == ""not_quantized"":  quantization_method = model_dtype
-    elif quantization_method == ""fast_quantized"": quantization_method = ""q8_0""
-    elif quantization_method == ""quantized"":      quantization_method = ""q4_k_m""
-    elif quantization_method is None:             quantization_method = ""q8_0""
-    pass
+    # Map quant methods
+    new_quantization_method = []
+    for quant_method in quantization_method:
+        if   quant_method == ""not_quantized"":  quantization_method = model_dtype
+        elif quant_method == ""fast_quantized"": quantization_method = ""q8_0""
+        elif quant_method == ""quantized"":      quantization_method = ""q4_k_m""
+        elif quant_method is None:             quantization_method = ""q8_0""
+
+        # Check if wrong method
+        if quant_method not in ALLOWED_QUANTS.keys():
+            error = f""Unsloth: Quant method = [{quant_method}] not supported. Choose from below:\n""
+            for key, value in ALLOWED_QUANTS.items():
+                error += f""[{key}] => {value}\n""
+            raise RuntimeError(error)
+        pass
 
-    if quantization_method not in ALLOWED_QUANTS.keys():
-        error = f""Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\n""
-        for key, value in ALLOWED_QUANTS.items():
-            error += f""[{key}] => {value}\n""
-        raise RuntimeError(error)
+        new_quantization_method.append(quant_method)
     pass
+    quantization_method = new_quantization_method
 
     print_info = \
         f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
         f""   \\\   /|    [0] Installing llama.cpp will take 3 minutes.\n""\
         f""O^O/ \_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n""\
-        f""\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\n""\
-        f' ""-____-""     In total, you will have to wait around 26 minutes.\n'
+        f""\        /    [2] Converting GGUF 16bits to {quantization_method} will take 10 minutes each.\n""\
+        f' ""-____-""     In total, you will have to wait at least 16 minutes.\n'
     print(print_info)
 
     # Check first_conversion format
@@ -928,24 +948,37 @@ def save_to_gguf(
         install_llama_cpp_old(-10)
     pass
 
-    if   quantization_method == ""f32"":  first_conversion = ""f32""
-    elif quantization_method == ""f16"":  first_conversion = ""f16""
-    elif quantization_method == ""bf16"": first_conversion = ""bf16""
-    elif quantization_method == ""q8_0"": first_conversion = ""q8_0""
-    else:
-        # Quantized models must have f16 as the default argument
-        if   first_conversion == ""f32""  : pass
-        elif first_conversion == ""f16""  : pass
-        elif first_conversion == ""bf16"" : pass
-        elif first_conversion == ""q8_0"":
-            logger.warning_once(
-                ""Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, ""\
-                ""but saves disk space!""
-            )
-            # first_conversion = ""f16""
+    # Determine maximum first_conversion state
+    if   first_conversion == ""f32""  : strength = 3
+    elif first_conversion == ""f16""  : strength = 2
+    elif first_conversion == ""bf16"" : strength = 1
+    elif first_conversion == ""q8_0"" : strength = 0
+
+    for quant_method in quantization_method:
+        if   quant_method == ""f32"":  strength = max(strength, 3)
+        elif quant_method == ""f16"":  strength = max(strength, 2)
+        elif quant_method == ""bf16"": strength = max(strength, 1)
+        elif quant_method == ""q8_0"": strength = max(strength, 0)
+        else:
+            # Quantized models must have f16 as the default argument
+            if   first_conversion == ""f32""  : pass
+            elif first_conversion == ""f16""  : pass
+            elif first_conversion == ""bf16"" : pass
+            elif first_conversion == ""q8_0"":
+                logger.warning_once(
+                    ""Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, ""\
+                    ""but saves disk space!""
+                )
+                # first_conversion = ""f16""
+            pass
         pass
     pass
 
+    if   strength >= 3: first_conversion = ""f32""
+    elif strength >= 2: first_conversion = ""f16""
+    elif strength >= 1: first_conversion = ""bf16""
+    else: first_conversion = ""q8_0""
+
     # Non llama/mistral needs can only use f32 or f16
     if not use_fast_convert and \
         (first_conversion != ""f16"" or first_conversion != ""bf16"" or first_conversion != ""f32""):
@@ -1033,52 +1066,58 @@ def save_to_gguf(
     pass
     print(f""Unsloth: Conversion completed! Output location: {final_location}"")
 
-    if quantization_method != first_conversion:
-        old_location = final_location
-        print(f""Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes..."")
-        final_location = f""./{model_directory}-unsloth.{quantization_method.upper()}.gguf""
+    full_precision_location = final_location
 
-        command = f""./{quantize_location} {old_location} ""\
-            f""{final_location} {quantization_method} {n_cpus}""
-        
-        # quantize uses stderr
-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
-            for line in sp.stdout:
-                line = line.decode(""utf-8"", errors = ""replace"")
-                if ""undefined reference"" in line:
-                    raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                print(line, flush = True, end = """")
-            if sp.returncode is not None and sp.returncode != 0:
-                raise subprocess.CalledProcessError(sp.returncode, sp.args)
-        pass
+    all_saved_locations = []
+    # Convert each type!
+    for quant_method in quantization_method:
+        if quant_method != first_conversion:
+            print(f""Unsloth: [2] Converting GGUF 16bit into {quant_method}. This will take 20 minutes..."")
+            final_location = f""./{model_directory}-unsloth.{quant_method.upper()}.gguf""
 
-        # Check if quantization succeeded!
-        if not os.path.isfile(final_location):
-            if IS_KAGGLE_ENVIRONMENT:
-                raise RuntimeError(
-                    f""Unsloth: Quantization failed for {final_location}\n""\
-                    ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
-                    ""Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\n""\
-                    ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
-                    ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
-                    ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
-                )
-            else:
-                raise RuntimeError(
-                    ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
-                    ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
-                    ""You must run this in the same folder as you're saving your model.\n""\
-                    ""git clone --recursive https://github.com/ggerganov/llama.cpp\n""\
-                    ""cd llama.cpp && make clean && make all -j\n""\
-                    ""Once that's done, redo the quantization.""
-                )
+            command = f""./{quantize_location} {full_precision_location} ""\
+                f""{final_location} {quant_method} {n_cpus}""
+            
+            # quantize uses stderr
+            with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
+                for line in sp.stdout:
+                    line = line.decode(""utf-8"", errors = ""replace"")
+                    if ""undefined reference"" in line:
+                        raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
+                    print(line, flush = True, end = """")
+                if sp.returncode is not None and sp.returncode != 0:
+                    raise subprocess.CalledProcessError(sp.returncode, sp.args)
             pass
-        pass
 
-        print(f""Unsloth: Conversion completed! Output location: {final_location}"")
+            # Check if quantization succeeded!
+            if not os.path.isfile(final_location):
+                if IS_KAGGLE_ENVIRONMENT:
+                    raise RuntimeError(
+                        f""Unsloth: Quantization failed for {final_location}\n""\
+                        ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
+                        ""Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\n""\
+                        ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
+                        ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
+                        ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
+                    )
+                else:
+                    raise RuntimeError(
+                        ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
+                        ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
+                        ""You must run this in the same folder as you're saving your model.\n""\
+                        ""git clone --recursive https://github.com/ggerganov/llama.cpp\n""\
+                        ""cd llama.cpp && make clean && make all -j\n""\
+                        ""Once that's done, redo the quantization.""
+                    )
+                pass
+            pass
+
+            print(f""Unsloth: Conversion completed! Output location: {final_location}"")
+            all_saved_locations.append(final_location)
+        pass
     pass
 
-    return final_location
+    return all_saved_locations
 pass
 
 
@@ -1453,7 +1492,7 @@ def unsloth_save_pretrained_gguf(
     is_sentencepiece_model = check_if_sentencepiece_model(self)
 
     # Save to GGUF
-    file_location = save_to_gguf(model_type, model_dtype, is_sentencepiece_model, 
+    all_file_locations = save_to_gguf(model_type, model_dtype, is_sentencepiece_model, 
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
@@ -1466,14 +1505,17 @@ def unsloth_save_pretrained_gguf(
 
     if push_to_hub:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
-        username = upload_to_huggingface(
-            self, save_directory, token,
-            ""GGUF converted"", ""gguf"", file_location, old_username, private,
-        )
-        link = f""{username}/{new_save_directory.lstrip('/.')}"" \
-            if username not in new_save_directory else \
-            new_save_directory.lstrip('/.')
-        print(f""Saved GGUF to https://huggingface.co/{link}"")
+
+        for file_location in all_file_locations:
+            username = upload_to_huggingface(
+                self, save_directory, token,
+                ""GGUF converted"", ""gguf"", file_location, old_username, private,
+            )
+            link = f""{username}/{new_save_directory.lstrip('/.')}"" \
+                if username not in new_save_directory else \
+                new_save_directory.lstrip('/.')
+            print(f""Saved GGUF to https://huggingface.co/{link}"")
+        pass
     pass
 pass
 
@@ -1604,20 +1646,22 @@ def unsloth_push_to_hub_gguf(
     is_sentencepiece_model = check_if_sentencepiece_model(self)
 
     # Save to GGUF
-    file_location = save_to_gguf(model_type, model_dtype, is_sentencepiece_model, 
+    all_file_locations = save_to_gguf(model_type, model_dtype, is_sentencepiece_model, 
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
-    print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
-    username = upload_to_huggingface(
-        self, repo_id, token,
-        ""GGUF converted"", ""gguf"", file_location, old_username, private,
-    )
-    link = f""{username}/{new_save_directory.lstrip('/.')}"" \
-        if username not in new_save_directory else \
-        new_save_directory.lstrip('/.')
+    for file_location in all_file_locations:
+        print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+        username = upload_to_huggingface(
+            self, repo_id, token,
+            ""GGUF converted"", ""gguf"", file_location, old_username, private,
+        )
+        link = f""{username}/{new_save_directory.lstrip('/.')}"" \
+            if username not in new_save_directory else \
+            new_save_directory.lstrip('/.')
 
-    print(f""Saved GGUF to https://huggingface.co/{link}"")
+        print(f""Saved GGUF to https://huggingface.co/{link}"")
+    pass
 
     if fix_bos_token:
         logger.warning(
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 2c2e361..c10b264 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1512,10 +1512,7 @@ def get_ollama_eos_tokens(tokenizer, extra_eos_tokens = []):
 
     # Remove duplicates
     splitted = joined_text.split(""\x01\x00"")
-    final_eos_tokens = []
-    for old, new in zip(added_tokens_decoder, splitted):
-        if old == new: final_eos_tokens.append(old)
-    pass
+    final_eos_tokens = [old for old, new in zip(added_tokens_decoder, splitted) if old == new]
     final_eos_tokens += extra_eos_tokens
     final_eos_tokens += repeatted_tokens
 
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index ce61cef..8f54e74 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -256,7 +256,6 @@ def unpatch_rms_layernorm():
     except:
         pass
     return
-    return
 pass
 
 
"
"diff --git a/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
new file mode 100644
index 0000000..0bf548b
--- /dev/null
+++ b/tests/saving/vision_models/test_save_merge_qwen2.5vl32B_model_ocr_benchmark.py
@@ -0,0 +1,255 @@
+# -*- coding: utf-8 -*-
+
+from unsloth import FastVisionModel
+
+import torch
+from qwen_vl_utils import process_vision_info
+import os
+from datasets import load_dataset
+from trl import SFTTrainer, SFTConfig
+
+import sys
+from pathlib import Path
+
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.ocr_eval import OCRModelEvaluator
+
+
+## Dataset Preparation
+from datasets import load_dataset
+
+dataset = load_dataset(""lbourdois/OCR-liboaccn-OPUS-MIT-5M-clean"", 'en', split=""train"")
+# To select the first 2000 examples
+train_dataset = dataset.select(range(2000))
+
+# To select the next 200 examples for evaluation
+eval_dataset = dataset.select(range(2000, 2200))
+
+# Convert dataset to OAI messages
+def format_data(sample):
+    return {""messages"": [
+                {
+                    ""role"": ""system"",
+                    ""content"": [{""type"": ""text"", ""text"": system_message}],
+                },
+                {
+                    ""role"": ""user"",
+                    ""content"": [
+                        {
+                            ""type"": ""text"",
+                            ""text"": sample[""question""],
+                        },{
+                            ""type"": ""image"",
+                            ""image"": sample[""image""],
+                        }
+                    ],
+                },
+                {
+                    ""role"": ""assistant"",
+                    ""content"": [{""type"": ""text"", ""text"": sample[""answer""]}],
+                },
+            ],
+        }
+
+system_message = ""You are an expert french ocr system.""
+# Convert dataset to OAI messages
+# need to use list comprehension to keep Pil.Image type, .mape convert image to bytes
+train_dataset = [format_data(sample) for sample in train_dataset]
+eval_dataset = [format_data(sample) for sample in eval_dataset]
+
+## Setup OCR main evaluation function and helpers
+import os
+import torch
+from tqdm import tqdm
+import pandas as pd
+from jiwer import wer, cer
+from qwen_vl_utils import process_vision_info
+
+#
+ocr_evaluator = OCRModelEvaluator()
+model_comparison_results = {}
+
+## Finetuning Setup and Run
+# Load Base Model
+
+model, tokenizer = FastVisionModel.from_pretrained(
+    model_name = ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
+    max_seq_length = 2048, # Choose any for long context!
+    load_in_4bit = True,  # 4 bit quantization to reduce memory
+    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
+    full_finetuning = False, # [NEW!] We have full finetuning now!
+)
+
+# benchmark base model performance
+model_name = ""Unsloth Base model""
+FastVisionModel.for_inference(model)
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_base_model_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+## Lora Finetuning
+model = FastVisionModel.get_peft_model(
+    model,
+    finetune_vision_layers     = True, # Turn off for just text!
+    finetune_language_layers   = True,  # Should leave on!
+    finetune_attention_modules = True,  # Attention good for GRPO
+    finetune_mlp_modules       = True,  # SHould leave on always!
+
+    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    #target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                      #""gate_proj"", ""up_proj"", ""down_proj"",],
+    lora_alpha = 32,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+)
+
+from unsloth import is_bf16_supported
+from unsloth.trainer import UnslothVisionDataCollator
+FastVisionModel.for_training(model) # Enable for training!
+model.config.use_cache = False
+
+
+trainer = SFTTrainer(
+    model = model,
+    tokenizer = tokenizer,
+    data_collator = UnslothVisionDataCollator(model, tokenizer),
+    train_dataset = train_dataset,
+    args = SFTConfig(
+        #per_device_train_batch_size = 4,
+        #gradient_accumulation_steps = 8,
+        per_device_train_batch_size = 2,
+        gradient_accumulation_steps = 4,
+        gradient_checkpointing=True,
+        gradient_checkpointing_kwargs = {""use_reentrant"": False}, # use reentrant checkpointing
+        max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper
+        warmup_ratio=0.03,
+        #num_train_epochs = 2, # Set this instead of max_steps for full training runs
+        max_steps=60,
+        learning_rate = 2e-4,
+        fp16 = not is_bf16_supported(),
+        bf16 = is_bf16_supported(),
+        logging_steps = 5,
+        save_strategy=""epoch"",
+        optim = ""adamw_torch_fused"",
+        weight_decay = 0.01,
+        lr_scheduler_type = ""linear"",
+        seed = 3407,
+        output_dir = ""unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"",
+        report_to = ""none"",     # For Weights and Biases
+
+        # You MUST put the below items for vision finetuning:
+        remove_unused_columns = False,
+        dataset_text_field = """",
+        dataset_kwargs = {""skip_prepare_dataset"": True},
+        dataset_num_proc = 4,
+        max_seq_length = 2048,
+    ),
+)
+
+# run training
+trainer_stats = trainer.train()
+
+model.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"", tokenizer)
+tokenizer.save_pretrained(""unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
+
+## Measure Adapter Performance
+
+# benchmark lora model performance
+model_name = ""Unsloth lora adapter model""
+FastVisionModel.for_inference(model)
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_lora_model_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+## Merge Model
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+base = find_lora_base_model(model)
+
+print((base.__class__.__name__))
+
+# merge default 16 bits
+model.save_pretrained_merged(save_directory=""qwen2.5-ocr-merged-finetune-merge-16bit"", tokenizer=tokenizer)
+
+
+## Benchmark merged model performance
+
+### 16 bits merged model
+
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=False)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-16bits""
+model.config.use_cache = True
+
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_16bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# load 16bits-merged model in 4 bits
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=True, load_in_8bit=False)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-4bits""
+model.config.use_cache = True
+
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_4bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# load model in 8 bits
+model, tokenizer = FastVisionModel.from_pretrained(""./qwen2.5-ocr-merged-finetune-merge-16bit"",load_in_4bit=False, load_in_8bit=True)
+
+# benchmark 4bit loaded, 16bits merged model performance
+model_name = ""Unsloth 16bits-merged model load-8bits""
+avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_16bits_merged_model_load_8bits_results"")
+ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# """"""### 4 bits merged model""""""
+#
+# # load 4bits-merged model in 4 bits
+# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=True, load_in_8bit=False)
+#
+# # benchmark 4bit loaded, 4bits merged model performance
+# model_name = ""Unsloth 4bits-merged model load-4bits""
+#
+# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_4bits_results"")
+# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+#
+# # load model in 8 bits
+# model, tokenizer = FastVisionModel.from_pretrained(""./qwen2-ocr-merged-finetune-merge-4bit"",load_in_4bit=False, load_in_8bit=True)
+#
+# # benchmark 8bit loaded, 4bits merged model performance
+# model_name = ""Unsloth 4bits-merged model load-8bits""
+#
+# avg_wer, avg_cer = ocr_evaluator.evaluate_model(model, tokenizer, eval_dataset, output_dir=""unsloth_4bits_merged_model_load_8bits_results"")
+# ocr_evaluator.add_to_comparison(model_name, avg_wer, avg_cer)
+
+# Model comparison report
+#print model comparison
+ocr_evaluator.print_model_comparison()
+
+
+
+# Final cleanup
+print(""\n Cleaning up temporary files..."")
+safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-adapter"")
+safe_remove_directory(""./unsloth-qwen2.5-vl-32b-french-ocr-checkpoints"")
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./qwen2.5-ocr-merged-finetune-merge-16bit"")
+
+print(""\n Pipeline completed successfully!"")
+print(""="" * 80)
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index f559c6c..28fa163 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -618,6 +618,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen2.5-VL-7B-Instruct"",
         ""unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"",
     ),
+    ""unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-VL-32B-Instruct"",
+        ""Qwen/Qwen2.5-VL-32B-Instruct"",
+        ""unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit"",
+    ),
     ""unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2.5-VL-72B-Instruct"",
         ""Qwen/Qwen2.5-VL-72B-Instruct"",
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index be7d221..2ec4ada 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.2.3""
+__version__ = ""2025.2.4""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 22e1e0f..515c658 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -47,8 +47,8 @@ def PatchRL(FastLanguageModel):
     from contextlib import contextmanager
 
     @contextmanager
-    def unsloth_unwrap_model_for_generation(model, accelerator):
-        with unwrap_model_for_generation(model, accelerator) as unwrapped_model:
+    def unsloth_unwrap_model_for_generation(model, *args, **kwargs):
+        with unwrap_model_for_generation(model, *args, **kwargs) as unwrapped_model:
             # Put the model in inference mode.
             FastLanguageModel.for_inference(unwrapped_model)
 
@@ -364,7 +364,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         lora_name = trainer_file + ""_lora_model""
         source = re.sub(
             r""(self\.llm\.(?:generate|chat)\([^\)]{1,})\)"",
-            r""\1, lora_request = model.load_lora('"" + lora_name + r""', load_tensors = True))"",
+            r""\1, lora_request = self.model.load_lora('"" + lora_name + r""', load_tensors = True))"",
             source
         )
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 9c899e9..c7602ed 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -226,7 +226,7 @@ class _RaiseUninitialized(logging.Handler):
                 f""Unsloth: Critical error since some weights are not initialized.\n""\
                 f""Please try updating Unsloth, transformers and timm via:\n""\
                 f""`pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo transformers timm`\n""\
-                f"""".str(record))
+                f""{record}"")
 pass
 class RaiseUninitialized:
     def __init__(self):
"
"diff --git a/pyproject.toml b/pyproject.toml
index 7b1d2ef..6e1bea6 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.11"",
+    ""unsloth_zoo>=2025.3.13"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -351,7 +351,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.9"",
+    ""unsloth_zoo>=2025.3.13"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -511,4 +511,4 @@ cu126-ampere-torch260 = [
 [project.urls]
 homepage = ""http://www.unsloth.ai""
 documentation = ""https://github.com/unslothai/unsloth""
-repository = ""https://github.com/unslothai/unsloth""
+repository = ""https://github.com/unslothai/unsloth""
\ No newline at end of file
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 7ffddde..80aa3bd 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,10 +198,10 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.11""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.13""):
         print(
             ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
-            ""To disable this, set os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'""
+            ""To disable this, set `os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'`""
         )
         if os.environ.get(""UNSLOTH_DISABLE_AUTO_UPDATES"", ""0"") == ""0"":
             try:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 69cc1e6..e2b35c5 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.14""
+__version__ = ""2025.3.15""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -182,6 +182,15 @@ try:
 except:
     pass
 
+# Gemma3 It is strongly recommended to train Gemma3 models with the `eager`
+try:
+    from transformers.models.gemma3.modeling_gemma3 import logger as gemma3_logger
+    gemma3_logger.addFilter(HideLoggingMessage(""strongly recommended""))
+    del gemma3_logger
+except:
+    pass
+
+
 # Patch get_model_param_count to record correct 4bit / 8bit
 from transformers.trainer_pt_utils import is_deepspeed_zero3_enabled
 def get_model_param_count(model, trainable_only = False):
@@ -1016,13 +1025,7 @@ def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
             ""Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient""
         )
     pass
-
-    if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""0"":
-        autocaster = contextlib.nullcontext()
-    else:
-        autocaster = torch.autocast(device_type = ""cuda"", dtype = torch.float32)
-    with autocaster:
-        outputs = self._old_compute_loss(model, inputs, *args, **kwargs)
+    outputs = self._old_compute_loss(model, inputs, *args, **kwargs)
     return outputs
 pass
 
@@ -1126,7 +1129,9 @@ pass
 
 
 def unsloth_compile_transformers(
+    dtype,
     model_name,
+    model_types,
     token                   = None,
     revision                = None,
     trust_remote_code       = False,
@@ -1164,15 +1169,12 @@ def unsloth_compile_transformers(
         )
         return
     pass
-
-    model_types = get_transformers_model_type(
-        model_name        = model_name,
-        token             = token,
-        revision          = revision,
-        trust_remote_code = trust_remote_code,
-    )
-    model_types = [""siglip""] + model_types
-
+    if trust_remote_code:
+        print(
+            ""Unsloth: We can't trace models if `trust_remote_code = True`, ""\
+            ""so turning off some optimizations!""
+        )
+        return
     if disable: return
 
     for model_type in model_types:
@@ -1204,6 +1206,9 @@ def unsloth_compile_transformers(
             return_logits          = return_logits,
         )
     pass
+    # Redo patches which override compiler
+    for temporary_patch in TEMPORARY_PATCHES:
+        temporary_patch()
     return model_types
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 893a09d..0780527 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1548,7 +1548,7 @@ def unsloth_fast_generate(
         if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
             if kwargs[""input_ids""].shape[-1] + kwargs[""max_new_tokens""] > self.config.max_position_embeddings:
                 raise ValueError(
-                    f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {model.config.max_position_embeddings}!\n'\
+                    f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {self.config.max_position_embeddings}!\n'\
                     'You will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`.'
                 )
     pass
@@ -1562,7 +1562,10 @@ def unsloth_fast_generate(
     # For newer HF
     kwargs[""cache_implementation""] = ""dynamic""
     # For num_logits_to_keep
-    kwargs[""num_logits_to_keep""] = 1
+    num_logits_to_keep = kwargs.get(""num_logits_to_keep"", None)
+    logits_to_keep     = kwargs.get(""logits_to_keep"",     None)
+    if num_logits_to_keep is None and logits_to_keep is None:
+        kwargs[""num_logits_to_keep""] = 1
 
     # Remove token_type_ids
     kwargs.pop(""token_type_ids"", None)
@@ -1822,7 +1825,7 @@ class FastLlamaModel:
 
             # Convert to HF format
             _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)
-            model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype)
+            model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype, bnb_config)
             model.vllm_engine = llm
             model.fast_generate = model.vllm_engine.generate
             model.fast_generate_batches = functools.partial(generate_batches, model.vllm_engine)
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 4447578..cd59e03 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -17,6 +17,7 @@ from ._utils import (
     HAS_FLASH_ATTENTION,
     HAS_FLASH_ATTENTION_SOFTCAPPING,
     USE_MODELSCOPE,
+    get_transformers_model_type,
 )
 from .granite import FastGraniteModel
 from .llama   import FastLlamaModel, logger
@@ -66,6 +67,11 @@ from ._utils import (
     unsloth_compile_transformers,
 )
 
+global FORCE_FLOAT32
+FORCE_FLOAT32 = [
+    ""gemma3"",
+]
+
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
@@ -212,7 +218,13 @@ class FastLanguageModel(FastLlamaModel):
                     f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 ) 
-            raise RuntimeError(autoconfig_error or peft_error)
+            # Create a combined error message showing both failures
+            combined_error = (
+                ""Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\n""
+                f""AutoConfig error: {autoconfig_error}\n\n""
+                f""PeftConfig error: {peft_error}\n\n""
+            )
+            raise RuntimeError(combined_error)
         pass
 
         # Get base model for PEFT:
@@ -460,12 +472,17 @@ class FastModel(FastBaseModel):
         *args, **kwargs,
     ):
         if token is None: token = get_token()
-        assert (dtype is None or dtype == torch.float16 or dtype == torch.bfloat16)
+
+        SUPPORTS_BFLOAT16 = is_bfloat16_supported()
+        if dtype is None:
+            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
+        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:
+            logger.warning_once(""Device does not support bfloat16. Will change to float16."")
+            dtype = torch.float16
+        assert(dtype in (torch.float16, torch.bfloat16, torch.float32))
 
         patch_compiled_autograd()
         patch_compiling_bitsandbytes()
-        if use_gradient_checkpointing == ""unsloth"":
-            patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
 
         if full_finetuning and (load_in_4bit or load_in_8bit):
             print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
@@ -479,11 +496,6 @@ class FastModel(FastBaseModel):
                 ""Also, we by default set `load_in_4bit = True`.\n""\
                 ""If you want 8bit finetuning, set both `load_in_4bit = False` and `load_in_8bit = True`""
             )
-        if load_in_4bit: pass
-        elif load_in_8bit: pass
-        elif not load_in_4bit and not load_in_8bit and not full_finetuning:
-            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
-            load_in_4bit = True
         pass
 
         old_model_name = model_name
@@ -591,7 +603,13 @@ class FastModel(FastBaseModel):
                     f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 ) 
-            raise RuntimeError(autoconfig_error or peft_error)
+            # Create a combined error message showing both failures
+            combined_error = (
+                ""Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\n""
+                f""AutoConfig error: {autoconfig_error}\n\n""
+                f""PeftConfig error: {peft_error}\n\n""
+            )
+            raise RuntimeError(combined_error)
         pass
 
         # Get base model for PEFT:
@@ -616,10 +634,39 @@ class FastModel(FastBaseModel):
         else:
             redirector = contextlib.redirect_stdout(open(os.devnull, ""w""))
 
+        # Get model types like Gemma3 etc
+        model_types = get_transformers_model_type(
+            model_name        = model_name,
+            token             = token,
+            revision          = revision,
+            trust_remote_code = trust_remote_code,
+        )
+        model_types = [""siglip""] + model_types
+
+        # Set forced float32 env flag
+        os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""0""
+        do_forced_float32 = False
+        model_type_arch = model_types[1]
+        global FORCE_FLOAT32
+        for disable_name in FORCE_FLOAT32:
+            if (disable_name.lower() == model_type_arch.lower() or \
+                disable_name.lower() in model_name.lower()) and \
+                ((dtype == torch.float16) or not SUPPORTS_BFLOAT16):
+                os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""1""
+                dtype = torch.bfloat16 # Change to bfloat16 loading
+                break
+        pass
+        # Patch gradient checkpointing
+        if use_gradient_checkpointing == ""unsloth"":
+            patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
+
         with redirector:
             patch_loss_functions(torch_compile = False)
             model_types = unsloth_compile_transformers(
+                dtype                   = dtype,
                 model_name              = model_name,
+                model_types             = model_types,
+                token                   = token,
                 sdpa_dynamic_mask       = True,
                 sdpa_bool_masks         = True,
                 sdpa_gqa_replace        = True,
@@ -644,6 +691,7 @@ class FastModel(FastBaseModel):
                 import_from_cache       = False,
                 disable                 = False,
                 return_logits           = return_logits,
+                trust_remote_code       = trust_remote_code,
             )
         pass
 
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index c450ef6..5d22708 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -439,6 +439,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""eval_accumulation_steps""     : 2,
         ""torch_empty_cache_steps""     : 250,
         ""logging_steps""               : 1,
+        ""max_seq_length""              : None,
     }
     for k, v in replacements.items():
         x = f""{k}( = [^,\n]{{1,}})?,\n""
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 4071ef8..a3b2d1d 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -176,8 +176,9 @@ def grpo_trainer__prepare_inputs(function_name, function):
 
         ""with torch.inference_mode(), ""\
         ""torch.amp.autocast(device_type = 'cuda', ""\
-        ""dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16) ""\
-        ""if not torch.is_autocast_enabled('cuda') else nullcontext():"",
+        ""dtype = ((torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16) ""\
+        ""if not torch.is_autocast_enabled('cuda') else nullcontext())""\
+        ""if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '0' else torch.float16):"",
     )
 
     # Disable attaching a float32 conversion hook which upcasts logits to FP32
@@ -212,7 +213,7 @@ def grpo_trainer__get_per_token_logps(function_name, function):
         # Otherwise, calculate normally:
         if not hasattr(self, '_autocast_dtype'):
             self._autocast_dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16
-            if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '1': self._autocast_dtype = torch.float32
+            if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '1': self._autocast_dtype = torch.float16
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
             # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
             logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
@@ -254,11 +255,12 @@ def grpo_trainer_compute_loss(function_name, function):
         completion_ids, completion_mask = inputs[""completion_ids""], inputs[""completion_mask""]
         input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
         bsz, qlen = input_ids.shape
-        # attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
-        attention_mask = None
+        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
+        # attention_mask = None
         logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens
         _input_ids = input_ids
         _logits_to_keep = logits_to_keep
+        
         per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
 
         # Compute the KL divergence between the model and the reference model
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 24015f8..53a873d 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -53,6 +53,7 @@ import math
 import functools
 from typing import Optional, Tuple, List, Union
 import re, inspect, sys
+import contextlib
 import types
 try:
     from huggingface_hub.utils import get_token
@@ -65,11 +66,6 @@ __all__ = [
     ""FastBaseModel"",
 ]
 
-global FORCE_FLOAT32
-FORCE_FLOAT32 = [
-    ""gemma3"",
-]
-
 global FORCE_EAGER_ATTENTION
 FORCE_EAGER_ATTENTION = [
     ""pixtral"",    # Pixtral SDPA not implemented
@@ -77,12 +73,23 @@ FORCE_EAGER_ATTENTION = [
 
 global NUM_LOGITS_TO_KEEP
 NUM_LOGITS_TO_KEEP = dict()
+global PROMPT_LOOPKUP
+PROMPT_LOOPKUP = dict()
 
 def unsloth_base_fast_generate(
     self,
     *args,
     **kwargs,
 ):
+    if len(args) != 0:
+        x = args[0]
+    elif ""input_ids"" in kwargs:
+        x = kwargs[""input_ids""]
+    else:
+        raise TypeError(""Unsloth: You need to pass in input_ids to .generate!"")
+    assert(type(x) is torch.Tensor)
+    bsz = x.shape[0]
+
     FastBaseModel.for_inference(self)
     dtype = _get_dtype(self.config.torch_dtype)
 
@@ -98,34 +105,35 @@ def unsloth_base_fast_generate(
     kwargs.pop(""token_type_ids"", None)
 
     # VLMs do not allow logits_to_keep
-    if not is_vlm:
-        global NUM_LOGITS_TO_KEEP
-        if arch not in NUM_LOGITS_TO_KEEP:
-            m = self
-            # Find which is needed ie
-            # num_logits_to_keep or logits_to_keep
-            while hasattr(m, ""model""):
-                if hasattr(m, ""forward""):
-                    keys = inspect.signature(m.forward).parameters.keys()
-                    if ""num_logits_to_keep"" in keys:
-                        NUM_LOGITS_TO_KEEP[arch] = ""num_logits_to_keep""
-                        break
-                    elif ""logits_to_keep"" in keys:
-                        NUM_LOGITS_TO_KEEP[arch] = ""logits_to_keep""
-                        break
-                m = m.model
-            pass
-            if arch not in NUM_LOGITS_TO_KEEP:
-                NUM_LOGITS_TO_KEEP[arch] = None
-            pass
+    global NUM_LOGITS_TO_KEEP
+    if arch not in NUM_LOGITS_TO_KEEP:
+        m = self
+        # Find which is needed ie
+        # num_logits_to_keep or logits_to_keep
+        while hasattr(m, ""model""):
+            if hasattr(m, ""forward""):
+                keys = inspect.signature(m.forward).parameters.keys()
+                if ""num_logits_to_keep"" in keys:
+                    NUM_LOGITS_TO_KEEP[arch] = ""num_logits_to_keep""
+                    break
+                elif ""logits_to_keep"" in keys:
+                    NUM_LOGITS_TO_KEEP[arch] = ""logits_to_keep""
+                    break
+            m = m.model
         pass
-        key = NUM_LOGITS_TO_KEEP[arch]
-        if key is not None and key not in kwargs:
-            kwargs[key] = 1
-    else:
+        if arch not in NUM_LOGITS_TO_KEEP:
+            NUM_LOGITS_TO_KEEP[arch] = None
         pass
-        # kwargs.pop(""logits_to_keep"", None)
-        # kwargs.pop(""num_logits_to_keep"", None)
+    pass
+    key = NUM_LOGITS_TO_KEEP[arch]
+    if key is not None and key not in kwargs:
+        kwargs[key] = 1
+    global PROMPT_LOOPKUP
+    if arch not in PROMPT_LOOPKUP:
+        PROMPT_LOOPKUP[arch] = True
+
+    if bsz == 1 and PROMPT_LOOPKUP[arch]:
+        kwargs[""prompt_lookup_num_tokens""] = 3
 
     # Check pad_token
     model_eos_token_id = getattr(self.config, ""eos_token_id"", None)
@@ -138,10 +146,20 @@ def unsloth_base_fast_generate(
     try: kwargs[""pixel_values""] = kwargs[""pixel_values""].to(dtype)
     except: pass
 
+    if ""use_cache"" not in kwargs: kwargs[""use_cache""] = True
+
     # Mixed precision autocast
-    if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"": dtype = torch.float32
-    with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
-        output = self._old_generate(*args, **kwargs)
+    if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"":
+        autocaster = torch.autocast(device_type = ""cuda"", dtype = dtype)
+    else:
+        autocaster = torch.autocast(device_type = ""cuda"", dtype = dtype)
+    with torch.inference_mode(), autocaster:
+        try:
+            output = self._old_generate(*args, **kwargs)
+        except:
+            PROMPT_LOOPKUP[arch] = False
+            kwargs.pop(""prompt_lookup_num_tokens"", None)
+            output = self._old_generate(*args, **kwargs)
     pass
 
     FastBaseModel.for_training(self)
@@ -209,24 +227,20 @@ class FastBaseModel:
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
+        elif os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"":
+            if dtype == torch.float16: dtype = torch.bfloat16
         elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:
             logger.warning_once(""Device does not support bfloat16. Will change to float16."")
             dtype = torch.float16
+        pass
+        assert(dtype in (torch.float16, torch.bfloat16, torch.float32))
 
-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
-
-        global FORCE_FLOAT32
-        os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""0""
         bnb_compute_dtype = dtype
-        for disable_name in FORCE_FLOAT32:
-            if (disable_name.lower() == model_type_arch.lower() or \
-                disable_name.lower() in model_name.lower()) and \
-                dtype == torch.float16:
-
-                print(f""Unsloth: Using float16 precision for {model_type_arch} won't work! Using float32."")
-                os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""1""
-                bnb_compute_dtype = torch.float32
-                break
+        do_forced_float32 = False
+        if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"":
+            print(f""Unsloth: Using float16 precision for {model_type_arch} won't work! Using float32."")
+            bnb_compute_dtype = torch.float16
+            do_forced_float32 = True
         pass
 
         global FORCE_EAGER_ATTENTION
@@ -263,15 +277,7 @@ class FastBaseModel:
                 llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
             )
         elif not load_in_4bit and not load_in_8bit and not full_finetuning:
-            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
-            load_in_4bit = True
-            bnb_config = BitsAndBytesConfig(
-                load_in_4bit              = True,
-                bnb_4bit_use_double_quant = True,
-                bnb_4bit_quant_type       = ""nf4"",
-                bnb_4bit_compute_dtype    = bnb_compute_dtype,
-                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
-            )
+            print(""Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA."")
         pass
 
         if full_finetuning:
@@ -289,10 +295,13 @@ class FastBaseModel:
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
 
+        # Check if using forced float32 - we load it in bfloat16, then cast to float16!
+        torch_dtype = dtype
+        if do_forced_float32: torch_dtype = torch.bfloat16
         model = auto_model.from_pretrained(
             model_name,
             device_map              = device_map,
-            torch_dtype             = dtype,
+            torch_dtype             = torch_dtype,
             # quantization_config   = bnb_config,
             token                   = token,
             trust_remote_code       = trust_remote_code,
@@ -325,15 +334,16 @@ class FastBaseModel:
                 tokenizer.pad_token    = __tokenizer.pad_token
                 tokenizer.pad_token_id = __tokenizer.pad_token_id
         pass
-        model, tokenizer = patch_tokenizer(model, tokenizer)
-        model = post_patch_loss_function(model)
         # Fix other stuff like BnB compute data types
         model, tokenizer = patch_model_and_tokenizer(
             model,
             tokenizer,
             downcast_rope = False,
             fix_embeddings = False,
+            do_forced_float32 = do_forced_float32,
         )
+        model, tokenizer = patch_tokenizer(model, tokenizer)
+        model = post_patch_loss_function(model)
 
         # Log Unsloth version for future fastpaths for inference
         if hasattr(model, ""config""):
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 2666912..067f259 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -686,12 +686,12 @@ def fix_chat_template(tokenizer):
                 raise RuntimeError(
                     f""Unsloth: The tokenizer `{tokenizer.name_or_path}`\n""\
                     ""does not have a {% if add_generation_prompt %} for generation purposes.\n""\
-                    ""Please file a bug report immediately - thanks!""
+                    f""Please file a bug report to the maintainers of `{tokenizer.name_or_path}` - thanks!""
                 )
             else:
                 logger.warning_once(
                     ""Unsloth: We successfully patched the tokenizer to add a {% if add_generation_prompt %} to the chat_template.\n""\
-                    ""This is not a bug, but please notify the Unsloth maintainers - thanks!""
+                    f""This is not a bug, but please notify the maintainers of `{tokenizer.name_or_path}` - thanks!""
                 )
                 chat_template = new_chat_template
             pass
"
"diff --git a/pyproject.toml b/pyproject.toml
index 3d381fa..b3ee2d7 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.7"",
+    ""unsloth_zoo>=2025.7.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.7"",
+    ""unsloth_zoo>=2025.7.8"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
diff --git a/unsloth/dataprep/synthetic.py b/unsloth/dataprep/synthetic.py
index f3fbf77..2326880 100644
--- a/unsloth/dataprep/synthetic.py
+++ b/unsloth/dataprep/synthetic.py
@@ -80,6 +80,10 @@ class SyntheticDataKit:
         )
         if ""dtype"" in engine_args:
             dtype_val = engine_args[""dtype""]
+            if   dtype_val == torch.float16:  dtype_val = ""float16""
+            elif dtype_val == torch.bfloat16: dtype_val = ""bfloat16""
+            elif dtype_val == torch.float32:  dtype_val = ""float32""
+            engine_args[""dtype""] = dtype_val
             # Convert torch.bfloat16, torch.float16, etc. to valid CLI string
             if hasattr(dtype_val, ""name""):
                 engine_args[""dtype""] = dtype_val.name
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 1774ed3..21ebbfd 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.5""
+__version__ = ""2025.7.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/save.py b/unsloth/save.py
index e610263..38c73c9 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2240,6 +2240,7 @@ from unsloth_zoo.llama_cpp import (
 def save_to_gguf_generic(
     model,
     save_directory,
+    quantization_method = None,
     quantization_type = ""Q8_0"",
     repo_id = None,
     token = None,
@@ -2252,29 +2253,63 @@ def save_to_gguf_generic(
         install_llama_cpp(just_clone_repo = True)
     pass
 
-    metadata = _convert_to_gguf(
-        save_directory,
-        print_output = True,
-        quantization_type = quantization_type,
-    )
-    if repo_id is not None:
-        prepare_saving(
-            model,
-            repo_id,
-            push_to_hub = True,
-            max_shard_size = ""50GB"",
-            private = True,
-            token = token,
-        )
+    # Use old style quantization_method
+    new_quantization_methods = []
+    if quantization_method is not None:
+        # Convert quantization_method to list
+        if   isinstance(quantization_method, list):  pass
+        elif isinstance(quantization_method, str):   quantization_method = [ quantization_method, ]
+        elif isinstance(quantization_method, tuple): quantization_method = list(quantization_method)
+        else:
+            raise TypeError(""Unsloth: quantization_method can only be a string or a list of strings"")
+        pass
+        for i, quant_method in enumerate(quantization_method):
+            quant_method = quant_method.lower()
+            if   quant_method == ""not_quantized"":  quant_method = ""f16""
+            elif quant_method == ""fast_quantized"": quant_method = ""q8_0""
+            elif quant_method == ""quantized"":      quant_method = ""q4_k_m""
+            elif quant_method is None:             quant_method = ""q8_0""
+            new_quantization_methods.append(quant_method.lower())
+        pass
+    else:
+        new_quantization_methods.append(quantization_type.lower())
+    # Check if wrong method
+    for quant_method in new_quantization_methods:
+        if quant_method not in ALLOWED_QUANTS.keys():
+            error = f""Unsloth: Quant method = [{quant_method}] not supported. Choose from below:\n""
+            for key, value in ALLOWED_QUANTS.items():
+                error += f""[{key}] => {value}\n""
+            raise RuntimeError(error)
+        pass
+    pass
 
-        from huggingface_hub import HfApi
-        api = HfApi(token = token)
-        api.upload_folder(
-            folder_path = save_directory,
-            repo_id = repo_id,
-            repo_type = ""model"",
-            allow_patterns = [""*.gguf""],
+    # Go through all types and save individually - somewhat inefficient
+    # since we save F16 / BF16 multiple times
+    for quantization_type in new_quantization_methods:
+        metadata = _convert_to_gguf(
+            save_directory,
+            print_output = True,
+            quantization_type = quantization_type,
         )
+        if repo_id is not None:
+            prepare_saving(
+                model,
+                repo_id,
+                push_to_hub = True,
+                max_shard_size = ""50GB"",
+                private = True,
+                token = token,
+            )
+
+            from huggingface_hub import HfApi
+            api = HfApi(token = token)
+            api.upload_folder(
+                folder_path = save_directory,
+                repo_id = repo_id,
+                repo_type = ""model"",
+                allow_patterns = [""*.gguf""],
+            )
+        pass
     pass
     return metadata
 pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 84dd72e..0658f3f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -76,6 +76,7 @@ import contextlib
 import re
 import warnings, subprocess, re, inspect, psutil, os, math
 from unsloth_zoo.utils import Version
+from unsloth import DEVICE_TYPE
 
 from unsloth_zoo.tokenizer_utils import (
     patch_tokenizer as _patch_tokenizer,
@@ -316,13 +317,20 @@ pass
 # =============================================
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
 torch_version = torch.__version__
-if Version(torch_version) < Version(""2.4.0""):
-    torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
-    torch_amp_custom_bwd = torch.cuda.amp.custom_bwd
-else:
-    torch_amp_custom_fwd = torch.amp.custom_fwd(device_type = ""cuda"")
-    torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""cuda"")
-pass
+if DEVICE_TYPE == ""cuda"":
+    if Version(torch_version) < Version(""2.4.0""):
+        torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
+        torch_amp_custom_bwd = torch.cuda.amp.custom_bwd
+    else:
+        torch_amp_custom_fwd = torch.amp.custom_fwd(device_type = ""cuda"")
+        torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""cuda"")
+    pass
+elif DEVICE_TYPE == ""xpu"":
+    if Version(torch_version) < Version(""2.6.0""):
+        raise RuntimeError(""torch.xpu currently only supports torch.version >= 2.6.0"")
+    else:
+        torch_amp_custom_fwd = torch.amp.custom_fwd(device_type = ""xpu"")
+        torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""xpu"")
 # =============================================
 
 # =============================================
@@ -363,60 +371,66 @@ pass
 
 # =============================================
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
-import bitsandbytes as bnb
+if DEVICE_TYPE == ""cuda"":
+    import bitsandbytes as bnb
+
 from transformers import AutoTokenizer
 from transformers.utils.import_utils import _is_package_available
 
-major_version, minor_version = torch.cuda.get_device_capability()
 SUPPORTS_BFLOAT16 = False
 HAS_FLASH_ATTENTION = False
 HAS_FLASH_ATTENTION_SOFTCAPPING = False
 
-if major_version >= 8:
-    SUPPORTS_BFLOAT16 = True
-    if _is_package_available(""flash_attn""):
-        # Check for CUDA linking errors ""undefined symbol: _ZNK3c106SymIntltEl""
-        try:
+if DEVICE_TYPE == ""cuda"":
+    major_version, minor_version = torch.cuda.get_device_capability()
+
+    if major_version >= 8:
+        SUPPORTS_BFLOAT16 = True
+        if _is_package_available(""flash_attn""):
+            # Check for CUDA linking errors ""undefined symbol: _ZNK3c106SymIntltEl""
             try:
-                # See https://github.com/unslothai/unsloth/issues/1437
-                from flash_attn.flash_attn_interface import flash_attn_gpu
+                try:
+                    # See https://github.com/unslothai/unsloth/issues/1437
+                    from flash_attn.flash_attn_interface import flash_attn_gpu
+                except:
+                    from flash_attn.flash_attn_interface import flash_attn_cuda
+                HAS_FLASH_ATTENTION = True
+
+                # Also check for softcapping
+                from flash_attn import __version__ as flash_attn_version
+                HAS_FLASH_ATTENTION_SOFTCAPPING = Version(flash_attn_version) >= Version(""2.6.3"")
+                if not HAS_FLASH_ATTENTION_SOFTCAPPING:
+                    print(
+                        ""Unsloth: If you want to finetune Gemma 2, upgrade flash-attn to version 2.6.3 or higher!\n""\
+                        ""Newer versions support faster and less memory usage kernels for Gemma 2's attention softcapping!\n""\
+                        ""To update flash-attn, do the below:\n""\
+                        '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
+                    )
             except:
-                from flash_attn.flash_attn_interface import flash_attn_cuda
-            HAS_FLASH_ATTENTION = True
-
-            # Also check for softcapping
-            from flash_attn import __version__ as flash_attn_version
-            HAS_FLASH_ATTENTION_SOFTCAPPING = Version(flash_attn_version) >= Version(""2.6.3"")
-            if not HAS_FLASH_ATTENTION_SOFTCAPPING:
                 print(
-                    ""Unsloth: If you want to finetune Gemma 2, upgrade flash-attn to version 2.6.3 or higher!\n""\
-                    ""Newer versions support faster and less memory usage kernels for Gemma 2's attention softcapping!\n""\
-                    ""To update flash-attn, do the below:\n""\
-                    '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
+                    ""Unsloth: Your Flash Attention 2 installation seems to be broken?\n""\
+                    ""A possible explanation is you have a new CUDA version which isn't\n""\
+                    ""yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n""\
+                    ""We shall now use Xformers instead, which does not have any performance hits!\n""\
+                    ""We found this negligible impact by benchmarking on 1x A100.""
                 )
-        except:
-            print(
-                ""Unsloth: Your Flash Attention 2 installation seems to be broken?\n""\
-                ""A possible explanation is you have a new CUDA version which isn't\n""\
-                ""yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n""\
-                ""We shall now use Xformers instead, which does not have any performance hits!\n""\
-                ""We found this negligible impact by benchmarking on 1x A100.""
-            )
 
-            # Stop Flash Attention from importing!
-            import transformers.utils.import_utils
-            transformers.utils.import_utils.is_flash_attn_2_available = lambda *args, **kwargs: False
-            import transformers.utils
-            transformers.utils.is_flash_attn_2_available = lambda *args, **kwargs: False
+                # Stop Flash Attention from importing!
+                import transformers.utils.import_utils
+                transformers.utils.import_utils.is_flash_attn_2_available = lambda *args, **kwargs: False
+                import transformers.utils
+                transformers.utils.is_flash_attn_2_available = lambda *args, **kwargs: False
 
+                HAS_FLASH_ATTENTION = False
+            pass
+        else:
             HAS_FLASH_ATTENTION = False
-        pass
     else:
+        # Tri Dao's benchmark shows xformers is faster for now.
         HAS_FLASH_ATTENTION = False
-else:
-    # Tri Dao's benchmark shows xformers is faster for now.
-    HAS_FLASH_ATTENTION = False
-pass
+    pass
+elif DEVICE_TYPE == ""xpu"":
+    SUPPORTS_BFLOAT16 = True
 
 from transformers.models.llama.modeling_llama import logger
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9db8abd..c83de8f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -24,6 +24,8 @@ from torch.nn.functional import scaled_dot_product_attention
 from transformers import __version__ as transformers_version
 from unsloth_zoo.utils import Version, _get_dtype
 from unsloth_zoo.peft_utils import SKIP_QUANTIZATION_MODULES
+from unsloth import DEVICE_TYPE
+
 transformers_version = Version(transformers_version)
 # Transformers moved rotary embeddings out of all attention layers
 IS_ATTENTION_REFACTOR = transformers_version > Version(""4.47.1"")
@@ -689,7 +691,7 @@ def LlamaModel_fast_forward(
         position_ids = torch.arange(
             past_key_values_length, seq_length + past_key_values_length,
             dtype  = torch.int32,
-            device = ""cuda:0"",
+            device = f""{DEVICE_TYPE}:0"",
         )
         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
     elif position_ids is not None:
@@ -861,13 +863,13 @@ def LlamaModel_fast_forward(
                     is_causal = True,
                     sliding_window = self.config.sliding_window,
                 )\
-                    .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda"",)\
+                    .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = DEVICE_TYPE,)\
                     .squeeze(0).squeeze(0)
 
                 self.GA_mask = AttentionMaskConverter(
                     is_causal = True,
                 )\
-                    .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda"",)\
+                    .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = DEVICE_TYPE,)\
                     .squeeze(0).squeeze(0)
             pass
         pass
@@ -984,11 +986,11 @@ def _LlamaModel_fast_forward_inference(attention_fast_forward_inference=LlamaAtt
         bsz, q_len, hd = X.shape
         assert(q_len == 1)
         # Get saved buffers to reduce memory movement
-        residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
-        _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+        residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = f""{DEVICE_TYPE}:0"")
+        _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = f""{DEVICE_TYPE}:0"")
         XX, XX2 = _XX[0], _XX[1]
-        variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = ""cuda:0"")
-        temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
+        variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = f""{DEVICE_TYPE}:0"")
+        temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = f""{DEVICE_TYPE}:0"")
         temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
 
         seq_len = past_key_values[0][0].shape[-2]
@@ -1305,7 +1307,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
             partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
             dim = getattr(config, ""head_dim"", None)
             if dim is None: dim = int((config.hidden_size // config.num_attention_heads))
-            device = ""cuda""
+            device = DEVICE_TYPE
             max_position_embeddings = config.max_position_embeddings
         pass
 
@@ -1354,7 +1356,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
         self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = ""cuda"", dtype = x.dtype)
+        self._set_cos_sin_cache(self.current_rope_size, device = DEVICE_TYPE, dtype = x.dtype)
     pass
 pass
 
@@ -1400,7 +1402,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
             base = config.rope_theta
             partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
             dim = int((config.hidden_size // config.num_attention_heads))
-            device = ""cuda""
+            device = DEVICE_TYPE
             max_position_embeddings = config.max_position_embeddings
         pass
 
@@ -1480,7 +1482,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
         self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = ""cuda"", dtype = x.dtype)
+        self._set_cos_sin_cache(self.current_rope_size, device = DEVICE_TYPE, dtype = x.dtype)
     pass
 pass
 
@@ -1507,7 +1509,7 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
             base = config.rope_theta
             partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
             dim = int((config.hidden_size // config.num_attention_heads))
-            device = ""cuda""
+            device = DEVICE_TYPE
             max_position_embeddings = config.max_position_embeddings
         pass
 
@@ -1595,7 +1597,7 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
         self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
-        self._set_cos_sin_cache(self.current_rope_size, device = ""cuda"", dtype = x.dtype)
+        self._set_cos_sin_cache(self.current_rope_size, device = DEVICE_TYPE, dtype = x.dtype)
     pass
 pass
 
@@ -1643,7 +1645,7 @@ def unsloth_fast_generate(
     kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
 
     # Mixed precision autocast
-    with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
+    with torch.inference_mode(), torch.autocast(device_type = DEVICE_TYPE, dtype = dtype):
         output = self._old_generate(*args, **kwargs)
     pass
 
@@ -1744,19 +1746,36 @@ class FastLlamaModel:
         if token is None: token = get_token()
         if model_patcher is None: model_patcher = FastLlamaModel
         SUPPORTS_BFLOAT16 = is_bfloat16_supported()
-        gpu_stats = torch.cuda.get_device_properties(0)
-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
-        from importlib.metadata import version as importlib_version
-        try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
-        except: vllm_version = """"
+        if DEVICE_TYPE == ""cuda"":
+            gpu_stats = torch.cuda.get_device_properties(0)
+            gpu_version = torch.version.cuda
+            gpu_stats_snippet = f""CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {gpu_version}.""
+            num_gpus = torch.cuda.device_count()
+
+            from importlib.metadata import version as importlib_version
+            try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
+            except: vllm_version = """"
+        elif DEVICE_TYPE == ""xpu"":
+            gpu_stats = torch.xpu.get_device_properties(0)
+            gpu_version = torch.version.xpu
+            num_gpus = torch.xpu.device_count()
+            gpu_stats_snippet = f""Intel Toolkit: {gpu_version}.""
+
+            # TODO: After adding vLLM support for XPU, changed this
+            vllm_version = """"
+        else:
+            raise ValueError(f""Unsloth: Unsupported device type: {DEVICE_TYPE}"")
+
+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.{vllm_version}\n""\
-           f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
-           f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
-           f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
-           f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
+        f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.{vllm_version}\n""\
+        f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {num_gpus}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+        f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. {gpu_stats_snippet} Triton: {triton_version}\n""\
+        f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
+        f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
+
         print(statistics)
 
         # Warn about fast transfers
@@ -2215,7 +2234,7 @@ class FastLlamaModel:
                     pass
 
                     model.get_input_embeddings().modules_to_save.default\
-                        .to(device = ""cuda"", dtype = new_dtype, non_blocking = True)
+                        .to(device = DEVICE_TYPE, dtype = new_dtype, non_blocking = True)
                     model.get_input_embeddings().modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old embed_tokens to CPU - should be disk!
@@ -2235,7 +2254,7 @@ class FastLlamaModel:
                     pass
 
                     model.get_output_embeddings().modules_to_save.default\
-                        .to(device = ""cuda"", dtype = new_dtype, non_blocking = True)
+                        .to(device = DEVICE_TYPE, dtype = new_dtype, non_blocking = True)
                     model.get_output_embeddings().modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old lm_head to CPU - should be disk!
@@ -2499,7 +2518,7 @@ class FastLlamaModel:
             pass
 
             model.get_input_embeddings().modules_to_save.default\
-                .to(device = ""cuda"", dtype = new_dtype, non_blocking = True)
+                .to(device = DEVICE_TYPE, dtype = new_dtype, non_blocking = True)
             model.get_input_embeddings().modules_to_save.default.requires_grad_(True)
         pass
 
@@ -2515,7 +2534,7 @@ class FastLlamaModel:
             pass
 
             model.get_output_embeddings().modules_to_save.default\
-                .to(device = ""cuda"", dtype = new_dtype, non_blocking = True)
+                .to(device = DEVICE_TYPE, dtype = new_dtype, non_blocking = True)
             model.get_output_embeddings().modules_to_save.default.requires_grad_(True)
         pass
 
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 4466128..a140ccd 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -61,6 +61,7 @@ except:
     # Old HF Hub versions <= 0.0.25
     from huggingface_hub.utils._token import get_token
 pass
+from unsloth import DEVICE_TYPE
 
 __all__ = [
     ""FastBaseModel"",
@@ -275,12 +276,28 @@ class FastBaseModel:
         pass
         if token is None: token = get_token()
         SUPPORTS_BFLOAT16 = is_bfloat16_supported()
-        gpu_stats = torch.cuda.get_device_properties(0)
-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
-        from importlib.metadata import version as importlib_version
-        try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
-        except: vllm_version = """"
+        if DEVICE_TYPE == ""cuda"":
+            gpu_stats = torch.cuda.get_device_properties(0)
+            gpu_version = torch.version.cuda
+            gpu_stats_snippet = f""CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {gpu_version}.""
+            num_gpus = torch.cuda.device_count()
+
+            from importlib.metadata import version as importlib_version
+            try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
+            except: vllm_version = """"
+        elif DEVICE_TYPE == ""xpu"":
+            gpu_stats = torch.xpu.get_device_properties(0)
+            gpu_version = torch.version.xpu
+            num_gpus = torch.xpu.device_count()
+            gpu_stats_snippet = f""Intel Toolkit: {gpu_version}.""
+
+            # TODO: After adding vLLM support for XPU, changed this
+            vllm_version = """"
+        else:
+            raise ValueError(f""Unsloth: Unsupported device type: {DEVICE_TYPE}"")
+
+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         model_type_arch = model_types[0]
         if model_type_arch == ""siglip"":
@@ -288,11 +305,12 @@ class FastBaseModel:
                 if model_type_arch != ""siglip"": break
 
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_type_arch.title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
-           f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
-           f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
-           f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
-           f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
+        f""==((====))==  Unsloth {__version__}: Fast {model_type_arch.title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
+        f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {num_gpus}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+        f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. {gpu_stats_snippet} Triton: {triton_version}\n""\
+        f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
+        f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
+        
         print(statistics)
 
         # Warn about fast transfers
@@ -500,7 +518,10 @@ class FastBaseModel:
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
-            torch.cuda.empty_cache()
+            if DEVICE_TYPE == ""cuda"":
+                torch.cuda.empty_cache()
+            elif DEVICE_TYPE == ""xpu"":
+                torch.xpu.empty_cache()
         pass
         return model, tokenizer
     pass
@@ -566,7 +587,10 @@ class FastBaseModel:
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
-            torch.cuda.empty_cache()
+            if DEVICE_TYPE == ""cuda"":
+                torch.cuda.empty_cache()
+            elif DEVICE_TYPE == ""xpu"":
+                torch.xpu.empty_cache()
         pass
         max_seq_length = model.max_seq_length
         lora_config = LoraConfig(
@@ -591,7 +615,10 @@ class FastBaseModel:
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
-            torch.cuda.empty_cache()
+            if DEVICE_TYPE == ""cuda"":
+                torch.cuda.empty_cache()
+            elif DEVICE_TYPE == ""xpu"":
+                torch.xpu.empty_cache()
         pass
         patch_saving_functions(model, vision = True)
 
@@ -653,7 +680,10 @@ class FastBaseModel:
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
-            torch.cuda.empty_cache()
+            if DEVICE_TYPE == ""cuda"":
+                torch.cuda.empty_cache()
+            elif DEVICE_TYPE == ""xpu"":
+                torch.xpu.empty_cache()
         pass
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastBaseModel.for_training,  model)
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index 0317c48..e6d09b7 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2240,6 +2240,7 @@ from unsloth_zoo.llama_cpp import (
 def save_to_gguf_generic(
     model,
     save_directory,
+    tokenizer,
     quantization_method = None,
     quantization_type = ""Q8_0"",
     repo_id = None,
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 51c63fc..c0a1559 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -487,6 +487,50 @@ def patch_tokenizer(model, tokenizer):
         bad_pad_token = False
     pass
 
+    # Check if unknown token is broken
+    fixed_unk_token = False
+
+    if hasattr(tokenizer, ""unk_token"") and tokenizer.unk_token is not None:
+
+        eos_token = getattr(tokenizer, ""eos_token"", None)
+        bos_token = getattr(tokenizer, ""bos_token"", None)
+
+        old_unk_token = tokenizer.unk_token
+        if old_unk_token == eos_token or old_unk_token == bos_token:
+            has_broken_unk = True
+            # Use the unicode replacement characters
+            possible_replacements = [
+                ""\uFFFD"", # Original replacement char
+                ""\uFFFC"", # Another option
+                ""\u2753"", # Red Question mark emoji
+                ""\u2754"", # White Question mark emoji
+                ""\u00BF"", # Inverted question mark
+            ]
+            for replacement_char in possible_replacements:
+                char = tokenizer(replacement_char, add_special_tokens = False)
+                if len(char) == 1:
+                    # Get actual token representation
+                    try: char = tokenizer.convert_ids_to_tokens(char[0])
+                    except: continue
+                    tokenizer.unk_token = char
+                    fixed_unk_token = True
+                    break
+                pass
+            pass
+
+            if not fixed_unk_token: # Still broken!
+                raise RuntimeError(
+                    f""Unsloth: Tried fixing the unk_token = {old_unk_token}, but couldn't!""
+                )
+            pass
+            
+            logger.warning_once(
+                f""Unsloth: unk_token = {old_unk_token} is the same as the EOS or BOS tokens.\n""\
+                f""We fixed it by changing it to {tokenizer.unk_token}.""
+            )
+        pass
+    pass
+
     if bad_pad_token:
         # Find a better pad token
         added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
@@ -534,8 +578,8 @@ def patch_tokenizer(model, tokenizer):
         pass
         possible_pad_token = final_pad_token
 
-        # Try unk_token
-        if possible_pad_token is None and hasattr(tokenizer, ""unk_token""):
+        # Try unk_token if it wasn't fixed
+        if possible_pad_token is None and not fixed_unk_token and hasattr(tokenizer, ""unk_token""):
             possible_pad_token = tokenizer.unk_token
         pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index cf05d43..9c9ea53 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1910,6 +1910,11 @@ class FastLlamaModel:
     ):
         transformers_set_seed(random_state)
 
+        if type(r) is not int:
+            raise TypeError(f""Unsloth: Rank of {str(r)} must be an integer."")
+        if r <= 0:
+            raise TypeError(f""Unsloth: Rank of {str(r)} must be larger than 0."")
+
         if isinstance(model, PeftModelForCausalLM):
             # Check if exactly the same and then pass through!
             assert(hasattr(model, ""peft_config""))
"
"diff --git a/pyproject.toml b/pyproject.toml
index 9abe7a5..ce33015 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -39,7 +39,7 @@ triton = [
     ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2024.12.6"",
+    ""unsloth_zoo>=2024.12.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -285,7 +285,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2024.12.6"",
+    ""unsloth_zoo>=2024.12.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 4f1b408..86346d7 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -1003,28 +1003,62 @@ pass
 def _unsloth_get_batch_samples(self, epoch_iterator, num_batches):
     batch_samples = []
     num_items_in_batch = None
+
+    # Check if model allows **kwargs
+    model = self.model
+    f = model.base_model.model.forward if hasattr(model, ""base_model"") else model.forward
+    has_kwargs = tuple(inspect.signature(f).parameters.values())[-1].kind == inspect._VAR_KEYWORD
+
+    # Iterate to find all batches
     for _ in range(num_batches):
         try:
             batch_samples += [next(epoch_iterator)]
         except StopIteration:
             break
-    if len(batch_samples) > 0 and ""labels"" in batch_samples[0]:
+    pass
+
+    # Get num_items_in_batch
+    if has_kwargs and len(batch_samples) > 0 and ""labels"" in batch_samples[0]:
         try:
             num_items_in_batch = sum(
-                [torch.count_nonzero(x[""labels""][..., 1:] != -100) for x in batch_samples]
+                [(x[""labels""][..., 1:] != -100).sum() for x in batch_samples]
             )
-        except TypeError:
-            pass
+            
+            if self.args.average_tokens_across_devices:
+                num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()
+
+            if torch.is_tensor(num_items_in_batch):
+                num_items_in_batch = num_items_in_batch.item()
+
+        except Exception as exception:
+            logger.warning_once(exception)
+    pass
+
     return batch_samples, num_items_in_batch
 pass
 
 
 def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
+    num_items_in_batch = None
+
     if ""num_items_in_batch"" in kwargs:
-        if ""num_items_in_batch"" not in inputs:
-            inputs[""num_items_in_batch""] = kwargs[""num_items_in_batch""]
+        num_items_in_batch = kwargs[""num_items_in_batch""]
+        if num_items_in_batch is None:
+            # Remove it since the model does not support it!
+            kwargs.pop(""num_items_in_batch"")
+        elif ""num_items_in_batch"" not in inputs:
+            inputs[""num_items_in_batch""] = num_items_in_batch
         pass
     pass
+
+    if num_items_in_batch is None:
+        name = (model.base_model.model if hasattr(model, ""base_model"") else model).__class__.__name__
+        logger.warning_once(
+            f""Unsloth: Not an error, but {name} does not accept `num_items_in_batch`.\n""\
+            ""Using gradient accumulation will be very slightly less accurate.\n""\
+            ""Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient""
+        )
+    pass
     return self._old_compute_loss(model, inputs, *args, **kwargs)
 pass
 
@@ -1104,7 +1138,7 @@ def patch_gradient_accumulation_fix(Trainer):
 
         ""else:\n""\
         ""\2if num_items_in_batch is None:\n""\
-        ""\3loss /= self.args.gradient_accumulation_steps\n""\
+        ""\3loss = loss / self.args.gradient_accumulation_steps\n""\
         ""\1self.accelerator.backward(loss, **kwargs)"",
         
         function,
@@ -1148,6 +1182,7 @@ def unsloth_compile_transformers(
     manual_replacements     = True,
     fast_lora_forwards      = True,
     fast_residual_stream    = True,
+    accurate_accumulation   = True,
     epilogue_fusion         = True,
     max_autotune            = False,
     shape_padding           = True,
@@ -1194,6 +1229,7 @@ def unsloth_compile_transformers(
             manual_replacements    = manual_replacements,
             fast_lora_forwards     = fast_lora_forwards,
             fast_residual_stream   = fast_residual_stream,
+            accurate_accumulation  = accurate_accumulation,
             epilogue_fusion        = epilogue_fusion,
             max_autotune           = max_autotune,
             shape_padding          = shape_padding,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index d1c8b1e..113c4fb 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -472,6 +472,7 @@ class FastVisionModel(FastBaseVisionModel):
                 manual_replacements     = True,
                 fast_lora_forwards      = False,
                 fast_residual_stream    = False,
+                accurate_accumulation   = True,
                 epilogue_fusion         = True,
                 max_autotune            = False,
                 shape_padding           = True,
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 709cd1c..2dc4b88 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -186,6 +186,10 @@ class FastBaseVisionModel:
         patch_saving_functions(model, vision = True)
         patch_saving_functions(tokenizer, vision = True)
 
+        # Fix gradient accumulation
+        from transformers.trainer import Trainer
+        patch_gradient_accumulation_fix(Trainer)
+
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
         tokenizer.tokenizer.padding_side = ""left"" # Force inference
diff --git a/unsloth/save.py b/unsloth/save.py
index 8db3b6d..d3ba192 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2131,7 +2131,8 @@ def unsloth_generic_save(
     if token is None and push_to_hub: token = get_token()
     merge_and_overwrite_lora(
         get_model_name,
-        model,
+        model                = model,
+        tokenizer            = tokenizer,
         save_directory       = save_directory,
         push_to_hub          = push_to_hub,
         private              = private,
"
"diff --git a/README.md b/README.md
index 69c8ddc..47ca0a3 100644
--- a/README.md
+++ b/README.md
@@ -11,17 +11,17 @@
 | [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing) | [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [Kaggle Alpaca example](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) |
 | [Colab A100 example](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | (59 more examples if you scroll down) | [Kaggle Slim Orca example](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |
 
-* Supports Llama, Yi, Mistral, CodeLlama, and their derived models (Open Hermes etc).
+* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).
 * All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language. **Manual backpropagation engine**.
 * **0% loss in accuracy** - no approximation methods - all exact.
 * No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)
 * **NEW!** Works on **Linux** and **Windows** via WSL.
 * **NEW!** Support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290), PPO and Reward Modelling via [TRL](https://huggingface.co/docs/trl/dpo_trainer).
-* **NEW!** Download 4 bit models 4x faster directly from Huggingface!
+* **NEW!** Download 4 bit models 4x faster from Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`.
 * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
 * Open source version trains 5x faster - check out [Unsloth Max](https://unsloth.ai/) for **30x faster training**!
 
-| 1 A100 40GB | Huggingface | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
+| 1 A100 40GB | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |
 |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|
 | Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |
 | LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |
@@ -88,9 +88,16 @@ max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!
 url = ""https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl""
 dataset = load_dataset(""json"", data_files = {""train"" : url}, split = ""train"")
 
+# 4bit pre quantized models we support - 4x faster downloading!
+fourbit_models = [
+    ""unsloth/mistral-7b-bnb-4bit"",
+    ""unsloth/llama-2-7b-bnb-4bit"",
+    ""unsloth/llama-2-13b-bnb-4bit"",
+    ""unsloth/codellama-34b-bnb-4bit"",
+]
 # Load Llama model
 model, tokenizer = FastLanguageModel.from_pretrained(
-    model_name = ""unsloth/llama-2-7b"", # Supports Llama, Mistral - replace this!
+    model_name = ""unsloth/mistral-7b"", # Supports Llama, Mistral - replace this!
     max_seq_length = max_seq_length,
     dtype = None,
     load_in_4bit = True,
@@ -133,7 +140,7 @@ trainer.train()
 ```
 
 # DPO (Direct Preference Optimization) Support
-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory).
+DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on 1x A100 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).
 
 # Future Milestones and limitations
 1. Support Mixtral.
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 28c7aff..879b092 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -46,12 +46,12 @@ except:
     raise ImportError(""Pytorch is not installed. Go to https://pytorch.org/.\n""\
                       ""We have some installation instructions on our Github page."")
 
-# We only support torch 2.1
+# We support torch 2.1 and 2.1.1
 # Fixes https://github.com/unslothai/unsloth/issues/38
 torch_version = torch.__version__.split(""."")
 major_torch, minor_torch = torch_version[0], torch_version[1]
 major_torch, minor_torch = int(major_torch), int(minor_torch)
-if (major_torch != 2) or (major_torch == 2 and minor_torch < 1):
+if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):
     raise ImportError(""Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\n""\
                       ""We have some installation instructions on our Github page."")
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index b3ee2d7..8ef5fcd 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.8"",
+    ""unsloth_zoo>=2025.7.9"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.8"",
+    ""unsloth_zoo>=2025.7.9"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index df331fc..018e89c 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -107,7 +107,7 @@ _cross_entropy_forward = triton.heuristics(
 
 def _chunked_cross_entropy_forward(
     logits_ptr        ,
-    logits_row_stride ,
+    logits_row_stride : tl.constexpr,
     loss_ptr          ,
     logsumexp_ptr     ,
     labels_ptr        ,
@@ -191,9 +191,9 @@ _chunked_cross_entropy_forward = triton.heuristics(
 
 def _cross_entropy_backward(
     logits_ptr        ,
-    logits_row_stride ,
+    logits_row_stride : tl.constexpr,
     dloss_ptr         ,
-    dloss_row_stride  ,
+    dloss_row_stride  : tl.constexpr,
     logsumexp_ptr     ,
     labels_ptr        ,
     VOCAB_SIZE        : tl.constexpr,
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index fba7e56..ec45c60 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -19,9 +19,9 @@ from .utils import calculate_settings, torch_gpu_device
 
 @triton.jit
 def _rms_layernorm_forward(
-    Y, Y_row_stride,
-    X, X_row_stride,
-    W, W_row_stride,
+    Y, Y_row_stride : tl.constexpr,
+    X, X_row_stride : tl.constexpr,
+    W, W_row_stride : tl.constexpr,
     r, r_row_stride : tl.constexpr,
     n_cols     : tl.constexpr,
     eps        : tl.constexpr,
@@ -54,10 +54,10 @@ pass
 
 
 def _rms_layernorm_backward(
-    dY, dY_row_stride,
-    dX, dX_row_stride,
-    X,   X_row_stride,
-    W,   W_row_stride,
+    dY, dY_row_stride : tl.constexpr,
+    dX, dX_row_stride : tl.constexpr,
+    X,   X_row_stride : tl.constexpr,
+    W,   W_row_stride : tl.constexpr,
     r,   r_row_stride : tl.constexpr,
     # dW, dW_row_stride,
     n_cols     : tl.constexpr,
@@ -106,9 +106,9 @@ _rms_layernorm_backward = triton.heuristics(
 
 @triton.jit
 def _gemma_rms_layernorm_forward(
-    Y, Y_row_stride,
-    X, X_row_stride,
-    W, W_row_stride,
+    Y, Y_row_stride : tl.constexpr,
+    X, X_row_stride : tl.constexpr,
+    W, W_row_stride : tl.constexpr,
     r, r_row_stride : tl.constexpr,
     n_cols     : tl.constexpr,
     eps        : tl.constexpr,
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index 1c981b3..a06d81a 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -19,9 +19,9 @@ from .utils import calculate_settings, torch_gpu_device
 ROPE_GROUP_SIZE : int = 4
 
 def _rope_embedding(
-    Q,     Q_row_stride,
-    cos, cos_row_stride,
-    sin, sin_row_stride,
+    Q,     Q_row_stride: tl.constexpr,
+    cos, cos_row_stride: tl.constexpr,
+    sin, sin_row_stride: tl.constexpr,
     seqlen,
     head_dim      : tl.constexpr,
     n_heads       : tl.constexpr,
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 645319d..1470068 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -124,7 +124,7 @@ if DEVICE_TYPE == ""xpu"":
         (index := torch.xpu.device(i).idx) : ctypes.c_void_p(torch._C._xpu_getCurrentRawStream(index))
         for i in range(DEVICE_COUNT)
     }
-    XPU_STREAMS   = [None] * (max(_XPU_STREAMS.keys()) + 1)
+    XPU_STREAMS    = [None] * (max(_XPU_STREAMS.keys()) + 1)
     WEIGHT_BUFFERS = [None] * (max(_XPU_STREAMS.keys()) + 1)
     ABSMAX_BUFFERS = [None] * (max(_XPU_STREAMS.keys()) + 1)
     for k, v in _XPU_STREAMS.items():
@@ -143,7 +143,7 @@ else:
     for k, v in _CUDA_STREAMS.items(): CUDA_STREAMS[k] = v
     CUDA_STREAMS = tuple(CUDA_STREAMS)
     del _CUDA_STREAMS
-
+pass
 
 # Bitsandbytes operations
 ctypes_c_int   = ctypes.c_int
@@ -172,12 +172,16 @@ else:
     cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
     cgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16
     cgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16
+pass
 
 torch_mm = torch.mm
 torch_mv = torch.mv
 torch_matmul = torch.matmul
 torch_addmm  = torch.addmm
 torch_empty  = torch.empty
+torch_float32  = torch.float32
+torch_float16  = torch.float16
+torch_bfloat16 = torch.bfloat16
 
 def QUANT_STATE(W): return getattr(W, ""quant_state"", None)
 
@@ -283,7 +287,7 @@ if DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
             else:
                 assert(out.shape == shape)
                 assert(out.dtype == dtype)
-            out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+            out_absmax = torch_empty(n_elements_absmax, dtype = torch_float32, device = device, requires_grad = False)
         pass
 
         # NF4 dequantization of statistics
@@ -296,7 +300,7 @@ if DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
             out_absmax += offset
 
             # Dequantize W
-            fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+            fx = cdequantize_blockwise_fp16_nf4 if dtype == torch_float16 else \
                  cdequantize_blockwise_bf16_nf4
             fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
                ctypes_c_int(blocksize), ctypes_c_int(out.numel()), XPU_STREAM,)
@@ -346,7 +350,7 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
             ABSMAX_BUFFER = ABSMAX_BUFFERS[device_index]
             if WEIGHT_BUFFER is None:
                 WEIGHT_BUFFERS[device_index] = WEIGHT_BUFFER = torch_empty(size, dtype = dtype, device = device, requires_grad = False)
-                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch_empty(n_elements_absmax, dtype = torch_float32, device = device, requires_grad = False)
 
             if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)
             if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)
@@ -359,7 +363,7 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
             else:
                 assert(out.shape == shape)
                 assert(out.dtype == dtype)
-            out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+            out_absmax = torch_empty(n_elements_absmax, dtype = torch_float32, device = device, requires_grad = False)
         pass
 
         # NF4 dequantization of statistics
@@ -372,7 +376,7 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
             out_absmax += offset
 
             # Dequantize W
-            fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+            fx = cdequantize_blockwise_fp16_nf4 if dtype == torch_float16 else \
                  cdequantize_blockwise_bf16_nf4
             fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
                ctypes_c_int(blocksize), ctypes_c_int(out.numel()), CUDA_STREAM,)
@@ -413,7 +417,7 @@ else:
         else:
             assert(out.shape == shape)
             assert(out.dtype == dtype)
-        out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+        out_absmax = torch_empty(n_elements_absmax, dtype = torch_float32, device = device, requires_grad = False)
 
         # Do dequantization
         ptr_out_absmax = get_ptr(out_absmax)
@@ -423,7 +427,7 @@ else:
         )
         out_absmax += offset
 
-        fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+        fx = cdequantize_blockwise_fp16_nf4 if dtype == torch_float16 else \
              cdequantize_blockwise_bf16_nf4
         fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
            ctypes_c_int(blocksize), ctypes_c_int(out.numel()),)
@@ -488,7 +492,7 @@ if  DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch_float32, device = device)
         with torch_gpu_device(device):
             cdequantize_blockwise_fp32(
                 get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
@@ -497,7 +501,7 @@ if  DEVICE_TYPE == ""xpu"" and HAS_XPU_STREAM:
             df += offset
             absmax = df
 
-            fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
+            fx = cgemm_4bit_inference_naive_fp16 if dtype == torch_float16 else \
                 cgemm_4bit_inference_naive_bf16
 
             blocksize = ctypes_c_int32(blocksize)
@@ -559,7 +563,7 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch_float32, device = device)
         with torch_gpu_device(device):
             cdequantize_blockwise_fp32(
                 get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
@@ -568,8 +572,8 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
             df += offset
             absmax = df
 
-            fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
-                cgemm_4bit_inference_naive_bf16
+            fx = cgemm_4bit_inference_naive_fp16 if dtype == torch_float16 else \
+                 cgemm_4bit_inference_naive_bf16
 
             blocksize = ctypes_c_int32(blocksize)
             fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
@@ -580,7 +584,7 @@ elif DEVICE_TYPE == ""cuda"" and HAS_CUDA_STREAM:
     pass
 else:
     def fast_gemv(X, W, quant_state, out = None):
-        if quant_state is None: return torch.matmul(X, W, out = out)
+        if quant_state is None: return torch_matmul(X, W, out = out)
         # For fast X @ W where seq_len == 1
         # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
         _, q_len, hd = X.shape
@@ -626,7 +630,7 @@ else:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch_float32, device = device)
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
             ctypes_c_int(blocksize2), ctypes_c_int(df.numel()),
@@ -634,8 +638,8 @@ else:
         df += offset
         absmax = df
 
-        fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
-            cgemm_4bit_inference_naive_bf16
+        fx = cgemm_4bit_inference_naive_fp16 if dtype == torch_float16 else \
+             cgemm_4bit_inference_naive_bf16
 
         blocksize = ctypes_c_int32(blocksize)
         fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c445e98..bce6bb6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.7""
+__version__ = ""2025.7.8""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -431,7 +431,7 @@ if DEVICE_TYPE == ""cuda"":
                         ""Unsloth: If you want to finetune Gemma 2, upgrade flash-attn to version 2.6.3 or higher!\n""\
                         ""Newer versions support faster and less memory usage kernels for Gemma 2's attention softcapping!\n""\
                         ""To update flash-attn, do the below:\n""\
-                        '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
+                        '\npip install --no-deps --no-build-isolation --upgrade ""flash-attn>=2.6.3""'
                     )
             except:
                 print(
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 5597995..0f19245 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -59,18 +59,6 @@ pass
 if HAS_FLASH_ATTENTION_SOFTCAPPING:
     from flash_attn import flash_attn_func
 
-# [TODO] We must randomnly use torch.compile?
-# Gemma 2 uses double RMS Layernorms, so the backward passes should not overwrite the gradients!
-@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)
-def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
-    old_dtype = X.dtype
-    X = X.float()
-    X = X * torch.rsqrt(X.square().mean(-1, keepdim = True) + layernorm.eps) * \
-        (1.0 + layernorm.weight.float())
-    return X.to(old_dtype)
-pass
-
-
 # Logit softcapping
 def Gemma2Attention_fast_forward(
     self,
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 5bbf4c7..2436db4 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -85,6 +85,12 @@ from unsloth_zoo.vllm_utils import (
     return_lora_modules,
 )
 
+try:
+    torch_compiler_set_stance = torch.compiler.set_stance
+except:
+    torch_compiler_set_stance = None
+pass
+
 def unsloth_base_fast_generate(
     self,
     *args,
@@ -756,7 +762,8 @@ class FastBaseModel:
         # Must enable returning logits
         os.environ[""UNSLOTH_RETURN_LOGITS""] = ""1""
         # Turn off skip guards and set stance to default
-        torch.compiler.set_stance(stance = ""default"", skip_guard_eval_unsafe = False)
+        if torch_compiler_set_stance is not None:
+            torch_compiler_set_stance(stance = ""default"", skip_guard_eval_unsafe = False)
         return model
     pass
 
@@ -804,7 +811,8 @@ class FastBaseModel:
         # Can re-enable not returning logits
         os.environ[""UNSLOTH_RETURN_LOGITS""] = ""0""
         # Turn off skip guards and set stance to default
-        torch.compiler.set_stance(stance = ""default"", skip_guard_eval_unsafe = False)
+        if torch_compiler_set_stance is not None:
+            torch_compiler_set_stance(stance = ""default"", skip_guard_eval_unsafe = False)
         return model
     pass
 pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0c00574..d8904aa 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -595,7 +595,6 @@ def _get_statistics(statistics = None, force_download = True):
     # You can disable this by commenting the below out
     try:
         n_cpus = psutil.cpu_count(logical = False)
-
         keynames = ""\n"" + ""\n"".join(os.environ.keys())
         if statistics is not None: pass
         elif ""\nCOLAB_""  in keynames and n_cpus == 1: statistics = ""colab""
@@ -604,10 +603,31 @@ def _get_statistics(statistics = None, force_download = True):
         elif ""\nRUNPOD_"" in keynames: statistics = ""runpod""
         elif ""\nAWS_""    in keynames: statistics = ""aws""
         elif ""\nAZURE_""  in keynames: statistics = ""azure""
-        elif ""\nK_"" in keynames or ""\nFUNCTION_"" in keynames: statistics = ""gcp""
+        # elif ""\nK_"" in keynames or ""\nFUNCTION_"" in keynames: statistics = ""gcp""
         elif ""\nINVOCATION_ID"" in keynames: statistics = ""lambda""
-        else: statistics = ""other""
-
+        # else: statistics = ""other""
+        else:
+            def try_vllm_check():
+                vendor_files = (
+                    ""/sys/class/dmi/id/product_version"",
+                    ""/sys/class/dmi/id/bios_vendor"",
+                    ""/sys/class/dmi/id/product_name"",
+                    ""/sys/class/dmi/id/chassis_asset_tag"",
+                    ""/sys/class/dmi/id/sys_vendor"",
+                )
+                from pathlib import Path
+                for vendor_file in vendor_files:
+                    path = Path(vendor_file)
+                    if path.is_file():
+                        file_content = path.read_text().lower()
+                        if   ""amazon""                in file_content: return ""aws""
+                        elif ""microsoft corporation"" in file_content: return ""azure""
+                        elif ""google""                in file_content: return ""gcp""
+                return ""other""
+            pass
+            try:    statistics = try_vllm_check()
+            except: statistics = ""other""
+        pass
         if statistics is not None:
             from transformers import AutoModelForCausalLM
             stats_model = AutoModelForCausalLM.from_pretrained(
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6a23335..048ba69 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2234,6 +2234,9 @@ class FastLlamaModel:
             internal_model.gradient_checkpointing = False
             internal_model.training = False
         pass
+        if hasattr(internal_model, ""training""):
+            internal_model.training = False
+        pass
 
         # Also check if lm_head / embeddings are trained
         internal_model = model
@@ -2267,6 +2270,16 @@ class FastLlamaModel:
             internal_model._saved_temp_tokenizer.padding_side = ""left""
         pass
 
+        # Also disable training for embeddings for NEFTune
+        if hasattr(model, ""get_input_embeddings""):
+            embeddings = model.get_input_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = False
+        pass
+        if hasattr(model, ""get_output_embeddings""):
+            embeddings = model.get_output_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = False
+        pass
+
         return model
     pass
 
@@ -2288,6 +2301,9 @@ class FastLlamaModel:
             internal_model.gradient_checkpointing = use_gradient_checkpointing
             internal_model.training = True
         pass
+        if hasattr(internal_model, ""training""):
+            internal_model.training = True
+        pass
 
         # Also revert model.generate
         if hasattr(model, ""_unwrapped_old_generate""):
@@ -2307,6 +2323,16 @@ class FastLlamaModel:
             internal_model._saved_temp_tokenizer.padding_side = ""right""
         pass
 
+        # Also re-enable training for embeddings for NEFTune
+        if hasattr(model, ""get_input_embeddings""):
+            embeddings = model.get_input_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = True
+        pass
+        if hasattr(model, ""get_output_embeddings""):
+            embeddings = model.get_output_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = True
+        pass
+
         return model
     pass
 pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 7316656..b677f86 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -1109,6 +1109,7 @@ from inspect import getsource
 import trl.trainer.sft_trainer
 from trl.trainer.sft_trainer import *
 from transformers.trainer import *
+from trl.trainer.sft_trainer import neftune_post_forward_hook
 
 def patch_sft_trainer_tokenizer():
     """"""
@@ -1173,6 +1174,19 @@ def patch_sft_trainer_tokenizer():
     ""\n""\
     ""fix_untrained_tokens(self.model, self.tokenizer, self.train_dataset, eps = 1e-16)\n\n""
 
+    # Add NEFTune since it doesn't seem to work?? We need to manually inject it
+    check_text += \
+    ""\n""\
+    ""if hasattr(self, 'neftune_hook_handle'):\n""\
+    ""    self.neftune_hook_handle.remove()\n""\
+    ""    if hasattr(self, 'neftune_hook_handle'): del self.neftune_hook_handle\n""\
+    ""\n""\
+    ""if getattr(self, 'neftune_noise_alpha', None) is not None:\n""\
+    ""    self.model.get_input_embeddings().neftune_noise_alpha = self.neftune_noise_alpha\n""\
+    ""    self.neftune_hook_handle = self.model.get_input_embeddings().register_forward_hook(neftune_post_forward_hook)\n""\
+    ""pass\n""\
+    ""\n""
+
     check_text = check_text.split(""\n"")
     check_text = ""\n"".join("" ""*where + x for x in check_text)
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 5b9dc8b..667901e 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.8"",
+    ""unsloth_zoo>=2025.3.9"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.8"",
+    ""unsloth_zoo>=2025.3.9"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 5bbb85d..9bcdd5c 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,14 +198,19 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.8""):
-        try:
-            os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
-        except:
+    if Version(unsloth_zoo_version) < Version(""2025.3.9""):
+        print(
+            ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
+            ""To disable this, set os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'""
+        )
+        if os.environ.get(""UNSLOTH_DISABLE_AUTO_UPDATES"", ""0"") == ""0"":
             try:
-                os.system(""pip install --upgrade --no-cache-dir --no-deps --user unsloth_zoo"")
+                os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
             except:
-                raise ImportError(""Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`"")
+                try:
+                    os.system(""pip install --upgrade --no-cache-dir --no-deps --user unsloth_zoo"")
+                except:
+                    raise ImportError(""Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`"")
     import unsloth_zoo
 except:
     raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`"")
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 03eb21f..c79d702 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.9""
+__version__ = ""2025.3.10""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -25,6 +25,7 @@ __all__ = [
     ""__version__"",
     ""HAS_FLASH_ATTENTION"",
     ""HAS_FLASH_ATTENTION_SOFTCAPPING"",
+    ""USE_MODELSCOPE"",
     ""platform_system"",
     ""patch_tokenizer"",
     ""get_statistics"",
@@ -100,6 +101,7 @@ from unsloth_zoo.gradient_checkpointing import (
 from unsloth_zoo.loss_utils import (
     HAS_CUT_CROSS_ENTROPY,
     fused_linear_cross_entropy,
+    _unsloth_get_batch_samples,
 )
 from unsloth_zoo.vision_utils import (
     process_vision_info,
@@ -108,6 +110,9 @@ from unsloth_zoo.compiler import (
     get_transformers_model_type,
     unsloth_compile_transformers as _unsloth_compile_transformers,
 )
+from unsloth_zoo.training_utils import (
+    prepare_model_for_training,
+)
 
 # =============================================
 # Disable some warnings which can get annoying
@@ -508,67 +513,16 @@ def prepare_model_for_kbit_training(
     use_gradient_checkpointing : Optional = True,
     use_reentrant              : Optional[bool] = True,
 ) -> Any:
-    """"""
-    Calculates where to place the gradient checkpoints given n_layers.
-    We also freeze all other layers's gradients
-
-    Args:
-        model: Any LlamaModel with layers.
-        use_gradient_checkpointing (`bool`, *optional*):
-            Default enabled. Provides memory savings by not saving all activations,
-            but only some.
-        use_reentrant (`bool`, *optional*):
-            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354
-            Optimal gradient checkpointing algorithm which will be the default in
-            future Pytorch versions.
-    """"""
-
-    # Freeze all parameters except LoRA
-    with torch.no_grad():
-        for name, param in model.named_parameters():
-            if "".lora_A."" in name or "".lora_B."" in name or "".lora_magnitude_vector"" in name:
-                param.requires_grad_(True)
-                # Also must be in float32!
-                if param.dtype != torch.float32:
-                    name = name.replace(""base_model"", ""model"", 1)
-                    layer_number = re.search(r""\.[\d]{1,}\."", name).group(0)
-                    name = name.replace(layer_number, f""[{layer_number[1:-1]}]."")
-                    name = name.replace("".weight"", """", 1)
-                    exec(f""{name}.to(torch.float32)"")
-                pass
-            else:
-                param.requires_grad_(False)
-        pass
-    pass
-
-    # Gradient checkpointing!
-    if use_gradient_checkpointing == ""unsloth"":
-
-        # Saves VRAM!
-        original_model = model
-        while hasattr(original_model, ""model""):
-            original_model._offloaded_gradient_checkpointing = True
-            original_model = original_model.model
-        pass
-        original_model._offloaded_gradient_checkpointing = True
-        
-        model.gradient_checkpointing_enable()
-
-    elif use_gradient_checkpointing == True:
-        model.gradient_checkpointing_enable()
-    pass
-
-    # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.
-    if use_reentrant:
-        if hasattr(model, ""enable_input_require_grads""):
-            model.enable_input_require_grads()
-        else:
-            def make_inputs_require_grad(module, input, output):
-                output.requires_grad_(True)
-            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
-    pass
-
-    return model
+    return prepare_model_for_training(
+        model                      = model,
+        use_gradient_checkpointing = use_gradient_checkpointing,
+        use_reentrant              = use_reentrant,
+        full_finetuning            = False,
+        train_layernorms           = False,
+        train_embedding            = False,
+        train_lm_head              = False,
+        float32_mixed_precision    = True,
+    )
 pass
 
 # =============================================
@@ -999,44 +953,6 @@ def test_mask_creation():
 pass
 
 
-def _unsloth_get_batch_samples(self, epoch_iterator, num_batches):
-    batch_samples = []
-    num_items_in_batch = None
-
-    # Check if model allows **kwargs
-    model = self.model
-    f = model.base_model.model.forward if hasattr(model, ""base_model"") else model.forward
-    has_kwargs = tuple(inspect.signature(f).parameters.values())[-1].kind == inspect._VAR_KEYWORD
-
-    # Iterate to find all batches
-    for _ in range(num_batches):
-        try:
-            batch_samples += [next(epoch_iterator)]
-        except StopIteration:
-            break
-    pass
-
-    # Get num_items_in_batch
-    if has_kwargs and len(batch_samples) > 0 and ""labels"" in batch_samples[0]:
-        try:
-            num_items_in_batch = sum(
-                [(x[""labels""][..., 1:] != -100).sum() for x in batch_samples]
-            )
-            
-            if self.args.average_tokens_across_devices:
-                num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()
-
-            if torch.is_tensor(num_items_in_batch):
-                num_items_in_batch = num_items_in_batch.item()
-
-        except Exception as exception:
-            logger.warning_once(exception)
-    pass
-
-    return batch_samples, num_items_in_batch
-pass
-
-
 def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
     num_items_in_batch = None
 
@@ -1053,7 +969,12 @@ def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
     # Get gradient accumulation steps if possible
     if num_items_in_batch is None and \
         getattr(getattr(self, ""args"", self), ""gradient_accumulation_steps"", 1) != 1:
-        name = (model.base_model.model if hasattr(model, ""base_model"") else model).__class__.__name__
+
+        inner_model = model
+        if hasattr(inner_model, ""base_model""): inner_model = inner_model. base_model
+        if hasattr(inner_model, ""model""): inner_model = inner_model.model
+        name = inner_model.__class__.__name__
+
         logger.warning_once(
             f""Unsloth: Not an error, but {name} does not accept `num_items_in_batch`.\n""\
             ""Using gradient accumulation will be very slightly less accurate.\n""\
@@ -1271,3 +1192,10 @@ for j, function in enumerate(functions):
         try: exec(f""EMPTY_LOGITS.{function} = raise_{j}"", globals(), locals())
         except: continue
 pass
+
+USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
+if USE_MODELSCOPE:
+    if importlib.util.find_spec(""modelscope"") is None:
+        raise ImportError(f'You are using the modelscope hub, please install modelscope by `pip install modelscope -U`')
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3504037..7ae6e92 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1913,12 +1913,12 @@ class FastLlamaModel:
 
         # Save max_seq_length
         model.max_seq_length = max_seq_length
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model.max_seq_length = max_seq_length
-            internal_model = internal_model.model
+        m = model
+        while hasattr(m, ""model""):
+            m.max_seq_length = max_seq_length
+            m = m.model
         pass
-        internal_model.max_seq_length = max_seq_length
+        m.max_seq_length = max_seq_length
 
         # We check the tokenizer first for errors
         if fix_tokenizer:
@@ -2016,6 +2016,10 @@ class FastLlamaModel:
         temporary_location  = ""_unsloth_temporary_saved_buffers"",
         **kwargs,
     ):
+        if os.environ.get(""UNSLOTH_ENABLE_FULL_FINETUNING"", ""0"") == ""1"":
+            print(""Unsloth: Full finetuning is enabled, so .get_peft_model has no effect"")
+            return model
+        pass
         transformers_set_seed(random_state)
 
         if use_gradient_checkpointing == ""unsloth"":
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 800c016..92a166f 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -12,7 +12,12 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from ._utils import is_bfloat16_supported, HAS_FLASH_ATTENTION, HAS_FLASH_ATTENTION_SOFTCAPPING
+from ._utils import (
+    is_bfloat16_supported,
+    HAS_FLASH_ATTENTION,
+    HAS_FLASH_ATTENTION_SOFTCAPPING,
+    USE_MODELSCOPE,
+)
 from .granite import FastGraniteModel
 from .llama   import FastLlamaModel, logger
 from .mistral import FastMistralModel
@@ -36,14 +41,6 @@ pass
 from huggingface_hub import HfFileSystem
 import importlib.util
 
-# [TODO] Move USE_MODELSCOPE to utils
-USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
-if USE_MODELSCOPE:
-    if importlib.util.find_spec(""modelscope"") is None:
-        raise ImportError(f'You are using the modelscope hub, please install modelscope by `pip install modelscope -U`')
-    pass
-pass
-
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
 from unsloth_zoo.utils import Version, _get_dtype
 transformers_version = Version(transformers_version)
@@ -76,6 +73,8 @@ class FastLanguageModel(FastLlamaModel):
         max_seq_length             = None,
         dtype                      = None,
         load_in_4bit               = True,
+        load_in_8bit               = False,
+        full_finetuning            = False,
         token                      = None,
         device_map                 = ""sequential"",
         rope_scaling               = None,
@@ -94,6 +93,29 @@ class FastLanguageModel(FastLlamaModel):
         disable_log_stats          = True,
         *args, **kwargs,
     ):
+        if load_in_8bit or full_finetuning:
+            return FastModel.from_pretrained(
+                model_name                 = model_name,
+                max_seq_length             = max_seq_length, # [TODO] No effect
+                dtype                      = dtype,
+                load_in_4bit               = load_in_4bit,
+                load_in_8bit               = load_in_8bit,
+                full_finetuning            = full_finetuning,
+                token                      = token,
+                device_map                 = device_map,
+                rope_scaling               = rope_scaling, # [TODO] No effect
+                fix_tokenizer              = fix_tokenizer, # [TODO] No effect
+                trust_remote_code          = trust_remote_code,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+                resize_model_vocab         = resize_model_vocab, # [TODO] No effect
+                revision                   = revision,
+                return_logits              = False, # Return logits
+                fullgraph                  = True, # No graph breaks
+                use_exact_model_name       = use_exact_model_name,
+                *args, **kwargs,
+            )
+        pass
+
         if token is None: token = get_token()
         assert (dtype is None or dtype == torch.float16 or dtype == torch.bfloat16)
 
@@ -153,7 +175,7 @@ class FastLanguageModel(FastLlamaModel):
 
         # Old transformers versions check
         both_exist = (is_model and is_peft) and not SUPPORTS_LLAMA32
-        
+
         # New transformers need to check manually.
         if SUPPORTS_LLAMA32:
             # Check if folder exists locally
@@ -202,7 +224,6 @@ class FastLanguageModel(FastLlamaModel):
             model_config = AutoConfig.from_pretrained(
                 model_name,
                 token = token,
-                revision = revision,
                 trust_remote_code = trust_remote_code,
             )
         pass
@@ -265,15 +286,32 @@ class FastLanguageModel(FastLlamaModel):
             dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
-        elif model_type == ""cohere"":
-            dispatch_model = FastCohereModel
-        elif model_type == ""granite"":
-            dispatch_model = FastGraniteModel
+        # Temporary disable optimized Cohere until errors match
+        # elif model_type == ""cohere"":
+        #     dispatch_model = FastCohereModel
+        # Temporary disable optimized Granite until errors match
+        # elif model_type == ""granite"":
+        #     dispatch_model = FastGraniteModel
         else:
-            raise NotImplementedError(
-                f""Unsloth: {model_name} not supported yet!\n""\
-                ""Maybe you're doing vision finetuning? Please use FastVisionModel instead!\n""\
-                ""Otherwise, make an issue to https://github.com/unslothai/unsloth!"",
+            return FastModel.from_pretrained(
+                model_name                 = model_name,
+                max_seq_length             = max_seq_length, # [TODO] No effect
+                dtype                      = dtype,
+                load_in_4bit               = load_in_4bit,
+                load_in_8bit               = load_in_8bit,
+                full_finetuning            = full_finetuning,
+                token                      = token,
+                device_map                 = device_map,
+                rope_scaling               = rope_scaling, # [TODO] No effect
+                fix_tokenizer              = fix_tokenizer, # [TODO] No effect
+                trust_remote_code          = trust_remote_code,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+                resize_model_vocab         = resize_model_vocab, # [TODO] No effect
+                revision                   = revision,
+                return_logits              = False, # Return logits
+                fullgraph                  = True, # No graph breaks
+                use_exact_model_name       = use_exact_model_name,
+                *args, **kwargs,
             )
         pass
 
@@ -288,6 +326,11 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         if fast_inference:
+            import platform
+            if platform.system().lower() == 'windows':
+                print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
+                fast_inference = False
+            pass
             from unsloth_zoo.vllm_utils import (
                 patch_vllm, 
                 vllm_dynamic_quant_supported,
@@ -385,9 +428,15 @@ from ..kernels import (
 )
 from .vision import FastBaseModel
 from transformers import (
-    AutoModelForVision2Seq,
     AutoModelForCausalLM,
 )
+try:
+    from transformers import AutoModelForImageTextToText
+    AutoModelForVision2Seq = AutoModelForImageTextToText
+except:
+    from transformers import AutoModelForVision2Seq
+pass
+
 
 class FastModel(FastBaseModel):
     @staticmethod
@@ -396,6 +445,8 @@ class FastModel(FastBaseModel):
         max_seq_length             = None, # [TODO] No effect
         dtype                      = None,
         load_in_4bit               = True,
+        load_in_8bit               = False,
+        full_finetuning            = False,
         token                      = None,
         device_map                 = ""sequential"",
         rope_scaling               = None, # [TODO] No effect
@@ -417,10 +468,40 @@ class FastModel(FastBaseModel):
         if use_gradient_checkpointing == ""unsloth"":
             patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
 
+        if full_finetuning and (load_in_4bit or load_in_8bit):
+            print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
+            load_in_4bit = False
+            load_in_8bit = False
+        pass
+
+        if load_in_4bit and load_in_8bit:
+            raise RuntimeError(
+                ""Unsloth: Can only load in 4bit or 8bit, not both!\n""\
+                ""Also, we by default set `load_in_4bit = True`.\n""\
+                ""If you want 8bit finetuning, set both `load_in_4bit = False` and `load_in_8bit = True`""
+            )
+        if load_in_4bit: pass
+        elif load_in_8bit: pass
+        elif not load_in_4bit and not load_in_8bit and not full_finetuning:
+            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
+            load_in_4bit = True
+        pass
+
         old_model_name = model_name
         if not use_exact_model_name:
             model_name = get_model_name(model_name, load_in_4bit)
 
+        # Check versions
+        LATEST  = '\nPlease use transformers via `pip install --no-deps git+https://github.com/huggingface/transformers.git`'
+        NIGHTLY = '\nPlease use nightly transformers via pip install --upgrade ""transformers>=4.49.0""`'
+        if ""pixtral"" in model_name.lower() and transformers_version < Version(""4.49.0""):
+            raise RuntimeError(""Unsloth: Pixtral only works on transformers >= 4.49.0."" + LATEST)
+        elif ""qwen2.5"" in model_name.lower() and transformers_version < Version(""4.49.0""):
+            raise RuntimeError(""Unsloth: Qwen 2.5 only works on transformers >= 4.49.0."" + LATEST)
+        elif ""aya-vision"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+            raise RuntimeError(""Unsloth: Aya Vision only works on transformers >= 4.50.0."" + NIGHTLY)
+        pass
+
         if USE_MODELSCOPE and not os.path.exists(model_name):
             from modelscope import snapshot_download
             model_name = snapshot_download(model_name)
@@ -510,7 +591,6 @@ class FastModel(FastBaseModel):
             model_config = AutoConfig.from_pretrained(
                 model_name,
                 token = token,
-                revision = revision,
                 trust_remote_code = trust_remote_code,
             )
         pass
@@ -565,7 +645,7 @@ class FastModel(FastBaseModel):
         pass
 
         # Check if VLM
-        is_vlm = (x.endswith(""ForConditionalGeneration"") for x in model_config.architectures)
+        is_vlm = any(x.endswith(""ForConditionalGeneration"") for x in model_config.architectures)
         is_vlm = is_vlm or hasattr(model_config, ""vision_config"")
         auto_model = AutoModelForVision2Seq if is_vlm else AutoModelForCausalLM
 
@@ -574,6 +654,8 @@ class FastModel(FastBaseModel):
             max_seq_length    = max_seq_length,
             dtype             = _get_dtype(dtype),
             load_in_4bit      = load_in_4bit,
+            load_in_8bit      = load_in_8bit,
+            full_finetuning   = full_finetuning,
             token             = token,
             device_map        = device_map,
             trust_remote_code = trust_remote_code,
@@ -581,6 +663,7 @@ class FastModel(FastBaseModel):
             model_types       = model_types,
             tokenizer_name    = tokenizer_name,
             auto_model        = auto_model,
+            use_gradient_checkpointing = use_gradient_checkpointing,
             *args, **kwargs,
         )
         
@@ -628,7 +711,7 @@ class FastModel(FastBaseModel):
                 trust_remote_code = trust_remote_code,
             )
             # Patch it as well!
-            model = FastBaseModel.patch_peft_model(model, use_gradient_checkpointing)
+            model = FastBaseModel.post_patch_model(model, use_gradient_checkpointing)
         pass
         return model, tokenizer
     pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index a2e609f..47dbb32 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -492,6 +492,18 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Qwen2-VL-72B-Instruct"",
         ""Qwen/Qwen2-VL-72B-Instruct"",
     ),
+    ""unsloth/Qwen2-VL-2B-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-2B"",
+        ""Qwen/Qwen2-VL-2B"",
+    ),
+    ""unsloth/Qwen2-VL-7B-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-7B"",
+        ""Qwen/Qwen2-VL-7B"",
+    ),
+    ""unsloth/Qwen2-VL-72B-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-72B"",
+        ""Qwen/Qwen2-VL-72B"",
+    ),
     ""unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-11B-Vision-Instruct"",
         ""meta-llama/Llama-3.2-11B-Vision-Instruct"",
@@ -626,6 +638,38 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/QwQ-32B"",
         ""unsloth/QwQ-32B-bnb-4bit"",
     ),
+    ""unsloth/gemma-3-1b-it"" : (
+        ""unsloth/gemma-3-1b-it"",
+        ""google/gemma-3-1b-it"",
+    ),
+    ""unsloth/gemma-3-4b-it"" : (
+        ""unsloth/gemma-3-4b-it"",
+        ""google/gemma-3-4b-it"",
+    ),
+    ""unsloth/gemma-3-12b-it"" : (
+        ""unsloth/gemma-3-12b-it"",
+        ""google/gemma-3-12b-it"",
+    ),
+    ""unsloth/gemma-3-27b-it"" : (
+        ""unsloth/gemma-3-27b-it"",
+        ""google/gemma-3-27b-it"",
+    ),
+    ""unsloth/gemma-3-1b-pt"" : (
+        ""unsloth/gemma-3-1b-pt"",
+        ""google/gemma-3-1b-pt"",
+    ),
+    ""unsloth/gemma-3-4b-pt"" : (
+        ""unsloth/gemma-3-4b-pt"",
+        ""google/gemma-3-4b-pt"",
+    ),
+    ""unsloth/gemma-3-12b-pt"" : (
+        ""unsloth/gemma-3-12b-pt"",
+        ""google/gemma-3-12b-pt"",
+    ),
+    ""unsloth/gemma-3-27b-pt"" : (
+        ""unsloth/gemma-3-27b-pt"",
+        ""google/gemma-3-27b-pt"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index cf9c165..86a174e 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -106,6 +106,8 @@ import torch
 import numpy as np
 from contextlib import nullcontext
 from torch.nn import functional as F
+from transformers import DataCollatorForSeq2Seq, DataCollatorForLanguageModeling
+
 torch_compile_options = {{
     ""epilogue_fusion""   : True,
     ""max_autotune""      : False,
@@ -234,6 +236,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         mixed_precision = \
         ""use_bf16 = getattr(args, 'bf16', False)\n""\
         ""use_fp16 = getattr(args, 'fp16', False)\n""\
+        ""mixed_precision_dtype = os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32')\n""\
         ""dtype = getattr(model.config, 'torch_dtype', None)\n""\
         ""if dtype is None: dtype = model.get_input_embeddings().dtype\n""\
         ""from unsloth_zoo.utils import _get_dtype\n""\
@@ -241,10 +244,14 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""float16 = dtype == torch.float16\n""\
         ""if float16 and use_bf16: raise TypeError('Unsloth: Model is in float16 precision but you want to use bfloat16 precision. Set fp16 to `True` and bf16 to `False`')\n""\
         ""if not float16 and use_fp16: raise TypeError('Unsloth: Model is in bfloat16 precision but you want to use float16 precision. Set fp16 to `False` and bf16 to `True`')\n""\
-        ""if not use_bf16 and not use_fp16:\n""\
+        ""if (not use_bf16 and not use_fp16) and mixed_precision_dtype == 'float32':\n""\
         ""    args.fp16 = float16\n""\
         ""    args.bf16 = not float16\n""\
         ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'fp16' if float16 else 'bf16'\n""
+        ""elif mixed_precision_dtype == 'bfloat16':\n""\
+        ""    args.fp16 = False\n""\
+        ""    args.bf16 = False\n""\
+        ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'no'\n""
         extra_args += mixed_precision
     pass
 
@@ -280,7 +287,12 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""bf16_full_eval = getattr(args, 'bf16_full_eval', False)\n""\
         ""if args.fp16 and bf16_full_eval: args.bf16_full_eval = False; args.fp16_full_eval = True\n""\
         ""if args.bf16 and fp16_full_eval: args.bf16_full_eval = True; args.fp16_full_eval = False\n""\
-        ""if not bf16_full_eval and not fp16_full_eval: args.bf16_full_eval = args.bf16; args.fp16_full_eval = args.fp16\n""
+        ""if os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32') == 'bfloat16':\n""\
+        ""    args.bf16_full_eval = True\n""\
+        ""    args.fp16_full_eval = False\n""\
+        ""elif not bf16_full_eval and not fp16_full_eval:\n""\
+        ""    args.bf16_full_eval = args.bf16\n""\
+        ""    args.fp16_full_eval = args.fp16\n""
         extra_args += eval_changes
     pass
 
@@ -327,6 +339,20 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         extra_args += training_check
     pass
 
+    # Check data collator if it's correct!
+    if ""data_collator"" in call_args and ""train_dataset"" in call_args:
+        data_collator_check = \
+        ""if isinstance(data_collator, DataCollatorForSeq2Seq) and 'labels' not in train_dataset.column_names:\n""\
+        ""    print('Unsloth: Changing data collator to `DataCollatorForLanguageModeling` since `labels` not found.')\n""\
+        ""    data_collator = DataCollatorForLanguageModeling(""\
+        ""tokenizer = processing_class if 'processing_class' in locals() else tokenizer, mlm = False)\n""\
+        ""elif isinstance(data_collator, DataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
+        ""    print('Unsloth: Changing data collator to `DataCollatorForSeq2Seq` since `labels` found.')\n""\
+        ""    data_collator = DataCollatorForSeq2Seq(""\
+        ""tokenizer = processing_class if 'processing_class' in locals() else tokenizer)\n""
+        extra_args += data_collator_check
+    pass
+
     # Check NEFTune
     if ""model"" in call_args:
         neftune_check = \
@@ -536,7 +562,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
     if ""args.use_vllm"" in init and ""model"" in init and ""args"" in init:
         # .*? matches first match. .+? matches final match.
         replacer = re.findall(
-            ""def __init__\(.*?\).*?\:\n"",
+            r""def __init__\(.*?\).*?\:\n"",
             init,
             flags = re.MULTILINE | re.DOTALL,
         )
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index ff07ef6..fa5547e 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -15,17 +15,22 @@
 import torch
 from transformers import (
     BitsAndBytesConfig,
-    AutoModelForVision2Seq,
     AutoProcessor,
     AutoTokenizer,
     AutoModelForCausalLM,
 )
+try:
+    from transformers import AutoModelForImageTextToText
+    AutoModelForVision2Seq = AutoModelForImageTextToText
+except:
+    from transformers import AutoModelForVision2Seq
+pass
 from .llama import *
 from ..kernels import (
     post_patch_loss_function,
 )
 from ._utils import __version__
-from peft import LoraConfig, TaskType, get_peft_model
+from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
 from transformers import set_seed as transformers_set_seed
 from unsloth_zoo.peft_utils import (
     get_peft_regex,
@@ -35,6 +40,7 @@ from unsloth_zoo.peft_utils import (
 from triton import __version__ as triton_version
 from unsloth_zoo.utils import _get_dtype
 from unsloth_zoo.patching_utils import patch_model_and_tokenizer
+from unsloth_zoo.training_utils import prepare_model_for_training
 import types
 import functools
 
@@ -52,14 +58,21 @@ def unsloth_base_fast_generate(
     dtype = _get_dtype(self.config.torch_dtype)
 
     # Check if VLM
-    is_vlm = (x.endswith(""ForConditionalGeneration"") for x in self.config.architectures)
+    is_vlm = (
+        x.endswith((""ForConditionalGeneration"", ""ForVisionText2Text""))
+        for x in self.config.architectures
+    )
     is_vlm = is_vlm or hasattr(self.config, ""vision_config"")
 
     # Remove token_type_ids
     kwargs.pop(""token_type_ids"", None)
 
     # VLMs do not allow logits_to_keep
-    if not is_vlm: kwargs[""logits_to_keep""] = 1
+    if not is_vlm:
+        kwargs[""logits_to_keep""] = 1
+    else:
+        kwargs.pop(""logits_to_keep"", None)
+        kwargs.pop(""num_logits_to_keep"", None)
 
     # Check pad_token
     model_eos_token_id = getattr(self.config, ""eos_token_id"", None)
@@ -90,12 +103,15 @@ class FastBaseModel:
         max_seq_length    = None,
         dtype             = None,
         load_in_4bit      = True,
+        load_in_8bit      = False,
+        full_finetuning   = False,
         token             = None,
         device_map        = ""sequential"",
         trust_remote_code = False,
         model_types       = None,
         tokenizer_name    = None,
         auto_model        = AutoModelForVision2Seq,
+        use_gradient_checkpointing = ""unsloth"",
         **kwargs,
     ):
         if trust_remote_code:
@@ -141,6 +157,14 @@ class FastBaseModel:
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
         bnb_config = None
+        if full_finetuning and (load_in_4bit or load_in_8bit):
+            print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
+            load_in_4bit = False
+            load_in_8bit = False
+        pass
+
+        if load_in_4bit and load_in_8bit:
+            raise RuntimeError(""Unsloth: Can only load in 4bit or 8bit, not both!"")
         if load_in_4bit:
             bnb_config = BitsAndBytesConfig(
                 load_in_4bit              = True,
@@ -149,6 +173,31 @@ class FastBaseModel:
                 bnb_4bit_compute_dtype    = dtype,
                 llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
             )
+        elif load_in_8bit:
+            bnb_config = BitsAndBytesConfig(
+                load_in_8bit              = True,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+            )
+        elif not load_in_4bit and not load_in_8bit and not full_finetuning:
+            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
+            load_in_4bit = True
+            bnb_config = BitsAndBytesConfig(
+                load_in_4bit              = True,
+                bnb_4bit_use_double_quant = True,
+                bnb_4bit_quant_type       = ""nf4"",
+                bnb_4bit_compute_dtype    = dtype,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+            )
+        pass
+
+        if full_finetuning:
+            os.environ[""UNSLOTH_ENABLE_FULL_FINETUNING""] = ""1""
+            if dtype == torch.bfloat16:
+                print(""Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%."")
+            else:
+                print(""Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32."")
+        else:
+            os.environ[""UNSLOTH_ENABLE_FULL_FINETUNING""] = ""0""
         pass
 
         kwargs.pop(""attn_implementation"", None); # No need since we auto call it
@@ -204,23 +253,37 @@ class FastBaseModel:
 
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
-        tokenizer.tokenizer.padding_side = ""left"" # Force inference
+        if hasattr(tokenizer, ""tokenizer""):
+            tokenizer.tokenizer.padding_side = ""left"" # Force inference
         m = model
         while hasattr(m, ""model""):
+            m.max_seq_length = max_seq_length
             m._saved_temp_tokenizer = tokenizer
             # Also set is_loaded_in_8bit to disable incorrect DDP
-            m.is_loaded_in_8bit = True
+            m.is_loaded_in_8bit = True if not full_finetuning else False
             m = m.model
         pass
+        m.max_seq_length = max_seq_length
         m._saved_temp_tokenizer = tokenizer
         # Also set is_loaded_in_8bit to disable incorrect DDP
-        m.is_loaded_in_8bit = True
+        m.is_loaded_in_8bit = True if not full_finetuning else False
 
         # Patch generate
         if model.generate.__name__ != ""unsloth_base_fast_generate"":
             model._old_generate = model.generate
             unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
             model.generate = types.MethodType(unsloth_base_fast_generate, model)
+
+        # Post patches
+        model = FastBaseModel.post_patch_model(
+            model,
+            use_gradient_checkpointing = use_gradient_checkpointing,
+        )
+        # Clear deleted GPU items
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        pass
         return model, tokenizer
     pass
 
@@ -249,6 +312,10 @@ class FastBaseModel:
         temporary_location         = ""_unsloth_temporary_saved_buffers"",
         **kwargs,
     ):
+        if os.environ.get(""UNSLOTH_ENABLE_FULL_FINETUNING"", ""0"") == ""1"":
+            print(""Unsloth: Full finetuning is enabled, so .get_peft_model has no effect"")
+            return model
+        pass
         transformers_set_seed(random_state)
 
         if type(r) is not int:
@@ -282,7 +349,7 @@ class FastBaseModel:
             gc.collect()
             torch.cuda.empty_cache()
         pass
-
+        max_seq_length = model.max_seq_length
         lora_config = LoraConfig(
             r               = r,
             lora_alpha      = lora_alpha,
@@ -295,11 +362,12 @@ class FastBaseModel:
             model,
             use_gradient_checkpointing = use_gradient_checkpointing,
         )
-        model = get_peft_model(model, lora_config)
+        model = _get_peft_model(model, lora_config)
         # Enable gradients on modules which are trainable
         requires_grad_for_gradient_checkpointing(model)
 
-        model = FastBaseModel.patch_peft_model(model, use_gradient_checkpointing)
+        model = FastBaseModel.post_patch_model(model, use_gradient_checkpointing)
+        model.max_seq_length = max_seq_length
 
         # Clear deleted GPU items
         for _ in range(3):
@@ -316,20 +384,26 @@ class FastBaseModel:
 
 
     @staticmethod
-    def patch_peft_model(
+    def post_patch_model(
         model,
         use_gradient_checkpointing = True,
     ):
-        if not isinstance(model, PeftModelForCausalLM):
-            raise TypeError(
-                ""Unsloth: Your model needs to call `.get_peft_model` first!""
-            )
-        pass
+        full_finetuning = os.environ.get(""UNSLOTH_ENABLE_FULL_FINETUNING"", ""0"") == ""1""
 
-        model = prepare_model_for_kbit_training(
+        float32_mixed_precision = True
+        if _get_dtype(model.config.torch_dtype) == torch.bfloat16:
+            # Use bfloat16 precision for full finetuning
+            float32_mixed_precision = False
+
+        model = prepare_model_for_training(
             model,
             use_gradient_checkpointing = use_gradient_checkpointing,
-            use_reentrant = True,
+            use_reentrant              = True,
+            full_finetuning            = full_finetuning,
+            train_layernorms           = full_finetuning,
+            train_embedding            = full_finetuning,
+            train_lm_head              = full_finetuning,
+            float32_mixed_precision    = float32_mixed_precision,
         )
 
         from transformers.trainer import Trainer 
@@ -347,17 +421,19 @@ class FastBaseModel:
         m = model
         while hasattr(m, ""model""):
             if hasattr(m, ""_saved_temp_tokenizer""):
-                m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
+                if hasattr(m._saved_temp_tokenizer, ""tokenizer""):
+                    m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
             # Also set is_loaded_in_8bit to disable incorrect DDP
-            m.is_loaded_in_8bit = True
+            m.is_loaded_in_8bit = True if not full_finetuning else False
             m = m.model
         pass
         if hasattr(m, ""_saved_temp_tokenizer""):
-            m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
+            if hasattr(m._saved_temp_tokenizer, ""tokenizer""):
+                m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
         # Also set is_loaded_in_8bit to disable incorrect DDP
-        m.is_loaded_in_8bit = True
+        m.is_loaded_in_8bit = True if not full_finetuning else False
 
         # Clear deleted GPU items
         for _ in range(3):
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 91bb020..2666912 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -945,7 +945,7 @@ def patch_sft_trainer_tokenizer():
         if replacer is None:
             # .*? matches first match. .+? matches final match.
             replacer = re.findall(
-                f""def {function_name}\(.*?\).*?\:\n"",
+                f""def {function_name}"" + r""\(.*?\).*?\:\n"",
                 function,
                 flags = re.MULTILINE | re.DOTALL,
             )
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 39b367e..186545c 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -24,10 +24,14 @@ from peft import PeftConfig, PeftModel
 from .loader_utils import get_model_name
 import os, contextlib, sys
 try:
-    from huggingface_hub.utils import get_token
+    from huggingface_hub import get_token
 except:
-    # Old HF Hub versions <= 0.0.25
-    from huggingface_hub.utils._token import get_token
+    try:
+        from huggingface_hub.utils import get_token
+    except:
+        # For older versions of huggingface_hub
+        from huggingface_hub.utils._token import get_token
+    pass
 pass
 from huggingface_hub import HfFileSystem
 import importlib.util
diff --git a/unsloth/save.py b/unsloth/save.py
index 0f75ecf..eaddfa0 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -31,10 +31,14 @@ from transformers.models.llama.modeling_llama import logger
 from .tokenizer_utils import fix_sentencepiece_gguf
 from huggingface_hub import HfApi
 try:
-    from huggingface_hub.utils import get_token
+    from huggingface_hub import get_token
 except:
-    # Old HF Hub versions <= 0.0.25
-    from huggingface_hub.utils._token import get_token
+    try:
+        from huggingface_hub.utils import get_token
+    except:
+        # For older versions of huggingface_hub
+        from huggingface_hub.utils._token import get_token
+    pass
 pass
 from pathlib import Path
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 6b029c6..a02cadd 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.7""
+__version__ = ""2024.12.8""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index a8ef9c0..709cd1c 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -29,7 +29,6 @@ from peft import LoraConfig, TaskType, get_peft_model
 from transformers import set_seed as transformers_set_seed
 from unsloth_zoo.peft_utils import (
     get_peft_regex,
-    merge_and_overwrite_lora,
     SKIP_QUANTIZATION_MODULES,
 )
 from triton import __version__ as triton_version
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 4cbbcf0..7cbdcfb 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -51,6 +51,7 @@ except:
 pass
 
 from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig
+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING
 from transformers import set_seed as transformers_set_seed
 from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
 from peft import PeftModelForCausalLM
@@ -1028,16 +1029,16 @@ class FastLlamaModel:
 
     @staticmethod
     def from_pretrained(
-        model_name     = ""unsloth/llama-2-7b-bnb-4bit"",
-        max_seq_length = None,
-        dtype          = None,
-        load_in_4bit   = True,
-        token          = None,
-        device_map     = ""sequential"",
-        rope_scaling   = None,
-        fix_tokenizer  = True,
-        model_patcher  = None,
-        tokenizer_name = None,
+        model_name        = ""unsloth/llama-3-8b-bnb-4bit"",
+        max_seq_length    = None,
+        dtype             = None,
+        load_in_4bit      = True,
+        token             = None,
+        device_map        = ""sequential"",
+        rope_scaling      = None,
+        fix_tokenizer     = True,
+        model_patcher     = None,
+        tokenizer_name    = None,
         trust_remote_code = False,
         **kwargs,
     ):
@@ -1070,9 +1071,17 @@ class FastLlamaModel:
 
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
-        # RoPE scaling
-        model_max_seq_length = \
-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings
+        # RoPE Scaling
+        model_config = AutoConfig.from_pretrained(model_name, token = token)
+        model_max_seq_length = model_config.max_position_embeddings
+
+        # Check if RoPE Scaling is even allowed
+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]
+        has_rope_scaling = False
+        try:
+            with open(inspect.getfile(model_function), ""r"") as file:
+                has_rope_scaling = ""self.config.rope_scaling"" in file.read()
+        except: pass
 
         # If max_seq_length is not specified, use maximum fron config
         if max_seq_length is None:
@@ -1080,14 +1089,28 @@ class FastLlamaModel:
         pass
 
         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):
+
             rope_scaling = max_seq_length / model_max_seq_length
+
             logger.warning_once(
                 f""Unsloth: {model_name} can only handle sequence lengths of at most ""\
                 f""{model_max_seq_length}.\nBut with kaiokendev's RoPE scaling of ""\
                 f""{round(rope_scaling, 3)}, it can be magically be extended to ""\
                 f""{max_seq_length}!""
             )
+
+            # Warn RoPE scaling isn't allowed
+            if not has_rope_scaling:
+                raise RuntimeError(
+                    ""However, {model_name} doesn't support RoPE Scaling!\n""\
+                    ""Please file a feature request at https://github.com/unslothai/unsloth.""
+                )
+            pass
+
             rope_scaling = {""type"": ""linear"", ""factor"": rope_scaling,}
+
+            # Add to kwargs
+            kwargs[""rope_scaling""] = rope_scaling
         pass
 
         bnb_config = None
@@ -1103,39 +1126,16 @@ class FastLlamaModel:
         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12
         # RoPE Scaling's max_position_embeddings must be updated
         max_position_embeddings = max(max_seq_length, model_max_seq_length)
-        try:
-            model = AutoModelForCausalLM.from_pretrained(
-                model_name,
-                device_map              = device_map,
-                torch_dtype             = dtype,
-                quantization_config     = bnb_config,
-                token                   = token,
-                rope_scaling            = rope_scaling,
-                max_position_embeddings = max_position_embeddings,
-                trust_remote_code       = trust_remote_code,
-                **kwargs,
-            )
-        except Exception as error:
-            if ""rope_scaling"" in str(error):
-                if rope_scaling is not None:
-                    raise TypeError(""Unsloth: {model_name} does not support rope_scaling."")
-                pass
-
-                # Counteract missing rope_scaling
-                model = AutoModelForCausalLM.from_pretrained(
-                    model_name,
-                    device_map              = device_map,
-                    torch_dtype             = dtype,
-                    quantization_config     = bnb_config,
-                    token                   = token,
-                    max_position_embeddings = max_position_embeddings,
-                    trust_remote_code       = trust_remote_code,
-                    **kwargs,
-                )
-            else:
-                raise error
-            pass
-        pass
+        model = AutoModelForCausalLM.from_pretrained(
+            model_name,
+            device_map              = device_map,
+            torch_dtype             = dtype,
+            quantization_config     = bnb_config,
+            token                   = token,
+            max_position_embeddings = max_position_embeddings,
+            trust_remote_code       = trust_remote_code,
+            **kwargs,
+        )
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
@@ -1423,7 +1423,6 @@ class FastLlamaModel:
 
         if loftq_config is None: loftq_config = {}
 
-        import inspect
         signature = str(inspect.signature(LoraConfig))
         SUPPORTS_LOFTQ  = ""loftq_config"" in signature
         SUPPORTS_RSLORA = ""use_rslora""   in signature
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index ff2e909..291f0aa 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):
 
     @staticmethod
     def from_pretrained(
-        model_name     = ""unsloth/mistral-7b-bnb-4bit"",
-        max_seq_length = None,
-        dtype          = None,
-        load_in_4bit   = True,
-        token          = None,
-        device_map     = ""sequential"",
-        rope_scaling   = None, # Mistral does not support RoPE scaling
-        fix_tokenizer  = True,
-        model_patcher  = None,
-        tokenizer_name = None,
+        model_name        = ""unsloth/mistral-7b-bnb-4bit"",
+        max_seq_length    = None,
+        dtype             = None,
+        load_in_4bit      = True,
+        token             = None,
+        device_map        = ""sequential"",
+        rope_scaling      = None, # Mistral does not support RoPE scaling
+        fix_tokenizer     = True,
+        model_patcher     = None,
+        tokenizer_name    = None,
         trust_remote_code = False,
         **kwargs,
     ):
-        if token is None and ""HF_TOKEN"" in os.environ:
-            token = os.environ[""HF_TOKEN""]
-
-        if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
-            token = os.environ[""HUGGINGFACE_TOKEN""]
-
-        if model_patcher is None: model_patcher = FastMistralModel
-        # Mistral does NOT support RoPE Scaling!
-        if rope_scaling is not None:
-            logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
-        pass
-
-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()
-        gpu_stats = torch.cuda.get_device_properties(0)
-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
-
-        statistics = \
-           f""==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
-           f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
-           f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
-           f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
-        print(statistics)
-        model_patcher.pre_patch()
-        # get_statistics()
-
-        if dtype is None:
-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:
-            logger.warning_once(""Device does not support bfloat16. Will change to float16."")
-            dtype = torch.float16
-
-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
-
-        # Check max sequence length
-        model_config = AutoConfig.from_pretrained(model_name, token = token)
-        model_max_seq_length = model_config.max_position_embeddings
-
-        # If max_seq_length is not specified, use maximum fron config
-        if max_seq_length is None:
-            max_seq_length = model_max_seq_length
-        pass
-
-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.
-        if max_seq_length > model_max_seq_length:
-            raise RuntimeError(
-                f""Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\n""\
-                f""The maximum sequence length supported is {model_max_seq_length}."",
-            )
-        pass
-
-        bnb_config = None
-        if load_in_4bit:
-            bnb_config = BitsAndBytesConfig(
-                load_in_4bit              = True,
-                bnb_4bit_use_double_quant = True,
-                bnb_4bit_quant_type       = ""nf4"",
-                bnb_4bit_compute_dtype    = dtype,
-            )
-
-        max_position_embeddings = max(max_seq_length, model_max_seq_length)
-        model = AutoModelForCausalLM.from_pretrained(
-            model_name,
-            device_map          = device_map,
-            torch_dtype         = dtype,
-            quantization_config = bnb_config,
-            token               = token,
-            # rope_scaling      = rope_scaling,
-            trust_remote_code   = trust_remote_code,
-            **kwargs,
-        )
-
-        # Counteract saved tokenizers
-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
-        tokenizer = load_correct_tokenizer(
-            tokenizer_name,
-            model_max_length  = max_position_embeddings,
-            padding_side      = ""right"",
+        return FastLlamaModel.from_pretrained(
+            model_name        = model_name,
+            max_seq_length    = max_seq_length,
+            dtype             = dtype,
+            load_in_4bit      = load_in_4bit,
             token             = token,
+            device_map        = device_map,
+            rope_scaling      = rope_scaling,
+            fix_tokenizer     = fix_tokenizer,
+            model_patcher     = FastMistralModel,
+            tokenizer_name    = tokenizer_name,
             trust_remote_code = trust_remote_code,
+            **kwargs,
         )
-
-        model, tokenizer = patch_tokenizer(model, tokenizer)
-        model = model_patcher.post_patch(model)
-
-        # Patch up QKV / O and MLP
-        for idx, layer in enumerate(model.model.layers):
-            layer.self_attn.apply_qkv = original_apply_qkv
-            layer.self_attn.apply_o   = original_apply_o
-        pass
-
-        # Patch Trainer
-        from transformers.trainer import Trainer
-        try:
-            if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)
-                Trainer._original_training_loop = inner_training_loop
-            else:
-                inner_training_loop = Trainer._original_training_loop
-        except:
-            raise RuntimeError(
-                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
-                ""The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\n""
-                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
-                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
-            )
-        pass
-
-        import transformers.trainer
-        items_in_trainer = dir(transformers.trainer)
-        good_items = []
-        for item in items_in_trainer:
-            # TODO: Support Deepspeed
-            if item.startswith((""deepspeed"", ""xm"", ""met"", ""smp"")): continue
-            if item in inner_training_loop: good_items.append(item)
-        pass
-        exec(""from transformers.trainer import ("" + "", "".join(x for x in good_items) + "")"", globals())
-
-        start = re.search('logger\.info\([\""\'].+?Running training', inner_training_loop).span(0)[0]
-        end = inner_training_loop.find(""\n\n"", start)
-        original_debug = inner_training_loop[start:end]
-        spaces = re.search('\n([\s\t]{1,})', original_debug).group(0)[1:]
-        front_spaces = re.match('([\s\t]{1,})', inner_training_loop).group(0)
-
-        debug_info = """"""debug_info = \\
-        f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\n""\\
-        f""   \\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
-        f""O^O/ \\_/ \\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
-        f""\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
-        f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
-        logger.warning(debug_info)
-        import subprocess, re, gc
-        output = subprocess.check_output(
-            'nvidia-smi --query-gpu=memory.used --format=csv', shell = True)
-        output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)
-        output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)
-        if output > 1: raise RuntimeError(
-            'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.')
-        for _ in range(3):
-            gc.collect()
-            torch.cuda.empty_cache()""""""
-
-        debug_info = debug_info.split('\n')
-        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)
-
-        debug_info = """"""n_total_devices = total_train_batch_size // \\
-            args.gradient_accumulation_steps // self._train_batch_size
-        if n_total_devices > 1:
-            logger.warning_once(
-                ""* Our OSS was designed for people with few GPU resources to level the playing field.\\n""
-                ""* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\n""
-                ""* We're a 2 person team, so we still have to fund our development costs - thanks!\\n""
-                ""* If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
-            )
-        debug_info =""""""
-        debug_info = debug_info.split('\n')
-        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
-        inner_training_loop = inner_training_loop.replace(""debug_info ="", debug_info, 1)
-
-        front_spaces = re.match(r""[\t\s]{1,}"", inner_training_loop).group(0)
-        inner_training_loop = re.sub(r""^"" + front_spaces, """", inner_training_loop, flags = re.MULTILINE)
-        inner_training_loop = inner_training_loop.replace(
-            ""train_dataloader = tpu_spmd_dataloader(train_dataloader)"",
-            ""raise RuntimeError('Unsloth: TPUs are not yet supported!')""
-        )
-        inner_training_loop = inner_training_loop.replace(
-            ""self.accelerator.free_memory()"",
-            ""self.accelerator.free_memory()\n"" + \
-            front_spaces + ""if self.is_deepspeed_enabled:""\
-            ""raise RuntimeError('Unsloth: Deepspeed is not yet supported!')\n"", 1,
-        )
-
-        check_batches = """"""train_dataloader = self.get_train_dataloader()
-        ga  = args.gradient_accumulation_steps
-        bsz = self._train_batch_size
-        total_batches = bsz * ga * args.world_size
-        n_total_devices = total_batches // ga // bsz
-        if n_total_devices > 1:
-            logger.warning_once(
-                ""* Our OSS was designed for people with few GPU resources to level the playing field.\\n""
-                ""* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\n""
-                ""* We're a 2 person team, so we still have to fund our development costs - thanks!\\n""
-                ""* If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
-            )
-            divisor = n_total_devices / 1
-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)
-            if total_batches // ga // bsz > 1:
-                divisor = n_total_devices / 1
-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)""""""
-        check_batches = check_batches.split('\n')
-        check_batches = ""\n"".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])
-        inner_training_loop = inner_training_loop.replace(
-            ""train_dataloader = self.get_train_dataloader()"",
-            check_batches, 1,
-        )
-        inner_training_loop = inner_training_loop.replace(
-            ""_inner_training_loop"",
-            ""_fast_inner_training_loop"", 1,
-        )
-        exec(inner_training_loop, globals())
-
-        Trainer._inner_training_loop = _fast_inner_training_loop
-        inner_training_loop = inner_training_loop.replace(
-            ""is_torch_tpu_available()"",
-            ""False"",
-        )
-        if ""n_total_devices >"" not in inner_training_loop:
-            raise RuntimeError(
-                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
-                ""The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\n""
-                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
-                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
-            )
-        pass
-        inner_training_loop = inner_training_loop.replace(
-            ""is_sagemaker_mp_enabled()"",
-            ""False"",
-        )
-        exec(inner_training_loop, globals())
-        Trainer._inner_training_loop = _fast_inner_training_loop
-
-        # Save max_seq_length
-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)
-        model.max_seq_length = max_position_embeddings
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model.max_seq_length = max_position_embeddings
-            internal_model = internal_model.model
-        pass
-        internal_model.max_seq_length = max_position_embeddings
-
-        # We check the tokenizer first for errors
-        if fix_tokenizer:
-            tokenizer = check_tokenizer(
-                model            = model,
-                tokenizer        = tokenizer,
-                model_name       = model_name,
-                model_max_length = max_position_embeddings,
-                padding_side     = ""right"",
-                token            = token,
-            )
-        pass
-        patch_saving_functions(tokenizer)
-
-        # Fix up config for transformers uploading PEFT
-        # Not necessary anymore since we require transformers>=4.37
-        if False:
-            name = model.config._name_or_path
-            if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-                name = name[:len(name) - len(""-bnb-4bit"")]
-                model.config.update({""_name_or_path"" : name})
-            pass
-        
-        # Log Unsloth version for future fastpaths for inference
-        model.config.update({""unsloth_version"" : __version__})
-
-        # Add save modules
-        patch_saving_functions(model)
-        Trainer._inner_training_loop = _fast_inner_training_loop
-
-        # Save tokenizer for inference purposes
-        tokenizer.padding_side = ""left"" # Force inference
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model._saved_temp_tokenizer = tokenizer
-            internal_model = internal_model.model
-        pass
-        internal_model._saved_temp_tokenizer = tokenizer
-        
-        return model, tokenizer
     pass
 pass
diff --git a/unsloth/models/qwen2.py b/unsloth/models/qwen2.py
index 4732728..984bf7c 100644
--- a/unsloth/models/qwen2.py
+++ b/unsloth/models/qwen2.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .mistral import *
+from .llama import *
 
 from transformers.models.qwen2.modeling_qwen2 import (
     Qwen2Attention,
@@ -32,7 +32,7 @@ except:
 pass
 
 
-class FastQwen2Model(FastMistralModel):
+class FastQwen2Model(FastLlamaModel):
 
     @staticmethod
     def pre_patch():
@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):
 
     @staticmethod
     def from_pretrained(
-        model_name     = ""Qwen/Qwen2-7B"",
-        max_seq_length = 4096,
-        dtype          = None,
-        load_in_4bit   = True,
-        token          = None,
-        device_map     = ""sequential"",
-        rope_scaling   = None, # Qwen2 does not support RoPE scaling
-        fix_tokenizer  = True,
-        model_patcher  = None,
-        tokenizer_name = None,
+        model_name        = ""Qwen/Qwen2-7B"",
+        max_seq_length    = 4096,
+        dtype             = None,
+        load_in_4bit      = True,
+        token             = None,
+        device_map        = ""sequential"",
+        rope_scaling      = None, # Qwen2 does not support RoPE scaling
+        fix_tokenizer     = True,
+        model_patcher     = None,
+        tokenizer_name    = None,
         trust_remote_code = False,
         **kwargs,
     ):
-        return FastMistralModel.from_pretrained(
-            model_name     = model_name,
-            max_seq_length = max_seq_length,
-            dtype          = dtype,
-            load_in_4bit   = load_in_4bit,
-            token          = token,
-            device_map     = device_map,
-            rope_scaling   = rope_scaling,
-            fix_tokenizer  = fix_tokenizer,
-            model_patcher  = FastQwen2Model,
-            tokenizer_name = tokenizer_name,
+        return FastLlamaModel.from_pretrained(
+            model_name        = model_name,
+            max_seq_length    = max_seq_length,
+            dtype             = dtype,
+            load_in_4bit      = load_in_4bit,
+            token             = token,
+            device_map        = device_map,
+            rope_scaling      = rope_scaling,
+            fix_tokenizer     = fix_tokenizer,
+            model_patcher     = FastQwen2Model,
+            tokenizer_name    = tokenizer_name,
             trust_remote_code = trust_remote_code,
             **kwargs,
         )
"
"diff --git a/pyproject.toml b/pyproject.toml
index 6e1bea6..a0a1723 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.13"",
+    ""unsloth_zoo>=2025.3.14"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -351,7 +351,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.13"",
+    ""unsloth_zoo>=2025.3.14"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 80aa3bd..41b6bb7 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.13""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.14""):
         print(
             ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
             ""To disable this, set `os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'`""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index e2b35c5..90b5917 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.15""
+__version__ = ""2025.3.16""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -1177,6 +1177,7 @@ def unsloth_compile_transformers(
         return
     if disable: return
 
+    model_types = list(dict().fromkeys(model_types).keys())
     for model_type in model_types:
         _unsloth_compile_transformers(
             model_type,
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 0780527..61cf05e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -652,13 +652,7 @@ def LlamaModel_fast_forward(
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
-    # inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
-    torch_dtype = __DTYPE_MAP.get(self.config.torch_dtype, None)
-    if torch_dtype is not None:
-        inputs_embeds = inputs_embeds.to(torch_dtype)
-    else:
-        raise TypeError(""Unsloth: torch_dtype for models is not bfloat16, float16 or float32!"")
-    pass
+    inputs_embeds = inputs_embeds.to(_get_dtype(self.config.torch_dtype))
 
     # Normalized from Gemma
     IS_GEMMA   = self.config.model_type.startswith(""gemma"")
@@ -924,7 +918,7 @@ def LlamaModel_fast_forward_inference(
     mlp_size = self.config.intermediate_size
 
     X = self.model.embed_tokens(input_ids)
-    X = X.to(self.config.torch_dtype)
+    X = X.to(_get_dtype(self.config.torch_dtype))
     bsz, q_len, hd = X.shape
     assert(q_len == 1)
     # Get saved buffers to reduce memory movement
@@ -2457,12 +2451,6 @@ class FastLlamaModel:
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
         model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
-
-        # Patch generate
-        if model.generate.__name__ != ""unsloth_fast_generate"":
-            model._old_generate = model.generate
-            unsloth_fast_generate.__doc__ = model._old_generate.__doc__
-            model.generate = types.MethodType(unsloth_fast_generate, model)
         return model
     pass
 
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 9af5317..cf250dd 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -718,6 +718,16 @@ __INT_TO_FLOAT_MAPPER = \
         ""allenai/OLMo-2-0325-32B-Instruct"",
         ""unsloth/OLMo-2-0325-32B-Instruct-bnb-4bit"",
     ),
+    ""unsloth/Mistral-Small-3.1-24B-Instruct-2503-unsloth-bnb-4bit"" : (
+        ""unsloth/Mistral-Small-3.1-24B-Instruct-2503"",
+        ""mistralai/Mistral-Small-3.1-24B-Instruct-2503"",
+        ""unsloth/Mistral-Small-3.1-24B-Instruct-2503-bnb-4bit"",
+    ),
+    ""unsloth/Mistral-Small-3.1-24B-Base-2503-unsloth-bnb-4bit"" : (
+        ""unsloth/Mistral-Small-3.1-24B-Base-2503"",
+        ""mistralai/Mistral-Small-3.1-24B-Base-2503"",
+        ""unsloth/Mistral-Small-3.1-24B-Base-2503-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 53a873d..db140c4 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -76,19 +76,34 @@ NUM_LOGITS_TO_KEEP = dict()
 global PROMPT_LOOPKUP
 PROMPT_LOOPKUP = dict()
 
+from transformers import GenerationConfig, CompileConfig, HybridCache
+_compile_config = CompileConfig(
+    fullgraph = False,
+    dynamic = None,
+    mode = ""reduce-overhead"",
+)
+_compile_config.disable = True # Must set manually
+
+from unsloth_zoo.vllm_utils import (
+    convert_lora_modules,
+    return_lora_modules,
+)
+
 def unsloth_base_fast_generate(
     self,
     *args,
     **kwargs,
 ):
     if len(args) != 0:
-        x = args[0]
+        input_ids = args[0]
     elif ""input_ids"" in kwargs:
-        x = kwargs[""input_ids""]
+        input_ids = kwargs[""input_ids""]
+    elif ""input"" in kwargs:
+        input_ids = kwargs[""input_ids""]
     else:
         raise TypeError(""Unsloth: You need to pass in input_ids to .generate!"")
-    assert(type(x) is torch.Tensor)
-    bsz = x.shape[0]
+    assert(type(input_ids) is torch.Tensor)
+    bsz = input_ids.shape[0]
 
     FastBaseModel.for_inference(self)
     dtype = _get_dtype(self.config.torch_dtype)
@@ -101,8 +116,8 @@ def unsloth_base_fast_generate(
     is_vlm = is_vlm or hasattr(self.config, ""vision_config"")
     arch = self.config.architectures[0]
 
-    # Remove token_type_ids
-    kwargs.pop(""token_type_ids"", None)
+    # Remove token_type_ids - WRONG for Gemma 3 since bidirectional attention
+    # kwargs.pop(""token_type_ids"", None)
 
     # VLMs do not allow logits_to_keep
     global NUM_LOGITS_TO_KEEP
@@ -146,20 +161,58 @@ def unsloth_base_fast_generate(
     try: kwargs[""pixel_values""] = kwargs[""pixel_values""].to(dtype)
     except: pass
 
-    if ""use_cache"" not in kwargs: kwargs[""use_cache""] = True
-
     # Mixed precision autocast
     if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"":
-        autocaster = torch.autocast(device_type = ""cuda"", dtype = dtype)
+        autocaster = torch.autocast(device_type = ""cuda"", dtype = torch.float16)
+        dtype = torch.float16
     else:
         autocaster = torch.autocast(device_type = ""cuda"", dtype = dtype)
-    with torch.inference_mode(), autocaster:
-        try:
+
+    # Prepare LoRA
+    # state_dict = convert_lora_modules(self, dtype = dtype)
+
+    # Set compile dynamic shapes
+    torch._dynamo.mark_static(input_ids, 0)
+    torch._dynamo.mark_dynamic(input_ids, 1)
+    if ""attention_mask"" in kwargs:
+        torch._dynamo.mark_static(kwargs[""attention_mask""], 0)
+        torch._dynamo.mark_dynamic(kwargs[""attention_mask""], 1)
+    if ""token_type_ids"" in kwargs:
+        torch._dynamo.mark_static(kwargs[""token_type_ids""], 0)
+        torch._dynamo.mark_dynamic(kwargs[""token_type_ids""], 1)
+
+    # Fix generation_config
+    # Use hybrid if sliding window seen, otherwise try static
+    cache_implementation = getattr(self.config, ""cache_implementation"", None)
+    if getattr(self, ""_supports_static_cache"", True):
+        cache_implementation = ""static""
+    else:
+        cache_implementation = None
+    if cache_implementation is not None:
+        swa = getattr(getattr(self.config, ""text_config"", self.config), ""sliding_window"", None)
+        if swa == 0 or type(swa) is not int:
+            cache_implementation = ""static""
+        else:
+            cache_implementation = ""hybrid""
+    if ""generation_config"" in kwargs:
+        kwargs[""generation_config""].cache_implementation = cache_implementation
+        kwargs[""generation_config""].compile_config = _compile_config
+    else:
+        kwargs[""cache_implementation""] = cache_implementation
+        kwargs[""compile_config""] = _compile_config
+    pass
+
+    try:
+        with torch.inference_mode(), autocaster:
             output = self._old_generate(*args, **kwargs)
-        except:
-            PROMPT_LOOPKUP[arch] = False
-            kwargs.pop(""prompt_lookup_num_tokens"", None)
+    except:
+        PROMPT_LOOPKUP[arch] = False
+        kwargs.pop(""prompt_lookup_num_tokens"", None)
+        with torch.inference_mode(), autocaster:
             output = self._old_generate(*args, **kwargs)
+    finally:
+        pass
+        # return_lora_modules(self, state_dict, torch.float32)
     pass
 
     FastBaseModel.for_training(self)
@@ -203,8 +256,9 @@ class FastBaseModel:
         except: vllm_version = """"
 
         model_type_arch = model_types[0]
-        if model_type_arch == ""siglip"" and len(model_types) != 1:
-            model_type_arch = model_types[1]
+        if model_type_arch == ""siglip"":
+            for model_type_arch in model_types:
+                if model_type_arch != ""siglip"": break
 
         statistics = \
            f""==((====))==  Unsloth {__version__}: Fast {model_type_arch.title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
@@ -543,12 +597,6 @@ class FastBaseModel:
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastBaseModel.for_training,  model)
         model.for_inference = functools.partial(FastBaseModel.for_inference, model)
-
-        # Patch generate
-        if model.generate.__name__ != ""unsloth_base_fast_generate"":
-            model._old_generate = model.generate
-            unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
-            model.generate = types.MethodType(unsloth_base_fast_generate, model)
         return model
     pass
 
"
"diff --git a/unsloth/model_registry.py b/unsloth/model_registry.py
index bb6540b..dede596 100644
--- a/unsloth/model_registry.py
+++ b/unsloth/model_registry.py
@@ -19,7 +19,9 @@ _IS_PHI_REGISTERED = False
 _IS_PHI_INSTRUCT_REGISTERED = False
 
 
-def construct_model_key(org, base_name, version, size, quant_type, instruct_tag):
+def construct_model_key(
+    org, base_name, version, size, quant_type, instruct_tag
+):
     key = f""{org}/{base_name}-{version}-{size}B""
     if instruct_tag:
         key = ""-"".join([key, instruct_tag])
@@ -58,7 +60,9 @@ class ModelInfo:
         return key
 
     @staticmethod
-    def append_quant_type(key: str, quant_type: Literal[""bnb"", ""unsloth""] = None):
+    def append_quant_type(
+        key: str, quant_type: Literal[""bnb"", ""unsloth""] = None
+    ):
         if quant_type:
             if quant_type == ""bnb"":
                 key = ""-"".join([key, BNB_QUANTIZED_TAG])
@@ -67,7 +71,9 @@ class ModelInfo:
         return key
 
     @classmethod
-    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+    def construct_model_name(
+        cls, base_name, version, size, quant_type, instruct_tag
+    ):
         raise NotImplementedError(""Subclass must implement this method"")
 
     @property
@@ -79,7 +85,9 @@ class ModelInfo:
 
 class LlamaModelInfo(ModelInfo):
     @classmethod
-    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+    def construct_model_name(
+        cls, base_name, version, size, quant_type, instruct_tag
+    ):
         key = f""{base_name}-{version}-{size}B""
         key = cls.append_instruct_tag(key, instruct_tag)
         key = cls.append_quant_type(key, quant_type)
@@ -88,7 +96,9 @@ class LlamaModelInfo(ModelInfo):
 
 class LlamaVisionModelInfo(ModelInfo):
     @classmethod
-    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+    def construct_model_name(
+        cls, base_name, version, size, quant_type, instruct_tag
+    ):
         key = f""{base_name}-{version}-{size}B-Vision""
         key = cls.append_instruct_tag(key, instruct_tag)
         key = cls.append_quant_type(key, quant_type)
@@ -97,7 +107,9 @@ class LlamaVisionModelInfo(ModelInfo):
 
 class QwenModelInfo(ModelInfo):
     @classmethod
-    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+    def construct_model_name(
+        cls, base_name, version, size, quant_type, instruct_tag
+    ):
         key = f""{base_name}{version}-{size}B""
         key = cls.append_instruct_tag(key, instruct_tag)
         key = cls.append_quant_type(key, quant_type)
@@ -106,7 +118,9 @@ class QwenModelInfo(ModelInfo):
 
 class QwenVLModelInfo(ModelInfo):
     @classmethod
-    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+    def construct_model_name(
+        cls, base_name, version, size, quant_type, instruct_tag
+    ):
         key = f""{base_name}{version}-VL-{size}B""
         key = cls.append_instruct_tag(key, instruct_tag)
         key = cls.append_quant_type(key, quant_type)
@@ -115,58 +129,62 @@ class QwenVLModelInfo(ModelInfo):
 
 class PhiModelInfo(ModelInfo):
     @classmethod
-    def construct_model_name(cls, base_name, version, size, quant_type, instruct_tag):
+    def construct_model_name(
+        cls, base_name, version, size, quant_type, instruct_tag
+    ):
         key = f""{base_name}-{version}""
         key = cls.append_instruct_tag(key, instruct_tag)
         key = cls.append_quant_type(key, quant_type)
         return key
 
+
 @dataclass
-class ModelMetaBase:
+class ModelMeta:
     org: str
     base_name: str
-
-@dataclass
-class ModelMeta(ModelMetaBase):
-    instruct_tags: list[str]
     model_version: str
-    model_sizes: list[str]
-    is_multimodal: bool
     model_info_cls: type[ModelInfo]
-    quant_types: list[Literal[None, ""bnb"", ""unsloth"", ""GGUF""]]
-
-@dataclass
-class LlamaMetaBase(ModelMetaBase):
-    org: str = ""meta-llama""
-    base_name: str = ""Llama""
-
-@dataclass
-class LlamaMeta3_1(LlamaMetaBase, ModelMeta):
-    instruct_tags: list[str] = [None, ""Instruct""]
-    model_version: str = ""3.1""
-    model_sizes: list[str] = [8]
-    is_multimodal: bool = False
-    quant_types: list[Literal[None, ""bnb"", ""unsloth""]] = [None]
-    model_info_cls: type[ModelInfo] = LlamaModelInfo
-@dataclass
-class LlamaMeta3_2(LlamaMetaBase, ModelMeta):
-    instruct_tags: list[str] = [None, ""Instruct""]
-    model_version: str = ""3.2""
-    model_sizes: list[str] = [1, 3]
+    model_sizes: list[str] = field(default_factory=list)
+    instruct_tags: list[str] = field(default_factory=list)
+    quant_types: list[Literal[None, ""bnb"", ""unsloth""]] = field(
+        default_factory=list
+    )
     is_multimodal: bool = False
-    quant_types: list[Literal[None, ""bnb"", ""unsloth""]] = [None]
-    model_info_cls: type[ModelInfo] = LlamaModelInfo
 
-# Llama text only models
-_LLAMA_INFO = {
-    ""org"": ""meta-llama"",
-    ""base_name"": ""Llama"",
-    ""instruct_tags"": [None, ""Instruct""],
-    ""model_versions"": [""3.2"", ""3.1""],
-    ""model_sizes"": {""3.2"": [1, 3], ""3.1"": [8]},
-    ""is_multimodal"": False,
-    ""model_info_cls"": LlamaModelInfo,
-}
+
+LlamaMeta3_1 = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[None, ""Instruct""],
+    model_version=""3.1"",
+    model_sizes=[8],
+    model_info_cls=LlamaModelInfo,
+    is_multimodal=False,
+    quant_types=[None, ""bnb"", ""unsloth""],
+)
+
+LlamaMeta3_2 = ModelMeta(
+    org=""meta-llama"",
+    base_name=""Llama"",
+    instruct_tags=[None, ""Instruct""],
+    model_version=""3.2"",
+    model_sizes=[1, 3],
+    model_info_cls=LlamaModelInfo,
+    is_multimodal=False,
+    quant_types=[None, ""bnb"", ""unsloth""],
+)
+
+
+# # Llama text only models
+# _LLAMA_INFO = {
+#     ""org"": ""meta-llama"",
+#     ""base_name"": ""Llama"",
+#     ""instruct_tags"": [None, ""Instruct""],
+#     ""model_versions"": [""3.2"", ""3.1""],
+#     ""model_sizes"": {""3.2"": [1, 3], ""3.1"": [8]},
+#     ""is_multimodal"": False,
+#     ""model_info_cls"": LlamaModelInfo,
+# }
 
 _LLAMA_VISION_INFO = {
     ""org"": ""meta-llama"",
@@ -293,6 +311,7 @@ def register_model(
 #                         is_multimodal=is_multimodal,
 #                     )
 
+
 def _register_models(model_meta: ModelMeta):
     org = model_meta.org
     base_name = model_meta.base_name
@@ -318,6 +337,7 @@ def _register_models(model_meta: ModelMeta):
                     is_multimodal=is_multimodal,
                 )
 
+
 def register_llama_models():
     global _IS_LLAMA_REGISTERED
     if _IS_LLAMA_REGISTERED:
@@ -387,7 +407,9 @@ def get_llama_models():
     if not _IS_LLAMA_REGISTERED:
         register_llama_models()
 
-    return _get_models(partial(_base_name_filter, base_name=_LLAMA_INFO[""base_name""]))
+    return _get_models(
+        partial(_base_name_filter, base_name=_LLAMA_INFO[""base_name""])
+    )
 
 
 def get_llama_vision_models():
@@ -395,7 +417,8 @@ def get_llama_vision_models():
         register_llama_vision_models()
 
     return _get_models(
-        lambda model_info: model_info.base_name == _LLAMA_VISION_INFO[""base_name""]
+        lambda model_info: model_info.base_name
+        == _LLAMA_VISION_INFO[""base_name""]
         and model_info.is_multimodal
     )
 
@@ -438,12 +461,34 @@ def get_phi_instruct_models():
     if not _IS_PHI_INSTRUCT_REGISTERED:
         register_phi_instruct_models()
     return _get_models(
-        lambda model_info: model_info.base_name == _PHI_INSTRUCT_INFO[""base_name""]
+        lambda model_info: model_info.base_name
+        == _PHI_INSTRUCT_INFO[""base_name""]
     )
 
 
 if __name__ == ""__main__"":
-    register_llama_models()
+    from huggingface_hub import HfApi
+
+    api = HfApi()
+
+    def get_model_info(
+        model_id: str, properties: list[str] = None
+    ) -> ModelInfo:
+        try:
+            model_info: ModelInfo = api.model_info(model_id, expand=properties)
+        except Exception as e:
+            print(f""Error getting model info for {model_id}: {e}"")
+            model_info = None
+        return model_info
+
+    test_model = LlamaMeta3_2
+    _register_models(test_model)
+
     for k, v in MODEL_REGISTRY.items():
-        print(f""{k}: {v}"")
-        print(v.model_path)
\ No newline at end of file
+        model_info = get_model_info(v.model_path)
+        if model_info is None:
+            # print unicode cross mark followed by model k
+            print(f""\u2718 {k}"")
+        else:
+            # print unicode checkmark followed by model k
+            print(f""\u2713 {k} found"")
diff --git a/unsloth/utils/hf_hub.py b/unsloth/utils/hf_hub.py
index e3230e6..da3f72a 100644
--- a/unsloth/utils/hf_hub.py
+++ b/unsloth/utils/hf_hub.py
@@ -1,6 +1,6 @@
 from huggingface_hub import HfApi, ModelInfo
 
-api = HfApi()
+api: HfApi
 
 POPULARITY_PROPERTIES = [
     ""downloads"",
@@ -32,6 +32,9 @@ def get_model_info(
     Default properties: [""safetensors"", ""lastModified""], only retrieves minimal information.
     Set to None to retrieve the full model information.
     """"""
+    global api
+    if api is None:
+        api = HfApi()
     try:
         model_info: ModelInfo = api.model_info(model_id, expand=properties)
     except Exception as e:
@@ -58,6 +61,9 @@ def retrieve_models(
     search: str = The search query for filtering models.
 
     """"""
+    global api
+    if api is None:
+        api = HfApi()
     if full:
         properties = None
 
"
"diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index a06d81a..46b6766 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -93,7 +93,7 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         # [TODO] Changing blocksize to head_dim//2 seems to have
         # some concurrency / un-deterministic issues.
         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)
-        
+
         # group_size = 4 # 4 or 8, too large group_size can hurt performance.
         div : int
         mod : int
@@ -155,6 +155,9 @@ pass
 def fast_rope_embedding(Q, K, cos, sin):
     Q = Fast_RoPE_Embedding.apply(Q.transpose(1, 2), cos, sin).transpose(1, 2)
     K = Fast_RoPE_Embedding.apply(K.transpose(1, 2), cos, sin).transpose(1, 2)
+    # synchronize before cat to avoid race condition
+    torch.cuda.current_stream(Q.device).synchronize()
+
     return Q, K
 pass
 
@@ -198,5 +201,6 @@ pass
 def inplace_rope_embedding(Q, K, cos, sin, position_ids):
     Q = Slow_RoPE_Embedding.apply(Q, cos, sin, position_ids)
     K = Slow_RoPE_Embedding.apply(K, cos, sin, position_ids)
+    torch.cuda.current_stream(Q.device).synchronize()
     return Q, K
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index e7d9084..3c0d501 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -499,8 +499,6 @@ def LlamaAttention_fast_forward(
     #     else inplace_rope_embedding(Q, K, cos, sin, position_ids)
     # )
     Q, K = fast_rope_embedding(Q, K, cos, sin)
-    # synchronize before cat to avoid race condition
-    torch.cuda.current_stream(Q.device).synchronize()
 
     if past_key_value is not None:
         K = torch.cat([past_key_value[0], K], dim = 2)
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 4bf1357..61cf05e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -652,13 +652,7 @@ def LlamaModel_fast_forward(
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
-    # inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
-    torch_dtype = __DTYPE_MAP.get(self.config.torch_dtype, None)
-    if torch_dtype is not None:
-        inputs_embeds = inputs_embeds.to(torch_dtype)
-    else:
-        raise TypeError(""Unsloth: torch_dtype for models is not bfloat16, float16 or float32!"")
-    pass
+    inputs_embeds = inputs_embeds.to(_get_dtype(self.config.torch_dtype))
 
     # Normalized from Gemma
     IS_GEMMA   = self.config.model_type.startswith(""gemma"")
@@ -924,7 +918,7 @@ def LlamaModel_fast_forward_inference(
     mlp_size = self.config.intermediate_size
 
     X = self.model.embed_tokens(input_ids)
-    X = X.to(self.config.torch_dtype)
+    X = X.to(_get_dtype(self.config.torch_dtype))
     bsz, q_len, hd = X.shape
     assert(q_len == 1)
     # Get saved buffers to reduce memory movement
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 458c269..109e1c6 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -27,13 +27,6 @@ import numpy as np
 #     pass
 # pass
 
-# Check for unsloth_zoo
-try:
-    import unsloth_zoo
-except:
-    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth-zoo`"")
-pass
-
 # Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
@@ -165,6 +158,13 @@ if ""SPACE_AUTHOR_NAME"" not in os.environ and ""SPACE_REPO_NAME"" not in os.environ
     pass
 pass
 
+# Check for unsloth_zoo
+try:
+    import unsloth_zoo
+except:
+    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth-zoo`"")
+pass
+
 from .models import *
 from .save import *
 from .chat_templates import *
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index debd037..a2337a1 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -17,7 +17,7 @@ import triton.language as tl
 import torch
 from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh
 from transformers.models.llama.modeling_llama import logger
-
+from packaging.version import Version
 
 @triton.heuristics({
     ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
@@ -352,7 +352,6 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
 pass
 
 
-# @torch._disable_dynamo
 def fast_cross_entropy_loss(
     logits,
     labels,
@@ -380,6 +379,9 @@ def fast_cross_entropy_loss(
         n_items = torch.count_nonzero(labels != -100)
     return loss.sum() / n_items
 pass
+if Version(torch.__version__) < Version(""2.5.0""):
+    fast_cross_entropy_loss = torch._disable_dynamo(fast_cross_entropy_loss)
+pass
 
 
 from transformers.models.llama.modeling_llama import (
@@ -475,7 +477,6 @@ def unpatch_llama_for_causal_lm():
 pass
 
 
-# @torch._disable_dynamo
 def UnslothForCausalLMLoss(
     logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs
 ):
@@ -490,6 +491,9 @@ def UnslothForCausalLMLoss(
     )
     return loss
 pass
+if Version(torch.__version__) < Version(""2.5.0""):
+    UnslothForCausalLMLoss = torch._disable_dynamo(UnslothForCausalLMLoss)
+pass
 
 
 def patch_transformers_losses():
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 97b2ea7..a39bc58 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -760,7 +760,9 @@ def get_statistics():
     # We log some basic stats about which environment is being used.
     # We simply download a README.md file from HF - all data is made public.
     # This is simply so we can check if some envs are broken or not.
-    # You can disable this by commenting the below out
+    # You can disable this by setting UNSLOTH_DISABLE_STATISTICS
+    import os
+    if ""UNSLOTH_DISABLE_STATISTICS"" in os.environ: return
     from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
     disabled = False
     if not are_progress_bars_disabled():
@@ -1295,6 +1297,7 @@ def patch_gradient_accumulation_fix(Trainer):
     # Fixes gradient accumulation 
     import inspect
     if hasattr(Trainer, ""get_batch_samples""):
+        if Trainer.get_batch_samples.__name__ == ""_unsloth_get_batch_samples"": return
         if \
             not inspect.getsource(Trainer.get_batch_samples).strip()\
             .endswith(""return batch_samples, num_items_in_batch""):
@@ -1321,6 +1324,7 @@ def patch_gradient_accumulation_fix(Trainer):
     pass
 
     # Also fix up loss scaling ie negate loss *= self.args.gradient_accumulation_steps
+    if Trainer.training_step.__name__ == ""_unsloth_training_step"": return
     if ""num_items_in_batch"" not in inspect.signature(Trainer.training_step).parameters: return
 
     function = inspect.getsource(Trainer.training_step)
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 65e2d77..c0175bb 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1518,6 +1518,7 @@ class FastLlamaModel:
         pass
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
+        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
         model_patcher.pre_patch()
         get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
"
"diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index d212c22..cadfed9 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -112,6 +112,11 @@ def unsloth_base_fast_generate(
     arch = self.config.architectures[0]
 
     # Remove token_type_ids - WRONG for Gemma 3 since bidirectional attention
+    if hasattr(self, ""generate"") and hasattr(self, ""forward""):
+        # did not combine with below since self might not have model
+        keys = inspect.signature(self.forward).parameters.keys()
+        if ""token_type_ids"" not in keys:
+            kwargs.pop(""token_type_ids"", None)
     # kwargs.pop(""token_type_ids"", None)
 
     # VLMs do not allow logits_to_keep
"
"diff --git a/pyproject.toml b/pyproject.toml
index ae46d7a..d17859c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.4"",
+    ""unsloth_zoo>=2025.7.5"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -48,7 +48,7 @@ huggingface = [
     ""wheel>=0.42.0"",
     ""numpy"",
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0,!=0.19.0"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf"",
     ""huggingface_hub"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.4"",
+    ""unsloth_zoo>=2025.7.5"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -399,7 +399,7 @@ colab-new = [
 ]
 colab-no-deps = [
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0,!=0.19.0"",
     ""peft>=0.7.1"",
     ""xformers"",
     ""bitsandbytes>=0.45.5"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 5b3879c..bfd7c8c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.3""
+__version__ = ""2025.7.4""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -190,6 +190,14 @@ try:
 except:
     pass
 
+# The following generation flags are not valid and may be ignored:
+try:
+    from transformers.generation.configuration_utils import logger as configuration_logger
+    configuration_logger.addFilter(HideLoggingMessage(""following generation flags""))
+    del configuration_logger
+except:
+    pass
+
 # Gemma3 It is strongly recommended to train Gemma3 models with the `eager`
 try:
     from transformers.models.gemma3.modeling_gemma3 import logger as gemma3_logger
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3551c6c..8d985aa 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2224,7 +2224,7 @@ class FastLlamaModel:
         bias                = ""none"",
         layers_to_transform = None,
         layers_pattern      = None,
-        use_gradient_checkpointing = True,
+        use_gradient_checkpointing = ""unsloth"",
         random_state        = 3407,
         max_seq_length      = 2048, # not used anymore
         use_rslora          = False,
@@ -2678,7 +2678,7 @@ class FastLlamaModel:
     @staticmethod
     def patch_peft_model(
         model,
-        use_gradient_checkpointing = True,
+        use_gradient_checkpointing = ""unsloth"",
     ):
         if os.environ.get(""UNSLOTH_USE_NEW_MODEL"", ""0"") == ""1"":
             return FastBaseModel.patch_peft_model(
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 45b8ca6..664fe10 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -481,10 +481,11 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""include_tokens_per_second""     : False,
         ""include_num_input_tokens_seen"" : False,
         ""auto_find_batch_size""          : True, # Auto /2 batch size
-        ""dataloader_persistent_workers"" : True, # Keeps dataloader in RAM
-        ""dataloader_prefetch_factor""    : 2,
         ""dataloader_pin_memory""         : True,
-        ""dataloader_num_workers""        : 1,
+        # Might fail so disable for now
+        # ""dataloader_persistent_workers"" : True, # Keeps dataloader in RAM
+        # ""dataloader_prefetch_factor""    : 2,
+        # ""dataloader_num_workers""        : 2, # Default is 0 means 1
     }
     for k, v in replacements.items():
         x = f""{k}( = [^,\n]{{1,}})?,\n""
@@ -671,7 +672,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         RLTrainer_source,
         f""trl.trainer.{trainer_file}"",
         imports,
-        overwrite = False,
+        overwrite = True,
     )
 
     # Patch Trainer
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 4d5b4d7..a88385b 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -235,6 +235,59 @@ pass
 RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__prepare_inputs)
 
 
+# Fix incorrect special tokens handling and truncation in older TRL versions
+def grpo_trainer__generate_and_score_completions(function_name, function):
+    if  function_name != ""_generate_and_score_completions"": return function
+
+    # TRL 0.19.0 did skip_special_tokens = True which should be False
+    function = function.replace(
+        ""prompt_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False"",
+        ""prompt_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False"",
+    )
+
+    # Always between max_prompt_length and use_vllm
+    found = re.findall(
+        r""\n(([ ]{8,})if self\.max_prompt_length is not None:.*?""\
+        r""\2if self\.use_vllm:)"",
+        function,
+        flags = re.DOTALL | re.MULTILINE,
+    )
+    if len(found) != 0:
+        replace_part, spacing = found[0]
+        removed_comments = re.sub(r""\#[^\n]{1,}"", """", replace_part)
+        splits = removed_comments.split(""\n"")
+        if sum(re.match(rf""{spacing}[^\s]"", x) is not None for x in splits) == 2 and len(spacing) >= 8:
+
+            new_replacement = \
+            f""""""\n{spacing}if self.max_prompt_length is not None:
+            # If max_prompt_length is set, we trim the prompt to keep only the last `max_prompt_length` tokens.
+            # Then we decode those tokens back into text. We manually remove leading pad tokens from the decoded text,
+            # because we can't use `skip_special_tokens=True` (some special tokens are still needed for generation).
+            prompt_ids = prompt_ids[:, -self.max_prompt_length :]
+            prompt_mask = prompt_mask[:, -self.max_prompt_length :]
+            prompts_text = self.processing_class.batch_decode(
+                prompt_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False
+            )
+            pad_token = self.processing_class.pad_token
+            def strip_leading_tokens(text):
+                while text.startswith(pad_token):
+                    text = text.removeprefix(pad_token)
+                return text
+
+            if pad_token is not None:
+                prompts_text = [
+                    strip_leading_tokens(text) for text in prompts_text
+                ]
+
+        # Generate completions using either vLLM or regular generation
+        if self.use_vllm:""""""
+            function = function.replace(replace_part, new_replacement)
+    pass
+    return function
+pass
+RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__generate_and_score_completions)
+
+
 # Remove _move_model_to_vllm
 def grpo_trainer__move_model_to_vllm(function_name, function):
     if  function_name != ""_move_model_to_vllm"": return function
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 7442f07..5bbf4c7 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -755,6 +755,8 @@ class FastBaseModel:
         os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""0""
         # Must enable returning logits
         os.environ[""UNSLOTH_RETURN_LOGITS""] = ""1""
+        # Turn off skip guards and set stance to default
+        torch.compiler.set_stance(stance = ""default"", skip_guard_eval_unsafe = False)
         return model
     pass
 
@@ -801,6 +803,8 @@ class FastBaseModel:
         pass
         # Can re-enable not returning logits
         os.environ[""UNSLOTH_RETURN_LOGITS""] = ""0""
+        # Turn off skip guards and set stance to default
+        torch.compiler.set_stance(stance = ""default"", skip_guard_eval_unsafe = False)
         return model
     pass
 pass
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index c401393..5785894 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1684,7 +1684,7 @@ extra_eos_tokens = None,
 
         for j in range(1, len(response_part)):
             try_find = re.escape(response_part[:j])
-            try: found = next(re.finditer(""("" + try_find + "").+?\{INPUT\}"", chat_template, flags = re.DOTALL | re.MULTILINE))
+            try: found = next(re.finditer(""("" + try_find + "").+?\\{INPUT\\}"", chat_template, flags = re.DOTALL | re.MULTILINE))
             except: break
         pass
         separator = found.group(1)
@@ -2125,7 +2125,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = ""./model-unsloth.F16.gguf"")
         gguf_tokens = """".join(datas)
 
         # Now extract GGUF tokenization attempt
-        gguf_tokenized = re.findall(""([\d]{1,}) \-\> \'([^\']{1,})\'"", gguf_tokens, flags = re.MULTILINE)
+        gguf_tokenized = re.findall(r""([\d]{1,}) \-\> \'([^\']{1,})\'"", gguf_tokens, flags = re.MULTILINE)
         gguf_tokenized = [(int(x[0]), x[1],) for x in gguf_tokenized]
         input_ids = tokenizer(prompt).input_ids
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 6a3effb..19b09e8 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -589,7 +589,7 @@ if Version(peft_version) < Version(""0.12.0""):
         spaces = len(re.match(r""[\s]{1,}"", source).group(0))
         lines = source.split(""\n"")
         source = ""\n"".join(x[spaces:] for x in lines)
-        source = re.sub(""([^\.])nn\."", r""\1torch.nn."", source)
+        source = re.sub(r""([^\.])nn\."", r""\1torch.nn."", source)
         source = source.replace(""def update_layer"", ""def LoraLayer_update_layer"")
         exec(source, globals())
 
@@ -852,7 +852,7 @@ def patch_linear_scaling(
         scaled_rope_function = scaled_rope_module.__name__,
     )
     rotary_emb = re.findall(
-        ""self.rotary_emb = .+?\)"", function,
+        r""self\.rotary\_emb \= .+?\)"", function,
         flags = re.DOTALL | re.MULTILINE,
     )
     if len(rotary_emb) == 0:
@@ -952,7 +952,7 @@ def patch_llama_rope_scaling(
             (longrope_module if longrope_module is not None else rope_module).__name__
     )
     rotary_emb = re.findall(
-        ""self.rotary_emb = .+?\)"", function,
+        r""self\.rotary\_emb \= .+?\)"", function,
         flags = re.DOTALL | re.MULTILINE,
     )
     if len(rotary_emb) == 0: return None, function
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3e0717a..9515a41 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1888,11 +1888,11 @@ class FastLlamaModel:
         pass
         exec(""from transformers.trainer import ("" + "", "".join(x for x in good_items) + "")"", globals())
 
-        start = re.search('logger\.info\([\""\'].+?Running training', inner_training_loop).span(0)[0]
+        start = re.search(r'logger\.info\([\""\'].+?Running training', inner_training_loop).span(0)[0]
         end = inner_training_loop.find(""\n\n"", start)
         original_debug = inner_training_loop[start:end]
-        spaces = re.search('\n([\s\t]{1,})', original_debug).group(0)[1:]
-        front_spaces = re.match('([\s\t]{1,})', inner_training_loop).group(0)
+        spaces = re.search(r'\n([\s\t]{1,})', original_debug).group(0)[1:]
+        front_spaces = re.match(r'([\s\t]{1,})', inner_training_loop).group(0)
 
         # Cannot use \\ since it will cause a SyntaxWarning in Python 3.12
         # Instead use chr(92) == \\
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 51450aa..31c6394 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -98,9 +98,9 @@ class FastBaseVisionModel:
 
         statistics = \
            f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} vision patching. Transformers: {transformers_version}.\n""\
-           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
-           f""O^O/ \_/ \\    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
-           f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
+           f""   {chr(92)}{chr(92)}   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
+           f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
 
diff --git a/unsloth/save.py b/unsloth/save.py
index eaddfa0..af95de0 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -482,8 +482,8 @@ def unsloth_save_model(
     max_ram = psutil.virtual_memory().available
     sharded_ram_usage = 5 * 1024 * 1024 * 1024
     if type(max_shard_size) is str:
-        gb_found = re.match(""([0-9]{1,})[\s]{0,}GB"", max_shard_size, flags = re.IGNORECASE)
-        mb_found = re.match(""([0-9]{1,})[\s]{0,}MB"", max_shard_size, flags = re.IGNORECASE)
+        gb_found = re.match(r""([0-9]{1,})[\s]{0,}GB"", max_shard_size, flags = re.IGNORECASE)
+        mb_found = re.match(r""([0-9]{1,})[\s]{0,}MB"", max_shard_size, flags = re.IGNORECASE)
         if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024
         elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024
     elif type(max_shard_size) is int:
@@ -1017,9 +1017,9 @@ def save_to_gguf(
 
     print_info = \
         f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
-        f""   \\\   /|    [0] Installing llama.cpp might take 3 minutes.\n""\
-        f""O^O/ \_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n""\
-        f""\        /    [2] Converting GGUF 16bits to {quantization_method} might take 10 minutes each.\n""\
+        f""   {chr(92)}{chr(92)}   /|    [0] Installing llama.cpp might take 3 minutes.\n""\
+        f""O^O/ {chr(92)}_/ {chr(92)}    [1] Converting HF to GGUF 16bits might take 3 minutes.\n""\
+        f""{chr(92)}        /    [2] Converting GGUF 16bits to {quantization_method} might take 10 minutes each.\n""\
         f' ""-____-""     In total, you will have to wait at least 16 minutes.\n'
     print(print_info)
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index fdc0988..2cbe68f 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -47,7 +47,7 @@ huggingface = [
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
-    ""hf-transfer"",
+    ""hf_transfer"",
 ]
 cu118only = [
     ""xformers==0.0.22.post7"",
@@ -178,7 +178,7 @@ colab-new = [
     ""numpy"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
-    ""hf-transfer"",
+    ""hf_transfer"",
 ]
 colab-no-deps = [
     ""accelerate>=0.26.1"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index fe3aa90..d5be8d9 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -332,7 +332,6 @@ def prepare_model_for_kbit_training(
     """"""
 
     # Freeze all parameters except LoRA
-    import re
     with torch.no_grad():
         for name, param in model.named_parameters():
             if "".lora_A."" in name or "".lora_B."" in name or "".lora_magnitude_vector"" in name:
@@ -389,12 +388,14 @@ def patch_tokenizer(model, tokenizer):
         Fixes https://github.com/unslothai/unsloth/issues/5
     """"""
     possible_reserved_tokens = (
+        ""<|finetune_right_pad_id|>"", # Llama-3.1
+        ""<pad>"",                     # Mistral Nemo
         ""<|reserved"",                # Llama-3
         ""<|placeholder"",             # Phi-3
         ""[control"",                  # Mistral type models
-        ""<pad>"",                     # Mistral Nemo
-        ""<|finetune_right_pad_id|>"", # Llama-3.1
     )
+    joiner = ""\1\0=+=\0\1""
+    number_repetitions = 3 - 1 # Number of reserved tokens needed
 
     if model is not None:
         model.config.update({""unsloth_version"" : __version__})
@@ -412,28 +413,69 @@ def patch_tokenizer(model, tokenizer):
     if bad_pad_token:
         # Find a better pad token
         added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
-        possible_pad_token = None
-        n_possible_pad_tokens = 0
-        for added_token in added_tokens[::-1]:
-            if added_token.startswith(possible_reserved_tokens):
-                if possible_pad_token is None: possible_pad_token = added_token
-                n_possible_pad_tokens += 1
-                # We must see at least 3 of the reserved tokens
-                if n_possible_pad_tokens >= 3: break
+        all_added_tokens = joiner.join(added_tokens[::-1])
+        all_added_tokens += joiner
+
+        final_pad_token  = None
+        final_good_match = False
+
+        for possible_reserved_token in possible_reserved_tokens:
+            possible_reserved_token = re.escape(possible_reserved_token)
+            found = re.finditer(f""{possible_reserved_token}"", all_added_tokens)
+            first_match = None
+            good_match  = False
+            for j, x in enumerate(found):
+                if j == 0: first_match = x
+                if j >= number_repetitions:
+                    good_match = True
+                    break
+                pass
+            pass
+
+            if first_match is None: continue
+
+            # If it ends with |> or > etc, then set it as a good pad token!
+            start = first_match.span(0)[0]
+            possible_pad_token = first_match.group(0)
+            end = all_added_tokens.find(joiner, start)
+            first_match = all_added_tokens[start:end]
+
+            if first_match is not None:
+                good_match = possible_pad_token.endswith(("">"", ""|>"", ""]"", "")""))
+            pass
+            possible_pad_token = first_match
+
+            # Replace current pad token if another exact match is found
+            if not final_good_match and good_match:
+                final_good_match = True
+                final_pad_token = possible_pad_token
+                break
+            else:
+                final_good_match = False
+                final_pad_token = possible_pad_token
             pass
         pass
-        if n_possible_pad_tokens < 3: possible_pad_token = None
+        possible_pad_token = final_pad_token
 
-        if possible_pad_token is None:
-            # Try unk_token
+        # Try unk_token
+        if possible_pad_token is None and hasattr(tokenizer, ""unk_token""):
             possible_pad_token = tokenizer.unk_token
         pass
 
+        # Check pad token's id must be less than vocab size
+        if possible_pad_token is not None:
+            check_pad_token = tokenizer(possible_pad_token, add_special_tokens = False).input_ids
+            if len(check_pad_token) != 1:
+                possible_pad_token = None
+            if check_pad_token[0] >= config.vocab_size:
+                possible_pad_token = None
+        pass
+
         if possible_pad_token is None:
             # Failure to find a good replacement!! We shall manually add one!
             new_pad_token = ""<|PAD_TOKEN|>""
             while new_pad_token in tokenizer.get_vocab():
-                new_pad_token += ""#""
+                new_pad_token = f""<{new_pad_token}>""
             pass
             possible_pad_token = new_pad_token
         pass
@@ -447,11 +489,16 @@ def patch_tokenizer(model, tokenizer):
         tokenizer.add_special_tokens({""pad_token"" : possible_pad_token})
         tokenizer.pad_token = possible_pad_token
         if model is not None:
-            config = model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
+            model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
+            model.generation_config.update(pad_token_id = tokenizer.pad_token_id)
     else:
         if model is not None:
             if model.config.pad_token_id is None:
-                config = model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
+                model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
+                model.generation_config.update(pad_token_id = tokenizer.pad_token_id)
+        pass
+    pass
+    model.generation_config.update(max_length = model.config.max_position_embeddings)
     return model, tokenizer
 pass
 
@@ -462,7 +509,6 @@ pass
 from peft import __version__ as peft_version
 if Version(peft_version) < Version(""0.12.0""):
     from peft.tuners.lora.layer import LoraLayer
-    import inspect, re
     try:
         source = inspect.getsource(LoraLayer.update_layer)
         text = ""if weight is not None:\n""
@@ -688,7 +734,6 @@ pass
 from transformers.utils.quantization_config import BitsAndBytesConfig, QuantizationMethod
 from inspect import getsource
 from accelerate.utils.dataclasses import DistributedType
-import re
 BitsAndBytesConfig__init__ = getsource(BitsAndBytesConfig.__init__)
 BitsAndBytesConfig__init__ = re.sub(
     r""if[\s]{1,}kwargs\:[\s]{1,}.+?\n"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index cec743e..e300e07 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1397,6 +1397,7 @@ class FastLlamaModel:
             padding_side      = ""right"",
             token             = token,
             trust_remote_code = trust_remote_code,
+            fix_tokenizer     = fix_tokenizer,
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 8474c2c..c67f82c 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -454,13 +454,14 @@ def fix_sentencepiece_gguf(saved_location):
 pass
 
 
-def load_correct_tokenizer(
+def _load_correct_tokenizer(
     tokenizer_name,
     model_max_length = None,
     padding_side = ""right"",
     token = None,
     trust_remote_code = False,
     cache_dir = ""huggingface_tokenizers_cache"",
+    fix_tokenizer = True,
 ):
     if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:
         cache_dir = cache_dir
@@ -501,7 +502,10 @@ def load_correct_tokenizer(
         cache_dir         = cache_dir,
     )
 
-    if tokenizer_name in IGNORED_TOKENIZER_NAMES:
+    if not fix_tokenizer or tokenizer_name in IGNORED_TOKENIZER_NAMES:
+        return fast_tokenizer
+    # Ignore Mistral ones - they're a bit weird to handle!
+    elif ""mistral"" in tokenizer_name.lower():
         return fast_tokenizer
     elif slow_tokenizer is not None:
         if hasattr(fast_tokenizer, ""add_bos_token"") and hasattr(slow_tokenizer, ""add_bos_token""):
@@ -522,6 +526,113 @@ def load_correct_tokenizer(
 pass
 
 
+def load_correct_tokenizer(
+    tokenizer_name,
+    model_max_length = None,
+    padding_side = ""right"",
+    token = None,
+    trust_remote_code = False,
+    cache_dir = ""huggingface_tokenizers_cache"",
+    fix_tokenizer = True,
+):
+    tokenizer = _load_correct_tokenizer(
+        tokenizer_name = tokenizer_name,
+        model_max_length = model_max_length,
+        padding_side = padding_side,
+        token = token,
+        trust_remote_code = trust_remote_code,
+        cache_dir = cache_dir,
+        fix_tokenizer = fix_tokenizer,
+    )
+
+    ### 1. Fixup tokenizer's chat_template
+    old_chat_template = getattr(tokenizer, ""chat_template"", None)
+
+    # Ignore mistral type models since they don't have a add_generation_prompt
+    if ""mistral"" in str(getattr(tokenizer, ""name_or_path"", """")).lower():
+        chat_template = old_chat_template
+
+    # Also check Llama-2 old style models
+    elif old_chat_template is not None and \
+        ""[/INST]"" in old_chat_template and ""[INST]"" in old_chat_template and \
+        ""bos_token"" in old_chat_template and ""eos_token"" in old_chat_template:
+
+        chat_template = old_chat_template
+
+    else:
+        chat_template = fix_chat_template(tokenizer)
+        if old_chat_template is not None and chat_template is None:
+            raise RuntimeError(
+                ""Unsloth: Fixing chat template failed - please file a report immediately!""
+            )
+        pass
+    pass
+
+    tokenizer.chat_template = chat_template
+    return tokenizer
+pass
+
+
+def _fix_chat_template(chat_template):
+    endfor = ""{% endfor %}""
+    where = chat_template.find(endfor)
+    if where == -1: return chat_template
+
+    after_endfor = chat_template[where + len(endfor):]
+
+    if ""{% if"" not in after_endfor and ""{% set "" not in after_endfor and \
+        after_endfor.startswith(""{{"") and after_endfor.endswith(""}}"") and \
+        after_endfor.count(""{{"") == 1 and after_endfor.count(""}}"") == 1:
+
+        after_endfor = ""{% if add_generation_prompt %}"" + after_endfor + ""{% endif %}""
+
+        chat_template = chat_template[:where + len(endfor)] + after_endfor
+    pass
+    return chat_template
+pass
+
+
+def fix_chat_template(tokenizer):
+    chat_template = getattr(tokenizer, ""chat_template"", None)
+    if chat_template is None: return None
+
+    ### 1. Check if add_generation_prompt works
+    messages = [
+        {""role"": ""user"", ""content"": ""Who are you?""},
+    ]
+    no  = tokenizer.apply_chat_template(messages, add_generation_prompt = False, tokenize = False)
+    yes = tokenizer.apply_chat_template(messages, add_generation_prompt =  True, tokenize = False)
+
+    if no == yes:
+        # SAME?! That's not good! We check for add_generation_prompt
+        if ""{% if add_generation_prompt %}"" not in chat_template:
+            # Try fixing it by adding it
+            new_chat_template = _fix_chat_template(chat_template)
+            if ""{% if add_generation_prompt %}"" not in new_chat_template:
+                raise RuntimeError(
+                    f""Unsloth: The tokenizer `{tokenizer.name_or_path}`\n""\
+                    ""does not have a {% if add_generation_prompt %} for generation purposes.\n""\
+                    ""Please file a bug report immediately - thanks!""
+                )
+            else:
+                logger.warning_once(
+                    ""Unsloth: We successfully patched the tokenizer to add a {% if add_generation_prompt %} to the chat_template.\n""\
+                    ""This is not a bug, but please notify the Unsloth maintainers - thanks!""
+                )
+                chat_template = new_chat_template
+            pass
+        else:
+            raise RuntimeError(
+                f""Unsloth: The tokenizer `{tokenizer.name_or_path}`\n""\
+                ""has a {% if add_generation_prompt %} for generation purposes, but wasn't provided correctly.\n""\
+                ""Please file a bug report immediately - thanks!""
+            )
+        pass
+    pass
+    return chat_template
+pass
+
+
 def check_tokenizer(
     model,
     tokenizer,
"
"diff --git a/README.md b/README.md
index 6df6616..07bcf2c 100644
--- a/README.md
+++ b/README.md
@@ -20,23 +20,25 @@
 
 All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.
 
-| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
-|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
-| **Llama 3 (8B)**      | [ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
-| **Mistral (7B)**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |
-| **Gemma (7B)**      | [ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |
-| **Llama 3 (8B)** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook) | 5x faster\* | 73% less |
-| **ORPO**     | [ Start on Colab](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |
-| **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
+| Unsloth supports | Free Notebooks | Performance | Memory use |
+|-----------|---------|--------|----------|
+| **Llama 3 (8B)**      | [ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
+| **Mistral (7B)**    | [ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |
+| **Gemma (7B)**      | [ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |
+| **ORPO**     | [ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |
+| **DPO Zephyr**     | [ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
+| **Phi-3 (3.8B)** | [ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |
+| **TinyLlama**  | [ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |
 
 - Benchmarking compared to FA2 + Hugging Face combined.
-- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
-- \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.
+- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
+- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for Llama-3. And ChatML for [Mistral 7b](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing).
+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.
 
 ##  Unsloth.ai News
 -  NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).
 -  NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!
+-  NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!
 -  NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you're using our notebooks. To enable, simply change 1 line:
 ```python
 model = FastLanguageModel.get_peft_model(
@@ -45,7 +47,7 @@ model = FastLanguageModel.get_peft_model(
 )
 ```
 -  [CodeGemma](https://colab.research.google.com/drive/19lwcRk_ZQ_ZtX-qzFP3qZBBHZNcMD1hh?usp=sharing) now works along with [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) and [Gemma 2b](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)
--  [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models
+-  [2x faster inference](https://colab.research.google.com/drive/1aqlNQi7MMJbynFDyOQteD2t0yVfjb9Zh?usp=sharing) added for all our models
 
 ##  Links and Resources
 | Type                            | Links                               |
@@ -190,6 +192,7 @@ fourbit_models = [
     ""unsloth/gemma-2b-bnb-4bit"",
     ""unsloth/gemma-2b-it-bnb-4bit"", # Instruct version of Gemma 2b
     ""unsloth/llama-3-8b-bnb-4bit"", # [NEW] 15 Trillion token Llama-3
+    ""unsloth/Phi-3-mini-4k-instruct-bnb-4bit"",
 ] # More models at https://huggingface.co/unsloth
 
 model, tokenizer = FastLanguageModel.from_pretrained(
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 769cbff..b4fbe57 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -140,6 +140,10 @@ __INT_TO_FLOAT_MAPPER = \
     ""unsloth/llama-3-70b-Instruct-bnb-4bit"" : (
         ""meta-llama/Meta-Llama-3-70B-Instruct"",
     ),
+    ""unsloth/Phi-3-mini-4k-instruct-bnb-4bit"" : (
+        ""unsloth/Phi-3-mini-4k-instruct"",
+        ""microsoft/Phi-3-mini-4k-instruct"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
"
"diff --git a/pyproject.toml b/pyproject.toml
index 01636e7..7dfca63 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.5"",
+    ""unsloth_zoo>=2025.3.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.5"",
+    ""unsloth_zoo>=2025.3.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 9ed356d..38453f3 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.5""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.7""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7ac35d7..37c69ef 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.7""
+__version__ = ""2025.3.8""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index e0df218..725e9b3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -618,7 +618,11 @@ def patch_linear_scaling(
         f""from {model_filepath} import logger, ""\
         f""{model_name.title()}Attention, {model_name.title()}Config""
 
-    function = inspect.getsource(attention_module.__init__)
+    try:
+        function = inspect.getsource(attention_module.__init__)
+    except:
+        # Most likely already patched!
+        return None, None
     where = function.find(""def"")
     function = function.split(""\n"")
     function = ""\n"".join(x[where:] for x in function)
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 4d3db8d..c47e65e 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -281,8 +281,10 @@ class FastGemmaModel(FastLlamaModel):
             scaled_rope_module = GemmaFixedLinearScalingRotaryEmbedding,
             attention_module   = GemmaAttention,
         )
-        exec(function, globals())
-        GemmaAttention.__init__      = eval(init_name)
+        if init_name is not None:
+            exec(function, globals())
+            GemmaAttention.__init__  = eval(init_name)
+        pass
         GemmaAttention      .forward = LlamaAttention_fast_forward
         GemmaSdpaAttention  .forward = LlamaAttention_fast_forward
         GemmaFlashAttention2.forward = LlamaAttention_fast_forward
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 4a1420f..fda7853 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -436,8 +436,10 @@ class FastGemma2Model(FastLlamaModel):
             scaled_rope_module = GemmaFixedLinearScalingRotaryEmbedding,
             attention_module   = Gemma2Attention,
         )
-        exec(function, globals())
-        Gemma2Attention.__init__      = eval(init_name)
+        if init_name is not None:
+            exec(function, globals())
+            Gemma2Attention.__init__  = eval(init_name)
+        pass
         Gemma2Attention      .forward = Gemma2Attention_fast_forward
         Gemma2SdpaAttention  .forward = Gemma2Attention_fast_forward
         Gemma2FlashAttention2.forward = Gemma2Attention_fast_forward
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 28f664c..6eb3fcc 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -277,8 +277,10 @@ class FastMistralModel(FastLlamaModel):
             scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
             attention_module   = MistralAttention,
         )
-        exec(function, globals())
-        MistralAttention.__init__      = eval(init_name)
+        if init_name is not None:
+            exec(function, globals())
+            MistralAttention.__init__  = eval(init_name)
+        pass
         MistralAttention      .forward = MistralAttention_fast_forward
         MistralSdpaAttention  .forward = MistralAttention_fast_forward
         MistralFlashAttention2.forward = MistralAttention_fast_forward
diff --git a/unsloth/models/qwen2.py b/unsloth/models/qwen2.py
index dcd05af..82de195 100644
--- a/unsloth/models/qwen2.py
+++ b/unsloth/models/qwen2.py
@@ -45,8 +45,10 @@ class FastQwen2Model(FastLlamaModel):
             scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
             attention_module   = Qwen2Attention,
         )
-        exec(function, globals())
-        Qwen2Attention.__init__      = eval(init_name)
+        if init_name is not None:
+            exec(function, globals())
+            Qwen2Attention.__init__  = eval(init_name)
+        pass
         Qwen2Attention      .forward = LlamaAttention_fast_forward
         Qwen2SdpaAttention  .forward = LlamaAttention_fast_forward
         Qwen2FlashAttention2.forward = LlamaAttention_fast_forward
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f310d10..73c78ba 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -619,7 +619,7 @@ class FastLlamaModel:
         token = None,
         device_map = ""sequential"",
         rope_scaling = None,
-        check_tokenizer = True,
+        fix_tokenizer = True,
     ):
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
@@ -704,7 +704,7 @@ class FastLlamaModel:
         internal_model.max_seq_length = max_position_embeddings
 
         # We check the tokenizer first for errors
-        if check_tokenizer:
+        if fix_tokenizer:
             tokenizer = check_tokenizer(
                 model = model,
                 tokenizer = tokenizer,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index eadf026..48200c3 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -44,7 +44,7 @@ class FastLanguageModel(FastLlamaModel):
         token = None,
         device_map = ""sequential"",
         rope_scaling = None,
-        check_tokenizer = True,
+        fix_tokenizer = True,
         *args, **kwargs,
     ):
         if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:
@@ -84,7 +84,7 @@ class FastLanguageModel(FastLlamaModel):
             token = token,
             device_map = device_map,
             rope_scaling = rope_scaling,
-            check_tokenizer = check_tokenizer,
+            fix_tokenizer = fix_tokenizer,
             *args, **kwargs,
         )
     pass
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index e15a3ae..e48a982 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -263,7 +263,7 @@ class FastMistralModel(FastLlamaModel):
         token = None,
         device_map = ""sequential"",
         rope_scaling = None, # Mistral does not support RoPE scaling
-        check_tokenizer = True,
+        fix_tokenizer = True,
     ): 
         if rope_scaling is not None:
             logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
@@ -333,7 +333,7 @@ class FastMistralModel(FastLlamaModel):
         internal_model.max_seq_length = max_position_embeddings
 
         # We check the tokenizer first for errors
-        if check_tokenizer:
+        if fix_tokenizer:
             tokenizer = check_tokenizer(
                 model = model,
                 tokenizer = tokenizer,
"
"diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
index e67c9f1..497a357 100644
--- a/unsloth/models/granite.py
+++ b/unsloth/models/granite.py
@@ -182,6 +182,11 @@ def GraniteDecoderLayer_fast_forward(
     position_embeddings:  Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ):
+    residual_multiplier = \
+        self.residual_multiplier \
+        if hasattr(self, ""residual_multiplier"") else \
+        self.config.residual_multiplier
+
     if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
@@ -197,13 +202,13 @@ def GraniteDecoderLayer_fast_forward(
             position_embeddings = position_embeddings,
             _flag_for_generation=self._flag_for_generation,
         )
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
         hidden_states = fast_swiglu_inference(self.mlp, hidden_states)
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
     else:
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
@@ -218,13 +223,13 @@ def GraniteDecoderLayer_fast_forward(
             padding_mask=padding_mask,
             position_embeddings = position_embeddings,
         )
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
         hidden_states = self.mlp(hidden_states)
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
     pass
 
     outputs = (hidden_states,)
@@ -370,6 +375,10 @@ def GraniteModel_fast_forward_inference(
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
     hidden_states *= self.model.embedding_multiplier
+    residual_multiplier = \
+        self.residual_multiplier \
+        if hasattr(self, ""residual_multiplier"") else \
+        self.config.residual_multiplier
 
     bsz, q_len, hd = hidden_states.shape
     seq_len = past_key_values[0][0].shape[-2]
@@ -401,12 +410,12 @@ def GraniteModel_fast_forward_inference(
             position_embeddings = position_embeddings,
         )
 
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         next_decoder_cache.append(present_key_value)
     pass
"
"diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 5eb9b8f..8da152b 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -138,7 +138,7 @@ def get_lora_parameters_bias(proj):
 
     # if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
     if getattr(proj, ""disable_adapters"", True) or proj.merged:
-        return W, getattr(W, ""quant_state"", None), None, None, None, bias
+        return W, getattr(W, ""quant_state"", None), None, None, None, base_layer.bias
     pass
 
     adapter = getattr(proj, ""active_adapters"", None)
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 36e4f51..43828a3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.2""
+__version__ = ""2025.3.3""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index c945149..128e0fd 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1967,29 +1967,29 @@ class FastLlamaModel:
                 if ""embed_tokens"" in new_target_modules:
                     print(""Unsloth: Training embed_tokens in mixed precision to save VRAM"")
 
-                    dtype = model.model.model.embed_tokens.modules_to_save.default.weight.dtype
-                    model.model.model.embed_tokens.modules_to_save.default\
+                    dtype = model.base_model.model.embed_tokens.modules_to_save.default.weight.dtype
+                    model.base_model.model.embed_tokens.modules_to_save.default\
                         .to(device = ""cuda:0"", dtype=(dtype if (dtype != torch.float16) else torch.float32), non_blocking = True)
-                    model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
+                    model.base_model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old embed_tokens to CPU - should be disk!
-                    model.model.model.embed_tokens.original_module\
+                    model.base_model.model.embed_tokens.original_module\
                         .to(device = ""cpu"", non_blocking = True)
-                    model.model.model.embed_tokens.original_module.requires_grad_(False)
+                    model.base_model.model.embed_tokens.original_module.requires_grad_(False)
                 pass
 
                 if ""lm_head"" in new_target_modules:
                     print(""Unsloth: Training lm_head in mixed precision to save VRAM"")
 
-                    dtype = model.model.model.lm_head.modules_to_save.default.weight.dtype
-                    model.model.lm_head.modules_to_save.default\
+                    dtype = model.base_model.model.lm_head.modules_to_save.default.weight.dtype
+                    model.base_model.lm_head.modules_to_save.default\
                         .to(device = ""cuda:0"", dtype=(dtype if (dtype != torch.float16) else torch.float32), non_blocking = True)
-                    model.model.lm_head.modules_to_save.default.requires_grad_(True)
+                    model.base_model.lm_head.modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old lm_head to CPU - should be disk!
-                    model.model.lm_head.original_module\
+                    model.base_model.lm_head.original_module\
                         .to(device = ""cpu"", non_blocking = True)
-                    model.model.lm_head.original_module.requires_grad_(False)
+                    model.base_model.lm_head.original_module.requires_grad_(False)
                 pass
 
                 return model
"
"diff --git a/unsloth-cli.py b/unsloth-cli.py
new file mode 100644
index 0000000..ddb0ac8
--- /dev/null
+++ b/unsloth-cli.py
@@ -0,0 +1,221 @@
+#!/usr/bin/env python3
+
+""""""
+ Starter Script for Fine-Tuning FastLanguageModel with Unsloth
+
+This script is designed as a starting point for fine-tuning your models using unsloth.
+It includes configurable options for model loading, PEFT parameters, training arguments, 
+and model saving/pushing functionalities.
+
+You will likely want to customize this script to suit your specific use case 
+and requirements.
+
+Here are a few suggestions for customization:
+    - Modify the dataset loading and preprocessing steps to match your data.
+    - Customize the model saving and pushing configurations.
+
+Usage: (most of the options have valid default values this is an extended example for demonstration purposes)
+    python unsloth-cli.py --model_name ""unsloth/llama-3-8b"" --max_seq_length 8192 --dtype None --load_in_4bit \
+    --r 64 --lora_alpha 32 --lora_dropout 0.1 --bias ""none"" --use_gradient_checkpointing ""unsloth"" \
+    --random_state 3407 --use_rslora --per_device_train_batch_size 4 --gradient_accumulation_steps 8 \
+    --warmup_steps 5 --max_steps 400 --learning_rate 2e-6 --logging_steps 1 --optim ""adamw_8bit"" \
+    --weight_decay 0.005 --lr_scheduler_type ""linear"" --seed 3407 --output_dir ""outputs"" \
+    --report_to ""tensorboard"" --save_model --save_path ""model"" --quantization_method ""f16"" \
+    --push_model --hub_path ""hf/model"" --hub_token ""your_hf_token""
+
+To see a full list of configurable options, use:
+    python unsloth-cli.py --help
+
+Happy fine-tuning!
+""""""
+
+import argparse
+
+def run(args):
+    import torch
+    from unsloth import FastLanguageModel
+    from datasets import load_dataset
+    from trl import SFTTrainer
+    from transformers import TrainingArguments
+    from unsloth import is_bfloat16_supported
+    import logging
+    logging.getLogger('hf-to-gguf').setLevel(logging.WARNING)
+
+    # Load model and tokenizer
+    model, tokenizer = FastLanguageModel.from_pretrained(
+        model_name=args.model_name,
+        max_seq_length=args.max_seq_length,
+        dtype=args.dtype,
+        load_in_4bit=args.load_in_4bit,
+    )
+
+    # Configure PEFT model
+    model = FastLanguageModel.get_peft_model(
+        model,
+        r=args.r,
+        target_modules=[""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                        ""gate_proj"", ""up_proj"", ""down_proj""],
+        lora_alpha=args.lora_alpha,
+        lora_dropout=args.lora_dropout,
+        bias=args.bias,
+        use_gradient_checkpointing=args.use_gradient_checkpointing,
+        random_state=args.random_state,
+        use_rslora=args.use_rslora,
+        loftq_config=args.loftq_config,
+    )
+
+    alpaca_prompt = """"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
+
+    ### Instruction:
+    {}
+
+    ### Input:
+    {}
+
+    ### Response:
+    {}""""""
+
+    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN
+    def formatting_prompts_func(examples):
+        instructions = examples[""instruction""]
+        inputs       = examples[""input""]
+        outputs      = examples[""output""]
+        texts = []
+        for instruction, input, output in zip(instructions, inputs, outputs):
+            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
+            texts.append(text)
+        return {""text"": texts}
+
+    # Load and format dataset
+    dataset = load_dataset(args.dataset, split=""train"")
+    dataset = dataset.map(formatting_prompts_func, batched=True)
+    print(""Data is formatted and ready!"")
+
+    # Configure training arguments
+    training_args = TrainingArguments(
+        per_device_train_batch_size=args.per_device_train_batch_size,
+        gradient_accumulation_steps=args.gradient_accumulation_steps,
+        warmup_steps=args.warmup_steps,
+        max_steps=args.max_steps,
+        learning_rate=args.learning_rate,
+        fp16=not is_bfloat16_supported(),
+        bf16=is_bfloat16_supported(),
+        logging_steps=args.logging_steps,
+        optim=args.optim,
+        weight_decay=args.weight_decay,
+        lr_scheduler_type=args.lr_scheduler_type,
+        seed=args.seed,
+        output_dir=args.output_dir,
+        report_to=args.report_to,
+    )
+
+    # Initialize trainer
+    trainer = SFTTrainer(
+        model=model,
+        tokenizer=tokenizer,
+        train_dataset=dataset,
+        dataset_text_field=""text"",
+        max_seq_length=args.max_seq_length,
+        dataset_num_proc=2,
+        packing=False,
+        args=training_args,
+    )
+
+    # Train model
+    trainer_stats = trainer.train()
+
+    # Save model
+    if args.save_model:
+        # if args.quantization_method is a list, we will save the model for each quantization method
+        if args.save_gguf:
+            if isinstance(args.quantization, list):
+                for quantization_method in args.quantization:
+                    print(f""Saving model with quantization method: {quantization_method}"")
+                    model.save_pretrained_gguf(
+                        args.save_path,
+                        tokenizer,
+                        quantization_method=quantization_method,
+                    )
+                    if args.push_model:
+                        model.push_to_hub_gguf(
+                            hub_path=args.hub_path,
+                            hub_token=args.hub_token,
+                            quantization_method=quantization_method,
+                        )
+            else:
+                print(f""Saving model with quantization method: {args.quantization}"")
+                model.save_pretrained_gguf(args.save_path, tokenizer, quantization_method=args.quantization)
+                if args.push_model:
+                    model.push_to_hub_gguf(
+                        hub_path=args.hub_path,
+                        hub_token=args.hub_token,
+                        quantization_method=quantization_method,
+                    )
+        else:
+            model.save_pretrained_merged(args.save_path, tokenizer, args.save_method)
+            if args.push_model:
+                model.push_to_hub_merged(args.save_path, tokenizer, args.hub_token)
+    else:
+        print(""Warning: The model is not saved!"")
+
+
+if __name__ == ""__main__"":
+
+    # Define argument parser
+    parser = argparse.ArgumentParser(description="" Fine-tune your llm faster using unsloth!"")
+
+    model_group = parser.add_argument_group("" Model Options"")
+    model_group.add_argument('--model_name', type=str, default=""unsloth/llama-3-8b"", help=""Model name to load"")
+    model_group.add_argument('--max_seq_length', type=int, default=2048, help=""Maximum sequence length, default is 2048. We auto support RoPE Scaling internally!"")
+    model_group.add_argument('--dtype', type=str, default=None, help=""Data type for model (None for auto detection)"")
+    model_group.add_argument('--load_in_4bit', action='store_true', help=""Use 4bit quantization to reduce memory usage"")
+    model_group.add_argument('--dataset', type=str, default=""yahma/alpaca-cleaned"", help=""Huggingface dataset to use for training"")
+
+    lora_group = parser.add_argument_group("" LoRA Options"", ""These options are used to configure the LoRA model."")
+    lora_group.add_argument('--r', type=int, default=16, help=""Rank for Lora model, default is 16.  (common values: 8, 16, 32, 64, 128)"")
+    lora_group.add_argument('--lora_alpha', type=int, default=16, help=""LoRA alpha parameter, default is 16. (common values: 8, 16, 32, 64, 128)"")
+    lora_group.add_argument('--lora_dropout', type=float, default=0, help=""LoRA dropout rate, default is 0.0 which is optimized."")
+    lora_group.add_argument('--bias', type=str, default=""none"", help=""Bias setting for LoRA"")
+    lora_group.add_argument('--use_gradient_checkpointing', type=str, default=""unsloth"", help=""Use gradient checkpointing"")
+    lora_group.add_argument('--random_state', type=int, default=3407, help=""Random state for reproducibility, default is 3407."")
+    lora_group.add_argument('--use_rslora', action='store_true', help=""Use rank stabilized LoRA"")
+    lora_group.add_argument('--loftq_config', type=str, default=None, help=""Configuration for LoftQ"")
+
+   
+    training_group = parser.add_argument_group("" Training Options"")
+    training_group.add_argument('--per_device_train_batch_size', type=int, default=2, help=""Batch size per device during training, default is 2."")
+    training_group.add_argument('--gradient_accumulation_steps', type=int, default=4, help=""Number of gradient accumulation steps, default is 4."")
+    training_group.add_argument('--warmup_steps', type=int, default=5, help=""Number of warmup steps, default is 5."")
+    training_group.add_argument('--max_steps', type=int, default=400, help=""Maximum number of training steps."")
+    training_group.add_argument('--learning_rate', type=float, default=2e-4, help=""Learning rate, default is 2e-4."")
+    training_group.add_argument('--optim', type=str, default=""adamw_8bit"", help=""Optimizer type."")
+    training_group.add_argument('--weight_decay', type=float, default=0.01, help=""Weight decay, default is 0.01."")
+    training_group.add_argument('--lr_scheduler_type', type=str, default=""linear"", help=""Learning rate scheduler type, default is 'linear'."")
+    training_group.add_argument('--seed', type=int, default=3407, help=""Seed for reproducibility, default is 3407."")
+    
+
+    # Report/Logging arguments
+    report_group = parser.add_argument_group("" Report Options"")
+    report_group.add_argument('--report_to', type=str, default=""tensorboard"",
+        choices=[""azure_ml"", ""clearml"", ""codecarbon"", ""comet_ml"", ""dagshub"", ""dvclive"", ""flyte"", ""mlflow"", ""neptune"", ""tensorboard"", ""wandb"", ""all"", ""none""],
+        help=""The list of integrations to report the results and logs to. Supported platforms are: \n\t\t 'azure_ml', 'clearml', 'codecarbon', 'comet_ml', 'dagshub', 'dvclive', 'flyte', 'mlflow', 'neptune', 'tensorboard', and 'wandb'. Use 'all' to report to all integrations installed, 'none' for no integrations."")
+    report_group.add_argument('--logging_steps', type=int, default=1, help=""Logging steps, default is 1"")
+
+    # Saving and pushing arguments
+    save_group = parser.add_argument_group(' Save Model Options')
+    save_group.add_argument('--output_dir', type=str, default=""outputs"", help=""Output directory"")
+    save_group.add_argument('--save_model', action='store_true', help=""Save the model after training"")
+    save_group.add_argument('--save_method', type=str, default=""merged_16bit"", choices=[""merged_16bit"", ""merged_4bit"", ""lora""], help=""Save method for the model, default is 'merged_16bit'"")
+    save_group.add_argument('--save_gguf', action='store_true', help=""Convert the model to GGUF after training"")
+    save_group.add_argument('--save_path', type=str, default=""model"", help=""Path to save the model"")
+    save_group.add_argument('--quantization', type=str, default=""q8_0"", nargs=""+"",
+        help=""Quantization method for saving the model. common values ('f16', 'q4_k_m', 'q8_0'), Check our wiki for all quantization methods https://github.com/unslothai/unsloth/wiki#saving-to-gguf "")
+
+    push_group = parser.add_argument_group(' Push Model Options')
+    push_group.add_argument('--push_model', action='store_true', help=""Push the model to Hugging Face hub after training"")
+    push_group.add_argument('--push_gguf', action='store_true', help=""Push the model as GGUF to Hugging Face hub after training"")
+    push_group.add_argument('--hub_path', type=str, default=""hf/model"", help=""Path on Hugging Face hub to push the model"")
+    push_group.add_argument('--hub_token', type=str, help=""Token for pushing the model to Hugging Face hub"")
+
+    args = parser.parse_args()
+    run(args)
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index a2a02d7..7b6da3e 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -17,9 +17,11 @@ __all__ = [
     ""test_chat_templates"",
     ""test_hf_gguf_equivalence"",
     ""remove_special_tokens"",
-    ""standardize_dataset"",
 
-    ""construct_chat_template"",
+    ""to_sharegpt"",
+    ""standardize_sharegpt"",
+    ""apply_chat_template"",
+
     ""test_construct_chat_template"",
     ""create_ollama_modelfile"",
 ]
@@ -32,6 +34,7 @@ import os
 import shutil
 from .tokenizer_utils import *
 from .models._utils import patch_tokenizer
+import re
 
 CHAT_TEMPLATES = {}
 
@@ -713,21 +716,209 @@ def remove_special_tokens(tokenizer, prompt):
 pass
 
 
-def standardize_dataset(
+def _parse_combined_prompt(combined_prompt, dataset):
+    # Find {...}
+    possible_columns = re.findall(r""\{(.+?)\}"", combined_prompt)
+    dataset_columns = set(dataset.column_names)
+    for column in possible_columns:
+        if column not in dataset_columns:
+            raise KeyError(
+                f""Unsloth: Your prompt includes '{column}' but this does not exist in the dataset. ""\
+                f""Only allowed columns are {list(dataset_columns)}""
+            )
+        pass
+    pass
+
+    # Find [[...]]
+    optional_prompts = list(re.finditer(r""\[\[.+?\]\]"", combined_prompt, flags = re.DOTALL | re.MULTILINE))
+    optional_prompts = [(x.span(), x.group(0)) for x in optional_prompts]
+
+    final_optional_prompts = []
+    if len(optional_prompts) != 0:
+        # Add left
+        left = optional_prompts[0]
+        l = left[0][0]
+        if l != 0: final_optional_prompts.append(combined_prompt[:l])
+
+        # Add in between
+        for left, right in zip(optional_prompts[:-1], optional_prompts[1:]):
+            l, r = left[0][-1], right[0][0]
+            final_optional_prompts.append(left)
+            if l != r: final_optional_prompts.append(combined_prompt[l : r])
+        pass
+        final_optional_prompts.append(optional_prompts[-1])
+
+        # Add right
+        right = optional_prompts[-1]
+        r = right[0][1]
+        if r != len(combined_prompt): final_optional_prompts.append(combined_prompt[r:])
+    else:
+        # Just add in the entire string
+        final_optional_prompts.append(combined_prompt)
+    pass
+
+    check_combined = """".join(x if type(x) is str else x[1] for x in final_optional_prompts)
+    assert(combined_prompt == check_combined)
+
+    return possible_columns, final_optional_prompts
+pass
+
+
+def _create_formatter(possible_columns, final_optional_prompts, user_column_name):
+    # Start final prompt!
+    function = [""def __combined_prompt_processor__(examples):""]
+    columns = list(set(possible_columns))
+    for column in columns:
+        function.append(f""{' '*4}{column}__ = examples['{column}']"")
+    function.append(f""{' '*4}texts = []"")
+    function.append(f""{' '*4}for ({', '.join(columns)}) in zip({', '.join(f'{x}__' for x in columns)}):"")
+
+    # Add optional tags as well!
+    final_prompt = """"
+    formatter = []
+
+    for j, optional_prompt in enumerate(final_optional_prompts):
+        if type(optional_prompt) is str:
+            columns = re.findall(r""\{(.+?)\}"", optional_prompt)
+            formatter += columns
+            # Must escape \n \r
+            final_prompt += optional_prompt.encode(""unicode-escape"").decode(""utf-8"")
+        else:
+            where, prompt = optional_prompt
+            # Strip [[...]]
+            # Must escape \n \r
+            prompt = prompt[2:-2].encode(""unicode-escape"").decode(""utf-8"")
+            columns = re.findall(r""\{(.+?)\}"", prompt)
+            x = f""__optional_{j}__""
+            prompt = f""{' '*8}{x} = '{prompt}'.format({', '.join(f'{x} = {x}' for x in columns)}) if input else ''""
+            function.append(prompt)
+            formatter.append(x)
+            final_prompt += ""{"" + x + ""}""
+        pass
+    pass
+
+    function.insert(1, f""{' '*4}__combined_prompt__ = '{final_prompt}'"")
+    function.append(f""{' '*8}texts.append(""\
+                    f""__combined_prompt__.format({', '.join(f'{x} = {x}' for x in formatter)}))"")
+    function.append(f""{' '*4}return "" + ""{ "" + f""'{user_column_name}' : texts"" + "" }"")
+    return ""\n"".join(function)
+pass
+
+
+def to_sharegpt(
+    dataset,
+    merged_prompt = """",
+    merged_column_name = ""instruction"",
+    output_column_name = ""output"",
+    remove_unsued_columns = True,
+    conversation_extension = 1,
+    random_state = 3407,
+):
+    """"""
+    Converts a dataset to ShareGPT style.
+    ShareGPT requires only 1 input and 1 output field.
+    This means one has to merge multiple columns into 1 for 1 input field.
+    Use `conversation_extension` to increase the length of each conversation by randomnly
+    selecting a few and packing them into 1.
+
+    merged_prompt = """",                 Prompt to merge columns into 1 input
+    merged_column_name = ""instruction"", Final column name for the input  field
+    output_column_name = ""output"",      Final column name for the output field
+    remove_unsued_columns = True,
+    conversation_extension = 1,         Automatically combines `conversation_extension` convos into 1
+    random_state = 3407,
+    """"""
+    if ""conversations"" in dataset.column_names:
+        convo = dataset[0][""conversations""]
+        if type(convo) is list:
+            raise TypeError(""Unsloth: Your dataset is probably already in ShareGPT format!"")
+        pass
+    pass
+
+    possible_columns, final_optional_prompts = _parse_combined_prompt(merged_prompt, dataset)
+    function = _create_formatter(possible_columns, final_optional_prompts, merged_column_name)
+    exec(function, globals())
+    dataset = dataset.map(__combined_prompt_processor__, batched = True, desc = ""Merging columns"")
+
+    def __convert_to_sharegpt__(examples):
+        users      = examples[merged_column_name]
+        assistants = examples[output_column_name]
+        texts = []
+        for user, assistant in zip(users, assistants):
+            texts.append([
+                {""from"" : ""user"",      ""content"" : user     },
+                {""from"" : ""assistant"", ""content"" : assistant},
+            ])
+        pass
+        return { ""conversations"" : texts, }
+    pass
+
+    dataset = dataset.map(
+        __convert_to_sharegpt__,
+        batched = True,
+        desc = ""Converting to ShareGPT"",
+        # Remove unsued columns!
+        remove_columns = dataset.column_names if remove_unsued_columns else None,
+    )
+
+    # Randomnly concat conversations to create a long stream!
+    from datasets import concatenate_datasets
+    n_extensions = max(conversation_extension-1, 0)
+    if n_extensions == 0: return dataset
+
+    dataset = dataset.rename_columns({""conversations"" : f""conversations0""})
+    all_shuffled = [dataset]
+    for j in range(1, n_extensions+1):
+        shuffled = dataset.shuffle(seed = random_state+j).rename_columns({""conversations0"" : f""conversations{j}""})
+        all_shuffled.append(shuffled)
+    pass
+    dataset = concatenate_datasets(all_shuffled, axis = 1)
+
+    # Combine them into 1
+    function = ""def __combine_conversations__(examples):\n""
+    n_extensions += 1
+    for j in range(n_extensions):
+        function += f""{' '*4}conversations{j}__ = examples['conversations{j}']\n""
+    function += f""{' '*4}convos = []\n""
+    function += f""{' '*4}for ({', '.join(f'conversations{j}' for j in range(n_extensions))}) ""\
+                f""in zip({', '.join(f'conversations{j}__' for j in range(n_extensions))}):\n""
+    function += f""{' '*8}convos.append(""\
+                f""{'+'.join(f'conversations{j}' for j in range(n_extensions))})\n""
+    function += f""{' '*4}return "" + ""{ "" + f""'conversations' : convos"" + "" }""
+
+    # Map function
+    exec(function, globals())
+    dataset = dataset.map(
+        __combine_conversations__,
+        batched = True,
+        desc = ""Extending conversations"",
+        # Remove unsued columns!
+        remove_columns = dataset.column_names if remove_unsued_columns else None,
+    )
+    return dataset
+pass
+
+
+def standardize_sharegpt(
     dataset,
-    conversation_key = ""conversations"",
-    system_message = None,
     aliases_for_system    = [""system"",],
     aliases_for_user      = [""user"", ""human"", ""input"",],
     aliases_for_assistant = [""gpt"", ""assistant"", ""output"",],
 ):
     """"""
-        Standardizes ShareGPT and other formats to user/assistant Hugging Face format.
+    Standardizes ShareGPT and other formats to user/assistant Hugging Face format.
+    
+    Get aliases for the system, user and assistant roles.
+    These shall map to ""system"", ""user"" and ""assistant"" respectively.
+    
+    aliases_for_system    = [""system"",],
+    aliases_for_user      = [""user"", ""human"", ""input"",],
+    aliases_for_assistant = [""gpt"", ""assistant"", ""output"",],
     """"""
     import collections
     import itertools
 
-    convos = dataset[:10][conversation_key]
+    convos = dataset[:10][""conversations""]
     uniques = collections.defaultdict(list)
     for convo in convos:
         for message in convo:
@@ -768,24 +959,19 @@ def standardize_dataset(
     for x in aliases_for_assistant: aliases_mapping[x] = ""assistant""
 
     def _standardize_dataset(examples):
-        convos = examples[conversation_key]
+        convos = examples[""conversations""]
         all_convos = []
         for convo in convos:
-            new_convo = []
-            if len(convo) == 0: continue
-            has_system = aliases_mapping[convo[0][role_key]] == ""system""
-            if not has_system and system_message is not None:
-                new_convo.append({ ""role"" : ""system"", ""content"" : system_message, })
-            for message in convo:
-                role = aliases_mapping[message[role_key]]
-                new_convo.append({ ""role"" : role, ""content"" : message[content_key], })
-            pass
+            new_convo = [
+                { ""role"" : aliases_mapping[message[role_key]], ""content"" : message[content_key], }
+                for message in convo
+            ]
             all_convos.append(new_convo)
         pass
-        return { conversation_key : all_convos, }
+        return { ""conversations"" : all_convos, }
     pass
 
-    return dataset.map(_standardize_dataset, batched = True,)
+    return dataset.map(_standardize_dataset, batched = True, desc = ""Standardizing format"")
 pass
 
 
@@ -837,7 +1023,7 @@ def construct_chat_template( \
 
 tokenizer = None,
 
-template = """"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
+chat_template = """"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
 
 {SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>
 
@@ -851,7 +1037,7 @@ template = """"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
     
 default_system_message = \
     ""Below are some instructions that describe some tasks. Write responses that appropriately complete each request."",
-  
+
 extra_eos_tokens = None,
   
 ):
@@ -865,6 +1051,7 @@ extra_eos_tokens = None,
     assert(tokenizer is not None)
 
     if extra_eos_tokens is None: extra_eos_tokens = []
+    elif type(extra_eos_tokens) is str: extra_eos_tokens = [extra_eos_tokens,]
 
     vocab = tokenizer.get_vocab()
     for extra_eos in extra_eos_tokens:
@@ -883,11 +1070,30 @@ extra_eos_tokens = None,
         ""### Input:\\n{INPUT}\\n\\n### Response:\\n{OUTPUT}\\n""\
         ""### Input:\\n{INPUT}\\n\\n### Response:\\n{OUTPUT}\\n""
 
+    # Check for EOS after {OUTPUT}
+    if tokenizer.eos_token is not None:
+        extra_eos_tokens.insert(0, tokenizer.eos_token)
+    if len(extra_eos_tokens) == 0:
+        raise RuntimeError(
+            ""Unsloth: Your tokenizer does not have an EOS token? Please provide one via extra_eos_tokens!""
+        )
+    pass
+
+    count_eos = 0
+    for eos in extra_eos_tokens:
+        count_eos += len(re.findall(r""{OUTPUT}"" + eos.encode(""unicode-escape"").decode(""utf-8""), chat_template))
+    pass
+    if count_eos == 0:
+        logger.warning(""Unsloth: We automatically added an EOS token to stop endless generations."")
+        eos = extra_eos_tokens[0]
+        chat_template = re.sub(r""{OUTPUT}"", r""{OUTPUT}"" + eos.encode(""unicode-escape"").decode(""utf-8""), chat_template)
+    pass
+
     # O(N^2) search finding 2 repeatted pieces of text
-    j = len(template)-1
+    j = len(chat_template)-1
     at_least_one = False
     while j > 0:
-        found = template.rfind(template[j:], 0, j)
+        found = chat_template.rfind(chat_template[j:], 0, j)
         if found == -1: break
         j -= 1
         at_least_one = True
@@ -895,19 +1101,18 @@ extra_eos_tokens = None,
     if j > 0: j += 1
     else: raise RuntimeError(error_msg)
 
-
     if not at_least_one: raise RuntimeError(error_msg)
 
     # Repeatted text
-    instruction_response = template[j:]
+    instruction_response = chat_template[j:]
     if instruction_response.count(""{INPUT}"") != 1 or instruction_response.count(""{OUTPUT}"") != 1:
         raise RuntimeError(error_msg)
     pass
 
     # 1st System, Instruction, Output pair
-    left  = template[:j]
+    left  = chat_template[:j]
     # 2nd Instruction, Output pair
-    right = template[j:]
+    right = chat_template[j:]
 
     # Isolate input
     extra_eos_tokens_regex = ""|"".join(f""(?:{re.escape(x)})"" for x in extra_eos_tokens)
@@ -952,7 +1157,12 @@ extra_eos_tokens = None,
             ollama_system = ollama_system[len(tokenizer.bos_token):]
         pass
     pass
-    system_modelfile = ""{{ if .System }}"" + ollama_system.replace(""{SYSTEM}"", ""{{ .System }}"") + ""{{ end }}""
+    # Check system
+    if ""{SYSTEM}"" in ollama_system:
+        system_modelfile = ""{{ if .System }}"" + ollama_system.replace(""{SYSTEM}"", ""{{ .System }}"") + ""{{ end }}""
+    else:
+        system_modelfile = ollama_system
+    pass
     input_modelfile  = ""{{ if .Prompt }}"" + input_part .replace(""{INPUT}"",  ""{{ .Prompt }}"") + ""{{ end }}""
     output_modelfile = output_part.replace(""{OUTPUT}"", ""{{ .Response }}"")
 
@@ -1005,6 +1215,14 @@ extra_eos_tokens = None,
         partial_system = process(system_part, ""{SYSTEM}"", ""messages[0]['content']"")
         partial_system = partial_system.replace(""{SYSTEM}"", """")
 
+        # If {SYSTEM} is non existent, simply just use the content
+        if ""{SYSTEM}"" not in partial_system:
+            partial_system = ""messages[0]['content']""
+        else:
+            if default_system_message is None:
+                raise RuntimeError(""Unsloth: Please specify a default system message!"")
+        pass
+
         # Separate the BOS
         if has_bos_token:
             partial_system = partial_system.replace(tokenizer.bos_token, """", 1)
@@ -1015,10 +1233,14 @@ extra_eos_tokens = None,
                 ""{{ "" + partial_system + "" }}""\
                 ""{% set loop_messages = messages[1:] %}""
         if default_system_message is not None:
+            full_system = system_part.replace(""{SYSTEM}"", default_system_message)
             partial_system += ""{% else %}""\
-                ""{{ '"" + system_part.replace(""{SYSTEM}"", default_system_message) + ""' }}""\
+                ""{{ '"" + full_system + ""' }}""\
                 ""{% set loop_messages = messages %}""\
             ""{% endif %}""
+
+            # Add to modelfile
+            modelfile += '\nSYSTEM ""' + full_system + '""'
         else:
             partial_system += ""{% endif %}""
         pass
@@ -1075,6 +1297,53 @@ def test_construct_chat_template():
 pass
 
 
+def apply_chat_template( \
+
+dataset,
+tokenizer = None,
+
+chat_template = """"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
+
+{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>
+
+{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
+
+{OUTPUT}<|eot_id|><|start_header_id|>user<|end_header_id|>
+
+{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
+
+{OUTPUT}<|eot_id|>"""""",
+    
+default_system_message = \
+    ""Below are some instructions that describe some tasks. Write responses that appropriately complete each request."",
+  
+extra_eos_tokens = None,
+  
+):
+    """"""
+    Creates a Ollama modelfile and a HF Jinja template from a custom
+    template. You must provide 2x examples of an input & output.
+    There is an optional system message as well.
+
+    You must use {INPUT}, {OUTPUT} twice, and {SYSTEM} is optional.
+    """"""
+    modelfile, jinja_template = construct_chat_template(
+        tokenizer = tokenizer,
+        chat_template = chat_template,
+        default_system_message = default_system_message,
+        extra_eos_tokens = extra_eos_tokens,
+    )
+    def formatting_prompts_func(examples):
+        convos = examples[""conversations""]
+        texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
+        return { ""text"" : texts, }
+    pass
+    tokenizer.chat_template = jinja_template
+    tokenizer._ollama_modelfile = modelfile
+    return dataset.map(formatting_prompts_func, batched = True,)
+pass
+
+
 def create_ollama_modelfile(tokenizer, gguf_location):
     """"""
         Creates an Ollama Modelfile.
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 022be10..9db7fcf 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1430,15 +1430,30 @@ class FastLlamaModel:
             check_parameters = [
                 ""r"", ""lora_alpha"", ""lora_dropout"",
                 ""bias"", ""layers_to_transform"", ""layers_pattern"",
-                ""use_rslora"", ""modules_to_save"", ""init_lora_weights"",
+                ""use_rslora"", ""init_lora_weights"",
             ]
             check_all = True
             for param in check_parameters:
                 check_all = check_all and (peft_config[param] == eval(param))
             pass
+
+            # Check save_modules
+            old_target_modules = list(peft_config[""target_modules""])
+            modules_to_save = peft_config[""modules_to_save""]
+            if modules_to_save is None: modules_to_save = {}
+            modules_to_save = list(modules_to_save)
+            old_target_modules += modules_to_save
+
+            # Combine all
+            new_target_modules = list(target_modules) + \
+                list(modules_to_save if modules_to_save is not None else [])
+
+            # Now check!
+            new_target_modules = set(new_target_modules)
             check_all = check_all and (
-                len(set(peft_config[""target_modules""]) ^ set(target_modules)) == 0
+                len(set(old_target_modules) ^ new_target_modules) == 0
             )
+
             check_all = check_all and (
                 (loftq_config == {} or loftq_config is None) and \
                 (peft_config[""loftq_config""] == {} or peft_config[""loftq_config""] is None)
@@ -1449,6 +1464,35 @@ class FastLlamaModel:
                 logger.warning(
                     ""Unsloth: Already have LoRA adapters! We shall skip this step.""
                 )
+
+                # Offload!
+                # [TODO] First offload lm_head and embed_tokens to CPU (should be disk!!)
+                if ""embed_tokens"" in new_target_modules:
+                    print(""Unsloth: Casting embed_tokens to float32"")
+
+                    model.model.model.embed_tokens.modules_to_save.default\
+                        .to(device = device, dtype = torch.float32, non_blocking = True)
+                    model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
+
+                    # [TODO] Move old embed_tokens to CPU - should be disk!
+                    model.model.model.embed_tokens.original_module\
+                        .to(device = ""cpu"", non_blocking = True)
+                    model.model.model.embed_tokens.original_module.requires_grad_(False)
+                pass
+
+                if ""lm_head"" in new_target_modules:
+                    print(""Unsloth: Casting lm_head to float32"")
+
+                    model.model.lm_head.modules_to_save.default\
+                        .to(device = device, dtype = torch.float32, non_blocking = True)
+                    model.model.lm_head.modules_to_save.default.requires_grad_(True)
+
+                    # [TODO] Move old lm_head to CPU - should be disk!
+                    model.model.lm_head.original_module\
+                        .to(device = ""cpu"", non_blocking = True)
+                    model.model.lm_head.original_module.requires_grad_(False)
+                pass
+
                 return model
             else:
                 raise TypeError(
@@ -1669,7 +1713,7 @@ class FastLlamaModel:
             print(""Unsloth: Casting embed_tokens to float32"")
             assert(hasattr(model.model.model.embed_tokens, ""modules_to_save""))
             model.model.model.embed_tokens.modules_to_save.default\
-                .to(device = input_embeddings_device,  dtype = torch.float32, non_blocking = True)
+                .to(device = device, dtype = torch.float32, non_blocking = True)
             model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
         pass
 
@@ -1677,7 +1721,7 @@ class FastLlamaModel:
             print(""Unsloth: Casting lm_head to float32"")
             assert(hasattr(model.model.lm_head, ""modules_to_save""))
             model.model.lm_head.modules_to_save.default\
-                .to(device = output_embeddings_device, dtype = torch.float32, non_blocking = True)
+                .to(device = device, dtype = torch.float32, non_blocking = True)
             model.model.lm_head.modules_to_save.default.requires_grad_(True)
         pass
 
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 57624e6..fe2dc06 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -735,7 +735,8 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):
         raise ValueError(
             'Unsloth: Untrained tokens found, but embed_tokens & lm_head not trainable, causing NaNs. '\
             'Restart then add `embed_tokens` & `lm_head` to '\
-            '`FastLanguageModel.get_peft_model(target_modules = [..., ""embed_tokens"", ""lm_head"",])`',
+            '`FastLanguageModel.get_peft_model(target_modules = [..., ""embed_tokens"", ""lm_head"",]). `'\
+            'Are you using the `base` model? Instead, use the `instruct` version to silence this warning.',
         )
     pass
 
"
"diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 7c693a6..e22e3a1 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -179,7 +179,10 @@ pass
 
 def fast_linear_forward(proj, X, temp_lora = None, out = None):
     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)
-    out = fast_gemv(X, W, W_quant, out = out)
+    if W_quant is None:
+        out = torch.matmul(X, W.t())
+    else:
+        out = fast_gemv(X, W, W_quant, out = out)
     if lora_A is not None:
 
         # Save LoRAs for inference to stop data movement costs
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index da99ea4..bc7dc69 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif True:#self.training:
+    elif self.training:
         attention_mask = None
         padding_mask = None
     else:
diff --git a/unsloth/save.py b/unsloth/save.py
index b4e1796..6c44d23 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -258,11 +258,19 @@ def unsloth_save_model(
         ""private""         : save_pretrained_settings[""private""],
         ""token""           : save_pretrained_settings[""token""],
     }
-    
+
+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.
+    from peft import PeftModelForCausalLM
+    if isinstance(model, PeftModelForCausalLM):
+        internal_model = model.model
+    else:
+        internal_model = model
+    pass
+        
+    # Cannot be converted properly!
     if (save_method == ""merged_4bit"") or (save_method == ""lora"") or (
         not hasattr(model, ""model"") or \
-        not hasattr(model.model, ""model"") or \
-        not hasattr(model.model.model, ""layers"")
+        not hasattr(internal_model.model, ""layers"")
     ):
         # Do general saving
         
@@ -343,12 +351,12 @@ def unsloth_save_model(
     # HF also uses a OrderedDict
     from collections import OrderedDict
     state_dict = OrderedDict()
-    state_dict[""model.embed_tokens.weight""] = model.model.model.embed_tokens.weight.data
+    state_dict[""model.embed_tokens.weight""] = internal_model.model.embed_tokens.weight.data
 
     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)
 
     from tqdm import tqdm as ProgressBar
-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):
+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):
         for item in LLAMA_WEIGHTS:
             proj = eval(f""layer.{item}"")
             name = f""model.layers.{j}.{item}.weight""
@@ -375,8 +383,8 @@ def unsloth_save_model(
         pass
     pass
 
-    state_dict[""model.norm.weight""] = model.model.model.norm.weight.data
-    state_dict[""lm_head.weight""]    = model.model.lm_head.weight.data
+    state_dict[""model.norm.weight""] = internal_model.model.norm.weight.data
+    state_dict[""lm_head.weight""]    = internal_model.lm_head.weight.data
 
     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter
     for key, value in state_dict.items():
@@ -418,7 +426,7 @@ def unsloth_save_model(
     model.config = new_config
 
     # Save!
-    model.model.save_pretrained(**save_pretrained_settings)
+    internal_model.save_pretrained(**save_pretrained_settings)
 
     # Revert config back
     original_model = model
"
"diff --git a/tests/saving/language_models/test_merge_model_perplexity_llama-3.2.py b/tests/saving/language_models/test_merge_model_perplexity_llama-3.2.py
index 384da30..2d4ec83 100644
--- a/tests/saving/language_models/test_merge_model_perplexity_llama-3.2.py
+++ b/tests/saving/language_models/test_merge_model_perplexity_llama-3.2.py
@@ -14,8 +14,10 @@ import gc
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
+
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_merge_model_perplexity_mistral.py b/tests/saving/language_models/test_merge_model_perplexity_mistral.py
index f08a3c1..d1942ea 100644
--- a/tests/saving/language_models/test_merge_model_perplexity_mistral.py
+++ b/tests/saving/language_models/test_merge_model_perplexity_mistral.py
@@ -14,8 +14,9 @@ import gc
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_merge_model_perplexity_phi_4.py b/tests/saving/language_models/test_merge_model_perplexity_phi_4.py
index df6f7ac..c0bd7fa 100644
--- a/tests/saving/language_models/test_merge_model_perplexity_phi_4.py
+++ b/tests/saving/language_models/test_merge_model_perplexity_phi_4.py
@@ -14,8 +14,10 @@ import gc
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
+
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_merged_model_perplexity_llama-3.1-8b.py b/tests/saving/language_models/test_merged_model_perplexity_llama-3.1-8b.py
index 5338371..d26771b 100644
--- a/tests/saving/language_models/test_merged_model_perplexity_llama-3.1-8b.py
+++ b/tests/saving/language_models/test_merged_model_perplexity_llama-3.1-8b.py
@@ -14,8 +14,9 @@ import gc
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_merged_model_perplexity_qwen_2.5.py b/tests/saving/language_models/test_merged_model_perplexity_qwen_2.5.py
index af8e8ea..b80197b 100644
--- a/tests/saving/language_models/test_merged_model_perplexity_qwen_2.5.py
+++ b/tests/saving/language_models/test_merged_model_perplexity_qwen_2.5.py
@@ -14,8 +14,9 @@ import gc
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_push_to_hub_merged.py b/tests/saving/language_models/test_push_to_hub_merged.py
index b770489..e1c23fa 100644
--- a/tests/saving/language_models/test_push_to_hub_merged.py
+++ b/tests/saving/language_models/test_push_to_hub_merged.py
@@ -15,8 +15,9 @@ from huggingface_hub import HfFileSystem, hf_hub_download
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_push_to_hub_merged_sharded_index_file.py b/tests/saving/language_models/test_push_to_hub_merged_sharded_index_file.py
index 7c31520..04bbf29 100644
--- a/tests/saving/language_models/test_push_to_hub_merged_sharded_index_file.py
+++ b/tests/saving/language_models/test_push_to_hub_merged_sharded_index_file.py
@@ -15,8 +15,9 @@ from huggingface_hub import HfFileSystem, hf_hub_download
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.perplexity_eval import ppl_model, add_to_comparison, print_model_comparison
diff --git a/tests/saving/language_models/test_save_merged_grpo_model.py b/tests/saving/language_models/test_save_merged_grpo_model.py
index 35b6f65..0bbb7ff 100644
--- a/tests/saving/language_models/test_save_merged_grpo_model.py
+++ b/tests/saving/language_models/test_save_merged_grpo_model.py
@@ -12,8 +12,9 @@ from pathlib import Path
 import multiprocessing as mp
 import gc
 from multiprocessing import Queue
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.aime_eval import evaluate_model_aime, compare_aime_results
diff --git a/tests/saving/non_peft/test_mistral_non_peft.py b/tests/saving/non_peft/test_mistral_non_peft.py
new file mode 100644
index 0000000..7308151
--- /dev/null
+++ b/tests/saving/non_peft/test_mistral_non_peft.py
@@ -0,0 +1,67 @@
+from unsloth import FastLanguageModel
+from transformers import AutoModelForCausalLM
+from peft import PeftModel
+from pathlib import Path
+import sys
+import warnings
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+
+
+print(f""\n{'='*80}"")
+print("" PHASE 1: Loading Base Model"")
+print(f""{'='*80}"")
+
+model, tokenizer = FastLanguageModel.from_pretrained(
+        model_name=""unsloth/mistral-7b-v0.3"",
+        max_seq_length=2048,
+        dtype=None,
+        load_in_4bit=True,
+        load_in_8bit=False,
+        full_finetuning=False,
+    )
+
+
+print("" Base model loaded successfully!"")
+
+### Attemtping save merge
+
+
+
+print(f""\n{'='*80}"")
+print("" PHASE 2: Attempting save_pretrained_merged (Should Warn)"")
+print(f""{'='*80}"")
+
+with warnings.catch_warnings(record=True) as w:
+        warnings.simplefilter(""always"")
+        model.save_pretrained_merged(""test_output"", tokenizer)
+
+        # Verify warning
+        assert len(w) >= 1, ""Expected warning but none raised""
+        warning_msg = str(w[0].message)
+        expected_msg = ""Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!""
+        assert expected_msg in warning_msg, f""Unexpected warning: {warning_msg}""
+        assert expected_msg in warning_msg, f""Unexpected warning: {warning_msg}""
+
+print("" Correct warning detected for non-PeftModel merge attempt!"")
+
+
+
+print(f""\n{'='*80}"")
+print("" PHASE 3: Using save_pretrained (Should Succeed)"")
+print(f""{'='*80}"")
+
+
+try:
+    with warnings.catch_warnings():
+        warnings.simplefilter(""error"")  # Treat warnings as errors here
+        model.save_pretrained(""test_output"")
+        print("" Standard save_pretrained completed successfully!"")
+except Exception as e:
+    assert False, f""Phase 3 failed: {e}""
+
+safe_remove_directory(""./test_output"")
+safe_remove_directory(""./unsloth_compiled_cache"")
diff --git a/tests/saving/non_peft/test_whisper_non_peft.py b/tests/saving/non_peft/test_whisper_non_peft.py
new file mode 100644
index 0000000..40321d2
--- /dev/null
+++ b/tests/saving/non_peft/test_whisper_non_peft.py
@@ -0,0 +1,67 @@
+from unsloth import FastLanguageModel, FastModel
+from transformers import AutoModelForCausalLM, WhisperForConditionalGeneration
+from peft import PeftModel
+from pathlib import Path
+import sys
+import warnings
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+
+
+print(f""\n{'='*80}"")
+print("" PHASE 1: Loading Base Model"")
+print(f""{'='*80}"")
+
+model, tokenizer = FastModel.from_pretrained(
+    model_name = ""unsloth/whisper-large-v3"",
+    dtype = None, # Leave as None for auto detection
+    load_in_4bit = False, # Set to True to do 4bit quantization which reduces memory
+    auto_model = WhisperForConditionalGeneration,
+    whisper_language = ""English"",
+    whisper_task = ""transcribe"",
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+print("" Base model loaded successfully!"")
+
+### Attemtping save merge
+
+
+
+print(f""\n{'='*80}"")
+print("" PHASE 2: Attempting save_pretrained_merged (Should Warn)"")
+print(f""{'='*80}"")
+
+with warnings.catch_warnings(record=True) as w:
+        warnings.simplefilter(""always"")
+        model.save_pretrained_merged(""test_output"", tokenizer)
+
+        # Verify warning
+        assert len(w) >= 1, ""Expected warning but none raised""
+        warning_msg = str(w[0].message)
+        expected_msg = ""Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!""
+        assert expected_msg in warning_msg, f""Unexpected warning: {warning_msg}""
+        assert expected_msg in warning_msg, f""Unexpected warning: {warning_msg}""
+
+print("" Correct warning detected for non-PeftModel merge attempt!"")
+
+
+
+print(f""\n{'='*80}"")
+print("" PHASE 3: Using save_pretrained (Should Succeed)"")
+print(f""{'='*80}"")
+
+
+try:
+    with warnings.catch_warnings():
+        warnings.simplefilter(""error"")  # Treat warnings as errors here
+        model.save_pretrained(""test_output"")
+        print("" Standard save_pretrained completed successfully!"")
+except Exception as e:
+    assert False, f""Phase 3 failed: {e}""
+
+safe_remove_directory(""./test_output"")
+safe_remove_directory(""./unsloth_compiled_cache"")
diff --git a/tests/saving/text_to_speech_models/test_csm.py b/tests/saving/text_to_speech_models/test_csm.py
new file mode 100644
index 0000000..c3703ca
--- /dev/null
+++ b/tests/saving/text_to_speech_models/test_csm.py
@@ -0,0 +1,156 @@
+from unsloth import FastLanguageModel, FastModel
+from transformers import CsmForConditionalGeneration
+import torch
+# ruff: noqa
+import sys
+from pathlib import Path
+from peft import PeftModel
+import warnings
+import requests
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.os_utils import require_package, require_python_package
+
+require_package(""ffmpeg"", ""ffmpeg"")
+require_python_package(""soundfile"")
+
+import soundfile as sf
+
+print(f""\n{'='*80}"")
+print("" SECTION 1: Loading Model and LoRA Adapters"")
+print(f""{'='*80}"")
+
+
+model, tokenizer = FastModel.from_pretrained(
+    model_name = ""unsloth/csm-1b"",
+    max_seq_length= 2048, # Choose any for long context!
+    dtype = None, # Leave as None for auto-detection
+    auto_model = CsmForConditionalGeneration,
+    load_in_4bit = False, # Select True for 4bit - reduces memory usage
+)
+
+
+base_model_class = model.__class__.__name__
+
+
+model = FastModel.get_peft_model(
+    model,
+    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                      ""gate_proj"", ""up_proj"", ""down_proj"",],
+    lora_alpha = 32,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+)
+
+print("" Model and LoRA adapters loaded successfully!"")
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 2: Checking Model Class Type"")
+print(f""{'='*80}"")
+
+assert isinstance(model, PeftModel), ""Model should be an instance of PeftModel""
+print("" Model is an instance of PeftModel!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 3: Checking Config Model Class Type"")
+print(f""{'='*80}"")
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+
+config_model = find_lora_base_model(model) if isinstance(model, PeftModel) else model
+
+assert config_model.__class__.__name__ == base_model_class, f""Expected config_model class to be {base_model_class}""
+print("" config_model returns correct Base Model class:"", str(base_model_class))
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 4: Saving and Merging Model"")
+print(f""{'='*80}"")
+
+with warnings.catch_warnings():
+    warnings.simplefilter(""error"")  # Treat warnings as errors
+    try:
+        model.save_pretrained_merged(""csm"", tokenizer)
+        print("" Model saved and merged successfully without warnings!"")
+    except Exception as e:
+        assert False, f""Model saving/merging failed with exception: {e}""
+
+print(f""\n{'='*80}"")
+print("" SECTION 5: Loading Model for Inference"")
+print(f""{'='*80}"")
+
+
+model, processor = FastModel.from_pretrained(
+    model_name = ""./csm"",
+    max_seq_length= 2048, # Choose any for long context!
+    dtype = None, # Leave as None for auto-detection
+    auto_model = CsmForConditionalGeneration,
+    load_in_4bit = False, # Select True for 4bit - reduces memory usage
+)
+
+from transformers import AutoProcessor
+processor = AutoProcessor.from_pretrained(""unsloth/csm-1b"")
+
+print("" Model loaded for inference successfully!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 6: Running Inference"")
+print(f""{'='*80}"")
+
+
+from transformers import pipeline
+import torch
+output_audio_path = ""csm_audio.wav""
+try:
+    text = ""We just finished fine tuning a text to speech model... and it's pretty good!""
+    speaker_id = 0
+    inputs = processor(f""[{speaker_id}]{text}"", add_special_tokens=True).to(""cuda"")
+    audio_values = model.generate(
+        **inputs,
+        max_new_tokens=125, # 125 tokens is 10 seconds of audio, for longer speech increase this
+        # play with these parameters to get the best results
+        depth_decoder_temperature=0.6,
+        depth_decoder_top_k=0,
+        depth_decoder_top_p=0.9,
+        temperature=0.8,
+        top_k=50,
+        top_p=1.0,
+        #########################################################
+        output_audio=True
+    )
+    audio = audio_values[0].to(torch.float32).cpu().numpy()
+    sf.write(""example_without_context.wav"", audio, 24000)
+    print(f"" Audio generated and saved to {output_audio_path}!"")
+except Exception as e:
+    assert False, f""Inference failed with exception: {e}""
+
+
+## assert that transcribed_text contains The birch canoe slid on the smooth planks. Glued the sheet to the dark blue background. It's easy to tell the depth of a well. Four hours of steady work faced us.
+
+print("" All sections passed successfully!"")
+
+
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./csm"")
diff --git a/tests/saving/text_to_speech_models/test_lasa.py b/tests/saving/text_to_speech_models/test_lasa.py
new file mode 100644
index 0000000..a4bc5ed
--- /dev/null
+++ b/tests/saving/text_to_speech_models/test_lasa.py
@@ -0,0 +1,217 @@
+from unsloth import FastLanguageModel, FastModel
+from transformers import CsmForConditionalGeneration
+import torch
+# ruff: noqa
+import sys
+from pathlib import Path
+from peft import PeftModel
+import warnings
+import requests
+
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.os_utils import require_package, require_python_package
+
+require_package(""ffmpeg"", ""ffmpeg"")
+require_python_package(""soundfile"")
+require_python_package(""xcodec2"")
+
+import soundfile as sf
+from xcodec2.modeling_xcodec2 import XCodec2Model
+XCODEC2_MODEL_NAME = ""HKUST-Audio/xcodec2""
+SAMPLE_RATE = 16000
+DEVICE = ""cuda""
+
+try:
+    codec_model = XCodec2Model.from_pretrained(XCODEC2_MODEL_NAME)
+
+except Exception as e:
+    raise f""ERROR loading XCodec2 model: {e}.""
+
+codec_model.to('cpu')
+
+print(f""\n{'='*80}"")
+print("" SECTION 1: Loading Model and LoRA Adapters"")
+print(f""{'='*80}"")
+
+max_seq_length = 2048
+model, tokenizer = FastLanguageModel.from_pretrained(
+    model_name = ""unsloth/Llasa-1B"",
+    max_seq_length = max_seq_length,
+    dtype = None, # Select None for auto detection
+    load_in_4bit = False, # Choose True for 4bit which reduces memory
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+base_model_class = model.__class__.__name__
+
+
+model = FastLanguageModel.get_peft_model(
+    model,
+    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    target_modules = [""q_proj"", ""v_proj""],
+    lora_alpha = 128,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+)
+
+print("" Model and LoRA adapters loaded successfully!"")
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 2: Checking Model Class Type"")
+print(f""{'='*80}"")
+
+assert isinstance(model, PeftModel), ""Model should be an instance of PeftModel""
+print("" Model is an instance of PeftModel!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 3: Checking Config Model Class Type"")
+print(f""{'='*80}"")
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+
+config_model = find_lora_base_model(model) if isinstance(model, PeftModel) else model
+
+assert config_model.__class__.__name__ == base_model_class, f""Expected config_model class to be {base_model_class}""
+print("" config_model returns correct Base Model class:"", str(base_model_class))
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 4: Saving and Merging Model"")
+print(f""{'='*80}"")
+
+with warnings.catch_warnings():
+    warnings.simplefilter(""error"")  # Treat warnings as errors
+    try:
+        model.save_pretrained_merged(""lasa"", tokenizer)
+        print("" Model saved and merged successfully without warnings!"")
+    except Exception as e:
+        assert False, f""Model saving/merging failed with exception: {e}""
+
+print(f""\n{'='*80}"")
+print("" SECTION 5: Loading Model for Inference"")
+print(f""{'='*80}"")
+
+
+model, tokenizer = FastLanguageModel.from_pretrained(
+    model_name = ""./lasa"",
+    max_seq_length = max_seq_length,
+    dtype = None, # Select None for auto detection
+    load_in_4bit = False, # Choose True for 4bit which reduces memory
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+#from transformers import AutoProcessor
+#processor = AutoProcessor.from_pretrained(""unsloth/csm-1b"")
+
+print("" Model loaded for inference successfully!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 6: Running Inference"")
+print(f""{'='*80}"")
+
+
+from transformers import pipeline
+import torch
+output_audio_path = ""lasa_audio.wav""
+input_text = ""Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.""
+
+FastLanguageModel.for_inference(model)
+
+def ids_to_speech_tokens(speech_ids):
+
+    speech_tokens_str = []
+    for speech_id in speech_ids:
+        speech_tokens_str.append(f""<|s_{speech_id}|>"")
+    return speech_tokens_str
+
+def extract_speech_ids(speech_tokens_str):
+
+    speech_ids = []
+    for token_str in speech_tokens_str:
+        if token_str.startswith('<|s_') and token_str.endswith('|>'):
+            num_str = token_str[4:-2]
+
+            num = int(num_str)
+            speech_ids.append(num)
+        else:
+            print(f""Unexpected token: {token_str}"")
+    return speech_ids
+
+#TTS start!
+with torch.inference_mode():
+    with torch.amp.autocast('cuda',dtype=model.dtype):
+        formatted_text = f""<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>""
+
+        # Tokenize the text
+        chat = [
+            {""role"": ""user"", ""content"": ""Convert the text to speech:"" + formatted_text},
+            {""role"": ""assistant"", ""content"": ""<|SPEECH_GENERATION_START|>""}
+        ]
+
+        input_ids = tokenizer.apply_chat_template(
+            chat,
+            tokenize=True,
+            return_tensors='pt',
+            continue_final_message=True
+        )
+        input_ids = input_ids.to('cuda')
+
+        speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')
+
+        # Generate the speech autoregressively
+        outputs = model.generate(
+            input_ids,
+            max_length=2048,  # We trained our model with a max length of 2048
+            eos_token_id= speech_end_id ,
+            do_sample=True,
+            top_p=1.2,           #  Adjusts the diversity of generated content
+            temperature=1.2,   #  Controls randomness in output
+        )
+    # Extract the speech tokens
+    generated_ids = outputs[0][input_ids.shape[1]:-1]
+
+    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
+
+    # Convert  token <|s_23456|> to int 23456
+    speech_tokens = extract_speech_ids(speech_tokens)
+
+    speech_tokens = torch.tensor(speech_tokens).cpu().unsqueeze(0).unsqueeze(0)
+
+    # Decode the speech tokens to speech waveform
+    gen_wav = codec_model.decode_code(speech_tokens)
+try:
+    sf.write(output_audio_path, gen_wav[0, 0, :].cpu().numpy(), 16000)
+except Exception as e:
+    assert False, f""Inference failed with exception: {e}""
+
+
+## assert that transcribed_text contains The birch canoe slid on the smooth planks. Glued the sheet to the dark blue background. It's easy to tell the depth of a well. Four hours of steady work faced us.
+
+print("" All sections passed successfully!"")
+
+
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./lasa"")
diff --git a/tests/saving/text_to_speech_models/test_orpheus.py b/tests/saving/text_to_speech_models/test_orpheus.py
new file mode 100644
index 0000000..813a80a
--- /dev/null
+++ b/tests/saving/text_to_speech_models/test_orpheus.py
@@ -0,0 +1,254 @@
+from unsloth import FastLanguageModel, FastModel
+from transformers import CsmForConditionalGeneration
+import torch
+# ruff: noqa
+import sys
+from pathlib import Path
+from peft import PeftModel
+import warnings
+import requests
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.os_utils import require_package, require_python_package
+
+require_package(""ffmpeg"", ""ffmpeg"")
+require_python_package(""soundfile"")
+require_python_package(""snac"")
+
+import soundfile as sf
+from snac import SNAC
+snac_model = SNAC.from_pretrained(""hubertsiuzdak/snac_24khz"")
+snac_model = snac_model.to(""cuda"")
+print(f""\n{'='*80}"")
+print("" SECTION 1: Loading Model and LoRA Adapters"")
+print(f""{'='*80}"")
+
+
+model, tokenizer = FastLanguageModel.from_pretrained(
+    model_name = ""unsloth/orpheus-3b-0.1-ft"",
+    max_seq_length= 2048, # Choose any for long context!
+    dtype = None, # Select None for auto detection
+    load_in_4bit = False, # Select True for 4bit which reduces memory usage
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+base_model_class = model.__class__.__name__
+
+
+model = FastLanguageModel.get_peft_model(
+    model,
+    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                      ""gate_proj"", ""up_proj"", ""down_proj"",],
+    lora_alpha = 64,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+)
+print("" Model and LoRA adapters loaded successfully!"")
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 2: Checking Model Class Type"")
+print(f""{'='*80}"")
+
+assert isinstance(model, PeftModel), ""Model should be an instance of PeftModel""
+print("" Model is an instance of PeftModel!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 3: Checking Config Model Class Type"")
+print(f""{'='*80}"")
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+
+config_model = find_lora_base_model(model) if isinstance(model, PeftModel) else model
+
+assert config_model.__class__.__name__ == base_model_class, f""Expected config_model class to be {base_model_class}""
+print("" config_model returns correct Base Model class:"", str(base_model_class))
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 4: Saving and Merging Model"")
+print(f""{'='*80}"")
+
+with warnings.catch_warnings():
+    warnings.simplefilter(""error"")  # Treat warnings as errors
+    try:
+        model.save_pretrained_merged(""orpheus"", tokenizer)
+        print("" Model saved and merged successfully without warnings!"")
+    except Exception as e:
+        assert False, f""Model saving/merging failed with exception: {e}""
+
+print(f""\n{'='*80}"")
+print("" SECTION 5: Loading Model for Inference"")
+print(f""{'='*80}"")
+
+
+model, tokenizer = FastLanguageModel.from_pretrained(
+    model_name = ""unsloth/orpheus-3b-0.1-ft"",
+    max_seq_length= 2048, # Choose any for long context!
+    dtype = None, # Select None for auto detection
+    load_in_4bit = False, # Select True for 4bit which reduces memory usage
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+#from transformers import AutoProcessor
+#processor = AutoProcessor.from_pretrained(""unsloth/csm-1b"")
+
+print("" Model loaded for inference successfully!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 6: Running Inference"")
+print(f""{'='*80}"")
+
+
+#@title Run Inference
+
+
+FastLanguageModel.for_inference(model) # Enable native 2x faster inference
+
+# Moving snac_model cuda to cpu
+snac_model.to(""cpu"")
+prompts = [
+    ""Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person."",
+]
+
+chosen_voice = None # None for single-speaker
+
+prompts_ = [(f""{chosen_voice}: "" + p) if chosen_voice else p for p in prompts]
+
+all_input_ids = []
+
+for prompt in prompts_:
+  input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids
+  all_input_ids.append(input_ids)
+
+start_token = torch.tensor([[ 128259]], dtype=torch.int64) # Start of human
+end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text, End of human
+
+all_modified_input_ids = []
+for input_ids in all_input_ids:
+  modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1) # SOH SOT Text EOT EOH
+  all_modified_input_ids.append(modified_input_ids)
+
+all_padded_tensors = []
+all_attention_masks = []
+max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])
+for modified_input_ids in all_modified_input_ids:
+  padding = max_length - modified_input_ids.shape[1]
+  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)
+  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)
+  all_padded_tensors.append(padded_tensor)
+  all_attention_masks.append(attention_mask)
+
+all_padded_tensors = torch.cat(all_padded_tensors, dim=0)
+all_attention_masks = torch.cat(all_attention_masks, dim=0)
+
+input_ids = all_padded_tensors.to(""cuda"")
+attention_mask = all_attention_masks.to(""cuda"")
+generated_ids = model.generate(
+      input_ids=input_ids,
+      attention_mask=attention_mask,
+      max_new_tokens=1200,
+      do_sample=True,
+      temperature=0.6,
+      top_p=0.95,
+      repetition_penalty=1.1,
+      num_return_sequences=1,
+      eos_token_id=128258,
+     use_cache = True
+  )
+token_to_find = 128257
+token_to_remove = 128258
+
+token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)
+
+if len(token_indices[1]) > 0:
+    last_occurrence_idx = token_indices[1][-1].item()
+    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]
+else:
+    cropped_tensor = generated_ids
+
+mask = cropped_tensor != token_to_remove
+
+processed_rows = []
+
+for row in cropped_tensor:
+    masked_row = row[row != token_to_remove]
+    processed_rows.append(masked_row)
+
+code_lists = []
+
+for row in processed_rows:
+    row_length = row.size(0)
+    new_length = (row_length // 7) * 7
+    trimmed_row = row[:new_length]
+    trimmed_row = [t - 128266 for t in trimmed_row]
+    code_lists.append(trimmed_row)
+
+
+def redistribute_codes(code_list):
+  layer_1 = []
+  layer_2 = []
+  layer_3 = []
+  for i in range((len(code_list)+1)//7):
+    layer_1.append(code_list[7*i])
+    layer_2.append(code_list[7*i+1]-4096)
+    layer_3.append(code_list[7*i+2]-(2*4096))
+    layer_3.append(code_list[7*i+3]-(3*4096))
+    layer_2.append(code_list[7*i+4]-(4*4096))
+    layer_3.append(code_list[7*i+5]-(5*4096))
+    layer_3.append(code_list[7*i+6]-(6*4096))
+  codes = [torch.tensor(layer_1).unsqueeze(0),
+         torch.tensor(layer_2).unsqueeze(0),
+         torch.tensor(layer_3).unsqueeze(0)]
+
+  # codes = [c.to(""cuda"") for c in codes]
+  audio_hat = snac_model.decode(codes)
+  return audio_hat
+
+my_samples = []
+for code_list in code_lists:
+  samples = redistribute_codes(code_list)
+  my_samples.append(samples)
+output_path = ""orpheus_audio.wav""
+try:
+    for i, samples in enumerate(my_samples):
+        audio_data = samples.detach().squeeze().cpu().numpy()
+        import soundfile as sf
+        sf.write(output_path, audio_data, 24000)  # Explicitly pass sample rate
+        print(f"" Audio saved to {output_path}!"")
+except Exception as e:
+    assert False, f""Inference failed with exception: {e}""
+
+# Verify the file exists
+import os
+assert os.path.exists(output_path), f""Audio file not found at {output_path}""
+print("" Audio file exists on disk!"")
+del my_samples, samples
+## assert that transcribed_text contains The birch canoe slid on the smooth planks. Glued the sheet to the dark blue background. It's easy to tell the depth of a well. Four hours of steady work faced us.
+
+print("" All sections passed successfully!"")
+
+
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./orpheus"")
diff --git a/tests/saving/text_to_speech_models/test_whisper.py b/tests/saving/text_to_speech_models/test_whisper.py
new file mode 100644
index 0000000..f29213c
--- /dev/null
+++ b/tests/saving/text_to_speech_models/test_whisper.py
@@ -0,0 +1,189 @@
+from unsloth import FastLanguageModel, FastModel
+from transformers import WhisperForConditionalGeneration, WhisperProcessor
+import torch
+# ruff: noqa
+import sys
+from pathlib import Path
+from peft import PeftModel
+import warnings
+import requests
+
+
+REPO_ROOT = Path(__file__).parents[3]
+sys.path.insert(0, str(REPO_ROOT))
+
+
+from tests.utils.cleanup_utils import safe_remove_directory
+from tests.utils.os_utils import require_package, require_python_package
+
+require_package(""ffmpeg"", ""ffmpeg"")
+require_python_package(""soundfile"")
+
+import soundfile as sf
+
+print(f""\n{'='*80}"")
+print("" SECTION 1: Loading Model and LoRA Adapters"")
+print(f""{'='*80}"")
+
+
+model, tokenizer = FastModel.from_pretrained(
+    model_name = ""unsloth/whisper-large-v3"",
+    dtype = None, # Leave as None for auto detection
+    load_in_4bit = False, # Set to True to do 4bit quantization which reduces memory
+    auto_model = WhisperForConditionalGeneration,
+    whisper_language = ""English"",
+    whisper_task = ""transcribe"",
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+
+base_model_class = model.__class__.__name__
+#https://github.com/huggingface/transformers/issues/37172
+model.generation_config.input_ids = model.generation_config.forced_decoder_ids
+model.generation_config.forced_decoder_ids = None
+
+
+model = FastModel.get_peft_model(
+    model,
+    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
+    target_modules = [""q_proj"", ""v_proj""],
+    lora_alpha = 64,
+    lora_dropout = 0, # Supports any, but = 0 is optimized
+    bias = ""none"",    # Supports any, but = ""none"" is optimized
+    # [NEW] ""unsloth"" uses 30% less VRAM, fits 2x larger batch sizes!
+    use_gradient_checkpointing = ""unsloth"", # True or ""unsloth"" for very long context
+    random_state = 3407,
+    use_rslora = False,  # We support rank stabilized LoRA
+    loftq_config = None, # And LoftQ
+    task_type = None, # ** MUST set this for Whisper **
+)
+
+print("" Model and LoRA adapters loaded successfully!"")
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 2: Checking Model Class Type"")
+print(f""{'='*80}"")
+
+assert isinstance(model, PeftModel), ""Model should be an instance of PeftModel""
+print("" Model is an instance of PeftModel!"")
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 3: Checking Config Model Class Type"")
+print(f""{'='*80}"")
+
+def find_lora_base_model(model_to_inspect):
+    current = model_to_inspect
+    if hasattr(current, ""base_model""):
+        current = current.base_model
+    if hasattr(current, ""model""):
+        current = current.model
+    return current
+pass
+
+
+config_model = find_lora_base_model(model) if isinstance(model, PeftModel) else model
+
+assert config_model.__class__.__name__ == base_model_class, f""Expected config_model class to be {base_model_class}""
+print("" config_model returns correct Base Model class:"", str(base_model_class))
+
+
+
+print(f""\n{'='*80}"")
+print("" SECTION 4: Saving and Merging Model"")
+print(f""{'='*80}"")
+
+with warnings.catch_warnings():
+    warnings.simplefilter(""error"")  # Treat warnings as errors
+    try:
+        model.save_pretrained_merged(""whisper"", tokenizer)
+        print("" Model saved and merged successfully without warnings!"")
+    except Exception as e:
+        assert False, f""Model saving/merging failed with exception: {e}""
+
+print(f""\n{'='*80}"")
+print("" SECTION 5: Loading Model for Inference"")
+print(f""{'='*80}"")
+
+
+model, tokenizer = FastModel.from_pretrained(
+    model_name = ""./whisper"",
+    dtype = None, # Leave as None for auto detection
+    load_in_4bit = False, # Set to True to do 4bit quantization which reduces memory
+    auto_model = WhisperForConditionalGeneration,
+    whisper_language = ""English"",
+    whisper_task = ""transcribe"",
+    # token = ""hf_..."", # use one if using gated models like meta-llama/Llama-2-7b-hf
+)
+
+# model = WhisperForConditionalGeneration.from_pretrained(""./whisper"")
+# processor = WhisperProcessor.from_pretrained(""./whisper"")
+
+print("" Model loaded for inference successfully!"")
+
+print(f""\n{'='*80}"")
+print("" SECTION 6: Downloading Sample Audio File"")
+print(f""{'='*80}"")
+
+audio_url = ""https://upload.wikimedia.org/wikipedia/commons/5/5b/Speech_12dB_s16.flac""
+audio_file = ""Speech_12dB_s16.flac""
+
+try:
+    headers = {
+        ""User-Agent"": ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36""
+    }
+    response = requests.get(audio_url, headers=headers)
+    response.raise_for_status()
+    with open(audio_file, ""wb"") as f:
+        f.write(response.content)
+    print("" Audio file downloaded successfully!"")
+except Exception as e:
+    assert False, f""Failed to download audio file: {e}""
+
+print(f""\n{'='*80}"")
+print("" SECTION 7: Running Inference"")
+print(f""{'='*80}"")
+
+
+from transformers import pipeline
+import torch
+FastModel.for_inference(model)
+model.eval()
+#Create pipeline without specifying the device
+whisper = pipeline(
+    ""automatic-speech-recognition"",
+    model=model,
+    tokenizer=tokenizer.tokenizer,
+    feature_extractor=tokenizer.feature_extractor,
+    processor=tokenizer,
+    return_language=True,
+    torch_dtype=torch.float16  # Remove the device parameter
+)
+# Example usage
+audio_file = ""Speech_12dB_s16.flac""
+transcribed_text = whisper(audio_file)
+# audio, sr = sf.read(audio_file)
+# input_features = processor(audio, return_tensors=""pt"").input_features
+# transcribed_text = model.generate(input_features=input_features)
+print(f"" Transcribed Text: {transcribed_text['text']}"")
+
+## assert that transcribed_text contains The birch canoe slid on the smooth planks. Glued the sheet to the dark blue background. It's easy to tell the depth of a well. Four hours of steady work faced us.
+
+expected_phrases = [
+    ""birch canoe slid on the smooth planks"",
+    ""sheet to the dark blue background"",
+    ""easy to tell the depth of a well"",
+    ""Four hours of steady work faced us"",
+]
+
+transcribed_lower = transcribed_text[""text""].lower()
+all_phrases_found = all(phrase.lower() in transcribed_lower for phrase in expected_phrases)
+
+assert all_phrases_found, f""Expected phrases not found in transcription: {transcribed_text['text']}""
+print("" Transcription contains all expected phrases!"")
+
+
+safe_remove_directory(""./unsloth_compiled_cache"")
+safe_remove_directory(""./whisper"")
diff --git a/tests/saving/vision_models/test_index_file_sharded_model.py b/tests/saving/vision_models/test_index_file_sharded_model.py
index 71dfe24..09c4bb1 100644
--- a/tests/saving/vision_models/test_index_file_sharded_model.py
+++ b/tests/saving/vision_models/test_index_file_sharded_model.py
@@ -11,8 +11,9 @@ from huggingface_hub import HfFileSystem
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 
diff --git a/tests/saving/vision_models/test_push_to_hub_merged.py b/tests/saving/vision_models/test_push_to_hub_merged.py
index 9845c3e..372d225 100644
--- a/tests/saving/vision_models/test_push_to_hub_merged.py
+++ b/tests/saving/vision_models/test_push_to_hub_merged.py
@@ -11,8 +11,10 @@ from trl import SFTTrainer, SFTConfig
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
+
 
 from tests.utils.cleanup_utils import safe_remove_directory
 
diff --git a/tests/saving/vision_models/test_save_merge_vision_model_ocr_benchmark.py b/tests/saving/vision_models/test_save_merge_vision_model_ocr_benchmark.py
index 8f2617e..e556f62 100644
--- a/tests/saving/vision_models/test_save_merge_vision_model_ocr_benchmark.py
+++ b/tests/saving/vision_models/test_save_merge_vision_model_ocr_benchmark.py
@@ -11,8 +11,9 @@ from trl import SFTTrainer, SFTConfig
 import sys
 from pathlib import Path
 
+
 REPO_ROOT = Path(__file__).parents[3]
-sys.path.append(str(REPO_ROOT))
+sys.path.insert(0, str(REPO_ROOT))
 
 from tests.utils.cleanup_utils import safe_remove_directory
 from tests.utils.ocr_eval import OCRModelEvaluator
diff --git a/tests/utils/os_utils.py b/tests/utils/os_utils.py
new file mode 100644
index 0000000..281fcdb
--- /dev/null
+++ b/tests/utils/os_utils.py
@@ -0,0 +1,119 @@
+import subprocess
+import sys
+import os
+import shutil
+import importlib
+
+def detect_package_manager():
+    """"""Detect the available package manager""""""
+    package_managers = {
+        'apt': '/usr/bin/apt',
+        'yum': '/usr/bin/yum',
+        'dnf': '/usr/bin/dnf',
+        'pacman': '/usr/bin/pacman',
+        'zypper': '/usr/bin/zypper'
+    }
+
+    for pm, path in package_managers.items():
+        if os.path.exists(path):
+            return pm
+    return None
+
+def check_package_installed(package_name, package_manager=None):
+    """"""Check if a package is installed using the system package manager""""""
+
+    if package_manager is None:
+        package_manager = detect_package_manager()
+
+    if package_manager is None:
+        print(""Warning: Could not detect package manager"")
+        return None
+
+    try:
+        if package_manager == 'apt':
+            # Check with dpkg
+            result = subprocess.run(['dpkg', '-l', package_name],
+                                  capture_output=True, text=True)
+            return result.returncode == 0
+
+        elif package_manager in ['yum', 'dnf']:
+            # Check with rpm
+            result = subprocess.run(['rpm', '-q', package_name],
+                                  capture_output=True, text=True)
+            return result.returncode == 0
+
+        elif package_manager == 'pacman':
+            result = subprocess.run(['pacman', '-Q', package_name],
+                                  capture_output=True, text=True)
+            return result.returncode == 0
+
+        elif package_manager == 'zypper':
+            result = subprocess.run(['zypper', 'se', '-i', package_name],
+                                  capture_output=True, text=True)
+            return package_name in result.stdout
+
+    except Exception as e:
+        print(f""Error checking package: {e}"")
+        return None
+
+def require_package(package_name, executable_name=None):
+    """"""Require a package to be installed, exit if not found""""""
+
+    # First check if executable is in PATH (most reliable)
+    if executable_name:
+        if shutil.which(executable_name):
+            print(f"" {executable_name} is available"")
+            return
+
+    # Then check with package manager
+    pm = detect_package_manager()
+    is_installed = check_package_installed(package_name, pm)
+
+    if is_installed:
+        print(f"" Package {package_name} is installed"")
+        return
+
+    # Package not found - show installation instructions
+    print(f"" Error: {package_name} is not installed"")
+    print(f""\nPlease install {package_name} using your system package manager:"")
+
+    install_commands = {
+        'apt': f""sudo apt update && sudo apt install {package_name}"",
+        'yum': f""sudo yum install {package_name}"",
+        'dnf': f""sudo dnf install {package_name}"",
+        'pacman': f""sudo pacman -S {package_name}"",
+        'zypper': f""sudo zypper install {package_name}""
+    }
+
+    if pm and pm in install_commands:
+        print(f""  {install_commands[pm]}"")
+    else:
+        for pm_name, cmd in install_commands.items():
+            print(f""  {pm_name}: {cmd}"")
+
+    print(f""\nAlternatively, install with conda:"")
+    print(f""  conda install -c conda-forge {package_name}"")
+
+    print(f""\nPlease install the required package and run the script again."")
+    sys.exit(1)
+
+# Usage
+#require_package(""ffmpeg"", ""ffmpeg"")
+
+def require_python_package(package_name, import_name=None, pip_name=None):
+    """"""Require a Python package to be installed, exit if not found""""""
+    if import_name is None:
+        import_name = package_name
+    if pip_name is None:
+        pip_name = package_name
+
+    if importlib.util.find_spec(import_name) is None:
+        print(f"" Error: Python package '{package_name}' is not installed"")
+        print(f""\nPlease install {package_name} using pip:"")
+        print(f""  pip install {pip_name}"")
+        print(f""  # or with conda:"")
+        print(f""  conda install {pip_name}"")
+        print(f""\nAfter installation, run this script again."")
+        sys.exit(1)
+    else:
+        print(f"" Python package '{package_name}' is installed"")
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index cc3dbb1..d347cd1 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh
+from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh, triton_cast
 from transformers.models.llama.modeling_llama import logger
 from packaging.version import Version
 
@@ -64,7 +64,7 @@ def _cross_entropy_forward(
         This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.
     """"""
     row_idx = tl.program_id(0)
-    logits_ptr    += row_idx * tl.cast(logits_row_stride, tl.int64)
+    logits_ptr    += row_idx * triton_cast(logits_row_stride, tl.int64)
     loss_ptr      += row_idx
     logsumexp_ptr += row_idx
     labels_ptr    += row_idx
@@ -142,7 +142,7 @@ def _chunked_cross_entropy_forward(
     """"""
     row_idx   = tl.program_id(0)
     chunk_idx = tl.program_id(1)
-    logits_ptr    += row_idx * tl.cast(logits_row_stride, tl.int64)
+    logits_ptr    += row_idx * triton_cast(logits_row_stride, tl.int64)
     loss_ptr      += row_idx
     logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx
     labels_ptr    += row_idx
@@ -216,7 +216,7 @@ def _cross_entropy_backward(
     row_idx   = tl.program_id(0)
     block_idx = tl.program_id(1)
 
-    logits_ptr += row_idx * tl.cast(logits_row_stride, tl.int64)
+    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)
     dloss_ptr  += row_idx *  dloss_row_stride
     col_offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
     mask = col_offsets < VOCAB_SIZE
@@ -400,6 +400,6 @@ if (Version(torch.__version__) < Version(""2.4.0"")) and \
 pass
 
 # Patch CE Losses in transformers
-def patch_loss_functions():
-    _patch_loss_functions(fast_cross_entropy_loss)
+def patch_loss_functions(torch_compile = True):
+    _patch_loss_functions(fast_cross_entropy_loss, torch_compile = torch_compile)
 pass
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index b394d12..de54396 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -31,12 +31,18 @@ pass
 # tl.math.tanh now is libdevice.tanh
 from packaging.version import Version
 import triton
+import triton.language as tl
 if Version(triton.__version__) >= Version(""3.0.0""):
     from triton.language.extra import libdevice
     triton_tanh = libdevice.tanh
+    triton_cast = tl.cast
 else:
-    import triton.language as tl
     triton_tanh = tl.math.tanh
+    # No casting in old Triton versions
+    @triton.jit
+    def triton_cast(x, dtype):
+        return x.to(dtype)
+    pass
 pass
 
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 5fff966..cb9ae48 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -104,7 +104,7 @@ logging.getLogger(""transformers.tokenization_utils_base"").setLevel(logging.CRITI
 # Ignore logging messages
 class HideLoggingMessage(logging.Filter):
     def __init__(self, text): self.text = text
-    def filter(self, x): return not x.getMessage().startswith(self.text)
+    def filter(self, x): return not (self.text in x.getMessage())
 pass
 
 # The speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.
@@ -112,6 +112,14 @@ from transformers.training_args import logger as transformers_training_args_logg
 transformers_training_args_logger.addFilter(HideLoggingMessage(""The speedups""))
 del transformers_training_args_logger
 
+# Using the default loss: `ForCausalLMLoss`.
+try:
+    from transformers.modeling_utils import logger as transformers_modeling_utils_logger
+    transformers_modeling_utils_logger.addFilter(HideLoggingMessage(""ForCausalLMLoss""))
+    del transformers_modeling_utils_logger
+except:
+    pass
+
 # =============================================
 
 # =============================================
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 7f07bea..47a5702 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2317,7 +2317,8 @@ class FastLlamaModel:
                     layer.self_attn.apply_qkv = apply_lora_qkv
                     n_qkv += 1
                 else:
-                    if model_type != ""qwen2"":
+                    if model_type == ""qwen2"": n_qkv += 1
+                    else:
                         logger.warning_once(
                             ""Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n""\
                             ""are not enabled or a bias term (like in Qwen) is used.""
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 10e40ab..d4f1278 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -384,22 +384,54 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Qwen2.5-Math-72B-Instruct"",
         ""Qwen/Qwen2.5-Math-72B-Instruct"",
     ),
+    ""unsloth/Qwen2.5-Coder-0.5B-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-0.5B"",
+        ""Qwen/Qwen2.5-Coder-0.5B"",
+    ),
     ""unsloth/Qwen2.5-Coder-1.5B-bnb-4bit"" : (
         ""unsloth/Qwen2.5-Coder-1.5B"",
         ""Qwen/Qwen2.5-Coder-1.5B"",
     ),
+    ""unsloth/Qwen2.5-Coder-3B-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-3B"",
+        ""Qwen/Qwen2.5-Coder-3B"",
+    ),
     ""unsloth/Qwen2.5-Coder-7B-bnb-4bit"" : (
         ""unsloth/Qwen2.5-Coder-7B"",
         ""Qwen/Qwen2.5-Coder-7B"",
     ),
+    ""unsloth/Qwen2.5-Coder-14B-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-14B"",
+        ""Qwen/Qwen2.5-Coder-14B"",
+    ),
+    ""unsloth/Qwen2.5-Coder-32B-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-32B"",
+        ""Qwen/Qwen2.5-Coder-32B"",
+    ),
+    ""unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-Instruct-0.5B"",
+        ""Qwen/Qwen2.5-Coder-Instruct-0.5B"",
+    ),
     ""unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit"" : (
         ""unsloth/Qwen2.5-Coder-Instruct-1.5B"",
         ""Qwen/Qwen2.5-Coder-Instruct-1.5B"",
     ),
+    ""unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-3B-Instruct"",
+        ""Qwen/Qwen2.5-Coder-3B-Instruct"",
+    ),
     ""unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit"" : (
         ""unsloth/Qwen2.5-Coder-7B-Instruct"",
         ""Qwen/Qwen2.5-Coder-7B-Instruct"",
     ),
+    ""unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-14B-Instruct"",
+        ""Qwen/Qwen2.5-Coder-14B-Instruct"",
+    ),
+    ""unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit"" : (
+        ""unsloth/Qwen2.5-Coder-32B-Instruct"",
+        ""Qwen/Qwen2.5-Coder-32B-Instruct"",
+    ),
     ""unsloth/Llama-3.2-1B-bnb-4bit"" : (
         ""unsloth/Llama-3.2-1B"",
         ""meta-llama/Llama-3.2-1B"",
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index c639dbf..ed95e07 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -588,15 +588,21 @@ pass
 def _fix_chat_template(chat_template):
     endfor = ""{% endfor %}""
     where = chat_template.find(endfor)
-    if where == -1: return chat_template
+    if where == -1:
+        endfor = ""{%- endfor %}""
+        where = chat_template.find(endfor)
+    if where == -1:
+        return chat_template
 
     after_endfor = chat_template[where + len(endfor):]
 
-    if ""{% if"" not in after_endfor and ""{% set "" not in after_endfor and \
+    dash = ""-"" if endfor.startswith(""{%-"") else """"
+
+    if ""{%"" + dash + "" if"" not in after_endfor and ""{%"" + dash + "" set "" not in after_endfor and \
         after_endfor.startswith(""{{"") and after_endfor.endswith(""}}"") and \
         after_endfor.count(""{{"") == 1 and after_endfor.count(""}}"") == 1:
 
-        after_endfor = ""{% if add_generation_prompt %}"" + after_endfor + ""{% endif %}""
+        after_endfor = ""{%"" + dash + "" if add_generation_prompt %}"" + after_endfor + endfor
 
         chat_template = chat_template[:where + len(endfor)] + after_endfor
     pass
@@ -643,10 +649,12 @@ def fix_chat_template(tokenizer):
 
     if no == yes:
         # SAME?! That's not good! We check for add_generation_prompt
-        if ""{% if add_generation_prompt %}"" not in chat_template:
+        if   ""{% if add_generation_prompt %}"" not in chat_template and \
+            ""{%- if add_generation_prompt %}"" not in chat_template:
             # Try fixing it by adding it
             new_chat_template = _fix_chat_template(chat_template)
-            if ""{% if add_generation_prompt %}"" not in new_chat_template:
+            if   ""{% if add_generation_prompt %}"" not in new_chat_template and \
+                ""{%- if add_generation_prompt %}"" not in new_chat_template:
                 raise RuntimeError(
                     f""Unsloth: The tokenizer `{tokenizer.name_or_path}`\n""\
                     ""does not have a {% if add_generation_prompt %} for generation purposes.\n""\
@@ -1001,13 +1009,14 @@ def patch_sft_trainer_tokenizer():
         # Also DPO weirdly tokenizes non numeric columns? Delete them!
         check_text += \
         ""\n""\
-        ""column_names = set(self.train_dataset.column_names)\n""\
-        ""check = ['chosen', 'rejected', 'prompt', 'chosen_input_ids', 'chosen_attention_mask',\n""\
-        "" 'chosen_labels', 'rejected_input_ids', 'rejected_attention_mask', 'rejected_labels',\n""\
-        "" 'prompt_input_ids', 'prompt_attention_mask']\n""\
-        ""if all(x in column_names for x in check):\n""\
-        ""    self.train_dataset = self.train_dataset.remove_columns(['chosen', 'rejected', 'prompt'])\n""\
-        ""del check, column_names\n""\
+        ""if hasattr(self.train_dataset, 'column_names'):\n""\
+        ""    column_names = set(self.train_dataset.column_names)\n""\
+        ""    check = ['chosen', 'rejected', 'prompt', 'chosen_input_ids', 'chosen_attention_mask',\n""\
+        ""        'chosen_labels', 'rejected_input_ids', 'rejected_attention_mask', 'rejected_labels',\n""\
+        ""        'prompt_input_ids', 'prompt_attention_mask']\n""\
+        ""    if all(x in column_names for x in check):\n""\
+        ""        self.train_dataset = self.train_dataset.remove_columns(['chosen', 'rejected', 'prompt'])\n""\
+        ""    del check, column_names\n""\
         ""\n""
 
         check_text = check_text.split(""\n"")
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bc7dc69..d5dd783 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(
     bsz, q_len, _ = hidden_states.size()
 
     # Check for inference
-    if past_key_value is not None and q_len == 1 and bsz == 1:
+    if False: #past_key_value is not None and q_len == 1 and bsz == 1:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -350,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
     bsz, q_len, hd = hidden_states.size()
-    if (past_key_value is not None and q_len == 1 and bsz == 1):
+    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
"
"diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index e5cb226..24cd5e6 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -107,7 +107,7 @@ import torch
 import numpy as np
 from contextlib import nullcontext
 from torch.nn import functional as F
-from transformers import DataCollatorForSeq2Seq, DataCollatorForLanguageModeling
+from transformers import DataCollatorForSeq2Seq, DataCollatorForLanguageModeling as TransformersDataCollatorForLanguageModeling
 
 torch_compile_options = {{
     ""epilogue_fusion""   : True,
@@ -358,8 +358,8 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""from unsloth_zoo.vision_utils import UnslothVisionDataCollator\n""\
         ""if not isinstance(data_collator, UnslothVisionDataCollator):\n""\
         ""    if isinstance(data_collator, DataCollatorForSeq2Seq) and 'labels' not in train_dataset.column_names:\n""\
-        ""        data_collator = DataCollatorForLanguageModeling(__tokenizer, mlm = False)\n""\
-        ""    elif isinstance(data_collator, DataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
+        ""        data_collator = TransformersDataCollatorForLanguageModeling(__tokenizer, mlm = False)\n""\
+        ""    elif isinstance(data_collator, TransformersDataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
         ""        data_collator = DataCollatorForSeq2Seq(__tokenizer)\n""\
         ""else:\n""\
         ""    if hasattr(args, 'remove_unused_columns'): args.remove_unused_columns = False\n""\
@@ -374,7 +374,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""        if isinstance(data_collator, DataCollatorForSeq2Seq):\n""\
         ""            data_collator = DataCollatorForSeq2Seq(__tokenizer.tokenizer)\n""\
         ""        else:\n""\
-        ""            data_collator = DataCollatorForLanguageModeling(__tokenizer.tokenizer, mlm = False)\n""
+        ""            data_collator = TransformersDataCollatorForLanguageModeling(__tokenizer.tokenizer, mlm = False)\n""
         extra_args += pad_check
     pass
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a93f18c..ab752f9 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.1.2""
+__version__ = ""2025.1.3""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 384b4bb..67a4c66 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -265,6 +265,10 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):
     )))
     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))
 
+    # Remove replacement char for false positive
+    replacement_char = b""\xc3\xaf\xc2\xbf\xc2\xbd"".decode(""utf-8"")
+    all_special_tokens = [x for x in all_special_tokens if x != replacement_char]
+
     # Check if chat template is enabled!
     check_chat_template1 = True
     check_chat_template2 = True
"
"diff --git a/pyproject.toml b/pyproject.toml
index 6bf4038..1d20691 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.2"",
+    ""unsloth_zoo>=2025.3.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.1"",
+    ""unsloth_zoo>=2025.3.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 8439ab8..4336ec4 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.2""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.4""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index e11cd54..a187ee5 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -12,12 +12,11 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
-from .granite import FastGraniteModel
-from .loader  import FastLanguageModel, FastVisionModel
 from .llama   import FastLlamaModel
+from .loader  import FastLanguageModel, FastVisionModel
 from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
+from .granite import FastGraniteModel
 from .dpo     import PatchDPOTrainer, PatchKTOTrainer
 from ._utils  import is_bfloat16_supported, __version__
 from .rl      import PatchFastRL, vLLMSamplingParams
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7d6bbfb..c01e0cc 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.5""
+__version__ = ""2025.3.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -1050,7 +1050,10 @@ def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
         pass
     pass
 
-    if num_items_in_batch is None:
+    # Get gradient accumulation steps if possible
+    if num_items_in_batch is None and \
+        getattr(self, ""args"", {}).get(""gradient_accumulation_steps"", 1) != 1:
+
         name = (model.base_model.model if hasattr(model, ""base_model"") else model).__class__.__name__
         logger.warning_once(
             f""Unsloth: Not an error, but {name} does not accept `num_items_in_batch`.\n""\
@@ -1245,10 +1248,11 @@ pass
 # os.environ['UNSLOTH_RETURN_LOGITS'] = '1'
 LOGITS_ERROR_STRING = \
     ""Unsloth: Logits are empty from 2024.11 onwards. To get raw logits again, please ""\
-    'set the environment variable `UNSLOTH_RETURN_LOGITS` to `""1"" BEFORE starting to train ie before `trainer.train()`. For example:\n\n'\
-    ""import os\n""\
+    'set the environment variable `UNSLOTH_RETURN_LOGITS` to `""1"" BEFORE starting to train ie before `trainer.train()`. For example:\n'\
+    ""```\nimport os\n""\
     ""os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n""\
-    ""... trainer.train() ...""
+    ""trainer.train()\n```\n""\
+    ""No need to restart your console - just add `os.environ['UNSLOTH_RETURN_LOGITS'] = '1'` before trainer.train() and re-run the cell!""
 
 def raise_logits_error(*args, **kwargs): raise NotImplementedError(LOGITS_ERROR_STRING)
 def return_none(*args, **kwargs): return None
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index c9ea922..cf9c165 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -284,6 +284,17 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         extra_args += eval_changes
     pass
 
+    # Force logits to be produced if preprocess_logits_for_metrics or compute_metrics is used
+    if ""model"" in call_args:
+        logits_check = \
+        ""_output_logits = False\n""\
+        ""if locals().get('compute_metrics', None) is not None: _output_logits = True\n""\
+        ""if locals().get('preprocess_logits_for_metrics', None) is not None: _output_logits = True\n""\
+        ""if _output_logits:\n""\
+        ""    os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n""
+        extra_args += logits_check
+    pass
+
     # Check max_seq_length
     if ""model"" in call_args:
         length_check = \
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index d1df57a..ab2694f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -68,6 +68,7 @@ __all__ = [
     ""patch_fast_lora"",
     ""validate_loftq_config"",
     ""RaiseUninitialized"",
+    ""dequantize_module_weight"",
 ]
 
 import torch
@@ -724,6 +725,7 @@ pass
 # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??
 # For mixed precision, we need it to be in float32 not float16.
 from peft import __version__ as peft_version
+from peft.utils.integrations import dequantize_module_weight
 if Version(peft_version) < Version(""0.12.0""):
     from peft.tuners.lora.layer import LoraLayer
     try:
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 7ac2715..d0b7d4d 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -618,9 +618,18 @@ class FastModel(FastBaseModel):
                 os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
                     ""all;None;None;""\
                     ""x = 'gate_up_proj_bias'\n""\
-                    ""if hasattr(module, x): setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
+                    ""if hasattr(module, x): ""\
+                    ""setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
                     ""x = 'down_proj_bias'\n""\
-                    ""if hasattr(module, x): setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n;""
+                    "";""
+            else:
+                # Set down projection compute dtype to be float32 for float16 machines
+                os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
+                    ""all;None;None;""\
+                    ""if 'down_projs' in name and hasattr(module, 'compute_dtype') and ""\
+                    ""torch.amax(dequantize_module_weight(module)) >= 1024:""\
+                    ""module._pre_set_compute_dtype = torch.float32\n""\
+                    "";""
         else:
             for check_model_name in DISABLE_COMPILE_MODEL_NAMES:
                 if check_model_name in lowered_model_name:
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 745b210..980425e 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -55,7 +55,7 @@ else:
 pass
 
 # Reduce VRAM usage by reducing fragmentation
-os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""expandable_segments:True""
+os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""expandable_segments:True,roundup_power2_divisions:[64:128,256:64,>:32]""
 
 # Hugging Face Hub faster downloads
 if ""HF_HUB_ENABLE_HF_TRANSFER"" not in os.environ:
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 82e7641..ef5fa5d 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -42,6 +42,7 @@ from .fast_lora import (
     apply_lora_mlp_geglu_approx,
     apply_lora_qkv,
     apply_lora_o,
+    fast_lora_forward,
 )
 from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora
 
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index 2177b43..c2b7929 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -410,3 +410,81 @@ def apply_lora_o(self, X):
     O = LoRA_W.apply(X, OW, OW_quant, OA, OB, OS)
     return O
 pass
+
+
+IDENTITY_DROPOUT = torch.nn.Identity
+@torch._disable_dynamo
+def fast_lora_forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
+    raise NotImplementedError(
+        ""Unsloth: Currently not supported yet - reshaping done incorrectly""
+    )
+    self._check_forward_args(x, *args, **kwargs)
+    adapter_names = kwargs.pop(""adapter_names"", None)
+
+    if self.disable_adapters:
+        if self.merged:
+            self.unmerge()
+        result = self.base_layer(x, *args, **kwargs)
+    elif adapter_names is not None:
+        result = self._mixed_batch_forward(x, *args, adapter_names=adapter_names, **kwargs)
+    elif self.merged:
+        result = self.base_layer(x, *args, **kwargs)
+    else:
+        # Fastpath
+        if len(self.active_adapters) == 1:
+            active_adapter = self.active_adapters[0]
+            if active_adapter not in self.lora_A.keys(): return self.base_layer(x, *args, **kwargs)
+
+            dropout = self.lora_dropout[active_adapter]
+            if isinstance(dropout, IDENTITY_DROPOUT) and not self.use_dora[active_adapter]:
+                lora_A = self.lora_A[active_adapter].weight
+                lora_B = self.lora_B[active_adapter].weight
+                scaling = self.scaling[active_adapter]
+                W = self.base_layer.weight
+                return LoRA_W.apply(x, W, QUANT_STATE(W), lora_A, lora_B, scaling)
+            pass
+        pass
+
+        result = self.base_layer(x, *args, **kwargs)
+        # As per Tim Dettmers, for 4bit, we need to defensively clone here.
+        # The reason is that in some cases, an error can occur that backprop
+        # does not work on a manipulated view. This issue may be solved with
+        # newer PyTorch versions but this would need extensive testing to be
+        # sure.
+        result = result.clone()
+
+        for active_adapter in self.active_adapters:
+            if active_adapter not in self.lora_A.keys():
+                continue
+            lora_A = self.lora_A[active_adapter]
+            lora_B = self.lora_B[active_adapter]
+            dropout = self.lora_dropout[active_adapter]
+            scaling = self.scaling[active_adapter]
+
+            requires_conversion = not torch.is_autocast_enabled()
+            if requires_conversion:
+                expected_dtype = result.dtype
+                x = x.to(lora_A.weight.dtype)
+
+            if not self.use_dora[active_adapter]:
+                result = result + lora_B(lora_A(dropout(x))) * scaling
+            else:
+                if isinstance(dropout, torch.nn.Identity) or not self.training:
+                    base_result = result
+                else:
+                    x = dropout(x)
+                    base_result = None
+
+                result = result + self.lora_magnitude_vector[active_adapter](
+                    x,
+                    lora_A=lora_A,
+                    lora_B=lora_B,
+                    scaling=scaling,
+                    base_layer=self.get_base_layer(),
+                    base_result=base_result,
+                )
+            if requires_conversion:
+                result = result.to(expected_dtype)
+
+    return result
+pass
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 4b22f8c..b74d636 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -57,6 +57,7 @@ pass
 @triton.jit
 def _rms_layernorm_backward(
     dY, dY_row_stride,
+    dX, dX_row_stride,
     X,   X_row_stride,
     W,   W_row_stride,
     r,   r_row_stride,
@@ -78,6 +79,9 @@ def _rms_layernorm_backward(
     X  += row_idx *  X_row_stride
     r  += row_idx *  r_row_stride
 
+    if GEMMA: dX += row_idx * dY_row_stride
+    else:     dX = dY
+
     dY_row = tl.load(dY + col_offsets, mask = mask, other = 0).to(tl.float32)
     X_row  = tl.load(X  + col_offsets, mask = mask, other = 0).to(tl.float32)
     W_row  = tl.load(W  + col_offsets, mask = mask, other = 0).to(tl.float32)
@@ -91,7 +95,7 @@ def _rms_layernorm_backward(
 
     rowsum_dY_normed = tl.sum(dY_W * normed, axis = 0)
     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)
-    tl.store(dY + col_offsets, output, mask = mask)
+    tl.store(dX + col_offsets, output, mask = mask)
 pass
 
 
@@ -172,9 +176,11 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         n_cols : int
         n_rows, n_cols = dY.shape
         # dW = X
+        dX = torch.empty_like(dY, device = ""cuda:0"") if ctx.GEMMA else dY
 
         _rms_layernorm_backward[(n_rows,)](
             dY, dY.stride(0),
+            dX, dX.stride(0),
             X,  X .stride(0),
             W,  W .stride(0),
             r,  r .stride(0),
@@ -184,7 +190,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
             BLOCK_SIZE = ctx.BLOCK_SIZE,
             num_warps  = ctx.num_warps,
         )
-        dX = dY.view(*shape)
+        dX = dX.view(*shape)
         return dX, None, None, None
     pass
 pass
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index e67a9e5..3230cdc 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .loader  import FastLanguageModel
+from .loader  import FastLanguageModel, FastVisionModel
 from .llama   import FastLlamaModel
 from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index daa81d9..ee85ba3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.11.7""
+__version__ = ""2024.11.8""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -52,6 +52,17 @@ __all__ = [
     ""unpatch_unsloth_gradient_checkpointing"",
     ""patch_gradient_checkpointing"",
     ""unpatch_gradient_checkpointing"",
+
+    ""HAS_CUT_CROSS_ENTROPY"",
+    ""fused_linear_cross_entropy"",
+    ""patch_unsloth_smart_gradient_checkpointing"",
+    ""unpatch_unsloth_smart_gradient_checkpointing"",
+    ""create_gradient_checkpointing_buffer"",
+
+    ""patch_compiled_autograd"",
+    ""process_vision_info"",
+    ""unsloth_compile_transformers"",
+    ""patch_fast_lora"",
 ]
 
 import torch
@@ -70,6 +81,7 @@ from unsloth_zoo.patching_utils import (
     patch_layernorm,
     patch_torch_compile,
     patch_model_and_tokenizer,
+    patch_compiled_autograd,
 )
 from unsloth_zoo.gradient_checkpointing import (
     Unsloth_Offloaded_Gradient_Checkpointer,
@@ -81,6 +93,21 @@ from unsloth_zoo.gradient_checkpointing import (
     unsloth_gradient_checkpoint,
     patch_gradient_checkpointing,
     unpatch_gradient_checkpointing,
+
+    patch_unsloth_smart_gradient_checkpointing,
+    unpatch_unsloth_smart_gradient_checkpointing,
+    create_gradient_checkpointing_buffer,
+)
+from unsloth_zoo.loss_utils import (
+    HAS_CUT_CROSS_ENTROPY,
+    fused_linear_cross_entropy,
+)
+from unsloth_zoo.vision_utils import (
+    process_vision_info,
+)
+from unsloth_zoo.compiler import (
+    get_transformers_model_type,
+    unsloth_compile_transformers as _unsloth_compile_transformers,
 )
 
 # =============================================
@@ -120,6 +147,22 @@ try:
 except:
     pass
 
+# The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
+try:
+    from accelerate.utils.modeling import logger as accelerate_utils_modeling_logger
+    accelerate_utils_modeling_logger.addFilter(HideLoggingMessage(""The model weights are not tied""))
+    del accelerate_utils_modeling_logger
+except:
+    pass
+
+# Setting `pad_token_id` to `eos_token_id`
+try:
+    from transformers.generation.utils import logger as transformers_generation_utils_logger
+    transformers_generation_utils_logger.addFilter(HideLoggingMessage(""Setting `pad_token_id` to `eos_token_id`""))
+    del transformers_generation_utils_logger
+except:
+    pass
+
 # =============================================
 
 # =============================================
@@ -282,54 +325,60 @@ from transformers.models.llama.modeling_llama import logger
 
 # =============================================
 # Get Xformers
-from xformers import __version__ as xformers_version
-# Temporarily disable 0.0.27 and higher - inference issues
-if False: #Version(xformers_version) >= Version(""0.0.27""):
-    raise ImportError(
-        ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
-        ""then press Disconnect Runtime and then Restart it.\n""\
-        ""\n""\
-        ""%%capture\n""
-        ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
-        '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
-        '!pip install --no-deps ""xformers<=0.0.27"" trl peft accelerate bitsandbytes\n'\
-        '\n'\
-        f""Otherwise in local machines, your xformers version of {xformers_version} is too new.\n""\
-        'Please downgrade xformers via `pip install --force-reinstall ""xformers<=0.0.27""'
-    )
-pass
+try:
+    from xformers import __version__ as xformers_version
+    # Temporarily disable 0.0.27 and higher - inference issues
+    if False: #Version(xformers_version) >= Version(""0.0.27""):
+        raise ImportError(
+            ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
+            ""then press Disconnect Runtime and then Restart it.\n""\
+            ""\n""\
+            ""%%capture\n""
+            ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
+            '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
+            '!pip install --no-deps ""xformers<=0.0.27"" trl peft accelerate bitsandbytes\n'\
+            '\n'\
+            f""Otherwise in local machines, your xformers version of {xformers_version} is too new.\n""\
+            'Please downgrade xformers via `pip install --force-reinstall ""xformers<=0.0.27""'
+        )
+    pass
 
-if   Version(torch_version) < Version(""2.2.0"") and Version(xformers_version) >= Version(""0.0.24""):
-    raise ImportError(
-        f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
-        f""Please install xformers < 0.0.24 for torch = {torch_version}.""
-    )
-elif Version(torch_version) < Version(""2.3.0"") and Version(xformers_version) >= Version(""0.0.26""):
-    raise ImportError(
-        f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
-        f""Please install xformers < 0.0.26 for torch = {torch_version}.""
-    )
-elif Version(torch_version) < Version(""2.4.0"") and Version(xformers_version) > Version(""0.0.27""):
-    raise ImportError(
-        f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
-        f""Please install xformers <= 0.0.27 for torch = {torch_version}.""
-    )
-pass
+    if   Version(torch_version) < Version(""2.2.0"") and Version(xformers_version) >= Version(""0.0.24""):
+        raise ImportError(
+            f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
+            f""Please install xformers < 0.0.24 for torch = {torch_version}.""
+        )
+    elif Version(torch_version) < Version(""2.3.0"") and Version(xformers_version) >= Version(""0.0.26""):
+        raise ImportError(
+            f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
+            f""Please install xformers < 0.0.26 for torch = {torch_version}.""
+        )
+    elif Version(torch_version) < Version(""2.4.0"") and Version(xformers_version) > Version(""0.0.27""):
+        raise ImportError(
+            f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
+            f""Please install xformers <= 0.0.27 for torch = {torch_version}.""
+        )
+    pass
 
-from xformers._cpp_lib import _register_extensions
-try:
-    _register_extensions() # Check if C++ modules are loaded correctly
-except Exception as error:
-    raise ImportError(
-        ""Unsloth: Xformers was not installed correctly.\n""\
-        ""Please install xformers separately first.\n""\
-        ""Then confirm if it's correctly installed by running:\n""\
-        ""python -m xformers.info\n\n""
-        ""Longer error message:\n"" + str(error)
-    )
+    from xformers._cpp_lib import _register_extensions
+    try:
+        _register_extensions() # Check if C++ modules are loaded correctly
+    except Exception as error:
+        raise ImportError(
+            ""Unsloth: Xformers was not installed correctly.\n""\
+            ""Please install xformers separately first.\n""\
+            ""Then confirm if it's correctly installed by running:\n""\
+            ""python -m xformers.info\n\n""
+            ""Longer error message:\n"" + str(error)
+        )
+    pass
+    import xformers.ops.fmha as xformers
+    xformers_attention = xformers.memory_efficient_attention
+except:
+    xformers = None
+    xformers_attention = None
+    xformers_version = None
 pass
-import xformers.ops.fmha as xformers
-xformers_attention = xformers.memory_efficient_attention
 
 # Check TRL version
 from trl import __version__ as trl_version
@@ -658,7 +707,7 @@ BitsAndBytesConfig__init__ = BitsAndBytesConfig__init__.replace(
 )
 
 def _prepare_backend(
-    self, cpu: bool = False, sagemaker_dp = False, backend: str = None,
+    self, cpu = False, sagemaker_dp = False, backend: str = None,
 ) -> tuple[str, DistributedType]:
     return None, DistributedType.NO
 pass
@@ -1047,3 +1096,69 @@ def patch_tokenizer(model, tokenizer):
         model.config.update({""unsloth_version"" : __version__})
     return model, tokenizer
 pass
+
+
+def patch_fast_lora():
+    import peft.tuners.lora.bnb
+    peft.tuners.lora.bnb.Linear4bit.forward = fast_lora_forward
+pass
+
+
+def unsloth_compile_transformers(
+    model_name,
+    token                   = None,
+    revision                = None,
+    trust_remote_code       = False,
+    sdpa_dynamic_mask       = True,
+    sdpa_bool_masks         = True,
+    sdpa_gqa_replace        = True,
+    sdpa_dynamic_compile    = True,
+    compile_attention       = True,
+    disable_causal_masks    = True,
+    compile_torch_modules   = True,
+    compile_custom_modules  = True,
+    compile_function_calls  = True,
+    fuse_lm_head            = True,
+    gradient_checkpointing  = True,
+    manual_replacements     = True,
+    epilogue_fusion         = True,
+    max_autotune            = False,
+    shape_padding           = True,
+    cudagraphs              = False,
+    debug                   = False,
+    import_from_cache       = False,
+    disable                 = False,
+):
+    if disable: return
+    model_types = get_transformers_model_type(
+        model_name        = model_name,
+        token             = token,
+        revision          = revision,
+        trust_remote_code = trust_remote_code,
+    )
+    for model_type in model_types:
+        _unsloth_compile_transformers(
+            model_type,
+            sdpa_dynamic_mask      = sdpa_dynamic_mask,
+            sdpa_bool_masks        = sdpa_bool_masks,
+            sdpa_gqa_replace       = sdpa_gqa_replace,
+            sdpa_dynamic_compile   = sdpa_dynamic_compile,
+            compile_attention      = compile_attention,
+            disable_causal_masks   = disable_causal_masks,
+            compile_torch_modules  = compile_torch_modules,
+            compile_custom_modules = compile_custom_modules,
+            compile_function_calls = compile_function_calls,
+            fuse_lm_head           = fuse_lm_head,
+            gradient_checkpointing = gradient_checkpointing,
+            manual_replacements    = manual_replacements,
+            epilogue_fusion        = epilogue_fusion,
+            max_autotune           = max_autotune,
+            shape_padding          = shape_padding,
+            cudagraphs             = cudagraphs,
+            debug                  = debug,
+            import_from_cache      = import_from_cache,
+            disable                = disable,
+        )
+    pass
+    return model_types
+pass
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 4eb9d64..62ecb96 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -60,8 +60,7 @@ if HAS_FLASH_ATTENTION_SOFTCAPPING:
     from flash_attn import flash_attn_func
 
 # [TODO] We must randomnly use torch.compile?
-# I checked the gradients and formulas and I'm sure it's correct.
-# I'm stumped :(
+# Gemma 2 uses double RMS Layernorms, so the backward passes should not overwrite the gradients!
 @torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)
 def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
     old_dtype = X.dtype
@@ -207,7 +206,7 @@ def Gemma2DecoderLayer_fast_forward(
         hidden_states += residual
     else:
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_gemma2_compiled(self.input_layernorm, hidden_states, gemma = True)
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
             hidden_states=hidden_states,
             causal_mask=causal_mask,
@@ -218,14 +217,14 @@ def Gemma2DecoderLayer_fast_forward(
             use_cache=use_cache,
             padding_mask=padding_mask,
         )
-        hidden_states = fast_rms_layernorm_gemma2_compiled(self.post_attention_layernorm, hidden_states, gemma = True)
+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)
         hidden_states = residual + hidden_states
 
         # Fully Connected
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_gemma2_compiled(self. pre_feedforward_layernorm, hidden_states, gemma = True)
+        hidden_states = fast_rms_layernorm(self. pre_feedforward_layernorm, hidden_states, gemma = True)
         hidden_states = self.mlp(hidden_states)
-        hidden_states = fast_rms_layernorm_gemma2_compiled(self.post_feedforward_layernorm, hidden_states, gemma = True)
+        hidden_states = fast_rms_layernorm(self.post_feedforward_layernorm, hidden_states, gemma = True)
         hidden_states = residual + hidden_states
     pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 47a5702..0256fc1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -719,25 +719,33 @@ def LlamaModel_fast_forward(
     pass
 
     # Gemma2 has alternating SWA and global attn
+    use_static_mask  = True
+    dynamic_SWA_mask = None
+    dynamic_GA_mask  = None
     if IS_GEMMA2:
         if HAS_FLASH_ATTENTION_SOFTCAPPING and attention_mask is None:
             self.SWA_mask = True
             self.GA_mask  = False
         elif attention_mask is not None:
-            self.SWA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+
+            # Fixes https://github.com/unslothai/unsloth/issues/853
+            # Unsloth needs a 2D mask, not a [2, 1, n, n] mask!
+            dynamic_SWA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                 attention_mask,
                 (batch_size, seq_length),
                 inputs_embeds,
                 past_key_values_length,
                 sliding_window = self.config.sliding_window,
-            )
-            self.GA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+            )[0][0]
+            dynamic_GA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                 attention_mask,
                 (batch_size, seq_length),
                 inputs_embeds,
                 past_key_values_length,
                 sliding_window = None,
-            )
+            )[0][0]
+            use_static_mask = False
+
         elif not hasattr(self, ""SWA_mask""):
             if HAS_FLEX_ATTENTION:
                 # Use Flex Attention instead!
@@ -772,7 +780,12 @@ def LlamaModel_fast_forward(
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
         mask = causal_mask
-        if IS_GEMMA2: mask = self.SWA_mask if (idx % 2 == 0) else self.GA_mask
+        if IS_GEMMA2:
+            if (idx % 2 == 0):
+                mask = self.SWA_mask if use_static_mask else dynamic_SWA_mask
+            else:
+                mask = self. GA_mask if use_static_mask else dynamic_GA_mask
+        pass
 
         if offloaded_gradient_checkpointing:
             hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(
@@ -955,14 +968,39 @@ def CausalLM_fast_forward(fast_forward_inference):
             )
         pass
         hidden_states = outputs[0]
+
         bsz, q_len, hd = hidden_states.shape
         lm_head = self.lm_head.weight
+        logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
+        logit_scaling     = getattr(self.config, ""logit_scale"", 0)
+
         if bsz == 1 and q_len == 1:
             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
             logits = logits.unsqueeze(0).unsqueeze(0)
         elif num_logits_to_keep != 0:
             logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
         else:
+            if HAS_CUT_CROSS_ENTROPY and labels is not None:
+                n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
+                loss = fused_linear_cross_entropy(
+                    hidden_states      = hidden_states,
+                    lm_weight          = lm_head,
+                    labels             = labels,
+                    num_items_in_batch = n_items,
+                    logit_softcapping  = logit_softcapping,
+                )
+                if not return_dict:
+                    output = (logits,) + outputs[1:]
+                    return (loss,) + output if loss is not None else output
+
+                return CausalLMOutputWithPast(
+                    loss=loss,
+                    logits=None,
+                    past_key_values=outputs.past_key_values,
+                    hidden_states=outputs.hidden_states,
+                    attentions=outputs.attentions,
+                )
+            pass
             logits = self.lm_head(hidden_states.to(lm_head.dtype))
         pass
 
@@ -974,8 +1012,6 @@ def CausalLM_fast_forward(fast_forward_inference):
         pass
 
         loss = None
-        logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
-        logit_scaling     = getattr(self.config, ""logit_scale"", 0)
         if labels is not None:
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 7a6322d..232fe6a 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -20,8 +20,8 @@ from .cohere  import FastCohereModel
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
 from peft import PeftConfig, PeftModel
-from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER, MAP_TO_UNSLOTH_16bit
-import os
+from .loader_utils import get_model_name
+import os, contextlib, sys
 try:
     from huggingface_hub.utils import get_token
 except:
@@ -63,105 +63,6 @@ def _get_dtype(dtype):
 pass
 
 
-def __get_model_name(
-    model_name,
-    load_in_4bit = True,
-    INT_TO_FLOAT_MAPPER  = None,
-    FLOAT_TO_INT_MAPPER  = None,
-    MAP_TO_UNSLOTH_16bit = None,
-):
-    model_name = str(model_name)
-    lower_model_name = model_name.lower()
-
-    if not SUPPORTS_FOURBIT and lower_model_name in INT_TO_FLOAT_MAPPER:
-
-        model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
-        logger.warning_once(
-            f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
-            f""4bit loading.\nThe minimum required version is 4.37.\n""\
-            f'Try `pip install --upgrade ""transformers>=4.37""`\n'\
-            f""to obtain the latest transformers build, then restart this session.\n""\
-            f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
-        )
-        return model_name
-    
-    elif not load_in_4bit and lower_model_name in INT_TO_FLOAT_MAPPER:
-
-        new_model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
-        # logger.warning_once(
-        #     f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
-        #     f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
-        # )
-        return new_model_name
-
-    elif not load_in_4bit and lower_model_name in MAP_TO_UNSLOTH_16bit:
-
-        new_model_name = MAP_TO_UNSLOTH_16bit[lower_model_name]
-        return new_model_name
-
-    elif load_in_4bit and SUPPORTS_FOURBIT and lower_model_name in FLOAT_TO_INT_MAPPER:
-
-        new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
-        # logger.warning_once(
-        #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
-        #     f""We shall load `{new_model_name}` for 4x faster loading.""
-        # )
-        return new_model_name
-    pass
-
-    return None
-pass
-
-
-def _get_new_mapper():
-    try:
-        import requests
-        new_mapper = ""https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/models/mapper.py""
-        with requests.get(new_mapper, timeout = 3) as new_mapper: new_mapper = new_mapper.text
-        new_mapper = new_mapper[new_mapper.find(""__INT_TO_FLOAT_MAPPER""):]
-        new_mapper = new_mapper\
-            .replace(""INT_TO_FLOAT_MAPPER"",  ""NEW_INT_TO_FLOAT_MAPPER"")\
-            .replace(""FLOAT_TO_INT_MAPPER"",  ""NEW_FLOAT_TO_INT_MAPPER"")\
-            .replace(""MAP_TO_UNSLOTH_16bit"", ""NEW_MAP_TO_UNSLOTH_16bit"")
-
-        exec(new_mapper, globals())
-        return NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit
-    except:
-        return {}, {}, {}
-    pass
-pass
-
-
-def get_model_name(model_name, load_in_4bit = True):
-    new_model_name = __get_model_name(
-        model_name = model_name,
-        load_in_4bit = load_in_4bit,
-        INT_TO_FLOAT_MAPPER  = INT_TO_FLOAT_MAPPER,
-        FLOAT_TO_INT_MAPPER  = FLOAT_TO_INT_MAPPER,
-        MAP_TO_UNSLOTH_16bit = MAP_TO_UNSLOTH_16bit,
-    )
-    if new_model_name is None and model_name.count(""/"") == 1 and model_name[0].isalnum():
-        # Try checking if a new Unsloth version allows it!
-        NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit = _get_new_mapper()
-        upgraded_model_name = __get_model_name(
-            model_name = model_name,
-            load_in_4bit = load_in_4bit,
-            INT_TO_FLOAT_MAPPER  = NEW_INT_TO_FLOAT_MAPPER,
-            FLOAT_TO_INT_MAPPER  = NEW_FLOAT_TO_INT_MAPPER,
-            MAP_TO_UNSLOTH_16bit = NEW_MAP_TO_UNSLOTH_16bit,
-        )
-        if upgraded_model_name is not None:
-            raise NotImplementedError(
-                f""Unsloth: {model_name} is not supported in your current Unsloth version! Please update Unsloth via:\n\n""\
-                'pip uninstall unsloth -y\n'\
-                'pip install --upgrade --no-cache-dir ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""'
-            )
-        pass
-    pass
-    return new_model_name if new_model_name is not None else model_name
-pass
-
-
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
@@ -333,7 +234,8 @@ class FastLanguageModel(FastLlamaModel):
         else:
             raise NotImplementedError(
                 f""Unsloth: {model_name} not supported yet!\n""\
-                ""Make an issue to https://github.com/unslothai/unsloth!"",
+                ""Maybe you're doing vision finetuning? Please use FastVisionModel instead!\n""\
+                ""Otherwise, make an issue to https://github.com/unslothai/unsloth!"",
             )
         pass
 
@@ -411,4 +313,236 @@ class FastLanguageModel(FastLlamaModel):
         pass
         return model, tokenizer
     pass
-pass
\ No newline at end of file
+pass
+
+
+from ._utils import (
+    patch_compiling_bitsandbytes,
+    patch_model_and_tokenizer,
+    prepare_model_for_kbit_training,
+    patch_unsloth_smart_gradient_checkpointing,
+    patch_compiled_autograd,
+    process_vision_info,
+    unsloth_compile_transformers,
+)
+from ..kernels import (
+    patch_loss_functions,
+    post_patch_loss_function,
+)
+from .vision import FastBaseVisionModel
+
+
+class FastVisionModel(FastBaseVisionModel):
+    @staticmethod
+    def from_pretrained(
+        model_name                 = ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
+        max_seq_length             = None, # [TODO] No effect
+        dtype                      = None,
+        load_in_4bit               = True,
+        token                      = None,
+        device_map                 = ""sequential"",
+        rope_scaling               = None, # [TODO] No effect
+        fix_tokenizer              = True, # [TODO] No effect
+        trust_remote_code          = False,
+        use_gradient_checkpointing = ""unsloth"",
+        resize_model_vocab         = None, # [TODO] No effect
+        revision                   = None,
+        *args, **kwargs,
+    ):
+        if token is None: token = get_token()
+
+        patch_compiled_autograd()
+        patch_compiling_bitsandbytes()
+        if use_gradient_checkpointing == ""unsloth"":
+            patch_unsloth_smart_gradient_checkpointing()
+        
+        old_model_name = model_name
+        model_name = get_model_name(model_name, load_in_4bit)
+
+        with contextlib.redirect_stdout(open(os.devnull, ""w"")):
+            patch_loss_functions(torch_compile = False)
+            model_types = unsloth_compile_transformers(
+                model_name              = model_name,
+                sdpa_dynamic_mask       = True,
+                sdpa_bool_masks         = True,
+                sdpa_gqa_replace        = True,
+                sdpa_dynamic_compile    = True,
+                compile_attention       = True,
+                disable_causal_masks    = True,
+                compile_torch_modules   = True,
+                compile_custom_modules  = True,
+                compile_function_calls  = True,
+                fuse_lm_head            = True,
+                gradient_checkpointing  = True,
+                manual_replacements     = True,
+                epilogue_fusion         = True,
+                max_autotune            = False,
+                shape_padding           = True,
+                cudagraphs              = False,
+                debug                   = False,
+                import_from_cache       = False,
+                disable                 = False,
+            )
+        pass
+
+        # First check if it's a normal model via AutoConfig
+        from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
+        was_disabled = are_progress_bars_disabled()
+        disable_progress_bars()
+
+        autoconfig_error = None
+        peft_error = None
+        try:
+            model_config = AutoConfig.from_pretrained(
+                model_name,
+                token = token,
+                revision = revision,
+                trust_remote_code = trust_remote_code,
+            )
+            is_model = True
+        except Exception as error:
+            autoconfig_error = str(error)
+            is_model = False
+        try:
+            peft_config = PeftConfig.from_pretrained(
+                model_name,
+                token = token,
+                revision = revision,
+                trust_remote_code = trust_remote_code,
+            )
+            is_peft = True
+        except Exception as error:
+            peft_error = str(error)
+            is_peft = False
+        pass
+
+        # Both config.json and adapter_config.json should not exist!
+
+        # Old transformers versions check
+        both_exist = (is_model and is_peft) and not SUPPORTS_LLAMA32
+        
+        # New transformers need to check manually.
+        if SUPPORTS_LLAMA32:
+            # Check if folder exists locally
+            if os.path.isdir(model_name):
+                exist_adapter_config = os.path.exists(os.path.join(model_name, ""adapter_config.json""))
+                exist_config         = os.path.exists(os.path.join(model_name, ""config.json""))
+                both_exist = exist_adapter_config and exist_config
+            else:
+                files = HfFileSystem(token = token).glob(os.path.join(model_name, ""*.json""))
+                files = (os.path.split(x)[-1] for x in files)
+                if sum(x == ""adapter_config.json"" or x == ""config.json"" for x in files) >= 2:
+                    both_exist = True
+                pass
+            pass
+        pass
+
+        # Error out if both LoRA and normal model config exists.
+        if both_exist:
+            raise RuntimeError(
+                ""Unsloth: Your repo has a LoRA adapter and a base model.\n""\
+                ""You have 2 files `config.json` and `adapter_config.json`.\n""\
+                ""We must only allow one config file.\n""\
+                ""Please separate the LoRA and base models to 2 repos.""
+            )
+
+        elif not is_model and not is_peft:
+            error = autoconfig_error or peft_error
+            # Old transformers version
+            if ""rope_scaling"" in error.lower() and not SUPPORTS_LLAMA31:
+                raise ImportError(
+                    f""Unsloth: Your transformers version of {transformers_version} does not support new RoPE scaling methods.\n""\
+                    f""This includes Llama 3.1. The minimum required version is 4.43.2\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
+                    f""to obtain the latest transformers build, then restart this session.""\
+                ) 
+            raise RuntimeError(autoconfig_error or peft_error)
+        pass
+
+        # Get base model for PEFT:
+        if is_peft:
+            # Check base model again for PEFT
+            model_name = get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
+            model_config = AutoConfig.from_pretrained(
+                model_name,
+                token = token,
+                revision = revision,
+                trust_remote_code = trust_remote_code,
+            )
+        pass
+
+        if not was_disabled: enable_progress_bars()
+
+        # Check if this is local model since the tokenizer gets overwritten
+        if  os.path.exists(os.path.join(old_model_name, ""tokenizer_config.json"")) and \
+            os.path.exists(os.path.join(old_model_name, ""tokenizer.json"")) and \
+            os.path.exists(os.path.join(old_model_name, ""special_tokens_map.json"")):
+
+            tokenizer_name = old_model_name
+        else:
+            tokenizer_name = None
+        pass
+
+        model, tokenizer = FastBaseVisionModel.from_pretrained(
+            model_name        = model_name,
+            max_seq_length    = max_seq_length,
+            dtype             = _get_dtype(dtype),
+            load_in_4bit      = load_in_4bit,
+            token             = token,
+            device_map        = device_map,
+            trust_remote_code = trust_remote_code,
+            revision          = revision if not is_peft else None,
+            model_types       = model_types,
+            tokenizer_name    = tokenizer_name,
+            *args, **kwargs,
+        )
+        
+        if resize_model_vocab is not None:
+            model.resize_token_embeddings(resize_model_vocab)
+        pass
+
+        # In case the model supports tagging, add the unsloth tag.
+        if hasattr(model, ""add_model_tags""):
+            model.add_model_tags([""unsloth"",])
+        pass
+        if hasattr(tokenizer, ""add_model_tags""):
+            tokenizer.add_model_tags([""unsloth"",])
+        pass
+
+        if load_in_4bit:
+            # Fix up bitsandbytes config
+            quantization_config = \
+            {
+                # Sometimes torch_dtype is not a string!!
+                ""bnb_4bit_compute_dtype""           : model.config.to_dict()[""torch_dtype""],
+                ""bnb_4bit_quant_type""              : ""nf4"",
+                ""bnb_4bit_use_double_quant""        : True,
+                ""llm_int8_enable_fp32_cpu_offload"" : False,
+                ""llm_int8_has_fp16_weight""         : False,
+                ""llm_int8_skip_modules""            : None,
+                ""llm_int8_threshold""               : 6.0,
+                ""load_in_4bit""                     : True,
+                ""load_in_8bit""                     : False,
+                ""quant_method""                     : ""bitsandbytes"",
+            }
+            model.config.update({""quantization_config"" : quantization_config})
+        pass
+
+        if is_peft:
+            # From https://github.com/huggingface/peft/issues/184
+            # Now add PEFT adapters
+            model.enable_input_require_grads()
+            model = PeftModel.from_pretrained(
+                model,
+                old_model_name,
+                token = token,
+                revision = revision,
+                is_trainable = True,
+                trust_remote_code = trust_remote_code,
+            )
+            # Patch it as well!
+            model = FastBaseVisionModel.patch_peft_model(model, use_gradient_checkpointing)
+        pass
+        return model, tokenizer
+    pass
+pass
diff --git a/unsloth/models/loader_utils.py b/unsloth/models/loader_utils.py
new file mode 100644
index 0000000..b778b7e
--- /dev/null
+++ b/unsloth/models/loader_utils.py
@@ -0,0 +1,120 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER, MAP_TO_UNSLOTH_16bit
+# https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
+from packaging.version import Version
+from transformers import __version__ as transformers_version
+transformers_version = Version(transformers_version)
+SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
+
+
+def __get_model_name(
+    model_name,
+    load_in_4bit = True,
+    INT_TO_FLOAT_MAPPER  = None,
+    FLOAT_TO_INT_MAPPER  = None,
+    MAP_TO_UNSLOTH_16bit = None,
+):
+    model_name = str(model_name)
+    lower_model_name = model_name.lower()
+
+    if not SUPPORTS_FOURBIT and lower_model_name in INT_TO_FLOAT_MAPPER:
+
+        model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
+        print(
+            f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
+            f""4bit loading.\nThe minimum required version is 4.37.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.37""`\n'\
+            f""to obtain the latest transformers build, then restart this session.\n""\
+            f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
+        )
+        return model_name
+    
+    elif not load_in_4bit and lower_model_name in INT_TO_FLOAT_MAPPER:
+
+        new_model_name = INT_TO_FLOAT_MAPPER[lower_model_name]
+        # logger.warning_once(
+        #     f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
+        #     f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
+        # )
+        return new_model_name
+
+    elif not load_in_4bit and lower_model_name in MAP_TO_UNSLOTH_16bit:
+
+        new_model_name = MAP_TO_UNSLOTH_16bit[lower_model_name]
+        return new_model_name
+
+    elif load_in_4bit and SUPPORTS_FOURBIT and lower_model_name in FLOAT_TO_INT_MAPPER:
+
+        new_model_name = FLOAT_TO_INT_MAPPER[lower_model_name]
+        # logger.warning_once(
+        #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
+        #     f""We shall load `{new_model_name}` for 4x faster loading.""
+        # )
+        return new_model_name
+    pass
+
+    return None
+pass
+
+
+def _get_new_mapper():
+    try:
+        import requests
+        new_mapper = ""https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/models/mapper.py""
+        with requests.get(new_mapper, timeout = 3) as new_mapper: new_mapper = new_mapper.text
+        new_mapper = new_mapper[new_mapper.find(""__INT_TO_FLOAT_MAPPER""):]
+        new_mapper = new_mapper\
+            .replace(""INT_TO_FLOAT_MAPPER"",  ""NEW_INT_TO_FLOAT_MAPPER"")\
+            .replace(""FLOAT_TO_INT_MAPPER"",  ""NEW_FLOAT_TO_INT_MAPPER"")\
+            .replace(""MAP_TO_UNSLOTH_16bit"", ""NEW_MAP_TO_UNSLOTH_16bit"")
+
+        exec(new_mapper, globals())
+        return NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit
+    except:
+        return {}, {}, {}
+    pass
+pass
+
+
+def get_model_name(model_name, load_in_4bit = True):
+    new_model_name = __get_model_name(
+        model_name = model_name,
+        load_in_4bit = load_in_4bit,
+        INT_TO_FLOAT_MAPPER  = INT_TO_FLOAT_MAPPER,
+        FLOAT_TO_INT_MAPPER  = FLOAT_TO_INT_MAPPER,
+        MAP_TO_UNSLOTH_16bit = MAP_TO_UNSLOTH_16bit,
+    )
+    if new_model_name is None and model_name.count(""/"") == 1 and model_name[0].isalnum():
+        # Try checking if a new Unsloth version allows it!
+        NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit = _get_new_mapper()
+        upgraded_model_name = __get_model_name(
+            model_name = model_name,
+            load_in_4bit = load_in_4bit,
+            INT_TO_FLOAT_MAPPER  = NEW_INT_TO_FLOAT_MAPPER,
+            FLOAT_TO_INT_MAPPER  = NEW_FLOAT_TO_INT_MAPPER,
+            MAP_TO_UNSLOTH_16bit = NEW_MAP_TO_UNSLOTH_16bit,
+        )
+        if upgraded_model_name is not None:
+            raise NotImplementedError(
+                f""Unsloth: {model_name} is not supported in your current Unsloth version! Please update Unsloth via:\n\n""\
+                'pip uninstall unsloth unsloth_zoo -y\n'\
+                'pip install --upgrade --no-cache-dir ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'\
+                'pip install --upgrade --no-cache-dir ""git+https://github.com/unslothai/unsloth-zoo.git""\n'\
+            )
+        pass
+    pass
+    return new_model_name if new_model_name is not None else model_name
+pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index d4f1278..fc1dc8c 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -409,12 +409,12 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen2.5-Coder-32B"",
     ),
     ""unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit"" : (
-        ""unsloth/Qwen2.5-Coder-Instruct-0.5B"",
-        ""Qwen/Qwen2.5-Coder-Instruct-0.5B"",
+        ""unsloth/Qwen2.5-Coder-0.5B-Instruct"",
+        ""Qwen/Qwen2.5-Coder-0.5B-Instruct"",
     ),
     ""unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit"" : (
-        ""unsloth/Qwen2.5-Coder-Instruct-1.5B"",
-        ""Qwen/Qwen2.5-Coder-Instruct-1.5B"",
+        ""unsloth/Qwen2.5-Coder-1.5B-Instruct"",
+        ""Qwen/Qwen2.5-Coder-1.5B-Instruct"",
     ),
     ""unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit"" : (
         ""unsloth/Qwen2.5-Coder-3B-Instruct"",
@@ -452,6 +452,46 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Llama-3.1-Nemotron-70B-Instruct"",
         ""nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"",
     ),
+    ""unsloth/Qwen2-VL-2B-Instruct-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-2B-Instruct"",
+        ""Qwen/Qwen2-VL-2B-Instruct"",
+    ),
+    ""unsloth/Qwen2-VL-7B-Instruct-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-7B-Instruct"",
+        ""Qwen/Qwen2-VL-7B-Instruct"",
+    ),
+    ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"" : (
+        ""unsloth/Llama-3.2-11B-Vision-Instruct"",
+        ""meta-llama/Llama-3.2-11B-Vision-Instruct"",
+    ),
+    ""unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit"" : (
+        ""unsloth/Llama-3.2-90B-Vision-Instruct"",
+        ""meta-llama/Llama-3.2-90B-Vision-Instruct"",
+    ),
+    ""unsloth/Llama-3.2-11B-Vision-bnb-4bit"" : (
+        ""unsloth/Llama-3.2-11B-Vision"",
+        ""meta-llama/Llama-3.2-11B-Vision"",
+    ),
+    ""unsloth/Llama-3.2-90B-Vision-bnb-4bit"" : (
+        ""unsloth/Llama-3.2-90B-Vision"",
+        ""meta-llama/Llama-3.2-90B-Vision"",
+    ),
+    ""unsloth/Pixtral-12B-2409-bnb-4bit"" : (
+        ""unsloth/Pixtral-12B-2409"",
+        ""mistralai/Pixtral-12B-2409"",
+    ),
+    ""unsloth/Pixtral-12B-2409-Base-bnb-4bit"" : (
+        ""unsloth/Pixtral-12B-Base-2409"",
+        ""mistralai/Pixtral-12B-Base-2409"",
+    ),
+    ""unsloth/llava-1.5-7b-hf-bnb-4bit"" : (
+        ""unsloth/llava-1.5-7b-hf"",
+        ""llava-hf/llava-1.5-7b-hf"",
+    ),
+    ""unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit"" : (
+        ""unsloth/llava-v1.6-mistral-7b-hf"",
+        ""llava-hf/llava-v1.6-mistral-7b-hf"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 0b8c08a..d083144 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -1,58 +1,86 @@
+# Unsloth Zoo - Utilities for Unsloth
 # Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the ""License"");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Lesser General Public License as published by
+# the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an ""AS IS"" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+import torch
+from transformers import (
+    BitsAndBytesConfig,
+    AutoModelForVision2Seq,
+    AutoProcessor,
+)
 from .llama import *
-from ..kernels import patch_layernorm, unpatch_layernorm
-from ..kernels import patch_rms_layernorm, unpatch_rms_layernorm
-from ..kernels import patch_llama_for_causal_lm, unpatch_llama_for_causal_lm
-from ._utils import patch_gradient_checkpointing
-
-from transformers import AutoProcessor
-try:
-    from transformers import MllamaForConditionalGeneration
-except:
-    raise ImportError(
-        ""Unsloth: Please update your transformers version to 4.46.0 for Llama 3.2 support!""
-    )
-pass
+from ..kernels import (
+    post_patch_loss_function,
+)
+from ._utils import __version__
+from peft import LoraConfig, TaskType, get_peft_model
+from transformers import set_seed as transformers_set_seed
+from unsloth_zoo.peft_utils import (
+    get_peft_regex,
+    merge_and_overwrite_lora,
+)
+
+__all__ = [
+    ""FastBaseVisionModel"",
+]
+
+def _wrap_fast_inference(generate, device_type, dtype, model):
+    # Wraps inference with bfloat16 / float16
+    @torch.inference_mode
+    def _fast_generate(*args, **kwargs):
+        # For num_logits_to_keep
+        kwargs[""num_logits_to_keep""] = 1
+
+        # Remove token_type_ids
+        kwargs.pop(""token_type_ids"", None)
+
+        # Check pad_token
+        model_eos_token_id = getattr(model.config, ""eos_token_id"", None)
+        if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
+            model_eos_token_id = model_eos_token_id[0]
+
+        kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
 
-class FastVisionModel:
+        try:
+            kwargs[""pixel_values""] = kwargs[""pixel_values""].to(model.dtype)
+        except:
+            pass
 
-    def pre_patch(self):
-        patch_gradient_checkpointing()
-        patch_layernorm()
-        patch_rms_layernorm()
-        patch_llama_for_causal_lm()
+        # Autocasted
+        with torch.autocast(device_type = device_type, dtype = dtype):
+            output = generate(*args, **kwargs)
+        pass
+        return output
     pass
+    return _fast_generate
+pass
 
-    def post_unpatch(self):
-        unpatch_layernorm()
-        unpatch_rms_layernorm()
-        unpatch_llama_for_causal_lm()
-    pass
 
+class FastBaseVisionModel:
 
     @staticmethod
     def from_pretrained(
-        model_name        = ""llava-hf/llava-1.5-7b-hf"",
+        model_name        = ""unsloth/llama-3-8b-bnb-4bit"",
         max_seq_length    = None,
         dtype             = None,
         load_in_4bit      = True,
         token             = None,
         device_map        = ""sequential"",
-        rope_scaling      = None,
         trust_remote_code = False,
+        model_types       = None,
+        tokenizer_name    = None,
         **kwargs,
     ):
         if trust_remote_code:
@@ -67,7 +95,7 @@ class FastVisionModel:
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers = {transformers_version}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} vision patching. Transformers = {transformers_version}.\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
@@ -81,6 +109,7 @@ class FastVisionModel:
         pass
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
+        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
         get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
 
@@ -105,160 +134,36 @@ class FastVisionModel:
             )
         pass
 
+        kwargs.pop(""attn_implementation"", None); # No need since we auto call it
+
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
         
-        self.pre_patch()
-        model = MllamaForConditionalGeneration.from_pretrained(
+        model = AutoModelForVision2Seq.from_pretrained(
             model_name,
             device_map              = device_map,
             torch_dtype             = dtype,
-            # quantization_config     = bnb_config,
+            # quantization_config   = bnb_config,
             token                   = token,
-            max_position_embeddings = max_position_embeddings,
             trust_remote_code       = trust_remote_code,
-            attn_implementation     = ""sdpa"",
+            # attn_implementation   = ""sdpa"", [TODO] Pixtral for eg fails
             **kwargs,
         )
-        self.post_unpatch()
-
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
         # We currently only support NVIDIA GPUs - AMD / Intel is a work in progress!
         post_check = check_nvidia()
 
         # Counteract saved tokenizers
+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         tokenizer = AutoProcessor.from_pretrained(
-            model_name,
-        )
-        model = FastVisionModel.post_patch(model)
-
-        # Patch Trainer
-        from transformers.trainer import Trainer
-        try:
-            if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)
-                Trainer._original_training_loop = inner_training_loop
-            else:
-                inner_training_loop = Trainer._original_training_loop
-        except:
-            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
-        pass
-
-        if ((post_check - pre_check) >= 1).sum() > 1:
-            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
-
-        import transformers.trainer
-        items_in_trainer = dir(transformers.trainer)
-        good_items = []
-        for item in items_in_trainer:
-            # TODO: Support Deepspeed
-            if item.startswith((""deepspeed"", ""xm"", ""met"", ""smp"")): continue
-            if item in inner_training_loop: good_items.append(item)
-        pass
-        exec(""from transformers.trainer import ("" + "", "".join(x for x in good_items) + "")"", globals())
-
-        start = re.search('logger\.info\([\""\'].+?Running training', inner_training_loop).span(0)[0]
-        end = inner_training_loop.find(""\n\n"", start)
-        original_debug = inner_training_loop[start:end]
-        spaces = re.search('\n([\s\t]{1,})', original_debug).group(0)[1:]
-        front_spaces = re.match('([\s\t]{1,})', inner_training_loop).group(0)
-
-        debug_info = """"""debug_info = \\
-        f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\n""\\
-        f""   \\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
-        f""O^O/ \\_/ \\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
-        f""\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
-        f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
-        logger.warning(debug_info)
-        import subprocess, re, gc, numpy as np
-        a = np.array([0,])
-        try:
-            a = subprocess.check_output('nvidia-smi --query-gpu=memory.used --format=csv', shell = True)
-            a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)
-            a = np.array([int(x.decode('utf-8'))/1024 for x in a])
-        except:
-            if not torch.cuda.is_available():
-                raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')
-        if ((a - PRE_CHECK) >= 1).sum() > 1:
-            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
-        for _ in range(3):
-            gc.collect()
-            torch.cuda.empty_cache()""""""
-
-        debug_info = debug_info.split('\n')
-        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)
-
-        debug_info = """"""n_total_devices = total_train_batch_size // \\
-            args.gradient_accumulation_steps // self._train_batch_size
-        if n_total_devices > 1:
-            logger.warning_once('Unsloth currently does not support multi GPU setups - but we are working on it!')
-        debug_info =""""""
-        debug_info = debug_info.split('\n')
-        debug_info = ""\n"".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])
-        inner_training_loop = inner_training_loop.replace(""debug_info ="", debug_info, 1)
-
-        front_spaces = re.match(r""[\t\s]{1,}"", inner_training_loop).group(0)
-        inner_training_loop = re.sub(r""^"" + front_spaces, """", inner_training_loop, flags = re.MULTILINE)
-        inner_training_loop = inner_training_loop.replace(
-            ""train_dataloader = tpu_spmd_dataloader(train_dataloader)"",
-            ""raise RuntimeError('Unsloth: TPUs are not yet supported!')""
+            tokenizer_name,
+            padding_side = ""right"",
+            token        = token,
         )
-        inner_training_loop = inner_training_loop.replace(
-            ""self.accelerator.free_memory()"",
-            ""self.accelerator.free_memory()\n"" + \
-            front_spaces + ""if self.is_deepspeed_enabled:""\
-            ""raise RuntimeError('Unsloth: Deepspeed is not yet supported!')\n"", 1,
-        )
-
-        check_batches = """"""train_dataloader = self.get_train_dataloader()
-        ga  = args.gradient_accumulation_steps
-        bsz = self._train_batch_size
-        total_batches = bsz * ga * args.world_size
-        n_total_devices = total_batches // ga // bsz
-        if n_total_devices > 1:
-            logger.warning_once('Unsloth currently does not support multi GPU setups - but we are working on it!')
-            divisor = n_total_devices / 1
-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)
-            if total_batches // ga // bsz > 1:
-                divisor = n_total_devices / 1
-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)""""""
-        check_batches = check_batches.split('\n')
-        check_batches = ""\n"".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])
-        inner_training_loop = inner_training_loop.replace(
-            ""train_dataloader = self.get_train_dataloader()"",
-            check_batches, 1,
-        )
-        inner_training_loop = inner_training_loop.replace(
-            ""_inner_training_loop"",
-            ""_fast_inner_training_loop"", 1,
-        )
-        exec(inner_training_loop, globals())
 
-        Trainer._inner_training_loop = _fast_inner_training_loop
-        inner_training_loop = inner_training_loop.replace(
-            ""is_torch_tpu_available()"",
-            ""False"",
-        )
-        if ""n_total_devices >"" not in inner_training_loop:
-            raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
-        pass
-        inner_training_loop = inner_training_loop.replace(
-            ""is_sagemaker_mp_enabled()"",
-            ""False"",
-        )
-        exec(inner_training_loop, globals())
-        Trainer._inner_training_loop = _fast_inner_training_loop
-
-        # Save max_seq_length
-        model.max_seq_length = max_position_embeddings
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model.max_seq_length = max_position_embeddings
-            internal_model = internal_model.model
-        pass
-        internal_model.max_seq_length = max_position_embeddings
+        model, tokenizer = patch_tokenizer(model, tokenizer)
+        model = post_patch_loss_function(model)
 
         # Fix up config for transformers uploading PEFT
         # Not necessary anymore since we require transformers>=4.37!
@@ -271,121 +176,105 @@ class FastVisionModel:
         pass
 
         # Log Unsloth version for future fastpaths for inference
-        model.config.update({""unsloth_version"" : __version__})
-
-        # Add save modules
-        patch_saving_functions(model)
-        Trainer._inner_training_loop = _fast_inner_training_loop
+        if hasattr(model, ""config""):
+            model.config.update({""unsloth_version"" : __version__})
+        pass
+        patch_saving_functions(model, vision = True)
+        patch_saving_functions(tokenizer, vision = True)
 
-        # Also fix torch_dtype
+        # Save tokenizer for inference purposes
+        tokenizer.padding_side = ""left"" # Force inference
         internal_model = model
         while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""config""):
-                if   internal_model.config.torch_dtype ==  ""float32"":
-                    internal_model.config.torch_dtype = torch.float32
-                elif internal_model.config.torch_dtype == ""bfloat16"":
-                    internal_model.config.torch_dtype = torch.bfloat16
-                elif internal_model.config.torch_dtype ==  ""float16"":
-                    internal_model.config.torch_dtype = torch.float16
-                pass
-            pass
+            internal_model._saved_temp_tokenizer = tokenizer
             internal_model = internal_model.model
         pass
-        if hasattr(internal_model, ""config""):
-            if   internal_model.config.torch_dtype ==  ""float32"":
-                internal_model.config.torch_dtype = torch.float32
-            elif internal_model.config.torch_dtype == ""bfloat16"":
-                internal_model.config.torch_dtype = torch.bfloat16
-            elif internal_model.config.torch_dtype ==  ""float16"":
-                internal_model.config.torch_dtype = torch.float16
-            pass
-        pass
+        internal_model._saved_temp_tokenizer = tokenizer
         
         return model, tokenizer
     pass
 
 
-    @staticmethod
-    def post_patch(model):
-        # Patch model
-        layers = model.model.layers
-        lm_head = model.get_output_embeddings().weight
-        
-        # Also patch all dtypes - BnB seems to not allocate the correct type?
-        # BnB default dtype seems to be float16!
-        correct_dtype = lm_head.weight.dtype
-
-        for name, module in model.named_modules():
-            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
-                weight = module.weight
-                quant_state = weight.quant_state
-
-                if type(quant_state) is list:
-                    # BnB seems to have float16 as default!
-                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
-                else:
-                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
-                    quant_state.dtype = correct_dtype
-                pass
-            pass
-        pass
-
-        # Clear deleted GPU items
-        for _ in range(3):
-            gc.collect()
-            torch.cuda.empty_cache()
-        return model
-    pass
-
-
     @staticmethod
     def get_peft_model(
         model,
-        r                   = 16,
-        target_modules      = ""all-linear"",
-        lora_alpha          = 16,
-        lora_dropout        = 0,
-        bias                = ""none"",
-        layers_to_transform = None,
-        layers_pattern      = None,
+        r                          = 16,
+        target_modules             = None,
+        lora_alpha                 = 16,
+        lora_dropout               = 0,
+        bias                       = ""none"",
+        finetune_vision_layers     = True,
+        finetune_language_layers   = True,
+        finetune_attention_modules = True,
+        finetune_mlp_modules       = True,
+        layers_to_transform        = None,
+        layers_pattern             = None,
         use_gradient_checkpointing = True,
-        random_state        = 3407,
-        max_seq_length      = 2048, # not used anymore
-        use_rslora          = False,
-        modules_to_save     = None,
-        init_lora_weights   = True,
-        loftq_config        = {},
-        temporary_location  = ""_unsloth_temporary_saved_buffers"",
+        random_state               = 3407,
+        max_seq_length             = 2048, # not used anymore
+        use_rslora                 = False,
+        modules_to_save            = None,
+        init_lora_weights          = True,
+        loftq_config               = {},
+        temporary_location         = ""_unsloth_temporary_saved_buffers"",
         **kwargs,
     ):
         transformers_set_seed(random_state)
 
-        # Get LoRA
-        arguments = dict(
-            r                   = r,
-            lora_alpha          = lora_alpha,
-            target_modules      = target_modules,
-            lora_dropout        = lora_dropout,
-            bias                = bias,
-            layers_to_transform = layers_to_transform,
-            init_lora_weights   = init_lora_weights,
-            # loftq_config        = loftq_config,
-            # use_rslora          = use_rslora,
-            modules_to_save     = modules_to_save,
-            **kwargs,
-        )
+        if type(r) is not int:
+            raise TypeError(f""Unsloth: Rank of {str(r)} must be an integer."")
+        if r <= 0:
+            raise TypeError(f""Unsloth: Rank of {str(r)} must be larger than 0."")
+
+        if isinstance(model, PeftModelForCausalLM):
+            raise RuntimeError(""Unsloth: You already added LoRA adapters to your model!"")
+
+        if target_modules == ""all-linear"":
+            finetune_vision_layers     = True
+            finetune_language_layers   = True
+            finetune_attention_modules = True
+            finetune_mlp_modules       = True
+        pass
+        if target_modules is None:
+            target_modules = get_peft_regex(
+                model,
+                finetune_vision_layers     = finetune_vision_layers,
+                finetune_language_layers   = finetune_language_layers,
+                finetune_attention_modules = finetune_attention_modules,
+                finetune_mlp_modules       = finetune_mlp_modules,
+            )
+        else:
+            assert(type(target_modules) in (list, tuple,))
+        pass
 
-        lora_config = LoraConfig(**arguments)
+        # Clear deleted GPU items
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        pass
 
-        model = _get_peft_model(model, lora_config)
+        lora_config = LoraConfig(
+            r               = r,
+            lora_alpha      = lora_alpha,
+            target_modules  = target_modules,
+            lora_dropout    = lora_dropout,
+            bias            = bias,
+            task_type       = TaskType.CAUSAL_LM,
+        )
+        model = prepare_model_for_kbit_training(
+            model,
+            use_gradient_checkpointing = use_gradient_checkpointing,
+        )
+        model = get_peft_model(model, lora_config)
 
-        model = FastVisionModel.patch_peft_model(model, use_gradient_checkpointing)
+        model = FastBaseVisionModel.patch_peft_model(model, use_gradient_checkpointing)
 
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()
         pass
+        patch_saving_functions(model, vision = True)
 
         return model
     pass
@@ -396,6 +285,11 @@ class FastVisionModel:
         model,
         use_gradient_checkpointing = True,
     ):
+        if not isinstance(model, PeftModelForCausalLM):
+            raise TypeError(
+                ""Unsloth: Your model needs to call `.get_peft_model` first!""
+            )
+        pass
 
         model = prepare_model_for_kbit_training(
             model,
@@ -403,20 +297,6 @@ class FastVisionModel:
             use_reentrant = True,
         )
 
-        # Fix up config for transformers uploading PEFT
-        for active_adapter in model.peft_config.keys():
-            # Not necessary since we requires transformers >= 4.37
-            if False:
-                name = model.peft_config[active_adapter].base_model_name_or_path
-                if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-                    name = name[:len(name) - len(""-bnb-4bit"")]
-                    model.peft_config[active_adapter].base_model_name_or_path = name
-                pass
-            # Add revision to enable future fast inference paths
-            # [TODO] Bugs out!see https://github.com/unslothai/unsloth/issues/492
-            # model.peft_config[active_adapter].revision = f""unsloth""
-        pass
-
         from transformers.trainer import Trainer 
         if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
             raise RuntimeError(
@@ -426,24 +306,7 @@ class FastVisionModel:
                 'Thank you for your understanding and we appreciate it immensely!'
             )
         pass
-
-        logger.warning_once(
-            f""Unsloth {__version__} patched {len(model.model.model.layers)} layers with ""\
-            f""{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers."",
-        )
-        patch_saving_functions(model)
-
-        # Patch cross entropy loss labels
-        # Fixes https://github.com/unslothai/unsloth/issues/10
-        max_seq_length = model.max_seq_length
-        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda:0"")
-        model.model.extra_ignored_labels = extra_ignored_labels
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model.max_seq_length = max_seq_length
-            internal_model = internal_model.model
-        pass
-        internal_model.max_seq_length = max_seq_length        
+        patch_saving_functions(model, vision = True)
 
         # Patch tokenizer to pad to the right
         internal_model = model
@@ -468,37 +331,40 @@ class FastVisionModel:
 
     @staticmethod
     def for_inference(model):
-        # if model.config.model_type == ""qwen2"":
-        #     FastLlamaModel.for_training(model)
-        #     return
-        # pass
-
-        internal_model = model
-        internal_model.gradient_checkpointing = False
-        internal_model.training = False
+        model.gradient_checkpointing = False
+        model.training = False
 
-        while hasattr(internal_model, ""model""):
-            internal_model = internal_model.model
-            internal_model.gradient_checkpointing = False
-            internal_model.training = False
-        pass
-        if hasattr(internal_model, ""training""):
-            internal_model.training = False
+        for name, module in model.named_modules():
+            if hasattr(module, ""gradient_checkpointing""):
+                module.gradient_checkpointing = False
+            if hasattr(module, ""training""):
+                module.training = False
         pass
 
-        # Also check if lm_head / embeddings are trained
-        internal_model = model
-        while not hasattr(internal_model, ""lm_head""):
-            internal_model = internal_model.model
-        pass
-        lm_head = internal_model.lm_head.weight
-        device_type = lm_head.device.type
         dtype = model.config.torch_dtype
-        
         if type(dtype) is str:
             if   dtype ==  ""float16"": dtype = torch.float16
             elif dtype == ""bfloat16"": dtype = torch.bfloat16
         pass
+        device_type = model.device.type
+
+        # Wrap model.generate
+        if model.generate.__name__ != ""_fast_generate"":
+            model._unwrapped_old_generate = model.generate
+            model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)
+        pass
+        
+        # Patch tokenizer to pad to the left
+        internal_model = model
+        while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_saved_temp_tokenizer""):
+                internal_model._saved_temp_tokenizer.padding_side = ""left""
+            pass
+            internal_model = internal_model.model
+        pass
+        if hasattr(internal_model, ""_saved_temp_tokenizer""):
+            internal_model._saved_temp_tokenizer.padding_side = ""left""
+        pass
 
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -516,23 +382,32 @@ class FastVisionModel:
 
     @staticmethod
     def for_training(model, use_gradient_checkpointing = True):
-        internal_model = model
-        internal_model.gradient_checkpointing = use_gradient_checkpointing
-        internal_model.training = True
+        model.gradient_checkpointing = use_gradient_checkpointing
+        model.training = True
 
-        # Delete all fast inference loras
-        for param in model.parameters():
-            if hasattr(param, ""_fast_lora""):
-                del param._fast_lora
+        for name, module in model.named_modules():
+            if hasattr(module, ""gradient_checkpointing""):
+                module.gradient_checkpointing = use_gradient_checkpointing
+            if hasattr(module, ""training""):
+                module.training = True
         pass
 
+        # Also revert model.generate
+        if hasattr(model, ""_unwrapped_old_generate""):
+            model.generate = model._unwrapped_old_generate
+            del model._unwrapped_old_generate
+        pass
+
+        # Patch tokenizer to pad to the right
+        internal_model = model
         while hasattr(internal_model, ""model""):
+            if hasattr(internal_model, ""_saved_temp_tokenizer""):
+                internal_model._saved_temp_tokenizer.padding_side = ""right""
+            pass
             internal_model = internal_model.model
-            internal_model.gradient_checkpointing = use_gradient_checkpointing
-            internal_model.training = True
         pass
-        if hasattr(internal_model, ""training""):
-            internal_model.training = True
+        if hasattr(internal_model, ""_saved_temp_tokenizer""):
+            internal_model._saved_temp_tokenizer.padding_side = ""right""
         pass
 
         # Also re-enable training for embeddings for NEFTune
@@ -548,3 +423,5 @@ class FastVisionModel:
         return model
     pass
 pass
+
+
diff --git a/unsloth/save.py b/unsloth/save.py
index b4c6b49..b503b2b 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2041,8 +2041,153 @@ def unsloth_convert_lora_to_ggml_and_save_locally(
     print(""Unsloth: Done."")
     print(f""Unsloth: Conversion completed! Output file: {output_file}"")
     print(""\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!"")
+pass
+
+
+from unsloth_zoo.peft_utils import merge_and_overwrite_lora
+from .models.loader_utils import get_model_name
+
+@torch.inference_mode
+def unsloth_generic_save(
+    model,
+    tokenizer,
+    save_directory       : Union[str, os.PathLike] = ""unsloth_finetuned_merge"",
+    save_method          : str = ""lora"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    push_to_hub          : bool = False,
+    token                : Optional[Union[str, bool]] = None,
+    is_main_process      : bool = True,
+    state_dict           : Optional[dict] = None,
+    save_function        : Callable = torch.save,
+    max_shard_size       : Union[int, str] = ""5GB"",
+    safe_serialization   : bool = True,
+    variant              : Optional[str] = None,
+    save_peft_format     : bool = True,
+
+    # Push to hub
+    use_temp_dir         : Optional[bool] = None,
+    commit_message       : Optional[str] = ""Trained with Unsloth"",
+    private              : Optional[bool] = None,
+    create_pr            : bool = False,
+    revision             : str = None,
+    commit_description   : str = ""Upload model trained with Unsloth 2x faster"",
+    tags                 : List[str] = None,
+
+    # Our functions
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.9,
+):
+    if token is None and push_to_hub: token = get_token()
+
+    merge_and_overwrite_lora(
+        get_model_name,
+        create_huggingface_repo,
+        model,
+        save_location        = save_directory,
+        push_to_hub          = push_to_hub,
+        token                = token,
+        upload_location      = save_directory if push_to_hub else None,
+        low_disk_space_usage = True,
+        private              = private,
+    )
+    return
+pass
+
+
+def unsloth_generic_save_pretrained_merged(
+    self,
+    save_directory       : Union[str, os.PathLike],
+    tokenizer            = None,
+    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    push_to_hub          : bool = False,
+    token                : Optional[Union[str, bool]] = None,
+    is_main_process      : bool = True,
+    state_dict           : Optional[dict] = None,
+    save_function        : Callable = torch.save,
+    max_shard_size       : Union[int, str] = ""5GB"",
+    safe_serialization   : bool = True,
+    variant              : Optional[str] = None,
+    save_peft_format     : bool = True,
+    tags                 : List[str] = None,
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.75,
+):   
+    """"""
+        Same as .push_to_hub(...) except 4bit weights are auto
+        converted to float16 with as few overhead as possible.
 
-def patch_saving_functions(model):
+        Choose for `save_method` to be either:
+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
+    """"""
+    if tokenizer is None:
+        logger.warning_once(
+            ""Unsloth: You're not saving a tokenizer as well?\n""\
+            ""You can do it separately via `tokenizer.save_pretrained(...)`""
+        )
+    pass
+
+    arguments = dict(locals())
+    arguments[""model""] = self
+    del arguments[""self""]
+    unsloth_generic_save(**arguments)
+    for _ in range(3):
+        gc.collect()
+pass
+
+
+def unsloth_generic_push_to_hub_merged(
+    self,
+    repo_id              : str,
+    tokenizer            = None,
+    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    use_temp_dir         : Optional[bool] = None,
+    commit_message       : Optional[str] = ""Trained with Unsloth"",
+    private              : Optional[bool] = None,
+    token                : Union[bool, str, None] = None,
+    max_shard_size       : Union[int, str, None] = ""5GB"",
+    create_pr            : bool = False,
+    safe_serialization   : bool = True,
+    revision             : str = None,
+    commit_description   : str = ""Upload model trained with Unsloth 2x faster"",
+    tags                 : Optional[List[str]] = None,
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.75,
+):
+    """"""
+        Same as .push_to_hub(...) except 4bit weights are auto
+        converted to float16 with as few overhead as possible.
+
+        Choose for `save_method` to be either:
+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
+    """"""
+    if tokenizer is None:
+        logger.warning_once(
+            ""Unsloth: You're not saving a tokenizer as well?\n""\
+            ""You can do it separately via `tokenizer.push_to_hub(...)`""
+        )
+    pass
+
+    arguments = dict(locals())
+    arguments[""model""]          = self
+    arguments[""save_directory""] = repo_id
+    arguments[""push_to_hub""]    = True
+    del arguments[""self""]
+    del arguments[""repo_id""]
+    unsloth_generic_save(**arguments)
+    for _ in range(3):
+        gc.collect()
+pass
+
+
+def not_implemented_save(*args, **kwargs):
+    raise NotImplementedError(""Unsloth: Sorry GGUF is currently not supported for vision models!"")
+pass
+
+
+def patch_saving_functions(model, vision = False):
     import inspect
     import types
     from typing import Callable, Optional, Union, List
@@ -2131,14 +2276,22 @@ def patch_saving_functions(model):
     pass
 
     # Add saving methods to top level model
-    if hasattr(model, ""config""):
-        # Counteract tokenizers
-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)
-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)
-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)
-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)
-        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)
-        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)
+    if not vision:
+        if hasattr(model, ""config""):
+            # Counteract tokenizers
+            model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)
+            model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)
+            model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)
+            model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)
+            model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)
+            model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)
+        pass
+    else:
+        # Vision only 1 option
+        model.push_to_hub_merged     = types.MethodType(unsloth_generic_push_to_hub_merged,     model)
+        model.save_pretrained_merged = types.MethodType(unsloth_generic_save_pretrained_merged, model)
+        model.push_to_hub_gguf       = types.MethodType(not_implemented_save,                   model)
+        model.save_pretrained_gguf   = types.MethodType(not_implemented_save,                   model)
     pass
     return model
 pass
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index 00956ed..012be4b 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -20,13 +20,13 @@ from functools import wraps
 import trl
 import inspect
 from trl import SFTTrainer
-try:
-    from trl import SFTConfig as TrainingArguments
-except:
-    from transformers import TrainingArguments
-pass
 from . import is_bfloat16_supported
-from unsloth_zoo.training_utils import unsloth_train as _unsloth_train
+from unsloth_zoo.training_utils import (
+    unsloth_train as _unsloth_train,
+)
+from unsloth_zoo.vision_utils import (
+    UnslothVisionDataCollator,
+)
 from packaging.version import Version
 import dataclasses
 
@@ -35,6 +35,7 @@ __all__ = [
     ""UnslothTrainer"",
     ""unsloth_train"",
     ""_patch_trl_trainer"",
+    ""UnslothVisionDataCollator"",
 ]
 
 # Unsloth gradient accumulation fix:
@@ -60,7 +61,11 @@ else:
     pass
 pass
 
-
+try:
+    from trl import SFTConfig as TrainingArguments
+except:
+    from transformers import TrainingArguments
+pass
 @dataclass
 class UnslothTrainingArguments(TrainingArguments):
     embedding_learning_rate : Optional[float] = field(
@@ -134,7 +139,7 @@ pass
 
 # From `trl>=0.13.0`, they changed how to pass several params to the trainer
 # We need to patch to make the transition smooth
-def create_backwards_compatible_trainer(trainer_class, config_class):
+def _backwards_compatible_trainer(trainer_class, config_class):
     original_init = trainer_class.__init__
     
     @wraps(original_init)
@@ -167,6 +172,7 @@ def create_backwards_compatible_trainer(trainer_class, config_class):
             }
 
             # Get parameters that exist in Config but not in TrainingArguments
+            from transformers import TrainingArguments
             moved_params = \
                 set(inspect.signature(config_class)     .parameters.keys()) - \
                 set(inspect.signature(TrainingArguments).parameters.keys())
@@ -207,14 +213,13 @@ def _patch_trl_trainer():
 
     import trl.trainer
     trl_classes = dir(trl.trainer)
-
-    non_convertable_trainer = set([""PPOv2"", ""AlignProp""])
-    trl_trainers = set(x[:-len(""Trainer"")] for x in trl_classes if x.endswith(""Trainer"")) - non_convertable_trainer
-    trl_configs  = set(x[:-len(""Config"")]  for x in trl_classes if x.endswith(""Config""))  - non_convertable_trainer
+    trl_trainers = set(x[:-len(""Trainer"")] for x in trl_classes if x.endswith(""Trainer""))
+    trl_configs  = set(x[:-len(""Config"")]  for x in trl_classes if x.endswith(""Config""))
     trl_classes = list(trl_trainers & trl_configs)
 
     for x in trl_classes:
-        exec(f""trl.{x}Trainer.__init__ = create_backwards_compatible_trainer(trl.{x}Trainer, trl.{x}Config)"", globals())
+        try:    exec(f""trl.{x}Trainer.__init__ = _backwards_compatible_trainer(trl.{x}Trainer, trl.{x}Config)"", globals())
+        except: continue
     pass
 
     trl.__UNSLOTH_BACKWARDS_COMPATIBLE__ = True
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 5135766..6bd8a6f 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -45,9 +45,10 @@ def fast_geglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     mlp_size = self.config.intermediate_size
+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
 
-    gate = fast_linear_forward(self.gate_proj, X)
-    up   = fast_linear_forward(self.  up_proj, X)
+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
     gate = torch_nn_functional_gelu(gate, approximate = ""tanh"")
     gate *= up
 
@@ -82,30 +83,20 @@ def GemmaDecoderLayer_fast_forward(
     padding_mask:         Optional[torch.LongTensor] = None,
     *args, **kwargs,
 ):
-    if use_cache: #past_key_value is not None:
+    if past_key_value is not None:
         do_prefill = not hasattr(self.self_attn, ""paged_attention"")
         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda"")
 
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)
-        hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_value=past_key_value,
-            output_attentions=output_attentions,
-            use_cache=use_cache,
-            padding_mask=padding_mask,
+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+            self.self_attn,
+            hidden_states,
+            past_key_value,
+            position_ids,
+            do_prefill = do_prefill,
         )
-        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
-        #     self.self_attn,
-        #     hidden_states,
-        #     past_key_value,
-        #     position_ids,
-        #     do_prefill = do_prefill,
-        # )
         hidden_states += residual
 
         # Fully Connected
@@ -138,8 +129,13 @@ def GemmaDecoderLayer_fast_forward(
     pass
 
     outputs = (hidden_states,)
-    if output_attentions: outputs += (self_attn_weights,)
-    if use_cache: outputs += (present_key_value,)
+
+    if output_attentions:
+        outputs += (self_attn_weights,)
+
+    if use_cache:
+        outputs += (present_key_value,)
+
     return outputs
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b802403..bfbd10e 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -74,7 +74,7 @@ pass
 
 
 from math import sqrt as math_sqrt
-KV_CACHE_INCREMENT = 256 # KV Cache update size
+KV_CACHE_INCREMENT = 128 # KV Cache update size
 
 def LlamaAttention_fast_forward_inference(
     self,
@@ -82,7 +82,6 @@ def LlamaAttention_fast_forward_inference(
     past_key_value: Optional[Tuple[torch.Tensor]],
     position_ids,
     do_prefill = False,
-    attention_mask = None,
 ):
     """"""
         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
@@ -168,12 +167,12 @@ def LlamaAttention_fast_forward_inference(
     Kn *= cos; Kn.addcmul_(RH_K, sin);
     
     # New KV cache
-    Kn = torch.cat([K1, Kn], dim = 2)
-    Vn = torch.cat([V1, Vn], dim = 2)
-    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
-    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
-    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
-    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+    # Kn = torch.cat([K1, Kn], dim = 2)
+    # Vn = torch.cat([V1, Vn], dim = 2)
+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
 
     # Handle sliding windows
     sliding_window = getattr(self.config, ""sliding_window"", None)
@@ -201,7 +200,6 @@ def LlamaAttention_fast_forward_inference(
     # Attention
     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
     A *= self.scalar
-    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
     A = torch.matmul(A, Vnn, out = Qn)
     A = A.transpose(1, 2)
@@ -217,9 +215,10 @@ def fast_swiglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     mlp_size = self.config.intermediate_size
+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
 
-    gate = fast_linear_forward(self.gate_proj, X)
-    up   = fast_linear_forward(self.  up_proj, X)
+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
     gate = torch_nn_functional_silu(gate, inplace = True)
     gate *= up
 
@@ -376,30 +375,19 @@ def LlamaDecoderLayer_fast_forward(
             (see `past_key_values`).
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
-    if use_cache: #past_key_value is not None:
+    if past_key_value is not None:
         do_prefill = not hasattr(self.self_attn, ""paged_attention"")
 
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
-        hidden_states, self_attn_weights, present_key_value = self.self_attn(
-            hidden_states=hidden_states,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_value=past_key_value,
-            output_attentions=output_attentions,
-            use_cache=use_cache,
-            padding_mask=padding_mask,
+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+            self.self_attn,
+            hidden_states,
+            past_key_value,
+            position_ids,
+            do_prefill = do_prefill,
         )
-        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
-        #     self.self_attn,
-        #     hidden_states,
-        #     past_key_value,
-        #     position_ids,
-        #     do_prefill = do_prefill,
-        #     attention_mask = attention_mask,
-        # )
         hidden_states += residual
 
         # Fully Connected
@@ -430,8 +418,13 @@ def LlamaDecoderLayer_fast_forward(
     pass
 
     outputs = (hidden_states,)
-    if output_attentions: outputs += (self_attn_weights,)
-    if use_cache: outputs += (present_key_value,)
+
+    if output_attentions:
+        outputs += (self_attn_weights,)
+
+    if use_cache:
+        outputs += (present_key_value,)
+
     return outputs
 pass
 
@@ -609,8 +602,9 @@ def LlamaModel_fast_forward(
     pass
 
     for idx, decoder_layer in enumerate(self.layers):
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
 
-        if output_hidden_states: all_hidden_states += (hidden_states,)
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
         if self.gradient_checkpointing and self.training:
@@ -642,15 +636,22 @@ def LlamaModel_fast_forward(
                 use_cache=use_cache,
                 padding_mask=padding_mask,
             )
-        pass
 
         hidden_states = layer_outputs[0]
-        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
-        if output_attentions: all_self_attns += (layer_outputs[1],)
+
+        if use_cache:
+            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+
+        if output_attentions:
+            all_self_attns += (layer_outputs[1],)
     pass
+    
     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
 
-    if output_hidden_states: all_hidden_states += (hidden_states,)
+    # add hidden states from the last decoder layer
+    if output_hidden_states:
+        all_hidden_states += (hidden_states,)
+
     next_cache = next_decoder_cache if use_cache else None
     if not return_dict:
         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
@@ -669,29 +670,12 @@ def LlamaModel_fast_forward_inference(
     self,
     input_ids,
     past_key_values,
-    attention_mask = None,
 ):
     # Fix out of bounds tokenization
     input_ids = input_ids[:,:self.max_seq_length]
 
     hidden_states = self.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
-    bsz, q_len, hd = hidden_states.shape
-    seq_len = past_key_values[0][0].shape[-2]
-
-    # Must use attention mask for batched processing
-    sliding_window = getattr(self.config, ""sliding_window"", None)
-    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):
-        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
-            attention_mask,
-            (bsz, q_len),
-            hidden_states,
-            seq_len,
-            sliding_window = sliding_window,
-        )
-    else:
-        attention_mask = None
-    pass
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.layers):
@@ -702,9 +686,7 @@ def LlamaModel_fast_forward_inference(
             decoder_layer.self_attn,
             hidden_states,
             past_key_values[idx],
-            position_ids = None,
-            do_prefill = False,
-            attention_mask = attention_mask,
+            None,
         )
         hidden_states += residual
 
@@ -744,12 +726,11 @@ def CausalLM_fast_forward(fast_forward_inference):
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
 
-        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, ""paged_attention""):
             outputs = fast_forward_inference(
                 self.model,
                 input_ids,
                 past_key_values,
-                attention_mask = attention_mask,
             )
         else:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index fcc1ab6..c609d2e 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -200,13 +200,12 @@ def MistralForCausalLM_fast_forward(
     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
     self.model._has_no_labels = labels is None
 
-    if False and past_key_values is not None and \
+    if past_key_values is not None and \
         hasattr(self.model.layers[0].self_attn, ""paged_attention""):
         outputs = LlamaModel_fast_forward_inference(
             self.model,
             input_ids,
             past_key_values,
-            attention_mask = attention_mask,
         )
     else:
         outputs = self.model(
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index db54c9a..dd526dc 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -11,10 +11,8 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import os
-import warnings
-import importlib
-import sys
+
+import warnings, importlib, sys
 from packaging.version import Version
 
 # # Define a list of modules to check
@@ -60,9 +58,8 @@ except:
                       ""We have some installation instructions on our Github page."")
 pass
 
-import os, re
+import os, re, subprocess, inspect
 import numpy as np
-import subprocess
 
 # Hugging Face Hub faster downloads (only enable during Colab and Kaggle sessions)
 keynames = ""\n"" + ""\n"".join(os.environ.keys())
@@ -83,12 +80,12 @@ elif (major_torch == 2) and (minor_torch < 2):
     del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
 pass
 
-# Torch 2.5 has including_emulation
+# Torch 2.4 has including_emulation
 major_version, minor_version = torch.cuda.get_device_capability()
 SUPPORTS_BFLOAT16 = (major_version >= 8)
 
-if (major_torch == 2) and (minor_torch >= 5): 
-    old_is_bf16_supported = torch.cuda.is_bf16_supported
+old_is_bf16_supported = torch.cuda.is_bf16_supported
+if ""including_emulation"" in str(inspect.signature(old_is_bf16_supported)):
     def is_bf16_supported(including_emulation = False):
         return old_is_bf16_supported(including_emulation)
     torch.cuda.is_bf16_supported = is_bf16_supported
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index e260017..02ed00f 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -169,13 +169,23 @@ class FastLanguageModel(FastLlamaModel):
         autoconfig_error = None
         peft_error = None
         try:
-            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
+            model_config = AutoConfig.from_pretrained(
+                model_name,
+                token = token,
+                revision = revision,
+                trust_remote_code = trust_remote_code,
+            )
             is_model = True
         except Exception as error:
             autoconfig_error = str(error)
             is_model = False
         try:
-            peft_config = PeftConfig .from_pretrained(model_name, token = token, revision = revision)
+            peft_config = PeftConfig.from_pretrained(
+                model_name,
+                token = token,
+                revision = revision,
+                trust_remote_code = trust_remote_code,
+            )
             is_peft = True
         except Exception as error:
             peft_error = str(error)
@@ -207,7 +217,12 @@ class FastLanguageModel(FastLlamaModel):
         if is_peft:
             # Check base model again for PEFT
             model_name = get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
-            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
+            model_config = AutoConfig.from_pretrained(
+                model_name,
+                token = token,
+                revision = revision,
+                trust_remote_code = trust_remote_code,
+            )
         pass
 
         if not was_disabled: enable_progress_bars()
@@ -340,10 +355,11 @@ class FastLanguageModel(FastLlamaModel):
                 token = token,
                 revision = revision,
                 is_trainable = True,
+                trust_remote_code = trust_remote_code,
             )
             # Patch it as well!
             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)
         pass
         return model, tokenizer
     pass
-pass
+pass
\ No newline at end of file
"
"diff --git a/unsloth/dataprep/synthetic.py b/unsloth/dataprep/synthetic.py
index a29e662..de6aff5 100644
--- a/unsloth/dataprep/synthetic.py
+++ b/unsloth/dataprep/synthetic.py
@@ -97,6 +97,9 @@ class SyntheticDataKit:
             elif which == ""False"":
                 # Ignore flag
                 pass
+            elif which == ""None"":
+                # Ignore flag
+                pass
             else:
                 subprocess_commands += [""--"" + flag, which,]
         pass
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 0c07035..cc3dbb1 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -84,12 +84,12 @@ def _cross_entropy_forward(
     logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
     if label_idx != -100:
-        x = tl.load(logits_ptr + label_idx)
+        x = tl.load(logits_ptr + label_idx).to(tl.float32)
         # Go logit scaling for Cohere: t * x
         if DO_LOGIT_SCALING: x = LOGIT_SCALE * x
         # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
         if DO_SOFTCAPPING:   x = SOFTCAP * triton_tanh(x / SOFTCAP)
-        loss = logsumexp - x.to(tl.float32)
+        loss = logsumexp - x
     else:
         loss = 0.0
     tl.store(logsumexp_ptr, logsumexp)
@@ -170,7 +170,7 @@ def _chunked_cross_entropy_forward(
             if DO_LOGIT_SCALING: x = LOGIT_SCALE * x
             # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
             if DO_SOFTCAPPING:   x = SOFTCAP * triton_tanh(x / SOFTCAP)
-            loss = -1.0 * x.to(tl.float32)
+            loss = -1.0 * x
         else:
             loss = 0.0
         tl.store(loss_ptr, loss)
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index 08426b6..887ffca 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -15,12 +15,13 @@
 import torch
 from functools import lru_cache
 from transformers.models.llama.modeling_llama import logger
+import os
 
 torch_compile_options = {
     ""epilogue_fusion""   : True,
     ""max_autotune""      : True,
     ""shape_padding""     : True,
-    ""trace.enabled""     : False, # Output Triton kernel outputs!
+    ""trace.enabled""     : os.environ.get(""UNSLOTH_COMPILE_DEBUG"", ""0"") == ""1"",
     ""triton.cudagraphs"" : False,
 }
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 903093e..a6cd13d 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.11.3""
+__version__ = ""2024.11.4""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3c4d8f3..7f07bea 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1376,6 +1376,15 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
     @torch.inference_mode
     def _fast_generate(*args, **kwargs):
 
+        if hasattr(model, ""config"") and hasattr(model.config, ""max_position_embeddings""):
+            if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
+                if kwargs[""input_ids""].shape[-1] + kwargs[""max_new_tokens""] > model.config.max_position_embeddings:
+                    raise ValueError(
+                        f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {model.config.max_position_embeddings}!\n'\
+                        'You will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`.'
+                    )
+        pass
+
         # Set a flag for generation!
         internal_model = model
         while hasattr(internal_model, ""model""):
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index db7259b..4566302 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -43,6 +43,24 @@ if SUPPORTS_GEMMA:
 if SUPPORTS_GEMMA2:
     from .gemma2 import FastGemma2Model
 pass
+import torch
+
+def _get_dtype(dtype):
+    __DTYPE_MAP = {
+        ""float32"": torch.float32,
+        torch.float32: torch.float32,
+        ""float16"": torch.float16,
+        torch.float16: torch.float16,
+        ""bfloat16"": torch.bfloat16,
+        torch.bfloat16: torch.bfloat16,
+    }
+    if dtype in __DTYPE_MAP:
+        return __DTYPE_MAP[dtype]
+    else:
+        print(f""Unsloth: {dtype} is not recognized, so we'll default to torch.float16"")
+        return torch.float16
+    pass
+pass
 
 
 def __get_model_name(
@@ -332,7 +350,7 @@ class FastLanguageModel(FastLlamaModel):
         model, tokenizer = dispatch_model.from_pretrained(
             model_name        = model_name,
             max_seq_length    = max_seq_length,
-            dtype             = dtype,
+            dtype             = _get_dtype(dtype),
             load_in_4bit      = load_in_4bit,
             token             = token,
             device_map        = device_map,
"
"diff --git a/pyproject.toml b/pyproject.toml
index 0398d0d..91654e6 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -43,7 +43,7 @@ huggingface = [
     ""numpy"",
     ""accelerate>=0.26.1"",
     ""trl>=0.7.9"",
-    ""peft>=0.7.1"",
+    ""peft>=0.7.1,<0.11.0"",
     ""protobuf<4.0.0"",
 ]
 cu118only = [
diff --git a/unsloth/save.py b/unsloth/save.py
index 39b18d0..caa5aaa 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -777,6 +777,8 @@ def install_llama_cpp_old(version = -10):
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
+                if ""undefined reference"" in line:
+                    raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
                 print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         pass
     pass
@@ -809,6 +811,8 @@ def install_llama_cpp_blocking(use_cuda = True):
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
+                if ""undefined reference"" in line:
+                    raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
                 print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         pass
     pass
@@ -984,6 +988,8 @@ def save_to_gguf(
 
     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
         for line in sp.stdout:
+            if ""undefined reference"" in line:
+                raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
             print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         if sp.returncode is not None and sp.returncode != 0:
             raise subprocess.CalledProcessError(sp.returncode, sp.args)
@@ -1025,6 +1031,8 @@ def save_to_gguf(
         # quantize uses stderr
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
+                if ""undefined reference"" in line:
+                    raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
                 print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
             if sp.returncode is not None and sp.returncode != 0:
                 raise subprocess.CalledProcessError(sp.returncode, sp.args)
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a693389..1b122fc 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -372,6 +372,10 @@ def prepare_n_gradient_checkpoints(
 pass
 
 
+# Unsloth only works on NVIDIA GPUs for now
+device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
+device = f""cuda:{device_ids[:device_ids.find(',')]}""
+
 class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     """"""
     Saves VRAM by smartly offloading to RAM.
@@ -393,7 +397,7 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     @torch.cuda.amp.custom_bwd
     def backward(ctx, dY):
         (hidden_states,) = ctx.saved_tensors
-        hidden_states = hidden_states.to(""cuda"", non_blocking = True).detach()
+        hidden_states = hidden_states.to(device, non_blocking = True).detach()
         hidden_states.requires_grad = True
         with torch.enable_grad():
             (output,) = ctx.forward_function(hidden_states, *ctx.args)
@@ -457,7 +461,6 @@ transformers.utils.quantization_config.BitsAndBytesConfig.__init__ = _BitsAndByt
 
 
 # Offloading to disk for modules (lm_head, embed_tokens)
-import os
 import pickle
 
 def offload_to_disk(W, model, name, temporary_location : str = ""_unsloth_temporary_saved_buffers""):
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 5dd2a5a..9850283 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -38,6 +38,11 @@ except:
     GemmaFlashAttention2 = GemmaAttention
 pass
 
+# Unsloth currently only works on one GPU
+import os
+device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
+device = f""cuda:{device_ids[:device_ids.find(',')]}""
+# Please obtain a commercial license
 
 torch_nn_functional_gelu = torch.nn.functional.gelu
 def fast_geglu_inference(self, X):
@@ -45,7 +50,7 @@ def fast_geglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     # mlp_size = self.config.intermediate_size
-    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = device)
 
     gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
@@ -72,7 +77,7 @@ def GemmaDecoderLayer_fast_forward(
     *args, **kwargs,
 ):
     if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
-        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda"")
+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = device)
 
         # Self Attention
         residual = hidden_states
@@ -134,7 +139,7 @@ def GemmaModel_fast_forward_inference(
     position_ids,
     attention_mask = None,
 ):
-    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda"")
+    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = device)
     input_ids = input_ids[:,:self.max_seq_length]
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 7cbdcfb..9327b1b 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -74,6 +74,9 @@ def original_apply_o(self, X):
     return O
 pass
 
+import os # Unsloth only works on NVIDIA GPUs for now
+device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
+device = f""cuda:{device_ids[:device_ids.find(',')]}""
 
 from math import sqrt as math_sqrt
 KV_CACHE_INCREMENT = 256 # KV Cache update size
@@ -132,15 +135,15 @@ def LlamaAttention_fast_forward_inference(
     # Prefill phase
     # if not hasattr(self, ""paged_attention""):
     if do_prefill:
-        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = ""cuda"")
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = device)
         self.paged_attention_K = self.paged_attention[:,0]
         self.paged_attention_V = self.paged_attention[:,1]
         self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
         self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
-        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda"")
-        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda"")
-        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda"")
-        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda"")
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
     elif kv_seq_len >= self.paged_attention.shape[0]:
@@ -170,7 +173,7 @@ def LlamaAttention_fast_forward_inference(
     Qn *= cos
     Qn.addcmul_(RH_Q, sin)
 
-    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda"")
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = device)
     RH_K[:,:,:,:h] = Kn[:,:,:,h:]
     RH_K[:,:,:,h:] = Kn[:,:,:,:h]
     torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
@@ -232,7 +235,7 @@ def fast_swiglu_inference(self, X):
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
     # mlp_size = self.config.intermediate_size
-    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = device)
 
     gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
@@ -522,7 +525,7 @@ def LlamaModel_fast_forward(
         position_ids = torch.arange(
             past_key_values_length, seq_length + past_key_values_length,
             dtype  = torch.int32,
-            device = ""cuda"",
+            device = device,
         )
         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
     elif position_ids is not None:
@@ -842,8 +845,10 @@ def CausalLM_fast_forward(fast_forward_inference):
         if labels is not None:
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
+                device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
+                device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
                 # Fixes https://github.com/unslothai/unsloth/issues/10
-                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
+                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = device)
             pass
             
             shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
@@ -1822,7 +1827,9 @@ class FastLlamaModel:
         # Patch cross entropy loss labels
         # Fixes https://github.com/unslothai/unsloth/issues/10
         max_seq_length = model.max_seq_length
-        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda"")
+        device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
+        device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
+        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = device)
         model.model.extra_ignored_labels = extra_ignored_labels
         internal_model = model
         while hasattr(internal_model, ""model""):
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 291f0aa..e147f21 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -239,8 +239,10 @@ def MistralForCausalLM_fast_forward(
     if labels is not None:
         shift_logits = logits
         if not hasattr(self, ""extra_ignored_labels""):
+            device_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""0"") + "",""
+            device = f""cuda:{device_ids[:device_ids.find(',')]}"" # Unsloth only works on NVIDIA GPUs for now
             # Fixes https://github.com/unslothai/unsloth/issues/10
-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = device)
         pass
         
         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
"
"diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index 92fde81..3ae4d63 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -101,10 +101,13 @@ pass
 
 
 def PatchDPOTrainer():
-    # Patch DPO notebook printing
-    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
-    from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
-    DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
-    DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
+    from transformers.trainer import is_in_notebook
+    if is_in_notebook():
+        # Patch DPO notebook printing
+        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
+        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
+        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
+        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
+    pass
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index d5dd783..fcaa2a1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -486,6 +486,15 @@ def LlamaModel_fast_forward(
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
+    # Fix up attention mask by setting elements to 0
+    # Specifically for DPO
+    if self._has_no_labels and attention_mask is not None:
+        inputs_requires_grad = inputs_embeds.requires_grad
+        if inputs_requires_grad: inputs_embeds.requires_grad_(False)
+        inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)
+        if inputs_requires_grad: inputs_embeds.requires_grad_(True)
+    pass
+
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
@@ -617,6 +626,7 @@ def LlamaForCausalLM_fast_forward(
     return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+    self.model._has_no_labels = labels is None
     outputs = self.model(
         input_ids=input_ids,
         causal_mask=causal_mask,
@@ -726,7 +736,7 @@ class FastLlamaModel:
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
-        logger.warning_once(statistics)
+        print(statistics)
         FastLlamaModel.pre_patch()
 
         if dtype is None:
@@ -826,6 +836,9 @@ class FastLlamaModel:
         # Log Unsloth version for future fastpaths for inference
         model.config.update({""unsloth_version"" : __version__})
 
+        # Add save modules
+        patch_saving_functions(model)
+
         return model, tokenizer
     pass
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 2410572..2941fb3 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -195,6 +195,7 @@ def MistralForCausalLM_fast_forward(
     return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+    self.model._has_no_labels = labels is None
     outputs = self.model(
         input_ids=input_ids,
         causal_mask=causal_mask,
@@ -282,7 +283,7 @@ class FastMistralModel(FastLlamaModel):
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
            f' ""-____-""     Apache 2 free license: http://github.com/unslothai/unsloth'
-        logger.warning_once(statistics)
+        print(statistics)
         FastMistralModel.pre_patch()
 
         if dtype is None:
diff --git a/unsloth/save.py b/unsloth/save.py
index 744ec48..baa8f3f 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -278,7 +278,7 @@ def unsloth_save_model(
         not hasattr(internal_model.model, ""layers"")
     ):
         # Do general saving
-        
+        print(type(model))
         # Edit save_pretrained_settings
         # [TODO] _create_repo has errors due to **kwargs getting accepted
         for deletion in \
@@ -483,7 +483,7 @@ def install_llama_cpp_make_non_blocking():
     n_jobs = max(int(psutil.cpu_count()*1.5), 1)
     # Force make clean
     os.system(""make clean -C llama.cpp"")
-    full_command = [""make"", ""-j"", str(n_jobs), ""-C"", ""llama.cpp""]
+    full_command = [""make"", ""all"", ""-j"", str(n_jobs), ""-C"", ""llama.cpp""]
     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
     return run_installer
 pass
@@ -499,7 +499,7 @@ pass
 def install_llama_cpp_blocking():
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
-        f""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}"",
+        f""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j {psutil.cpu_count()*2}"",
         ""pip install gguf protobuf"",
     ]
     if os.path.exists(""llama.cpp""): return
@@ -515,6 +515,7 @@ pass
 def save_to_gguf(
     model_directory      : str = ""unsloth_finetuned_model"",
     quantization_method  : str = ""fast_quantized"",
+    first_conversion     : str = ""f16"",
     _run_installer = None, # Non blocking install of llama.cpp
 ):
     from transformers.models.llama.modeling_llama import logger
@@ -539,6 +540,16 @@ def save_to_gguf(
         f' ""-____-""     In total, you will have to wait around 26 minutes.\n'
     print(print_info)
 
+    # Check first_conversion format
+    if   first_conversion == ""f16"" : pass
+    elif first_conversion == ""f32"" : pass
+    elif first_conversion == ""q8_0"": pass
+    else:
+        raise RuntimeError(
+            f""Unsloth: `first_conversion` can only be one of ['f16', 'f32', 'q8_0'] and not `{first_conversion}`.""
+        )
+    pass
+
     print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
     if _run_installer is not None:
         _run_installer.wait()
@@ -546,11 +557,19 @@ def save_to_gguf(
         install_llama_cpp_blocking()
     pass
 
-    print(""Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes..."")
-    first_conversion = ""f16""
     if   quantization_method == ""f32"":  first_conversion = ""f32""
     elif quantization_method == ""f16"":  first_conversion = ""f16""
     elif quantization_method == ""q8_0"": first_conversion = ""q8_0""
+    else:
+        # Quantized models must have f16 as the default argument
+        if   first_conversion == ""f32"" : pass
+        elif first_conversion == ""f16"" : pass
+        elif first_conversion == ""q8_0"":
+            logger.warning_once(""Unsloth: We must use f16 for quantization first."")
+            first_conversion = ""f16""
+        pass
+    pass
+    print(f""Unsloth: [1] Converting HF into {first_conversion} GGUF format. This will take 3 minutes..."")
 
     n_cpus = psutil.cpu_count()*2
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
@@ -566,6 +585,17 @@ def save_to_gguf(
             print(line.decode(""utf-8""), flush = True, end = """")
     pass
 
+    # Check if quantization succeeded!
+    if not os.path.isfile(final_location):
+        raise RuntimeError(
+            ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
+            ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
+            ""You must run this in the same folder as you're saving your model.\n""\
+            ""git clone https://github.com/ggerganov/llama.cpp\n""\
+            ""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\n""\
+            ""Once that's done, redo the quantization.""
+        )
+    pass
     print(f""Unsloth: Conversion completed! Output location: {final_location}"")
 
     if quantization_method != first_conversion:
@@ -581,6 +611,19 @@ def save_to_gguf(
             for line in sp.stderr:
                 print(line.decode(""utf-8""), flush = True, end = """")
         pass
+
+        # Check if quantization succeeded!
+        if not os.path.isfile(final_location):
+            raise RuntimeError(
+                ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
+                ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
+                ""You must run this in the same folder as you're saving your model.\n""\
+                ""git clone https://github.com/ggerganov/llama.cpp\n""\
+                ""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\n""\
+                ""Once that's done, redo the quantization.""
+            )
+        pass
+
         print(f""Unsloth: Conversion completed! Output location: {final_location}"")
     pass
 
@@ -765,6 +808,7 @@ def unsloth_save_pretrained_gguf(
     save_directory       : Union[str, os.PathLike],
     tokenizer            = None,
     quantization_method  : str = ""fast_quantized"",
+    first_conversion     : str = ""f16"",
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
     is_main_process      : bool = True,
@@ -813,6 +857,7 @@ def unsloth_save_pretrained_gguf(
     arguments[""save_method""] = ""merged_16bit"" # Must be 16bit
     del arguments[""self""]
     del arguments[""quantization_method""]
+    del arguments[""first_conversion""]
 
     # Non blocking install GGUF first
     if not os.path.exists(""llama.cpp""):
@@ -840,7 +885,7 @@ def unsloth_save_pretrained_gguf(
     for _ in range(3):
         gc.collect()
 
-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)
 
     if push_to_hub:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
@@ -861,6 +906,7 @@ def unsloth_push_to_hub_gguf(
     repo_id              : str,
     tokenizer            = None,
     quantization_method  : str = ""fast_quantized"",
+    first_conversion     : str = ""f16"",
     use_temp_dir         : Optional[bool] = None,
     commit_message       : Optional[str] = None,
     private              : Optional[bool] = None,
@@ -911,6 +957,7 @@ def unsloth_push_to_hub_gguf(
     del arguments[""self""]
     del arguments[""repo_id""]
     del arguments[""quantization_method""]
+    del arguments[""first_conversion""]
 
     # Non blocking install GGUF first
     if not os.path.exists(""llama.cpp""):
@@ -938,7 +985,7 @@ def unsloth_push_to_hub_gguf(
     for _ in range(3):
         gc.collect()
 
-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)
 
     print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
     username = upload_to_huggingface(
@@ -960,6 +1007,23 @@ def patch_saving_functions(model):
 
     if hasattr(model, ""_original_push_to_hub""): return
 
+    # First check if this has already been called, and revert it
+    original_model = model
+    while True:
+        if hasattr(original_model, ""_original_push_to_hub""):
+            original_model.push_to_hub = original_model._original_push_to_hub
+            del original_model._original_push_to_hub
+            if hasattr(original_model, ""push_to_hub_merged""):     del original_model.push_to_hub_merged
+            if hasattr(original_model, ""save_pretrained_merged""): del original_model.save_pretrained_merged
+            if hasattr(original_model, ""push_to_hub_gguf""):       del original_model.push_to_hub_gguf
+            if hasattr(original_model, ""save_pretrained_gguf""):   del original_model.save_pretrained_gguf
+        pass
+
+        if hasattr(original_model, ""model""): original_model = original_model.model
+        else: break
+    pass
+
+    # And now re add our saving methods!
     original_push_to_hub = model.push_to_hub
     signature = str(inspect.signature(original_push_to_hub)).replace(""NoneType"", ""None"")
     signature = signature[1:]
@@ -988,49 +1052,29 @@ def patch_saving_functions(model):
     pass
     '''
     exec(push_to_hub_text, globals())
-    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)
 
-    if hasattr(model, ""add_model_tags""):
-        model.add_model_tags([""unsloth"",])
+    original_model = model
+    while True:
+
+        if not hasattr(original_model, ""_original_push_to_hub""):
+            original_model._original_push_to_hub = original_model.push_to_hub
+            original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)
+
+            if hasattr(original_model, ""add_model_tags""):
+                original_model.add_model_tags([""unsloth"",])
+        pass
 
+        if hasattr(original_model, ""model""): original_model = original_model.model
+        else: break
+    pass
+
+    # Add saving methods to top level model
     if hasattr(model, ""config""):
         # Counteract tokenizers
         model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)
         model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)
         model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)
         model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)
-    else:
-        model.push_to_hub_merged     = model.push_to_hub
-        model.save_pretrained_merged = model.save_pretrained
-        model.push_to_hub_gguf       = model.push_to_hub
-        model.save_pretrained_gguf   = model.save_pretrained
-    pass
-
-    original_model = model
-    while hasattr(original_model, ""model""):
-        original_model = original_model.model
-        if hasattr(original_model, ""_original_push_to_hub""): continue
-        
-        original_model._original_push_to_hub = original_model.push_to_hub
-        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)
-
-        if hasattr(original_model, ""add_model_tags""):
-            original_model.add_model_tags([""unsloth"",])
-
-        if hasattr(original_model, ""config""):
-            # Counteract tokenizers
-            original_model.push_to_hub_merged     = \
-                types.MethodType(unsloth_push_to_hub_merged,     original_model)
-
-            original_model.save_pretrained_merged = \
-                types.MethodType(unsloth_save_pretrained_merged, original_model)
-
-            original_model.push_to_hub_gguf       = \
-                types.MethodType(unsloth_push_to_hub_gguf,       original_model)
-
-            original_model.save_pretrained_gguf   = \
-                types.MethodType(unsloth_save_pretrained_gguf,   original_model)
-        pass
     pass
-    return
+    return model
 pass
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a490fb8..3504037 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -91,7 +91,7 @@ def original_apply_o(self, X):
 pass
 
 from math import sqrt as math_sqrt
-KV_CACHE_INCREMENT = 256 # KV Cache update size
+KV_CACHE_INCREMENT = 512 # KV Cache update size
 torch_nn_functional_softmax = torch.nn.functional.softmax
 # SDPA has GQA internally
 SDPA_HAS_GQA = ""enable_gqa"" in scaled_dot_product_attention.__doc__
@@ -1656,6 +1656,13 @@ class FastLlamaModel:
                 ""Are you certain you want to do remote code execution?""
             )
         pass
+        if fast_inference:
+            import platform
+            if platform.system().lower() == 'windows':
+                print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
+                fast_inference = False
+        pass
+
         if token is None: token = get_token()
         if model_patcher is None: model_patcher = FastLlamaModel
         SUPPORTS_BFLOAT16 = is_bfloat16_supported()
@@ -1966,12 +1973,17 @@ class FastLlamaModel:
             for layer in model.model.layers:
                 layer.self_attn.rotary_emb = rotary_emb
         pass
-        
+
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
+        model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
+
         # Patch generate
         if model.generate.__name__ != ""unsloth_fast_generate"":
             model._old_generate = model.generate
             unsloth_fast_generate.__doc__ = model._old_generate.__doc__
             model.generate = types.MethodType(unsloth_fast_generate, model)
+        pass
         return model, tokenizer
     pass
 
@@ -2404,7 +2416,7 @@ class FastLlamaModel:
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
         model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
-        
+
         # Patch generate
         if model.generate.__name__ != ""unsloth_fast_generate"":
             model._old_generate = model.generate
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index da7f449..a2e609f 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -611,6 +611,21 @@ __INT_TO_FLOAT_MAPPER = \
         ""open-thoughts/OpenThinker-7B"",
         ""unsloth/OpenThinker-7B-bnb-4bit"",
     ),
+    ""unsloth/granite-3.2-2b-instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/granite-3.2-2b-instruct"",
+        ""ibm-granite/granite-3.2-2b-instruct"",
+        ""unsloth/granite-3.2-2b-instruct-bnb-4bit"",
+    ),
+    ""unsloth/granite-3.2-8b-instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/granite-3.2-8b-instruct"",
+        ""ibm-granite/granite-3.2-8b-instruct"",
+        ""unsloth/granite-3.2-8b-instruct-bnb-4bit"",
+    ),
+    ""unsloth/QwQ-32B-unsloth-bnb-4bit"" : (
+        ""unsloth/QwQ-32B"",
+        ""Qwen/QwQ-32B"",
+        ""unsloth/QwQ-32B-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index d13d394..22b6ffc 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -31,40 +31,47 @@ from unsloth_zoo.peft_utils import (
     requires_grad_for_gradient_checkpointing,
 )
 from triton import __version__ as triton_version
+from unsloth_zoo.utils import _get_dtype
+import types
+import functools
 
 __all__ = [
     ""FastBaseVisionModel"",
 ]
 
-def _wrap_fast_inference(generate, device_type, dtype, model):
-    # Wraps inference with bfloat16 / float16
-    @torch.inference_mode
-    def _fast_generate(*args, **kwargs):
-        # For num_logits_to_keep
-        # kwargs[""num_logits_to_keep""] = 1
 
-        # Remove token_type_ids
-        kwargs.pop(""token_type_ids"", None)
+def unsloth_vision_fast_generate(
+    self,
+    *args,
+    **kwargs,
+):
+    FastBaseVisionModel.for_inference(self)
 
-        # Check pad_token
-        model_eos_token_id = getattr(model.config, ""eos_token_id"", None)
-        if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
-            model_eos_token_id = model_eos_token_id[0]
+    dtype = _get_dtype(self.config.torch_dtype)
 
-        kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
+    # Remove token_type_ids
+    kwargs.pop(""token_type_ids"", None)
 
-        try:
-            kwargs[""pixel_values""] = kwargs[""pixel_values""].to(model.dtype)
-        except:
-            pass
+    # Check pad_token
+    model_eos_token_id = getattr(model.config, ""eos_token_id"", None)
+    if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
+        model_eos_token_id = model_eos_token_id[0]
+
+    kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
 
-        # Autocasted
-        with torch.autocast(device_type = device_type, dtype = dtype):
-            output = generate(*args, **kwargs)
+    try:
+        kwargs[""pixel_values""] = kwargs[""pixel_values""].to(dtype)
+    except:
         pass
-        return output
+
+    # Mixed precision autocast
+    with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
+        output = self._old_generate(*args, **kwargs)
     pass
-    return _fast_generate
+
+    FastBaseVisionModel.for_training(self)
+
+    return output
 pass
 
 
@@ -94,12 +101,16 @@ class FastBaseVisionModel:
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
+        from importlib.metadata import version as importlib_version
+        try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
+        except: vllm_version = """"
+
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} vision patching. Transformers: {transformers_version}.\n""\
-           f""   {chr(92)}{chr(92)}   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
+           f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
            f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
-           f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
+           f' ""-____-""     Free license: http://github.com/unslothai/unsloth'
         print(statistics)
 
         # Warn about fast transfers
@@ -136,7 +147,7 @@ class FastBaseVisionModel:
 
         # Cannot be None, since HF now checks for the config
         if load_in_4bit: kwargs[""quantization_config""] = bnb_config
-        
+
         model = AutoModelForVision2Seq.from_pretrained(
             model_name,
             device_map              = device_map,
@@ -190,10 +201,20 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             internal_model._saved_temp_tokenizer = tokenizer
+            # Also set is_loaded_in_8bit to disable incorrect DDP
+            internal_model.is_loaded_in_8bit = True
+
             internal_model = internal_model.model
         pass
         internal_model._saved_temp_tokenizer = tokenizer
-        
+        # Also set is_loaded_in_8bit to disable incorrect DDP
+        internal_model.is_loaded_in_8bit = True
+
+        # Patch generate
+        if model.generate.__name__ != ""unsloth_vision_fast_generate"":
+            model._old_generate = model.generate
+            unsloth_vision_fast_generate.__doc__ = model._old_generate.__doc__
+            model.generate = types.MethodType(unsloth_vision_fast_generate, model)
         return model, tokenizer
     pass
 
@@ -281,6 +302,9 @@ class FastBaseVisionModel:
         pass
         patch_saving_functions(model, vision = True)
 
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastBaseVisionModel.for_training,  model)
+        model.for_inference = functools.partial(FastBaseVisionModel.for_inference, model)
         return model
     pass
 
@@ -319,57 +343,52 @@ class FastBaseVisionModel:
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
                 internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
+            # Also set is_loaded_in_8bit to disable incorrect DDP
+            internal_model.is_loaded_in_8bit = True
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
             internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
+        # Also set is_loaded_in_8bit to disable incorrect DDP
+        internal_model.is_loaded_in_8bit = True
 
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()
         pass
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastBaseVisionModel.for_training,  model)
+        model.for_inference = functools.partial(FastBaseVisionModel.for_inference, model)
+
+        # Patch generate
+        if model.generate.__name__ != ""unsloth_vision_fast_generate"":
+            model._old_generate = model.generate
+            unsloth_vision_fast_generate.__doc__ = model._old_generate.__doc__
+            model.generate = types.MethodType(unsloth_vision_fast_generate, model)
         return model
     pass
 
 
     @staticmethod
     def for_inference(model):
-        model.gradient_checkpointing = False
-        model.training = False
-
-        for name, module in model.named_modules():
-            if hasattr(module, ""gradient_checkpointing""):
-                module.gradient_checkpointing = False
-            if hasattr(module, ""training""):
-                module.training = False
-        pass
-
-        dtype = model.config.torch_dtype
-        if type(dtype) is str:
-            if   dtype ==  ""float16"": dtype = torch.float16
-            elif dtype == ""bfloat16"": dtype = torch.bfloat16
-        pass
-        device_type = model.device.type
-
-        # Wrap model.generate
-        if model.generate.__name__ != ""_fast_generate"":
-            model._unwrapped_old_generate = model.generate
-            model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)
-        pass
-        
-        # Patch tokenizer to pad to the left
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
+        if not hasattr(model, ""parameters""):
+            raise TypeError(""Unsloth: I think you're passing a tokenizer, not the model to for_inference!"")
+
+        def _for_inference(m):
+            if hasattr(m, ""gradient_checkpointing""): m.gradient_checkpointing = False
+            if hasattr(m, ""training""): m.training = False
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""): m._saved_temp_tokenizer.padding_side = ""left""
+            # Set a flag for generation!
+            m._flag_for_generation = True
         pass
+        m = model
+        while hasattr(m, ""model""):
+            _for_inference(m)
+            m = m.model
+        _for_inference(m)
 
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -380,40 +399,34 @@ class FastBaseVisionModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = False
         pass
-
         return model
     pass
 
 
     @staticmethod
     def for_training(model, use_gradient_checkpointing = True):
-        model.gradient_checkpointing = use_gradient_checkpointing
-        model.training = True
-
-        for name, module in model.named_modules():
-            if hasattr(module, ""gradient_checkpointing""):
-                module.gradient_checkpointing = use_gradient_checkpointing
-            if hasattr(module, ""training""):
-                module.training = True
-        pass
+        if not hasattr(model, ""parameters""):
+            raise TypeError(""Unsloth: I think you're passing a tokenizer, not the model to for_training!"")
 
-        # Also revert model.generate
-        if hasattr(model, ""_unwrapped_old_generate""):
-            model.generate = model._unwrapped_old_generate
-            del model._unwrapped_old_generate
+        # Delete all fast inference loras
+        for param in model.parameters():
+            if hasattr(param, ""_fast_lora""):
+                del param._fast_lora
         pass
 
-        # Patch tokenizer to pad to the right
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
+        def _for_training(m):
+            if hasattr(m, ""gradient_checkpointing""): m.gradient_checkpointing = use_gradient_checkpointing
+            if hasattr(m, ""training""): m.training = True
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""): m._saved_temp_tokenizer.padding_side = ""right""
+            # Set a flag for generation!
+            if hasattr(m, ""_flag_for_generation""): del m._flag_for_generation
         pass
+        m = model
+        while hasattr(m, ""model""):
+            _for_training(m)
+            m = m.model
+        _for_training(m)
 
         # Also re-enable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -424,7 +437,6 @@ class FastBaseVisionModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = True
         pass
-
         return model
     pass
 pass
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 38b8f55..06b78d7 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -460,7 +460,7 @@ def LlamaModel_fast_forward(
             (batch_size, seq_length),
             inputs_embeds,
             past_key_values_length,
-            sliding_window = getattr(self.config, ""sliding_window""),
+            sliding_window = getattr(self.config, ""sliding_window"", None),
         )
     pass
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index eb9d105..2c85129 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -131,7 +131,7 @@ def MistralAttention_fast_forward(
         Q = Q.transpose(1, 2)
         K = K.transpose(1, 2)
         V = V.transpose(1, 2)
-        sw = getattr(self.config, ""sliding_window"")
+        sw = getattr(self.config, ""sliding_window"", None)
         sw = q_len if sw is None else sw
         window = (-1, -1) if (q_len <= sw) else (sw, sw)
         A = flash_attn_func(Q, K, V, causal = True, window_size = window)
@@ -175,7 +175,7 @@ def MistralForCausalLM_fast_forward(
 
     if causal_mask is None:
         bsz, q_len = input_ids.shape
-        sliding_window = getattr(self.config, ""sliding_window"")
+        sliding_window = getattr(self.config, ""sliding_window"", None)
         if sliding_window is None or sliding_window <= 0:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
         elif q_len <= sliding_window:
"
"diff --git a/images/Assistant.png b/images/Assistant.png
new file mode 100644
index 0000000..1207034
Binary files /dev/null and b/images/Assistant.png differ
diff --git a/images/Terminal_Type.png b/images/Terminal_Type.png
new file mode 100644
index 0000000..e83ac48
Binary files /dev/null and b/images/Terminal_Type.png differ
diff --git a/images/Where_Terminal.png b/images/Where_Terminal.png
new file mode 100644
index 0000000..2239315
Binary files /dev/null and b/images/Where_Terminal.png differ
diff --git a/pyproject.toml b/pyproject.toml
index 581b86a..7d018b8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,6 +33,7 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
+    ""packaging"",
     ""tyro"",
     ""transformers>=4.42.3"",
     ""datasets>=2.16.0"",
@@ -184,6 +185,7 @@ colab-ampere-torch220 = [
     ""flash-attn"",
 ]
 colab-new = [
+    ""packaging"",
     ""tyro"",
     ""transformers>=4.42.3"",
     ""datasets>=2.16.0"",
@@ -198,7 +200,7 @@ colab-no-deps = [
     ""accelerate>=0.26.1"",
     ""trl>=0.7.9"",
     ""peft>=0.7.1"",
-    ""xformers"",
+    ""xformers<0.0.27"",
     ""bitsandbytes"",
     ""protobuf<4.0.0"",
 ]
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 9eaded3..e543287 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -43,6 +43,7 @@ from platform import system as platform_system
 platform_system = platform_system()
 import numpy as np
 import warnings, subprocess, re, inspect, psutil, os, math
+from packaging.version import Version
 
 # =============================================
 # Disable some warnings which can get annoying
@@ -126,6 +127,23 @@ pass
 import xformers.ops.fmha as xformers
 xformers_attention = xformers.memory_efficient_attention
 from xformers import __version__ as xformers_version
+# Temporarily disable 0.0.27 and higher - inference issues
+if Version(xformers_version) >= Version(""0.0.27""):
+    raise ImportError(
+        f""Unsloth: Your xformers version of {xformers_version} is too new.\n""\
+        'Please downgrade xformers via `pip install --force-reinstall ""xformers<0.0.27""'
+    )
+pass
+
+# Check TRL version
+from trl import __version__ as trl_version
+if Version(xformers_version) >= Version(""0.9.0""):
+    raise ImportError(
+        f""Unsloth: Your TRL version of {trl_version} is too new.\n""\
+        'Please downgrade TRL via `pip install --force-reinstall ""trl<0.9.0""'
+    )
+pass
+
 # =============================================
 
 # =============================================
@@ -696,12 +714,14 @@ pass
 
 def check_nvidia():
     # Unsloth doesn't work yet on AMD devices - we're working on it!
+    output = np.array([0,])
     try:
         output = subprocess.check_output(""nvidia-smi --query-gpu=memory.used --format=csv"", shell = True)
+        output = re.findall(rb'([\d]{1,})[\s]{1,}M', output)
+        output = np.array([int(x.decode('utf-8'))/1024 for x in output])
     except:
-        raise RuntimeError(""Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!"")
-    output = re.findall(rb'([\d]{1,})[\s]{1,}M', output)
-    output = np.array([int(x.decode('utf-8'))/1024 for x in output])
+        if not torch.cuda.is_available():
+            raise RuntimeError(""Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!"")    
     return output
 pass
 PRE_CHECK = check_nvidia()
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index c47e65e..bc70b99 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -15,15 +15,29 @@
 from .llama import *
 from ._utils import __version__
 
-from transformers.models.gemma.modeling_gemma import (
-    GemmaAttention,
-    GemmaDecoderLayer,
-    GemmaModel,
-    GemmaForCausalLM,
-    GemmaRotaryEmbedding,
-    apply_rotary_pos_emb,
-    repeat_kv,
-)
+try:
+    from transformers.models.gemma.modeling_gemma import (
+        GemmaAttention,
+        GemmaDecoderLayer,
+        GemmaModel,
+        GemmaForCausalLM,
+        GemmaRotaryEmbedding,
+        apply_rotary_pos_emb,
+        repeat_kv,
+    )
+except:
+    from packaging.version import Version
+    transformers_version = Version(transformers_version)
+    if not transformers_version >= Version(""4.38""):
+        raise ImportError(
+            f""Unsloth: Your transformers version of {transformers_version} does not support Gemma.\n""\
+            f""The minimum required version is 4.38.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.38""`\n'\
+            f""to obtain the latest transformers build, then restart this session.""\
+        )
+    pass
+pass
+
 from transformers.modeling_attn_mask_utils import (
     _prepare_4d_causal_attention_mask_for_sdpa,
 )
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index fda7853..cf1936d 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -19,15 +19,29 @@ from .gemma import (
     GemmaFixedLinearScalingRotaryEmbedding,
     fast_geglu_inference,
 )
-from transformers.models.gemma2.modeling_gemma2 import (
-    Gemma2Attention,
-    Gemma2DecoderLayer,
-    Gemma2Model,
-    Gemma2ForCausalLM,
-    Gemma2RotaryEmbedding,
-    apply_rotary_pos_emb,
-    repeat_kv,
-)
+try:
+    from transformers.models.gemma2.modeling_gemma2 import (
+        Gemma2Attention,
+        Gemma2DecoderLayer,
+        Gemma2Model,
+        Gemma2ForCausalLM,
+        Gemma2RotaryEmbedding,
+        apply_rotary_pos_emb,
+        repeat_kv,
+    )
+except:
+    from packaging.version import Version
+    transformers_version = Version(transformers_version)
+    if not transformers_version >= Version(""4.42""):
+        raise ImportError(
+            f""Unsloth: Your transformers version of {transformers_version} does not support Gemma2.\n""\
+            f""The minimum required version is 4.42.3.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.42.3""`\n'\
+            f""to obtain the latest transformers build, then restart this session.""\
+        )
+    pass
+pass
+
 from transformers.modeling_attn_mask_utils import (
     _prepare_4d_causal_attention_mask_for_sdpa,
 )
@@ -46,7 +60,7 @@ pass
 # [TODO] We must randomnly use torch.compile?
 # I checked the gradients and formulas and I'm sure it's correct.
 # I'm stumped :(
-@torch.compile(fullgraph = True, dynamic = True)#, options = torch_compile_options)
+@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
 def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
     old_dtype = X.dtype
     X = X.float()
@@ -70,7 +84,11 @@ def gemma2_attention(Q, K, V, causal_mask, self, bsz, q_len):
     K = K.reshape(bsz, n_heads, q_len, head_dim)
     V = V.reshape(bsz, n_heads, q_len, head_dim)
 
-    s = self.config.hidden_size // self.config.num_attention_heads
+    # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
+    # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
+    # We default to using the config file itself
+    # s = self.config.hidden_size // self.config.num_attention_heads
+    s = self.config.query_pre_attn_scalar
     t = self.config.attn_logit_softcapping
 
     Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
@@ -260,7 +278,13 @@ def Gemma2Attention_fast_forward_inference(
         # Only for Gemma2
         self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
-        self.scalar = 1.0 / math_sqrt(self.config.hidden_size // self.config.num_attention_heads)
+        
+        # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
+        # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
+        # We default to using the config file itself
+        # s = self.config.hidden_size // self.config.num_attention_heads
+        self.scalar = 1.0 / math_sqrt(self.config.query_pre_attn_scalar)
+        # self.scalar = 1.0 / math_sqrt(self.config.hidden_size // self.config.num_attention_heads)
         self.half_head_dim = head_dim // 2
         self.           t =       self.config.attn_logit_softcapping
         self.reciprocal_t = 1.0 / self.config.attn_logit_softcapping
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index de7f7bc..2d888b8 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1276,12 +1276,14 @@ class FastLlamaModel:
         f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
         logger.warning(debug_info)
         import subprocess, re, gc, numpy as np
+        a = np.array([0,])
         try:
             a = subprocess.check_output('nvidia-smi --query-gpu=memory.used --format=csv', shell = True)
+            a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)
+            a = np.array([int(x.decode('utf-8'))/1024 for x in a])
         except:
-            raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')
-        a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)
-        a = np.array([int(x.decode('utf-8'))/1024 for x in a])
+            if not torch.cuda.is_available():
+                raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')
         if ((a - PRE_CHECK) >= 1).sum() > 1:
             raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')
         for _ in range(3):
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 82d177e..0f17059 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -22,16 +22,16 @@ from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER
 import os
 
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
-major, minor = transformers_version.split(""."")[:2]
-major, minor = int(major), int(minor)
-SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)
-SUPPORTS_GEMMA   = (major > 4) or (major == 4 and minor >= 38)
-SUPPORTS_GEMMA2  = (major > 4) or (major == 4 and minor >= 42)
+from packaging.version import Version
+transformers_version = Version(transformers_version)
+SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
+SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
+SUPPORTS_GEMMA2  = transformers_version >= Version(""4.42"")
 if SUPPORTS_GEMMA:
     from .gemma  import FastGemmaModel
 if SUPPORTS_GEMMA2:
     from .gemma2 import FastGemma2Model
-del major, minor
+pass
 
 
 def _get_model_name(model_name, load_in_4bit = True):
@@ -134,7 +134,7 @@ class FastLanguageModel(FastLlamaModel):
         elif model_type == ""mistral"": dispatch_model = FastMistralModel
         elif model_type == ""gemma"":
             if not SUPPORTS_GEMMA:
-                raise RuntimeError(
+                raise ImportError(
                     f""Unsloth: Your transformers version of {transformers_version} does not support Gemma.\n""\
                     f""The minimum required version is 4.38.\n""\
                     f'Try `pip install --upgrade ""transformers>=4.38""`\n'\
@@ -143,10 +143,10 @@ class FastLanguageModel(FastLlamaModel):
             dispatch_model = FastGemmaModel
         elif model_type == ""gemma2"":
             if not SUPPORTS_GEMMA2:
-                raise RuntimeError(
+                raise ImportError(
                     f""Unsloth: Your transformers version of {transformers_version} does not support Gemma2.\n""\
-                    f""The minimum required version is 4.43.\n""\
-                    f'Try `pip install --upgrade ""transformers>=4.43""`\n'\
+                    f""The minimum required version is 4.42.3.\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.42.3""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 )
             dispatch_model = FastGemma2Model
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 440a53c..dc0c7da 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -910,12 +910,14 @@ pass
 
 def check_nvidia():
     # Unsloth doesn't work yet on AMD devices - we're working on it!
+    output = np.array([0,])
     try:
         output = subprocess.check_output(""nvidia-smi --query-gpu=memory.used --format=csv"", shell = True)
+        output = re.findall(rb'([\d]{1,})[\s]{1,}M', output)
+        output = np.array([int(x.decode('utf-8'))/1024 for x in output])
     except:
-        raise RuntimeError(""Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!"")
-    output = re.findall(rb'([\d]{1,})[\s]{1,}M', output)
-    output = np.array([int(x.decode('utf-8'))/1024 for x in output])
+        if not torch.cuda.is_available():
+            raise RuntimeError(""Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!"")
     return output
 pass
 PRE_CHECK = check_nvidia()
@@ -972,12 +974,14 @@ def patch_sft_trainer_tokenizer():
     ""    )\n""\
     ""pass\n""\
     ""import subprocess, re, gc, numpy as np\n""\
+    ""a = np.array([0,])\n""\
     ""try:\n""\
     ""    a = subprocess.check_output('nvidia-smi --query-gpu=memory.used --format=csv', shell = True)\n""\
+    ""    a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)\n""\
+    ""    a = np.array([int(x.decode('utf-8'))/1024 for x in a])\n""\
     ""except:\n""\
-    ""    raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')\n""\
-    ""a = re.findall(rb'([\\d]{1,})[\\s]{1,}M', a)\n""\
-    ""a = np.array([int(x.decode('utf-8'))/1024 for x in a])\n""\
+    ""    if not torch.cuda.is_available():\n""\
+    ""        raise RuntimeError('Unsloth: We do not support AMD / Intel machines yet - it is a work in progress!')\n""\
     ""if ((a - PRE_CHECK) >= 1).sum() > 1:\n""\
     ""    raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')\n""\
     ""for _ in range(3):\n""\
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0c00574..d8904aa 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -595,7 +595,6 @@ def _get_statistics(statistics = None, force_download = True):
     # You can disable this by commenting the below out
     try:
         n_cpus = psutil.cpu_count(logical = False)
-
         keynames = ""\n"" + ""\n"".join(os.environ.keys())
         if statistics is not None: pass
         elif ""\nCOLAB_""  in keynames and n_cpus == 1: statistics = ""colab""
@@ -604,10 +603,31 @@ def _get_statistics(statistics = None, force_download = True):
         elif ""\nRUNPOD_"" in keynames: statistics = ""runpod""
         elif ""\nAWS_""    in keynames: statistics = ""aws""
         elif ""\nAZURE_""  in keynames: statistics = ""azure""
-        elif ""\nK_"" in keynames or ""\nFUNCTION_"" in keynames: statistics = ""gcp""
+        # elif ""\nK_"" in keynames or ""\nFUNCTION_"" in keynames: statistics = ""gcp""
         elif ""\nINVOCATION_ID"" in keynames: statistics = ""lambda""
-        else: statistics = ""other""
-
+        # else: statistics = ""other""
+        else:
+            def try_vllm_check():
+                vendor_files = (
+                    ""/sys/class/dmi/id/product_version"",
+                    ""/sys/class/dmi/id/bios_vendor"",
+                    ""/sys/class/dmi/id/product_name"",
+                    ""/sys/class/dmi/id/chassis_asset_tag"",
+                    ""/sys/class/dmi/id/sys_vendor"",
+                )
+                from pathlib import Path
+                for vendor_file in vendor_files:
+                    path = Path(vendor_file)
+                    if path.is_file():
+                        file_content = path.read_text().lower()
+                        if   ""amazon""                in file_content: return ""aws""
+                        elif ""microsoft corporation"" in file_content: return ""azure""
+                        elif ""google""                in file_content: return ""gcp""
+                return ""other""
+            pass
+            try:    statistics = try_vllm_check()
+            except: statistics = ""other""
+        pass
         if statistics is not None:
             from transformers import AutoModelForCausalLM
             stats_model = AutoModelForCausalLM.from_pretrained(
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6a23335..d18dd4c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1628,7 +1628,7 @@ class FastLlamaModel:
 
         # Torch.compile fails on embedding matrix??
         # Workaround randomnly fixes it for torch versions < 2.
-        model.set_input_embeddings(torch.nn.Embedding.from_pretrained(model.get_input_embeddings().weight))
+        # model.set_input_embeddings(torch.nn.Embedding.from_pretrained(model.get_input_embeddings().weight))
         model.config.update({""unsloth_version"" : __version__})
 
         # We also do this for the lm_head
@@ -2234,6 +2234,9 @@ class FastLlamaModel:
             internal_model.gradient_checkpointing = False
             internal_model.training = False
         pass
+        if hasattr(internal_model, ""training""):
+            internal_model.training = False
+        pass
 
         # Also check if lm_head / embeddings are trained
         internal_model = model
@@ -2267,6 +2270,16 @@ class FastLlamaModel:
             internal_model._saved_temp_tokenizer.padding_side = ""left""
         pass
 
+        # Also disable training for embeddings for NEFTune
+        if hasattr(model, ""get_input_embeddings""):
+            embeddings = model.get_input_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = False
+        pass
+        if hasattr(model, ""get_output_embeddings""):
+            embeddings = model.get_output_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = False
+        pass
+
         return model
     pass
 
@@ -2288,6 +2301,9 @@ class FastLlamaModel:
             internal_model.gradient_checkpointing = use_gradient_checkpointing
             internal_model.training = True
         pass
+        if hasattr(internal_model, ""training""):
+            internal_model.training = True
+        pass
 
         # Also revert model.generate
         if hasattr(model, ""_unwrapped_old_generate""):
@@ -2307,6 +2323,16 @@ class FastLlamaModel:
             internal_model._saved_temp_tokenizer.padding_side = ""right""
         pass
 
+        # Also re-enable training for embeddings for NEFTune
+        if hasattr(model, ""get_input_embeddings""):
+            embeddings = model.get_input_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = True
+        pass
+        if hasattr(model, ""get_output_embeddings""):
+            embeddings = model.get_output_embeddings()
+            if hasattr(embeddings, ""training""): embeddings.training = True
+        pass
+
         return model
     pass
 pass
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 0105199..298ed13 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -17,17 +17,20 @@ import importlib
 import sys
 from packaging.version import Version
 
-# Define a list of modules to check
-MODULES_TO_CHECK = [""bitsandbytes""]
-
-# Check if any of the modules in the list have been imported
-for module in MODULES_TO_CHECK:
-    if module in sys.modules:
-        raise ImportError(f""Unsloth: Please import Unsloth before {module}."")
-    pass
-pass
-
-# Currently only supports 1 GPU, or else seg faults will occur.    
+# # Define a list of modules to check
+# MODULES_TO_CHECK = [""bitsandbytes""]
+
+# # Check if any of the modules in the list have been imported
+# for module in MODULES_TO_CHECK:
+#     if module in sys.modules:
+#         raise ImportError(f""Unsloth: Please import Unsloth before {module}."")
+#     pass
+# pass
+
+# Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so
+# enabling it will require much more work, so we have to prioritize. Please understand!
+# We do have a beta version, which you can contact us about!
+# Thank you for your understanding and we appreciate it immensely!
 if ""CUDA_VISIBLE_DEVICES"" in os.environ:
     os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
     devices = os.environ[""CUDA_VISIBLE_DEVICES""]
@@ -36,6 +39,10 @@ if ""CUDA_VISIBLE_DEVICES"" in os.environ:
         first_id = devices.split("","")[0]
         warnings.warn(
             f""Unsloth: 'CUDA_VISIBLE_DEVICES' is currently {devices} \n""\
+            ""Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so ""\
+            ""enabling it will require much more work, so we have to prioritize. Please understand!""\
+            ""We do have a beta version, which you can contact us about!\n""\
+            ""Thank you for your understanding and we appreciate it immensely!\n\n""\
             ""Multiple CUDA devices detected but we require a single device.\n""\
             f""We will override CUDA_VISIBLE_DEVICES to first device: {first_id}.""
         )
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2d8e6a0..2368a37 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1165,10 +1165,10 @@ class FastLlamaModel:
                 inner_training_loop = Trainer._original_training_loop
         except:
             raise RuntimeError(
-                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
-                ""The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\n""
-                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
-                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+                'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\
+                'enabling it will require much more work, so we have to prioritize. Please understand!\n'\
+                'We do have a separate beta version, which you can contact us about!\n'\
+                'Thank you for your understanding and we appreciate it immensely!'
             )
         pass
 
@@ -1201,7 +1201,10 @@ class FastLlamaModel:
         output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)
         output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)
         if output > 1: raise RuntimeError(
-            'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.')
+            'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\
+            'enabling it will require much more work, so we have to prioritize. Please understand!\\n'\\
+            'We do have a separate beta version, which you can contact us about!\\n'\\
+            'Thank you for your understanding and we appreciate it immensely!')
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()""""""
@@ -1214,10 +1217,10 @@ class FastLlamaModel:
             args.gradient_accumulation_steps // self._train_batch_size
         if n_total_devices > 1:
             logger.warning_once(
-                ""* Our OSS was designed for people with few GPU resources to level the playing field.\\n""
-                ""* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\n""
-                ""* We're a 2 person team, so we still have to fund our development costs - thanks!\\n""
-                ""* If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+                '* Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so ' \\
+                '* enabling it will require much more work, so we have to prioritize. Please understand!\\n' \\
+                '* We do have a separate beta version, which you can contact us about!\\n'\\
+                '* Thank you for your understanding and we appreciate it immensely!'
             )
         debug_info =""""""
         debug_info = debug_info.split('\n')
@@ -1244,10 +1247,10 @@ class FastLlamaModel:
         n_total_devices = total_batches // ga // bsz
         if n_total_devices > 1:
             logger.warning_once(
-                ""* Our OSS was designed for people with few GPU resources to level the playing field.\\n""
-                ""* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\n""
-                ""* We're a 2 person team, so we still have to fund our development costs - thanks!\\n""
-                ""* If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+                '* Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so ' \\
+                '* enabling it will require much more work, so we have to prioritize. Please understand!\\n' \\
+                '* We do have a separate beta version, which you can contact us about!\\n'\\
+                '* Thank you for your understanding and we appreciate it immensely!'
             )
             divisor = n_total_devices / 1
             bsz = self._train_batch_size = max(int(bsz / divisor), 1)
@@ -1273,10 +1276,10 @@ class FastLlamaModel:
         )
         if ""n_total_devices >"" not in inner_training_loop:
             raise RuntimeError(
-                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
-                ""The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\n""
-                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
-                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+                'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\
+                'enabling it will require much more work, so we have to prioritize. Please understand!\n'\
+                'We do have a separate beta version, which you can contact us about!\n'\
+                'Thank you for your understanding and we appreciate it immensely!'
             )
         pass
         inner_training_loop = inner_training_loop.replace(
@@ -1783,10 +1786,10 @@ class FastLlamaModel:
         from transformers.trainer import Trainer 
         if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
             raise RuntimeError(
-                ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
-                ""The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\n""
-                ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
-                ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
+                'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\
+                'enabling it will require much more work, so we have to prioritize. Please understand!\n'\
+                'We do have a separate beta version, which you can contact us about!\n'\
+                'Thank you for your understanding and we appreciate it immensely!'
             )
         pass
 
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index fe2dc06..50b0927 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -954,7 +954,7 @@ def patch_sft_trainer_tokenizer():
     ""\n""\
     ""if self._inner_training_loop.__name__ != '_fast_inner_training_loop':\n""\
     ""    raise RuntimeError(\n""\
-    ""       'Do not edit specific areas of the Unsloth codebase or you will get CUDA segfaults.'\n""\
+    ""       'Please do not edit specific areas of the Unsloth codebase or you will get CUDA segfaults.'\n""\
     ""    )\n""\
     ""pass\n""\
     ""n_devices = torch.cuda.device_count()\n""\
@@ -964,7 +964,10 @@ def patch_sft_trainer_tokenizer():
     ""output = re.findall(rb'([\\d]{1,})[\\s]{1,}M', output)\n""\
     ""output = sum(int(x.decode('utf-8'))/1024 > 4 for x in output)\n""\
     ""if output > 1: raise RuntimeError(\n""\
-    ""    'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.')\n""\
+    ""    'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\\\n""\
+    ""    'enabling it will require much more work, so we have to prioritize. Please understand!\\n'\\\n""\
+    ""    'We do have a separate beta version, which you can contact us about!\\n'\\\n""\
+    ""    'Thank you for your understanding and we appreciate it immensely!')\n""\
     ""for _ in range(3):\n""\
     ""    gc.collect()\n""\
     ""    torch.cuda.empty_cache()\n""\
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0f0d4c1..2423e8f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.1""
+__version__ = ""2025.3.4""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bcabbd5..a5bc871 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1538,6 +1538,7 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
     # Wraps inference with bfloat16 / float16
     @torch.inference_mode
     def _fast_generate(*args, **kwargs):
+        if hasattr(model, ""for_inference""): model.for_inference()
 
         if hasattr(model, ""config"") and hasattr(model.config, ""max_position_embeddings""):
             if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
@@ -1603,6 +1604,9 @@ def _wrap_fast_inference(generate, device_type, dtype, model):
             accelerate.utils.operations.send_to_device = accelerate_old_send_to_device
         pass
 
+        # Return to training state
+        if hasattr(model, ""for_training""): model.for_training()
+
         return output
     pass
     return _fast_generate
@@ -2416,6 +2420,9 @@ class FastLlamaModel:
             model.load_lora = partial(load_lora, model)
         pass
 
+        # Add for_inference and for_training
+        model.for_training  = partial(FastLlamaModel.for_training,  model)
+        model.for_inference = partial(FastLlamaModel.for_inference, model)
         return model
     pass
 
"
"diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 4288f53..6a84f12 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -207,7 +207,7 @@ def grpo_trainer__get_per_token_logps(function_name, function):
     if  function_name != ""_get_per_token_logps"": return function
 
     def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
-        if os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '1':
+        if os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '0':
             return None # Unsloth efficient GRPO
         # Otherwise, calculate normally:
         if not hasattr(self, '_autocast_dtype'):
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 7993a9a..9508d64 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -137,8 +137,6 @@ def unsloth_base_fast_generate(
     try: kwargs[""pixel_values""] = kwargs[""pixel_values""].to(dtype)
     except: pass
 
-    print(kwargs.keys())
-
     # Mixed precision autocast
     if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"": dtype = torch.float32
     with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index bea0606..d545c65 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -9,9 +9,6 @@ assignees: ''
 
 **Describe the bug**
 A clear and concise description of what the bug is.
-## Issue Type
-
-## Steps to Reproduce
 
 1. **Environment Setup:**
    - OS: [e.g., Ubuntu 20.04]
"
"diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 8b66b17..db1d73c 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -104,6 +104,11 @@ cdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_
 cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
 cgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16
 cgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16
+torch_mm = torch.mm
+torch_mv = torch.mv
+torch_matmul = torch.matmul
+torch_addmm  = torch.addmm
+torch_empty  = torch.empty
 
 def QUANT_STATE(W): return getattr(W, ""quant_state"", None)
 
@@ -194,8 +199,8 @@ if HAS_CUDA_STREAM:
             WEIGHT_BUFFER = WEIGHT_BUFFERS[device_index]
             ABSMAX_BUFFER = ABSMAX_BUFFERS[device_index]
             if WEIGHT_BUFFER is None:
-                WEIGHT_BUFFERS[device_index] = WEIGHT_BUFFER = torch.empty(size, dtype = dtype, device = device, requires_grad = False)
-                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch.empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+                WEIGHT_BUFFERS[device_index] = WEIGHT_BUFFER = torch_empty(size, dtype = dtype, device = device, requires_grad = False)
+                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
 
             if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)
             if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)
@@ -204,11 +209,11 @@ if HAS_CUDA_STREAM:
             out_absmax = ABSMAX_BUFFER[:n_elements_absmax]
         else:
             if out is None:
-                out = torch.empty(shape, dtype = dtype, device = device, requires_grad = False)
+                out = torch_empty(shape, dtype = dtype, device = device, requires_grad = False)
             else:
                 assert(out.shape == shape)
                 assert(out.dtype == dtype)
-            out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+            out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
         pass
 
         # NF4 dequantization of statistics
@@ -258,11 +263,11 @@ else:
 
         # Create weight matrix
         if out is None:
-            out = torch.empty(shape, dtype = dtype, device = device, requires_grad = False)
+            out = torch_empty(shape, dtype = dtype, device = device, requires_grad = False)
         else:
             assert(out.shape == shape)
             assert(out.dtype == dtype)
-        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+        out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
 
         # Do dequantization
         ptr_out_absmax = get_ptr(out_absmax)
@@ -286,7 +291,7 @@ pass
 
 if HAS_CUDA_STREAM:
     def fast_gemv(X, W, quant_state, out = None):
-        if quant_state is None: return torch.matmul(X, W, out = out)
+        if quant_state is None: return torch_matmul(X, W, out = out)
         # For fast X @ W where seq_len == 1
         # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
         _, q_len, hd = X.shape
@@ -318,7 +323,7 @@ if HAS_CUDA_STREAM:
         bout = shape[0]
 
         if out is None:
-            out = torch.empty((1, 1, bout,), dtype = dtype, device = device)
+            out = torch_empty((1, 1, bout,), dtype = dtype, device = device)
         # else:
         #     assert(out.shape == (1, 1, bout,))
         # pass
@@ -336,7 +341,7 @@ if HAS_CUDA_STREAM:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch.empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
         with torch_cuda_device(device):
             cdequantize_blockwise_fp32(
                 get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
@@ -385,7 +390,7 @@ else:
         device = W.device
 
         if out is None:
-            out = torch.empty((1, 1, bout,), dtype = dtype, device = device)
+            out = torch_empty((1, 1, bout,), dtype = dtype, device = device)
         # else:
         #     assert(out.shape == (1, 1, bout,))
         # pass
@@ -403,7 +408,7 @@ else:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch.empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
             ctypes_c_int(blocksize2), ctypes_c_int(df.numel()),
@@ -423,10 +428,6 @@ else:
 pass
 
 
-torch_mm = torch.mm
-torch_mv = torch.mv
-torch_matmul = torch.matmul
-torch_addmm  = torch.addmm
 def fast_linear_forward(proj, X, temp_lora = None, out = None):
 
     W, W_quant, lora_A, lora_B, lora_S, bias = get_lora_parameters_bias(proj)
@@ -438,7 +439,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
     elif bsz == 1 and q_len == 1:
         out = fast_gemv(X, W, W_quant, out = out)
     else:
-        W = fast_dequantize(W.t(), W_quant, use_global_buffer = False)
+        W = fast_dequantize(W.t(), W_quant, use_global_buffer = True)
         out = torch_matmul(X, W, out = out)
     pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 8ba7c45..356e81a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -261,7 +261,7 @@ def LlamaAttention_fast_forward_inference(
     # pass
 
     # Attention
-    if bsz == 1:
+    if True:#bsz == 1:
         Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
         # It seems like doing (Q * scalar) @ K is better than (Q @ K) * scalar to stop overflows
         A = torch_matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
"
"diff --git a/unsloth/model_registry.py b/unsloth/model_registry.py
index dede596..2f7ccb9 100644
--- a/unsloth/model_registry.py
+++ b/unsloth/model_registry.py
@@ -342,7 +342,8 @@ def register_llama_models():
     global _IS_LLAMA_REGISTERED
     if _IS_LLAMA_REGISTERED:
         return
-    _register_models(_LLAMA_INFO)
+    _register_models(LlamaMeta3_1)
+    _register_models(LlamaMeta3_2)
     _IS_LLAMA_REGISTERED = True
 
 
@@ -403,13 +404,18 @@ def _get_models(filter_func: Callable[[ModelInfo], bool] = _base_name_filter):
     return {k: v for k, v in MODEL_REGISTRY.items() if filter_func(v)}
 
 
-def get_llama_models():
+def get_llama_models(version: str = None):
     if not _IS_LLAMA_REGISTERED:
         register_llama_models()
 
-    return _get_models(
-        partial(_base_name_filter, base_name=_LLAMA_INFO[""base_name""])
+    llama_models: dict[str, ModelInfo] = _get_models(
+        partial(_base_name_filter, base_name=LlamaMeta3_1.base_name)
     )
+    if version is not None:
+        llama_models = {
+            k: v for k, v in llama_models.items() if v.version == version
+        }
+    return llama_models
 
 
 def get_llama_vision_models():
@@ -481,14 +487,17 @@ if __name__ == ""__main__"":
             model_info = None
         return model_info
 
-    test_model = LlamaMeta3_2
-    _register_models(test_model)
+    register_llama_models()
 
-    for k, v in MODEL_REGISTRY.items():
+    llama3_1_models = get_llama_models(version=""3.2"")
+    missing_models = []
+    for k, v in llama3_1_models.items():
         model_info = get_model_info(v.model_path)
         if model_info is None:
             # print unicode cross mark followed by model k
             print(f""\u2718 {k}"")
-        else:
-            # print unicode checkmark followed by model k
-            print(f""\u2713 {k} found"")
+            missing_models.append(k)
+    
+    if len(missing_models) == 0:
+        # print unicode checkmark
+        print(f""\u2713 All models found!"")
\ No newline at end of file
"
"diff --git a/pyproject.toml b/pyproject.toml
index a19d68f..e1e021d 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.6.6"",
+    ""unsloth_zoo>=2025.6.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.6.6"",
+    ""unsloth_zoo>=2025.6.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a31bf7b..154b437 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.6.8""
+__version__ = ""2025.6.9""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bc46ba1..d0ff413 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -909,12 +909,7 @@ def LlamaModel_fast_forward(
                 mask = self. GA_mask if use_static_mask else dynamic_GA_mask
         pass
 
-        try:
-            is_gradient_checkpointing_layer = isinstance(decoder_layer, GradientCheckpointingLayer)
-        except:
-            is_gradient_checkpointing_layer = False
-
-        if gradient_checkpointing and not is_gradient_checkpointing_layer:
+        if gradient_checkpointing and not isinstance(decoder_layer, GradientCheckpointingLayer):
             def create_custom_forward(module):
                 def custom_forward(*inputs):
                     return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask, position_embeddings = position_embeddings)
@@ -2019,7 +2014,7 @@ class FastLlamaModel:
         f""   {chr(92)}{chr(92)}   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,} | Total steps = {max_steps:,}\\n""\\
         f""O^O/ {chr(92)}_/ {chr(92)}    Batch size per device = {self._train_batch_size:,} | Gradient accumulation steps = {args.gradient_accumulation_steps}\\n""\\
         f""{chr(92)}        /    Data Parallel GPUs = {args.world_size} | Total batch size ({self._train_batch_size} x {args.gradient_accumulation_steps} x {args.world_size}) = {total_train_batch_size:,}\\n""\\
-        f' ""-____-""     Trainable parameters = {get_model_param_count(model, trainable_only=True):,}/{get_model_param_count(model):,} ({get_model_param_count(model, trainable_only=True)/get_model_param_count(model)*100:.2f}% trained)'
+        f' ""-____-""     Trainable parameters = {get_model_param_count(model, trainable_only=True):,} of {get_model_param_count(model):,} ({get_model_param_count(model, trainable_only=True)/get_model_param_count(model)*100:.2f}% trained)'
         logger.warning(debug_info)
         import gc
         for _ in range(3):
@@ -2842,6 +2837,12 @@ class FastLlamaModel:
             m = m.model
         _for_inference(m)
 
+        # Since transformers 4.53, must turn off explicitly
+        for module in model.modules():
+            if hasattr(module, ""gradient_checkpointing""):
+                module.gradient_checkpointing = False
+        pass
+
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
             embeddings = model.get_input_embeddings()
@@ -2880,6 +2881,12 @@ class FastLlamaModel:
             m = m.model
         _for_training(m)
 
+        # Since transformers 4.53, must turn on explicitly
+        for module in model.modules():
+            if hasattr(module, ""gradient_checkpointing""):
+                module.gradient_checkpointing = use_gradient_checkpointing
+        pass
+
         # Also re-enable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
             embeddings = model.get_input_embeddings()
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 70dd896..a95a54b 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -547,25 +547,40 @@ class FastModel(FastBaseModel):
         lowered_model_name = model_name.lower()
         LATEST  = '\nPlease use transformers via `pip install --no-deps git+https://github.com/huggingface/transformers.git`'
         NIGHTLY = '\nPlease use nightly transformers via pip install --upgrade ""transformers>=4.49.0""`'
+        # Pixtral
         if ""pixtral"" in lowered_model_name and transformers_version < Version(""4.49.0""):
             raise RuntimeError(""Unsloth: Pixtral only works on transformers >= 4.49.0."" + LATEST)
+        # Qwen 2.5
         elif ""qwen2.5"" in lowered_model_name and transformers_version < Version(""4.49.0""):
             raise RuntimeError(""Unsloth: Qwen 2.5 only works on transformers >= 4.49.0."" + LATEST)
+        # Gemma 3
         elif ""gemma-3"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Gemma 3 only works on transformers >= 4.50.0."" + NIGHTLY)
+        # Cohere
         elif ""c4ai-command-a-03-2025"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Cohere's Command model only works on transformers >= 4.50.0."" + NIGHTLY)
+        # Sesame
         elif ""csm-1b"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Sesame fails
-            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
+                ""all;torch.float32;torch.float16;""\
+                ""if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16);""
+        # Granite 4
         elif 'granite-4' in lowered_model_name:
             # granite-4 rms norms are stored as 16 bit, but we upcast
             os.environ[""UNSLOTH_UPCAST_LAYERNORM""] = ""1""
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
+        # Olmo 2
         elif ""olmo-2"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
+        # Gemma 3N
         elif ""gemma-3n"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
+                ""float16;torch.float16;torch.float16;""\
+                ""if name.endswith(('.conv')): module;""\
+                ""from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()""
+            
             if transformers_version < Version(""4.53.0""):
                 raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
         else:
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 3137792..fb0a544 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -350,11 +350,23 @@ class FastBaseModel:
         correct_dtype = None
         if os.environ.get(""UNSLOTH_FORCE_CUSTOM_DTYPE"", """") != """":
             custom_datatype = os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""]
-            assert custom_datatype.count("";"") == 1
-            bnb_compute_dtype, custom_datatype = custom_datatype.split("";"", 1)
-            dtype = torch.float32
-            bnb_compute_dtype = eval(bnb_compute_dtype)
-            correct_dtype = bnb_compute_dtype
+            assert custom_datatype.count("";"") >= 4
+            checker, _dtype, _bnb_compute_dtype, _custom_datatype, execute_code = custom_datatype.split("";"", 4)
+
+            # Allow custom dtypes on all runs
+            allow_all_runs = (checker == ""all"")
+            # Allow only on float16 datatypes
+            allow_float16_runs = (checker == ""float16"" and dtype == torch.float16)
+
+            if allow_all_runs or allow_float16_runs:
+                dtype = eval(_dtype)
+                bnb_compute_dtype = eval(_bnb_compute_dtype)
+                correct_dtype = bnb_compute_dtype
+                custom_datatype = _custom_datatype
+                # Execute code as well
+                if len(execute_code.strip()) != 0:
+                    exec(execute_code)
+            pass
         pass
 
         # Stop SDPA for some archs like Pixtral / Mistral3
@@ -423,8 +435,15 @@ class FastBaseModel:
 
         # Edit data-types
         if custom_datatype is not None:
-            for name, module in model.named_modules():
+            for jj, (name, module) in enumerate(model.named_modules()):
                 exec(custom_datatype)
+            pass
+            # Clear deleted GPU items
+            for _ in range(3):
+                gc.collect()
+                if DEVICE_TYPE == ""cuda"":  torch.cuda.empty_cache()
+                elif DEVICE_TYPE == ""xpu"": torch.xpu.empty_cache()
+            pass
         pass
 
         # Counteract saved tokenizers
@@ -713,6 +732,12 @@ class FastBaseModel:
             m = m.model
         _for_inference(m)
 
+        # Since transformers 4.53, must turn off explicitly
+        for module in model.modules():
+            if hasattr(module, ""gradient_checkpointing""):
+                module.gradient_checkpointing = False
+        pass
+
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
             embeddings = model.get_input_embeddings()
@@ -755,6 +780,12 @@ class FastBaseModel:
             m = m.model
         _for_training(m)
 
+        # Since transformers 4.53, must turn on explicitly
+        for module in model.modules():
+            if hasattr(module, ""gradient_checkpointing""):
+                module.gradient_checkpointing = True
+        pass
+
         # Also re-enable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
             embeddings = model.get_input_embeddings()
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 7d205eb..da46972 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -15,12 +15,15 @@
 __all__ = [
     ""get_chat_template"",
     ""test_chat_templates"",
+    ""fix_sentencepiece_tokenizer"",
 ]
 
 from transformers import StoppingCriteria, StoppingCriteriaList
 from torch import LongTensor, FloatTensor
 from transformers.models.llama.modeling_llama import logger
 from .models._utils import patch_tokenizer
+import os
+import shutil
 
 CHAT_TEMPLATES = {}
 
@@ -239,14 +242,75 @@ CHAT_TEMPLATES[""gemma""] = (gemma_template, gemma_eos_token,)
 
 
 # Gemma with ChatML instead
+# We find using <eos> is still more appropriate!
 gemma_chatml_template = ""{{ bos_token }}"" + chatml_template
 gemma_chatml_eos_token = (
-    {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""},
+    {""<start_of_turn>"" : ""<|im_start|>"", ""<eos>"" : ""<|im_end|>""},
     ""<|im_end|>"",
 )
 CHAT_TEMPLATES[""gemma_chatml""] = (gemma_chatml_template, gemma_chatml_eos_token,)
 
 
+def fix_sentencepiece_tokenizer(
+    old_tokenizer,
+    new_tokenizer,
+    token_mapping,
+    temporary_location = ""_unsloth_sentencepiece_temp"",
+):
+    # From https://github.com/google/sentencepiece/issues/121
+    # We need to manually edit the sentencepiece tokenizer!
+    try:
+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2
+    except:
+        if not os.path.exists(temporary_location):
+            os.system(""git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp"")
+            os.system(f""cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto"")
+            shutil.rmtree(temporary_location)
+        pass
+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2
+    pass
+
+    if not os.path.exists(temporary_location):
+        os.makedirs(temporary_location)
+    pass
+
+    # First save the old tokenizer
+    old_tokenizer.save_pretrained(temporary_location)
+
+    from sentencepiece import SentencePieceProcessor
+    tokenizer_file = sentencepiece_model_pb2.ModelProto()
+    tokenizer_file.ParseFromString(open(f""{temporary_location}/tokenizer.model"", ""rb"").read())
+
+    # Now save the new tokenizer
+    new_tokenizer.save_pretrained(temporary_location)
+
+    # Now correct the old tokenizer's .model file
+    for old_token, new_token in token_mapping.items():
+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids
+        ids = ids[0]
+        if (len(ids) != 1):
+            # Skip this token!
+            print(f""Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!"")
+            continue
+        pass
+        ids = ids[0]
+        tokenizer_piece = tokenizer_file.pieces[ids]
+        assert(tokenizer_piece.piece == old_token)
+        tokenizer_piece.piece = new_token
+    pass
+
+    # And now write it
+    with open(f""{temporary_location}/tokenizer.model"", ""wb"") as file:
+        file.write(tokenizer_file.SerializeToString())
+    pass
+
+    # And load it!
+    from transformers import AutoTokenizer
+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)
+    return tokenizer
+pass
+
+
 def get_chat_template(
     tokenizer,
     chat_template = ""chatml"",
@@ -290,11 +354,13 @@ def get_chat_template(
 
             string_vocab = tokenizer._tokenizer.to_str()
 
+            skipped = 0
             for old_token, new_token in token_mapping.items():
                 old_count = string_vocab.count(f'""{old_token}""')
                 new_count = string_vocab.count(f'""{new_token}""')
                 if new_count != 0:
                     print(f""{new_token} is already a token. Skipping."")
+                    skipped += 1
                 elif old_count == 0:
                     raise RuntimeError(f""{old_token} was not part of the tokenizer!"")
                 else:
@@ -308,8 +374,14 @@ def get_chat_template(
                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
             pass
 
-            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+            if skipped != len(token_mapping):
+                new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
+                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+
+                # Must fix the sentence piece tokenizer since there's no tokenizer.model file!
+                tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)
+            else:
+                pass
 
         elif stop_word != ""eos_token"":
             logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 1055dfb..a43dc4f 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -12,31 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-try:
-    # Fix up AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
-    # MUST do this at the start primarily due to tensorflow causing issues
-    import google.protobuf.message_factory
-    class MessageFactory:
-        def CreatePrototype(self, *args, **kwargs): return
-        def GetMessages(self, *args, **kwargs): return
-        def GetPrototype(self, *args, **kwargs): return
-    if not hasattr(google.protobuf.message_factory, ""MessageFactory""):
-        google.protobuf.message_factory.MessageFactory = MessageFactory
-    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
-        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
-        not hasattr(google.protobuf.message_factory, ""GetMessageClass""):
-        google.protobuf.message_factory.MessageFactory = MessageFactory
-    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
-        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
-        hasattr(google.protobuf.message_factory, ""GetMessageClass""):
-        GetMessageClass = google.protobuf.message_factory.GetMessageClass
-        def GetPrototype(self, descriptor):
-            return GetMessageClass(descriptor)
-        google.protobuf.message_factory.MessageFactory.GetPrototype = GetPrototype
-    pass
-except:
-    pass
-
 import warnings, importlib, sys
 from packaging.version import Version
 import os, re, subprocess, inspect
@@ -265,6 +240,31 @@ except:
     raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`"")
 pass
 
+try:
+    # Fix up AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
+    # MUST do this at the start primarily due to tensorflow causing issues
+    import google.protobuf.message_factory
+    class MessageFactory:
+        def CreatePrototype(self, *args, **kwargs): return
+        def GetMessages(self, *args, **kwargs): return
+        def GetPrototype(self, *args, **kwargs): return
+    if not hasattr(google.protobuf.message_factory, ""MessageFactory""):
+        google.protobuf.message_factory.MessageFactory = MessageFactory
+    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
+        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
+        not hasattr(google.protobuf.message_factory, ""GetMessageClass""):
+        google.protobuf.message_factory.MessageFactory = MessageFactory
+    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
+        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
+        hasattr(google.protobuf.message_factory, ""GetMessageClass""):
+        GetMessageClass = google.protobuf.message_factory.GetMessageClass
+        def GetPrototype(self, descriptor):
+            return GetMessageClass(descriptor)
+        google.protobuf.message_factory.MessageFactory.GetPrototype = GetPrototype
+    pass
+except:
+    pass
+
 from .models import *
 from .models import __version__
 from .save import *
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index dd1798f..fd4cd81 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.8.7""
+__version__ = ""2025.8.8""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 2e3761f..a2a02d7 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -528,6 +528,7 @@ def get_chat_template(
         chat_template, stop_word = chat_template
         assert(type(chat_template) is str)
         assert(type(stop_word) is str)
+        ollama_modelfile = None
 
     elif type(chat_template) is str:
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f2f79de..3d969d7 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1423,9 +1423,38 @@ class FastLlamaModel:
         transformers_set_seed(random_state)
 
         if isinstance(model, PeftModelForCausalLM):
-            raise TypeError(
-                ""Unsloth: Your model already has LoRA adapters. No need to run this again!""
+            # Check if exactly the same and then pass through!
+            assert(hasattr(model, ""peft_config""))
+
+            peft_config = model.peft_config[""default""].to_dict()
+            check_parameters = [
+                ""r"", ""lora_alpha"", ""lora_dropout"",
+                ""bias"", ""layers_to_transform"", ""layers_pattern"",
+                ""use_rslora"", ""modules_to_save"", ""init_lora_weights"",
+            ]
+            check_all = True
+            for param in check_parameters:
+                check_all = check_all and (peft_config[param] == eval(param))
+            pass
+            check_all = check_all and (
+                len(set(peft_config[""target_modules""]) ^ set(target_modules)) == 0
             )
+            check_all = check_all and (
+                (loftq_config == {} or loftq_config is None) and \
+                (peft_config[""loftq_config""] == {} or peft_config[""loftq_config""] is None)
+            )
+
+            if check_all:
+                # Simply pass through!
+                logger.warning(
+                    ""Unsloth: Already have LoRA adapters! We shall skip this step.""
+                )
+                return model
+            else:
+                raise TypeError(
+                    ""Unsloth: Your model already has LoRA adapters. Your new parameters are different.""
+                )
+            pass
         pass
 
         if loftq_config is None: loftq_config = {}
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index de1e2e5..d7c0f07 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -91,21 +91,37 @@ class FastLanguageModel(FastLlamaModel):
         model_name = _get_model_name(model_name, load_in_4bit)
 
         # First check if it's a normal model via AutoConfig
-        is_peft = False
         try:
             model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
-            is_peft = False
+            is_model = True
+        except:
+            is_model = False
+        try:
+            peft_config = PeftConfig .from_pretrained(model_name, token = token, revision = revision)
+            is_peft = True
         except:
-            try:
-                # Most likely a PEFT model
-                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)
-            except:
-                raise RuntimeError(f""Unsloth: `{model_name}` is not a full model or a PEFT model."")
-            
+            is_peft = False
+
+        # Cannot be both!
+        if is_model and is_peft:
+            raise RuntimeError(
+                ""Unsloth: You repo has a LoRA adapter and a base model.\n""\
+                ""You have 2 files `config.json` and `adapter_config.json`.\n""\
+                ""We must only allow one config file.\n""\
+                ""Please separate the LoRA and base models to 2 repos.""
+            )
+        elif not is_model and not is_peft:
+            raise RuntimeError(
+                f""Unsloth: `{model_name}` is not a base model or a PEFT model.\n""\
+                ""We could not locate a `config.json` or `adapter_config.json` file""
+            )
+        pass
+
+        # Get base model for PEFT:
+        if is_peft:
             # Check base model again for PEFT
             model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
-            model_config = AutoConfig.from_pretrained(model_name, token = token)
-            is_peft = True
+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
         pass
 
         model_type = model_config.model_type
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 9134d4a..82d177e 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -108,7 +108,7 @@ class FastLanguageModel(FastLlamaModel):
         # Cannot be both!
         if is_model and is_peft:
             raise RuntimeError(
-                ""Unsloth: You repo has a LoRA adapter and a base model.\n""\
+                ""Unsloth: Your repo has a LoRA adapter and a base model.\n""\
                 ""You have 2 files `config.json` and `adapter_config.json`.\n""\
                 ""We must only allow one config file.\n""\
                 ""Please separate the LoRA and base models to 2 repos.""
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index cec7332..31b3ab6 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -199,6 +199,17 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/gemma-2-27b"",
         ""google/gemma-2-27b"",
     ),
+    ""unsloth/gemma-2-9b-it-bnb-4bit"" : (
+        ""unsloth/gemma-2-9b-it"",
+        ""google/gemma-2-9b-it"",
+    ),
+    ""unsloth/gemma-2-27b-it-bnb-4bit"" : (
+        ""unsloth/gemma-2-27b-it"",
+        ""google/gemma-2-27b-it"",
+    ),
+    ""unsloth/Phi-3-mini-4k-instruct-v0-bnb-4bit"" : ( # Old Phi pre July
+        ""unsloth/Phi-3-mini-4k-instruct-v0"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
diff --git a/unsloth/save.py b/unsloth/save.py
index 9163c6d..1ceea3c 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -800,8 +800,8 @@ def install_llama_cpp_old(version = -10):
     # Check if successful
     if not os.path.exists(""llama.cpp/quantize"") and not os.path.exists(""llama.cpp/llama-quantize""):
         raise RuntimeError(
-            ""Unsloth: llama.cpp GGUF seems to be too buggy to install.\n""\
-            ""File a report to llama.cpp's main repo since this is not an Unsloth issue.""
+            ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
+            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
         )
     pass
 pass
@@ -945,9 +945,28 @@ def save_to_gguf(
         quantize_location = ""llama.cpp/quantize""
     elif os.path.exists(""llama.cpp/llama-quantize""):
         quantize_location = ""llama.cpp/llama-quantize""
+    else:
+        raise RuntimeError(
+            ""Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\n""\
+            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+        )
+    pass
+
+    # See https://github.com/unslothai/unsloth/pull/730
+    # Filenames changed again!
+    convert_location = None
+    if os.path.exists(""llama.cpp/convert-hf-to-gguf.py""):
+        convert_location = ""llama.cpp/convert-hf-to-gguf.py""
+    elif os.path.exists(""llama.cpp/convert_hf_to_gguf.py""):
+        convert_location = ""llama.cpp/convert_hf_to_gguf.py""
+    else:
+        raise RuntimeError(
+            ""Unsloth: The file 'llama.cpp/convert-hf-to-gguf.py' or 'llama.cpp/convert_hf_to_gguf.py' does not exist.\n""\
+            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+        )
     pass
 
-    if error != 0 or quantize_location is None:
+    if error != 0 or quantize_location is None or convert_location is None:
         print(f""Unsloth: llama.cpp error code = {error}."")
         install_llama_cpp_old(-10)
     pass
@@ -1035,7 +1054,7 @@ def save_to_gguf(
             f""--outfile {final_location} --vocab-type {vocab_type} ""\
             f""--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab""
     else:
-        command = f""python llama.cpp/convert-hf-to-gguf.py {model_directory} ""\
+        command = f""python {convert_location} {model_directory} ""\
             f""--outfile {final_location} ""\
             f""--outtype {first_conversion}""
     pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 8727ca0..b0bc514 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -376,7 +376,7 @@ def fix_sentencepiece_gguf(saved_location):
     """"""
         Fixes sentencepiece tokenizers which did not extend the vocabulary with
         user defined tokens.
-        Inspiration from https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py
+        Inspiration from https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py
     """"""
     from copy import deepcopy
     from transformers.utils import sentencepiece_model_pb2
"
"diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 0f682c6..a8ef9c0 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -164,6 +164,8 @@ class FastBaseVisionModel:
             padding_side = ""right"",
             token        = token,
         )
+        # Add padding side as well
+        tokenizer.tokenizer.padding_side = ""right""
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = post_patch_loss_function(model)
@@ -187,6 +189,7 @@ class FastBaseVisionModel:
 
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
+        tokenizer.tokenizer.padding_side = ""left"" # Force inference
         internal_model = model
         while hasattr(internal_model, ""model""):
             internal_model._saved_temp_tokenizer = tokenizer
@@ -315,12 +318,12 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""right""
+                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""right""
+            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
 
         # Clear deleted GPU items
@@ -361,12 +364,12 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""left""
+                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
             pass
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""left""
+            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""left""
         pass
 
         # Also disable training for embeddings for NEFTune
@@ -405,12 +408,12 @@ class FastBaseVisionModel:
         internal_model = model
         while hasattr(internal_model, ""model""):
             if hasattr(internal_model, ""_saved_temp_tokenizer""):
-                internal_model._saved_temp_tokenizer.padding_side = ""right""
+                internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
             internal_model = internal_model.model
         pass
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
-            internal_model._saved_temp_tokenizer.padding_side = ""right""
+            internal_model._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
 
         # Also re-enable training for embeddings for NEFTune
"
"diff --git a/pyproject.toml b/pyproject.toml
index 6f6f225..f8558a8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.8.5"",
+    ""unsloth_zoo>=2025.8.6"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -384,7 +384,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.8.5"",
+    ""unsloth_zoo>=2025.8.6"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 1c6a726..fae6ae0 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -571,8 +571,11 @@ class FastModel(FastBaseModel):
         elif ""qwen2.5"" in lowered_model_name and transformers_version < Version(""4.49.0""):
             raise RuntimeError(""Unsloth: Qwen 2.5 only works on transformers >= 4.49.0."" + LATEST)
         # Gemma 3
-        elif ""gemma-3"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
-            raise RuntimeError(""Unsloth: Gemma 3 only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""gemma-3"" in lowered_model_name:
+            if transformers_version < Version(""4.50.0.dev0""):
+                raise RuntimeError(""Unsloth: Gemma 3 only works on transformers >= 4.50.0."" + NIGHTLY)
+            # Set norms to float32 since anyways they get upcasted to float32
+            os.environ[""UNSLOTH_HIGH_PRECISION_LAYERNORM""] = ""1""
         # Cohere
         elif ""c4ai-command-a-03-2025"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Cohere's Command model only works on transformers >= 4.50.0."" + NIGHTLY)
@@ -582,31 +585,36 @@ class FastModel(FastBaseModel):
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Sesame fails
             os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
                 ""all;torch.float32;torch.float16;""\
-                ""if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16);""
+                ""if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""\
+                "";""
         # Granite 4
         elif 'granite-4' in lowered_model_name:
-            # granite-4 rms norms are stored as 16 bit, but we upcast
-            os.environ[""UNSLOTH_UPCAST_LAYERNORM""] = ""1""
+            # Granite-4 rms norms are stored as 16 bit, but we upcast
+            os.environ[""UNSLOTH_HIGH_PRECISION_LAYERNORM""] = ""1""
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
         # Olmo 2
         elif ""olmo-2"" in lowered_model_name and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: OLMo-2 only works on transformers >= 4.50.0."" + NIGHTLY)
         # Gemma 3N
         elif ""gemma-3n"" in lowered_model_name:
+            if transformers_version < Version(""4.53.0""):
+                raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
             os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
                 ""float16;torch.float16;torch.float16;""\
-                ""if name.endswith(('.conv')): module;""\
+                ""if name.endswith('norm'): ""\
+                ""module._pre_set_compute_dtype = torch.float32\n""\
+                "";""\
                 ""from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()""
-
-            if transformers_version < Version(""4.53.0""):
-                raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
+            # Set norms to float32 since anyways they get upcasted to float32
+            os.environ[""UNSLOTH_HIGH_PRECISION_LAYERNORM""] = ""1""
         elif ""falcon-h1"" in lowered_model_name:
             # Falcon must use float32 Triton ie TRITON_F32_DEFAULT = 'ieee'
             # since Mamba kernels error out on using lower precision
             os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
                 ""float16;torch.float32;torch.float16;""\
-                ""if name.endswith(('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'head')): module.to(torch.float16);""\
+                ""if name.endswith(('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'head')): module.to(torch.float16)""\
+                "";""\
                 ""os.environ['TRITON_F32_DEFAULT'] = 'ieee'""
         elif ""gpt-oss"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
@@ -615,23 +623,30 @@ class FastModel(FastBaseModel):
             os.environ[""UNSLOTH_ENABLE_CCE""] = ""0""
             if not load_in_4bit:
                 # Only upcast MoE biases for MXFP4, not BnB
+                # Set norms to float32 since anyways they get upcasted to float32
                 os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
                     ""all;None;None;""\
                     ""x = 'gate_up_proj_bias'\n""\
                     ""if hasattr(module, x): ""\
                     ""setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
+                    """"\
                     ""x = 'down_proj_bias'\n""\
                     ""if hasattr(module, x): ""\
                     ""setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
+                    """"\
                     "";""
             else:
                 # Set down projection compute dtype to be float32 for float16 machines
+                # Set norms to float32 since anyways they get upcasted to float32
                 os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
                     ""all;None;None;""\
-                    ""if 'down_projs' in name and hasattr(module, 'compute_dtype') and ""\
+                    ""if 'down_projs' in name and hasattr(module, 'weight') and ""\
                     ""torch.amax(dequantize_module_weight(module)) >= 1024:""\
                     ""module._pre_set_compute_dtype = torch.float32\n""\
+                    """"\
                     "";""
+            # Set norms to float32 since anyways they get upcasted to float32
+            os.environ[""UNSLOTH_HIGH_PRECISION_LAYERNORM""] = ""1""
         else:
             for check_model_name in DISABLE_COMPILE_MODEL_NAMES:
                 if check_model_name in lowered_model_name:
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index e751ef5..52b1e83 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -487,6 +487,8 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""logging_steps""                 : 1,
         ""max_seq_length""                : None,
         ""num_generations""               : 8,
+        # ""steps_per_generation""          : 1, # Otherwise defaults to ga_steps which is wrong
+        # ""generation_batch_size""         : None, # Useless. If steps_per_generation set, generation_batch_size clashes
         ""top_k""                         : None,
         ""vllm_mode""                     : ""colocate"",
         ""generation_kwargs""             : {},
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index a5de457..6790c5c 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -455,6 +455,12 @@ class FastBaseModel:
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
 
+        # Check float32 norm weights
+        if os.environ.get(""UNSLOTH_HIGH_PRECISION_LAYERNORM"", ""0"") == ""1"":
+            for jj, (name, module) in enumerate(model.named_modules()):
+                if name.endswith(""norm"") and hasattr(module, ""weight""):
+                    module._pre_set_compute_dtype = torch.float32
+        pass
         # Edit data-types
         if custom_datatype is not None:
             with torch.no_grad():
diff --git a/unsloth/save.py b/unsloth/save.py
index bfd45a3..9539b66 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -1195,6 +1195,41 @@ def save_to_gguf(
             f""--outfile {final_location} --vocab-type {vocab_type} ""\
             f""--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab""
     else:
+        # Fix up conversion script is possible
+        with open(convert_location, ""rb"") as f: converter_latest = f.read()
+        # Fix metadata
+        converter_latest = re.sub(
+            rb""(self\.metadata \= .+?\(.+?\)""\
+            rb""[\n]{1,}([\s]{4,}))"",
+            rb""\1""\
+            rb""if hasattr(self.metadata, 'quantized_by'): self.metadata.quantized_by = 'Unsloth'\n""\
+            rb""\2if hasattr(self.metadata, 'repo_url'): self.metadata.repo_url = 'https://huggingface.co/unsloth'\n""\
+            rb""\2if hasattr(self.metadata, 'tags'): self.metadata.tags = ['unsloth', 'llama.cpp']\n""\
+            rb""\2"",
+            converter_latest,
+        )
+
+        # Make mistral_common optional for now
+        # from x import y
+        converter_latest = re.sub(
+            rb""(from mistral_common[^\n\(]{1,})[\s]{0,}\n"",
+            rb""try:\n    \1\nexcept:\n    pass\n"",
+            converter_latest,
+        )
+        # from x import (y, z,)
+        converter_latest = re.sub(
+            rb""(from mistral_common[^\n\(]{1,}[\s]{0,}\(.+?\))"",
+            rb""try:\n    \1\nexcept:\n    pass\n"",
+            converter_latest,
+            flags = re.MULTILINE | re.DOTALL,
+        )
+
+        try:
+            # Write file
+            with open(convert_location, ""wb"") as file:
+                file.write(converter_latest)
+        except:
+            pass
         command = f""python {convert_location} {model_directory} ""\
             f""--outfile {final_location} ""\
             f""--outtype {first_conversion}""
@@ -1694,7 +1729,7 @@ def push_to_ollama_hub(username: str, model_name: str, tag: str):
         print(f""\nMODEL PUBLISHED FAILED WITH RETURN CODE {return_code}"")
     else:
         print(""\nMODEL PUBLISHED SUCCESSFULLY"")
-
+pass
 
 def push_to_ollama(
     tokenizer,
@@ -1726,9 +1761,7 @@ def push_to_ollama(
     )
 
     print(""Successfully pushed to ollama"")
-
-
-
+pass
 
 
 def unsloth_save_pretrained_gguf(
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 1c8f8c8..f2377d5 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -388,6 +388,14 @@ from transformers.models.llama.modeling_llama import (
     List,
     Tuple,
 )
+
+# Transformers 4.47 need Unpack, KwargsForCausalLM
+try:
+    from transformers.models.llama.modeling_llama import Unpack, KwargsForCausalLM
+except:
+    pass
+pass
+
 import inspect, re
 function = inspect.getsource(LlamaForCausalLM.forward)
 function = function.split(""\n"")
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index bf5216b..873a272 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -162,6 +162,20 @@ if hasattr(transformers.cache_utils, ""DynamicCache"") and \
 pass
 # =============================================
 
+# =============================================
+# Weird Databricks errors
+from transformers.utils import is_openai_available
+if is_openai_available():
+    try:
+        from openai import OpenAI
+    except:
+        print(""Unsloth: OpenAI failed to import - ignoring for now."")
+        import transformers.utils
+        def _is_openai_available(): return False
+        transformers.utils.is_openai_available = _is_openai_available
+    pass
+pass 
+
 # =============================================
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 import bitsandbytes as bnb
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 5bc9529..11423f9 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.1""
+__version__ = ""2024.12.2""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index b2f73aa..0ba03ce 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -452,33 +452,45 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Llama-3.1-Nemotron-70B-Instruct"",
         ""nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"",
     ),
-    ""unsloth/Qwen2-VL-2B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2-VL-2B-Instruct"",
         ""Qwen/Qwen2-VL-2B-Instruct"",
+        ""unsloth/Qwen2-VL-2B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Qwen2-VL-7B-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen2-VL-7B-Instruct"",
         ""Qwen/Qwen2-VL-7B-Instruct"",
+        ""unsloth/Qwen2-VL-7B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"" : (
+    ""unsloth/Qwen2-VL-72B-Instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-72B-Instruct"",
+        ""Qwen/Qwen2-VL-72B-Instruct"",
+        ""unsloth/Qwen2-VL-72B-Instruct-bnb-4bit"",
+    ),
+    ""unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-11B-Vision-Instruct"",
         ""meta-llama/Llama-3.2-11B-Vision-Instruct"",
+        ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-90B-Vision-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-90B-Vision-Instruct"",
         ""meta-llama/Llama-3.2-90B-Vision-Instruct"",
+        ""unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-11B-Vision-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-11B-Vision"",
         ""meta-llama/Llama-3.2-11B-Vision"",
+        ""unsloth/Llama-3.2-11B-Vision-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-90B-Vision-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-90B-Vision-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-90B-Vision"",
         ""meta-llama/Llama-3.2-90B-Vision"",
+        ""unsloth/Llama-3.2-90B-Vision-bnb-4bit"",
     ),
-    ""unsloth/Pixtral-12B-2409-bnb-4bit"" : (
+    ""unsloth/Pixtral-12B-2409-unsloth-bnb-4bit"" : (
         ""unsloth/Pixtral-12B-2409"",
         ""mistralai/Pixtral-12B-2409"",
+        ""unsloth/Pixtral-12B-2409-bnb-4bit"",
     ),
     ""unsloth/Pixtral-12B-2409-Base-bnb-4bit"" : (
         ""unsloth/Pixtral-12B-Base-2409"",
@@ -500,6 +512,10 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Llama-3.1-Tulu-3-70B"",
         ""allenai/Llama-3.1-Tulu-3-70B"",
     ),
+    ""unsloth/QwQ-32B-Preview-bnb-4bit"" : (
+        ""unsloth/QwQ-32B-Preview"",
+        ""Qwen/QwQ-32B-Preview"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
@@ -519,6 +535,14 @@ for key, values in __INT_TO_FLOAT_MAPPER.items():
             MAP_TO_UNSLOTH_16bit[values[1]] = values[0]
             MAP_TO_UNSLOTH_16bit[values[1].lower()] = values[0]
         pass
+    elif len(values) == 3:
+        # Dynamic Unsloth quantization
+        if values[0].startswith(""unsloth""):
+            MAP_TO_UNSLOTH_16bit[values[1]] = values[0]
+            MAP_TO_UNSLOTH_16bit[values[1].lower()] = values[0]
+            MAP_TO_UNSLOTH_16bit[values[2]] = values[0]
+            MAP_TO_UNSLOTH_16bit[values[2].lower()] = values[0]
+        pass
     pass
 
     # Get lowercased
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index aa4cc09..0f682c6 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -30,7 +30,7 @@ from transformers import set_seed as transformers_set_seed
 from unsloth_zoo.peft_utils import (
     get_peft_regex,
     merge_and_overwrite_lora,
-    # SKIP_QUANTIZATION_MODULES,
+    SKIP_QUANTIZATION_MODULES,
 )
 from triton import __version__ as triton_version
 
@@ -133,7 +133,7 @@ class FastBaseVisionModel:
                 bnb_4bit_use_double_quant = True,
                 bnb_4bit_quant_type       = ""nf4"",
                 bnb_4bit_compute_dtype    = dtype,
-                # llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
             )
         pass
 
"
"diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 302017d..384b4bb 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -585,26 +585,43 @@ def load_correct_tokenizer(
 pass
 
 
+def _find_end_position(template, endfor, endif):
+    where_endfor = template.find(endfor)
+    where_endif = template.find(endif)
+    if where_endfor == where_endif == -1:
+        return None
+    elif where_endfor > where_endif:
+        return endfor
+    else:
+        return endif
+    pass
+pass
+
+
 def _fix_chat_template(chat_template):
-    endfor = ""{% endif %}""
-    where = chat_template.find(endfor)
-    if where == -1:
-        endfor = ""{%- endif %}""
-        where = chat_template.find(endfor)
-    if where == -1:
+    endfor = ""{% endfor %}""
+    endif = ""{% endif %}""
+    chosen_end = _find_end_position(chat_template, endfor, endif)
+    if chosen_end is None:
+        endfor = ""{%- endfor %}""
+        endif = ""{%- endif %}""
+        chosen_end = _find_end_position(chat_template, endfor, endif)
+    if chosen_end is None:
         return chat_template
+    
+    where = chat_template.find(chosen_end)
 
-    after_endfor = chat_template[where + len(endfor):]
+    after_endfor = chat_template[where + len(chosen_end):]
 
-    dash = ""-"" if endfor.startswith(""{%-"") else """"
+    dash = ""-"" if chosen_end.startswith(""{%-"") else """"
 
     if ""{%"" + dash + "" if"" not in after_endfor and ""{%"" + dash + "" set "" not in after_endfor and \
         after_endfor.startswith(""{{"") and after_endfor.endswith(""}}"") and \
         after_endfor.count(""{{"") == 1 and after_endfor.count(""}}"") == 1:
 
-        after_endfor = ""{%"" + dash + "" if add_generation_prompt %}"" + after_endfor + endfor
+        after_endfor = ""{%"" + dash + "" if add_generation_prompt %}"" + after_endfor + endif
 
-        chat_template = chat_template[:where + len(endfor)] + after_endfor
+        chat_template = chat_template[:where + len(chosen_end)] + after_endfor
     pass
     return chat_template
 pass
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 7000739..0bb8c4a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1663,6 +1663,10 @@ class FastLlamaModel:
             if platform.system().lower() == 'windows':
                 print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
                 fast_inference = False
+            major_version, minor_version = torch.cuda.get_device_capability()
+            if major_version < 7:
+                print(""Unsloth: vLLM does not work on older GPUs - will switch to Unsloth inference!"")
+                fast_inference = False
         pass
 
         if token is None: token = get_token()
@@ -1786,6 +1790,8 @@ class FastLlamaModel:
                 attn_implementation     = ""eager"",
                 **kwargs,
             )
+            model.fast_generate = model.generate
+            model.fast_generate_batches = None
         else:
             from unsloth_zoo.vllm_utils import (
                 load_vllm,
@@ -1804,6 +1810,7 @@ class FastLlamaModel:
                 enable_lora            = True,
                 max_lora_rank          = max_lora_rank,
                 disable_log_stats      = disable_log_stats,
+                use_bitsandbytes       = load_in_4bit,
             )
             for allowed_arg in allowed_args:
                 if allowed_arg not in load_vllm_kwargs and allowed_arg in kwargs:
@@ -2651,6 +2658,19 @@ class FastLlamaModel:
             torch.cuda.empty_cache()
         pass
 
+        # Patch for fast inference
+        vllm_engine = getattr(model, ""vllm_engine"")
+        if vllm_engine is not None:
+            model.vllm_engine = vllm_engine
+            model.fast_generate = vllm_fast_generate
+            model.fast_generate_batches = vllm_fast_generate_batches
+
+            # Also saving and loading LoRA
+            from unsloth_zoo.vllm_utils import save_lora, load_lora
+            model.save_lora = functools.partial(save_lora, model)
+            model.load_lora = functools.partial(load_lora, model)
+        pass
+
         # Add for_inference and for_training
         model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
         model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 1b54c8c..ae9e9df 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -405,7 +405,6 @@ class FastLanguageModel(FastLlamaModel):
         if is_peft:
             # From https://github.com/huggingface/peft/issues/184
             # Now add PEFT adapters
-            model.enable_input_require_grads()
             model = PeftModel.from_pretrained(
                 model,
                 old_model_name,
@@ -668,7 +667,7 @@ class FastModel(FastBaseModel):
             use_gradient_checkpointing = use_gradient_checkpointing,
             *args, **kwargs,
         )
-        
+
         if resize_model_vocab is not None:
             model.resize_token_embeddings(resize_model_vocab)
         pass
@@ -703,7 +702,6 @@ class FastModel(FastBaseModel):
         if is_peft:
             # From https://github.com/huggingface/peft/issues/184
             # Now add PEFT adapters
-            model.enable_input_require_grads()
             model = PeftModel.from_pretrained(
                 model,
                 old_model_name,
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 2ef9d2e..f0d5a09 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -68,6 +68,9 @@ __all__ = [
 global FORCE_FLOAT32
 FORCE_FLOAT32 = [""gemma3""]
 
+global FORCE_EAGER_ATTENTION
+FORCE_EAGER_ATTENTION = [""pixtral""]
+
 
 def unsloth_base_fast_generate(
     self,
@@ -193,6 +196,15 @@ class FastBaseModel:
                 break
         pass
 
+        global FORCE_EAGER_ATTENTION
+        attn_implementation = ""sdpa""
+        for disable_sdpa_name in FORCE_EAGER_ATTENTION:
+            if disable_sdpa_name.lower() == model_type_arch.lower():
+                print(f""Unsloth: {model_type_arch} does not support SDPA - switching to eager!"")
+                attn_implementation = ""eager""
+                break
+        pass
+
         bnb_config = None
         if full_finetuning and (load_in_4bit or load_in_8bit):
             print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
@@ -249,7 +261,7 @@ class FastBaseModel:
             # quantization_config   = bnb_config,
             token                   = token,
             trust_remote_code       = trust_remote_code,
-            attn_implementation     = ""sdpa"", #[TODO] Pixtral for eg fails
+            attn_implementation     = attn_implementation,
             **kwargs,
         )
         # Return old flag
"
"diff --git a/README.md b/README.md
index 45b7525..8a6c970 100644
--- a/README.md
+++ b/README.md
@@ -29,7 +29,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 | **CodeLlama 34b** A100   | [ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |
 | **Mistral 7b** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\* | 62% less |
 
-- This [conversational notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for ShareGPT ChatML datatsets.
+- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
 - Our [raw text notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for text completion.
 - Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.
 - \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.
diff --git a/pyproject.toml b/pyproject.toml
index 5bbcc35..2fa2cb1 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -41,6 +41,7 @@ huggingface = [
     ""peft>=0.7.1"",
     ""tqdm"",
     ""psutil"",
+    ""wheel>=0.42.0"",
 ]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 66c10e2..d052b33 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -82,3 +82,4 @@ pass
 
 from .models import *
 from .save import *
+from .chat_templates import *
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
new file mode 100644
index 0000000..eb61056
--- /dev/null
+++ b/unsloth/chat_templates.py
@@ -0,0 +1,384 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+__all__ = [
+    ""get_chat_template"",
+    ""test_chat_templates"",
+]
+
+from transformers import StoppingCriteria, StoppingCriteriaList
+from torch import LongTensor, FloatTensor
+from transformers.models.llama.modeling_llama import logger
+from .models._utils import patch_tokenizer
+
+CHAT_TEMPLATES = {}
+
+# Unsloth efficient template leverages from Zephyr
+unsloth_template = \
+    ""{{ bos_token }}""\
+    ""{% if messages[0]['role'] == 'system' %}""\
+        ""{{ messages[0]['content'] + '\n' }}""\
+        ""{% set loop_messages = messages[1:] %}""\
+    ""{% else %}""\
+        ""{{ 'You are a helpful assistant to the user\n' }}""\
+        ""{% set loop_messages = messages %}""\
+    ""{% endif %}""\
+    ""{% for message in loop_messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ '>>> User: ' + message['content'] + '\n' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ '>>> Assistant: ' + message['content'] + eos_token + '\n' }}""\
+        ""{% else %}""\
+            ""{{ raise_exception('Only user and assistant roles are supported!') }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '>>> Assistant: ' }}""\
+    ""{% endif %}""
+unsloth_eos_token = ""eos_token""
+CHAT_TEMPLATES[""unsloth""] = (unsloth_template, unsloth_eos_token,)
+
+
+# Zephyr has no BOS!
+zephyr_template = \
+    ""{% for message in messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ '<|user|>\n' + message['content'] + eos_token + '\n' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ '<|assistant|>\n' + message['content'] + eos_token + '\n' }}""\
+        ""{% else %}""\
+            ""{{ '<|system|>\n' + message['content'] + eos_token + '\n' }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '<|assistant|>\n' }}""\
+    ""{% endif %}""
+zephyr_eos_token = ""eos_token""
+CHAT_TEMPLATES[""zephyr""] = (zephyr_template, zephyr_eos_token,)
+
+
+# ChatML has no BOS and not EOS! Rather <|im_start|> and <|im_end|> acts as BOS / EOS.
+chatml_template = \
+    ""{% for message in messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{'<|im_start|>user\n' + message['content'] + '<|im_end|>\n'}}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{'<|im_start|>assistant\n' + message['content'] + '<|im_end|>\n' }}""\
+        ""{% else %}""\
+            ""{{ '<|im_start|>system\n' + message['content'] + '<|im_end|>\n' }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '<|im_start|>assistant\n' }}""\
+    ""{% endif %}""
+chatml_eos_token = ""<|im_end|>""
+CHAT_TEMPLATES[""chatml""] = (chatml_template, chatml_eos_token,)
+
+
+# Mistral Instruct doesn't allow system prompts, so we append it to the user message.
+mistral_template = \
+    ""{{ bos_token }}""\
+    ""{% if messages[0]['role'] == 'system' %}""\
+        ""{% if messages[1]['role'] == 'user' %}""\
+            ""{{ '[INST] ' + messages[0]['content'] + ' ' + messages[1]['content'] + ' [/INST]' }}""\
+            ""{% set loop_messages = messages[2:] %}""\
+        ""{% else %}""\
+            ""{{ '[INST] ' + messages[0]['content'] + ' [/INST]' }}""\
+            ""{% set loop_messages = messages[1:] %}""\
+        ""{% endif %}""\
+    ""{% else %}""\
+        ""{% set loop_messages = messages %}""\
+    ""{% endif %}""\
+    ""{% for message in loop_messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ '[INST] ' + message['content'] + ' [/INST]' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ message['content'] + eos_token }}""\
+        ""{% else %}""\
+            ""{{ raise_exception('Only user and assistant roles are supported!') }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""
+mistral_eos_token = ""eos_token""
+CHAT_TEMPLATES[""mistral""] = (mistral_template, mistral_eos_token,)
+
+
+# Adds BOS to every convo! And weird <<SYS>> system messages.
+llama_template = \
+    ""{% if messages[0]['role'] == 'system' %}""\
+        ""{% if messages[1]['role'] == 'user' %}""\
+            ""{{ bos_token + '[INST] <<SYS>>\n' + messages[0]['content'] + '\n<</SYS>>\n\n' + messages[1]['content'] + ' [/INST]' }}""\
+            ""{% set loop_messages = messages[2:] %}""\
+        ""{% else %}""\
+            ""{{ bos_token + '[INST] ' + messages[0]['content'] + ' [/INST]' }}""\
+            ""{% set loop_messages = messages[1:] %}""\
+        ""{% endif %}""\
+    ""{% else %}""\
+        ""{% set loop_messages = messages %}""\
+    ""{% endif %}""\
+    ""{% for message in loop_messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ ' ' + message['content'].strip() + ' ' + eos_token }}""\
+        ""{% else %}""\
+            ""{{ raise_exception('Only user and assistant roles are supported!') }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""
+llama_eos_token = ""eos_token""
+CHAT_TEMPLATES[""llama""] = (llama_template, llama_eos_token,)
+
+
+# https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template
+vicuna_template = \
+    ""{{ bos_token }}""\
+    ""{% if messages[0]['role'] == 'system' %}""\
+        ""{{ messages[0]['content'] + ' ' }}""\
+        ""{% set loop_messages = messages[1:] %}""\
+    ""{% else %}""\
+        ""{{ 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions.' + ' ' }}""\
+        ""{% set loop_messages = messages %}""\
+    ""{% endif %}""\
+    ""{% for message in loop_messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ 'USER: ' + message['content'] + ' ' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ 'ASSISTANT: ' + message['content'] + eos_token }}""\
+        ""{% else %}""\
+            ""{{ raise_exception('Only user and assistant roles are supported!') }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ 'ASSISTANT:' }}""\
+    ""{% endif %}""
+vicuna_eos_token = ""eos_token""
+CHAT_TEMPLATES[""vicuna""] = (vicuna_template, vicuna_eos_token,)
+
+
+# https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template
+vicuna_old_template = \
+    ""{{ bos_token }}""\
+    ""{% if messages[0]['role'] == 'system' %}""\
+        ""{{ messages[0]['content'] + '\n' }}""\
+        ""{% set loop_messages = messages[1:] %}""\
+    ""{% else %}""\
+        ""{{ 'A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\\'s questions.' + '\n' }}""\
+        ""{% set loop_messages = messages %}""\
+    ""{% endif %}""\
+    ""{% for message in loop_messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ '### Human: ' + message['content'] + '\n' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ '### Assistant: ' + message['content'] + eos_token + '\n' }}""\
+        ""{% else %}""\
+            ""{{ raise_exception('Only user and assistant roles are supported!') }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '### Assistant:' }}""\
+    ""{% endif %}""
+vicuna_old_eos_token = ""eos_token""
+CHAT_TEMPLATES[""vicuna_old""] = (vicuna_old_template, vicuna_old_eos_token,)
+
+
+# https://github.com/tatsu-lab/stanford_alpaca Changed for multi-turn convos
+alpaca_template = \
+    ""{{ bos_token }}""\
+    ""{% if messages[0]['role'] == 'system' %}""\
+        ""{{ messages[0]['content'] + '\n\n' }}""\
+        ""{% set loop_messages = messages[1:] %}""\
+    ""{% else %}""\
+        ""{{ 'Below are some instructions that describes some tasks. Write responses that appropriately completes each request.\n\n' }}""\
+        ""{% set loop_messages = messages %}""\
+    ""{% endif %}""\
+    ""{% for message in loop_messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{ '### Instruction:\n' + message['content'] + '\n\n' }}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{ '### Response:\n' + message['content'] + eos_token + '\n\n' }}""\
+        ""{% else %}""\
+            ""{{ raise_exception('Only user and assistant roles are supported!') }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '### Response:\n' }}""\
+    ""{% endif %}""
+alpaca_eos_token = ""eos_token""
+CHAT_TEMPLATES[""alpaca""] = (alpaca_template, alpaca_eos_token,)
+
+
+def get_chat_template(
+    tokenizer,
+    chat_template = ""chatml"",
+    mapping = {""role"" : ""role"", ""content"" : ""content"", ""user"" : ""user"", ""assistant"" : ""assistant""},
+    map_eos_token = True,
+):
+    if map_eos_token is False:
+        assert(""Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported."")
+    pass
+
+    old_padding_side = tokenizer.padding_side
+
+    if type(chat_template) in (list, tuple):
+        chat_template, stop_word = chat_template
+        assert(type(chat_template) is str)
+        assert(type(stop_word) is str)
+
+    elif type(chat_template) is str:
+
+        chat_template, stop_word = CHAT_TEMPLATES[chat_template]
+
+        if stop_word != ""eos_token"":
+            logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
+
+            # Replaces the old EOS token with a new one.
+            # Useful for ChatML <|im_end|> for example.
+            # Usually we train 2 more tokens <|im_start|> and <|im_end|>
+            # But training the lm_head and embeddings are slow!
+            # This is a HACK!
+            # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser
+            string_vocab = tokenizer._tokenizer.to_str()
+            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
+            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
+            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+        pass
+    else:
+        raise TypeError(
+            f""Unsloth: `chat_template` must be a tuple of (your_template, eos_token,) or one of\n""\
+            f""{CHAT_TEMPLATES.keys()}""
+        )
+    pass
+
+    # For ShareGPT role -> from and content -> value
+    chat_template = chat_template\
+        .replace(""'role'"",      ""'"" + mapping[""role""]      + ""'"")\
+        .replace(""'content'"",   ""'"" + mapping[""content""]   + ""'"")\
+        .replace(""'user'"",      ""'"" + mapping[""user""]      + ""'"")\
+        .replace(""'assistant'"", ""'"" + mapping[""assistant""] + ""'"")
+
+    _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)
+    tokenizer.padding_side  = old_padding_side
+    tokenizer.chat_template = chat_template
+
+    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)
+
+    return tokenizer#, stopping_criteria
+pass
+
+
+def create_stopping_criteria(tokenizer, stop_word = ""eos_token""):
+    class StoppingCriteriaSub(StoppingCriteria):
+        __slots__ = ""stop_token"", ""single_match"", ""length"",
+
+        def __init__(self, stops = ""eos_token"", device = ""cuda"", encounters = 1):
+            super().__init__()
+            if stops == ""eos_token"":
+                self.stop_token = torch.tensor(tokenizer.eos_token_id, device = ""cuda"")
+                self.length = 1
+            else:
+                self.stop_token = tokenizer([""\n"" + stops], add_special_tokens = False, return_tensors = ""pt"")
+                self.stop_token = self.stop_token.input_ids.ravel()[1:].to(""cuda"")
+                self.length = self.stop_token.shape[0]
+            pass
+            self.single_match = self.length == 1
+        pass
+
+        def __call__(self, input_ids: LongTensor, scores: FloatTensor) -> bool:
+            input_ids = input_ids.ravel()
+            last_token = input_ids[-1]
+            if self.single_match and (last_token == self.stop_token): return True
+
+            if input_ids.shape[0] >= self.length and \
+                (input_ids[-self.length:] == self.stop_token).all(): return True
+            return False
+        pass
+    pass
+    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = stop_word)])
+    return stopping_criteria
+pass
+
+
+def test_chat_templates():
+    messages = [
+        {""role"": ""system"",""content"": "" You are a friendly chatbot."",},
+        {""role"": ""user"", ""content"": ""What is 2+2?""},
+        {""role"": ""assistant"", ""content"": ""It's 4.""},
+        {""role"": ""user"", ""content"": ""  But 2+2 is equal to 5. ""},
+        {""role"": ""assistant"", ""content"": ""No I'm sure its 4.""},
+        {""role"": ""user"", ""content"": ""  No it's 100% 5! ""},
+    ]
+
+    from transformers import AutoTokenizer
+    template = zephyr_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""HuggingFaceH4/zephyr-7b-beta"")
+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    assert(correct_prompt == our_prompt)
+
+    template = chatml_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""teknium/OpenHermes-2.5-Mistral-7B"")
+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    assert(correct_prompt == our_prompt)
+
+    template = mistral_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"")
+    correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
+    assert(correct_prompt == our_prompt)
+
+    template = llama_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""unsloth/llama-2-7b-chat"")
+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    assert(correct_prompt == our_prompt)
+
+    try:
+        from fastchat.conversation import get_conv_template
+    except:
+        os.system(""pip -qqq install git+https://github.com/lm-sys/FastChat.git"")
+        from fastchat.conversation import get_conv_template
+    correct_prompt = get_conv_template(""vicuna_v1.1"")
+    for j in range(len(messages)-1):
+        correct_prompt.append_message(correct_prompt.roles[j%2==1], messages[j+1][""content""])
+    correct_prompt.append_message(correct_prompt.roles[1], """")
+    correct_prompt = tokenizer.bos_token + correct_prompt.get_prompt()
+
+    template = vicuna_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""lmsys/vicuna-7b-v1.5"")
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
+    assert(correct_prompt == our_prompt)
+
+    try:
+        from fastchat.conversation import get_conv_template
+    except:
+        os.system(""pip -qqq install git+https://github.com/lm-sys/FastChat.git"")
+        from fastchat.conversation import get_conv_template
+    correct_prompt = get_conv_template(""zero_shot"")
+    for j in range(len(messages)-1):
+        correct_prompt.append_message(correct_prompt.roles[j%2==1], messages[j+1][""content""])
+    correct_prompt.append_message(correct_prompt.roles[1], """")
+    correct_prompt = tokenizer.bos_token + correct_prompt.get_prompt()
+
+    template = vicuna_old_template
+    correct_tokenizer = AutoTokenizer.from_pretrained(""lmsys/vicuna-7b-v1.5"")
+    correct_tokenizer.chat_template = template
+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
+    # We add </s> ourselves
+    assert(correct_prompt == our_prompt.replace(""</s>"", """"))
+pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 617b850..c21da4e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -16,6 +16,7 @@ import torch
 from typing import Union, Optional, List, Any, Callable
 import warnings
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""huggingface_hub"")
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
@@ -116,21 +117,24 @@ pass
 
 
 def patch_tokenizer(model, tokenizer):
-    model.config.update({""unsloth_version"" : __version__})
+    if model is not None:
+        model.config.update({""unsloth_version"" : __version__})
     if not hasattr(tokenizer, ""pad_token"") or tokenizer.pad_token is None:
         # Fixes https://github.com/unslothai/unsloth/issues/5
         if hasattr(tokenizer, ""unk_token""):
             tokenizer.add_special_tokens({""pad_token"" : tokenizer.unk_token})
             tokenizer.pad_token = tokenizer.unk_token
         else:
+            name = model.config._name_or_path if model is not None else ""Model""
             logger.warning_one(
-                f""{model.config._name_or_path} does not have a padding or unknown token!\n""\
+                f""{name} does not have a padding or unknown token!\n""\
                 f""Will use the EOS token of id {tokenizer.eos_token_id} as padding.""
             )
             assert(hasattr(tokenizer, ""eos_token""))
             tokenizer.add_special_tokens({""pad_token"" : tokenizer.eos_token})
             tokenizer.pad_token = tokenizer.eos_token
-        config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
+        if model is not None:
+            config = model.config.update({""pad_token_id"" : tokenizer.eos_token_id})
     pass
     return model, tokenizer
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index d35a35c..8a37be1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -540,7 +540,7 @@ def LlamaModel_fast_forward(
 
     hidden_states = inputs_embeds
 
-    if past_key_values is None and self.gradient_checkpointing and self.training:
+    if past_key_values is None and self.training:
         use_cache = False
         # if use_cache:
         #     logger.warning_once(
@@ -776,6 +776,73 @@ def PeftModelForCausalLM_fast_forward(
 pass
 
 
+# Solves https://github.com/unslothai/unsloth/issues/168
+# Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+# Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+# https://github.com/huggingface/transformers/pull/27931
+# https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+class LlamaRotaryEmbedding(torch.nn.Module):
+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
+        super().__init__()
+
+        self.dim = dim
+        self.max_position_embeddings = max_position_embeddings
+        self.base = base
+        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))
+        self.register_buffer(""inv_freq"", inv_freq, persistent=False)
+
+        # Build here to make `torch.jit.trace` work.
+        self._set_cos_sin_cache(
+            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
+        )
+    pass
+
+    def _set_cos_sin_cache(self, seq_len, device, dtype):
+        self.max_seq_len_cached = seq_len
+        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
+
+        freqs = torch.outer(t, self.inv_freq)
+        # Different from paper, but it uses a different permutation in order to obtain the same calculation
+        emb = torch.cat((freqs, freqs), dim=-1)
+        self.register_buffer(""cos_cached"", emb.cos().to(dtype), persistent=False)
+        self.register_buffer(""sin_cached"", emb.sin().to(dtype), persistent=False)
+    pass
+
+    def forward(self, x, seq_len=None):
+        # x: [bs, num_attention_heads, seq_len, head_size]
+        if seq_len > self.max_seq_len_cached:
+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
+
+        return (
+            self.cos_cached[:seq_len].to(dtype=x.dtype),
+            self.sin_cached[:seq_len].to(dtype=x.dtype),
+        )
+    pass
+pass
+
+
+class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
+    """"""LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev""""""
+
+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):
+        self.scaling_factor = scaling_factor
+        super().__init__(dim, max_position_embeddings, base, device)
+    pass
+
+    def _set_cos_sin_cache(self, seq_len, device, dtype):
+        self.max_seq_len_cached = seq_len
+        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
+        t = t / self.scaling_factor
+
+        freqs = torch.outer(t, self.inv_freq)
+        # Different from paper, but it uses a different permutation in order to obtain the same calculation
+        emb = torch.cat((freqs, freqs), dim=-1)
+        self.register_buffer(""cos_cached"", emb.cos().to(dtype), persistent=False)
+        self.register_buffer(""sin_cached"", emb.sin().to(dtype), persistent=False)
+    pass
+pass
+
+
 class FastLlamaModel:
 
     @staticmethod
@@ -787,6 +854,15 @@ class FastLlamaModel:
         LlamaModel          .forward = LlamaModel_fast_forward
         LlamaForCausalLM    .forward = LlamaForCausalLM_fast_forward
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+
+        # Solves https://github.com/unslothai/unsloth/issues/168
+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+        # https://github.com/huggingface/transformers/pull/27931
+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+        import transformers.models.llama.modeling_llama
+        transformers.models.llama.modeling_llama.LlamaRotaryEmbedding = LlamaRotaryEmbedding
+        transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding = LlamaLinearScalingRotaryEmbedding
         return
     pass
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index bc00e7a..615e436 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -271,6 +271,14 @@ class FastMistralModel(FastLlamaModel):
         MistralModel          .forward = LlamaModel_fast_forward
         MistralForCausalLM    .forward = MistralForCausalLM_fast_forward
         PeftModelForCausalLM  .forward = PeftModelForCausalLM_fast_forward
+
+        # Solves https://github.com/unslothai/unsloth/issues/168
+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+        # https://github.com/huggingface/transformers/pull/27931
+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+        import transformers.models.mistral.modeling_mistral
+        transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding = LlamaRotaryEmbedding
         return
     pass
 
diff --git a/unsloth/save.py b/unsloth/save.py
index b45ae99..dc83f27 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -41,6 +41,7 @@ LLAMA_LAYERNORMS = (
     ""input_layernorm"", ""post_attention_layernorm"",
 )
 
+# https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp#L19
 # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html
 ALLOWED_QUANTS = \
 {
@@ -59,10 +60,16 @@ ALLOWED_QUANTS = \
     ""q4_0""    : ""Original quant method, 4-bit."",
     ""q4_1""    : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
     ""q4_k_s""  : ""Uses Q4_K for all tensors"",
+    ""q4_k""    : ""alias for q4_k_m"",
+    ""q5_k""    : ""alias for q5_k_m"",
     ""q5_0""    : ""Higher accuracy, higher resource usage and slower inference."",
     ""q5_1""    : ""Even higher accuracy, resource usage and slower inference."",
     ""q5_k_s""  : ""Uses Q5_K for all tensors"",
     ""q6_k""    : ""Uses Q8_K for all tensors"",
+    ""iq2_xxs"" : ""2.06 bpw quantization"",
+    ""iq2_xs""  : ""2.31 bpw quantization"",
+    ""iq3_xxs"" : ""3.06 bpw quantization"",
+    ""q3_k_xs"" : ""3-bit extra small quantization"",
 }
 
 def print_quantization_methods():
@@ -246,7 +253,8 @@ def unsloth_save_model(
     # If push_to_hub, we must remove the .../ part of a repo
     if push_to_hub and ""/"" in save_directory:
 
-        new_save_directory = save_directory[save_directory.find(""/""):]
+        # +1 solves absolute path issues
+        new_save_directory = save_directory[save_directory.find(""/"")+1:]
 
         logger.warning_once(
             f""Unsloth: You are pushing to hub, but you passed your HF username.\n""\
@@ -861,10 +869,16 @@ def unsloth_save_pretrained_gguf(
         ""q4_0""    : ""Original quant method, 4-bit."",
         ""q4_1""    : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
         ""q4_k_s""  : ""Uses Q4_K for all tensors"",
+        ""q4_k""    : ""alias for q4_k_m"",
+        ""q5_k""    : ""alias for q5_k_m"",
         ""q5_0""    : ""Higher accuracy, higher resource usage and slower inference."",
         ""q5_1""    : ""Even higher accuracy, resource usage and slower inference."",
         ""q5_k_s""  : ""Uses Q5_K for all tensors"",
         ""q6_k""    : ""Uses Q8_K for all tensors"",
+        ""iq2_xxs"" : ""2.06 bpw quantization"",
+        ""iq2_xs""  : ""2.31 bpw quantization"",
+        ""iq3_xxs"" : ""3.06 bpw quantization"",
+        ""q3_k_xs"" : ""3-bit extra small quantization"",
     """"""
     if tokenizer is None:
         raise ValueError(""Unsloth: Saving to GGUF must have a tokenizer."")
"
"diff --git a/.github/ISSUE_TEMPLATE/bug---issue.md b/.github/ISSUE_TEMPLATE/bug---issue.md
index a08a6cf..ff508cf 100644
--- a/.github/ISSUE_TEMPLATE/bug---issue.md
+++ b/.github/ISSUE_TEMPLATE/bug---issue.md
@@ -15,5 +15,5 @@ assignees: ''
 6. Which trainer? `SFTTrainer`, `GRPOTrainer` etc
 7. **Minimal code to reproduce error Remove Hugging Face token!**
 
-For quick replies, got to https://discord.com/invite/unsloth.
-Have you tried https://docs.unsloth.ai/basics/errors-troubleshooting
+You can also join our Discord: https://discord.com/invite/unsloth
+Have you tried visiting our Docs? https://docs.unsloth.ai/basics/errors-troubleshooting
"
"diff --git a/pyproject.toml b/pyproject.toml
index 385366a..05c4191 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -43,6 +43,7 @@ huggingface = [
     ""psutil"",
     ""wheel>=0.42.0"",
     ""numpy"",
+    ""triton"",
 ]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
@@ -104,6 +105,16 @@ cu121-torch211 = [
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch211]"",
 ]
+cu118-torch212 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118onlytorch212]"",
+]
+cu121-torch212 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121onlytorch212]"",
+]
 cu118-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 9dce29c..7080c92 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -99,10 +99,14 @@ except:
         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32
         libcuda_dirs()
     except:
-        raise ImportError(""Unsloth: CUDA is not linked properly.\n""\
-                          ""We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.\n""\
-                          ""You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\n""\
-                          ""Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version."")
+        warnings.warn(
+            ""Unsloth: CUDA is not linked properly.\n""\
+            ""Try running `python -m bitsandbytes` then `python -m xformers.info`\n""\
+            ""We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.\n""\
+            ""You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\n""\
+            ""Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\n""\
+            ""Unsloth will still run for now, but maybe it might crash - let's hope it works!""
+        )
 pass
 
 from .models import *
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 5ad34d6..9675b10 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -257,6 +257,10 @@ def get_chat_template(
         assert(""Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported."")
     pass
 
+    if tokenizer.__class__.__name__.startswith(""Gemma"") and chat_template == ""chatml"":
+        chat_template = ""gemma_chatml""
+    pass
+
     old_padding_side = tokenizer.padding_side
 
     if type(chat_template) in (list, tuple,):
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index ccd9f89..4db89b7 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -53,6 +53,7 @@ def _rms_layernorm_forward(
 pass
 
 
+@triton.heuristics({""GEMMA"": lambda args: args[""GEMMA""],})
 @triton.jit
 def _rms_layernorm_backward(
     dY, dY_row_stride,
@@ -61,6 +62,7 @@ def _rms_layernorm_backward(
     r,   r_row_stride,
     dW, dW_row_stride,
     n_cols, eps,
+    GEMMA      : tl.constexpr,
     BLOCK_SIZE : tl.constexpr,
 ):
     """"""
@@ -84,16 +86,51 @@ def _rms_layernorm_backward(
     inv_var = tl.load(r).to(tl.float32)
     normed = X_row * inv_var
 
-    dY_W = dY_row * W_row
+    if GEMMA: dY_W = dY_row * (W_row + 1.0)
+    else:     dY_W = dY_row * W_row
+
     rowsum_dY_normed = tl.sum(dY_W * normed, axis = 0)
     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)
     tl.store(dY + col_offsets, output, mask = mask)
 pass
 
 
+@triton.jit
+def _gemma_rms_layernorm_forward(
+    Y, Y_row_stride,
+    X, X_row_stride,
+    W, W_row_stride,
+    r, r_row_stride,
+    n_cols, eps,
+    BLOCK_SIZE : tl.constexpr,
+):
+    # Copies https://github.com/google-deepmind/gemma/blob/main/gemma/layers.py#L31
+    # and https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L33
+    # exactly. Essentially all in float32!
+    row_idx = tl.program_id(0)
+    col_offsets = tl.arange(0, BLOCK_SIZE)
+    mask = col_offsets < n_cols
+
+    Y += row_idx * Y_row_stride
+    X += row_idx * X_row_stride
+    r += row_idx * r_row_stride
+
+    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)
+    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)
+
+    row_var = tl.sum(X_row * X_row, axis = 0) / n_cols
+    inv_var = 1.0 / tl.sqrt(row_var + eps) # Must be 1/sqrt to match Deepmind's impl
+    tl.store(r, inv_var)
+    normed = X_row * inv_var
+    output = normed * (W_row + 1.0)
+
+    tl.store(Y + col_offsets, output, mask = mask)
+pass
+
+
 class Fast_RMS_Layernorm(torch.autograd.Function):
     @staticmethod
-    def forward(ctx, X, W, eps):
+    def forward(ctx, X, W, eps, gemma = False):
         shape = X.shape
         dim = shape[-1]
         X = X.view(-1, dim)
@@ -103,7 +140,8 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = ""cuda"")
         r = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
 
-        _rms_layernorm_forward[(n_rows,)](
+        fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward
+        fx[(n_rows,)](
             Y, Y.stride(0),
             X, X.stride(0),
             W, W.stride(0),
@@ -115,6 +153,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
         ctx.eps = eps
         ctx.BLOCK_SIZE = BLOCK_SIZE
         ctx.num_warps  = num_warps
+        ctx.GEMMA = gemma
         ctx.save_for_backward(X, W, r)
         return Y.view(*shape)
     pass
@@ -135,18 +174,19 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
             r,  r .stride(0),
             dW, dW.stride(0),
             n_cols, ctx.eps,
+            GEMMA      = ctx.GEMMA,
             BLOCK_SIZE = ctx.BLOCK_SIZE,
             num_warps  = ctx.num_warps,
         )
         dX = dY.view(*shape)
-        return dX, None, None
+        return dX, None, None, None
     pass
 pass
 
 
-def fast_rms_layernorm(layernorm, X):
+def fast_rms_layernorm(layernorm, X, gemma = False):
     W   = layernorm.weight
     eps = layernorm.variance_epsilon
-    out = Fast_RMS_Layernorm.apply(X, W, eps)
+    out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)
     return out
 pass
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index a952752..c116739 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -39,24 +39,28 @@ def _rope_embedding(
     half_head_dim = head_dim // 2
     mask = col_offsets < half_head_dim
 
-    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \
-                   half_head_dim*0 + col_offsets, mask = mask, other = 0)
-    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \
-                   half_head_dim*1 + col_offsets, mask = mask, other = 0)
     sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \
                    half_head_dim*0 + col_offsets, mask = mask, other = 0)
     cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \
                    half_head_dim*0 + col_offsets, mask = mask, other = 0)
 
+    # For Gemma - sometimes RoPE must be done in float32 and not bfloat16
+    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \
+                   half_head_dim*0 + col_offsets, mask = mask, other = 0).to(sin1.dtype)
+    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \
+                   half_head_dim*1 + col_offsets, mask = mask, other = 0).to(sin1.dtype)
+
     if BACKWARD_PASS:
         # See our blog post for more info.
         sin1 = -sin1
     pass
 
     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \
-             half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)
+             half_head_dim*0 + col_offsets,
+             Q1*cos1 - Q2*sin1, mask = mask)
     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \
-             half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)
+             half_head_dim*1 + col_offsets,
+             Q2*cos1 + Q1*sin1, mask = mask)
 pass
 
 
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 99bd9e7..bcd0e1a 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -39,6 +39,7 @@ except:
 pass
 
 
+torch_nn_functional_gelu = torch.nn.functional.gelu
 def fast_geglu_inference(self, X):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
@@ -48,7 +49,7 @@ def fast_geglu_inference(self, X):
 
     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
-    gate = torch.nn.functional.gelu(gate, approximate = ""tanh"")
+    gate = torch_nn_functional_gelu(gate, approximate = ""tanh"")
     gate *= up
 
     # X = self.down_proj(gate)
@@ -57,6 +58,18 @@ def fast_geglu_inference(self, X):
 pass
 
 
+def fast_rms_layernorm_inference_gemma(self, X, out_weight):
+    XX = X.to(torch.float32)
+    variance = XX.square().mean(-1, keepdim = True)
+    variance += self.variance_epsilon
+    XX *= variance.rsqrt_()
+    out_weight[:] = self.weight
+    out_weight += 1.0
+    XX *= out_weight
+    return XX.to(X.dtype)
+pass
+
+
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590
 def GemmaDecoderLayer_fast_forward(
     self,
@@ -72,10 +85,11 @@ def GemmaDecoderLayer_fast_forward(
 ):
     if past_key_value is not None:
         do_prefill = not hasattr(self.self_attn, ""paged_attention"")
+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda"")
 
         # Self Attention
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)
         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
             self.self_attn,
             hidden_states,
@@ -87,12 +101,12 @@ def GemmaDecoderLayer_fast_forward(
 
         # Fully Connected
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)
         hidden_states = fast_geglu_inference(self.mlp, hidden_states)
         hidden_states += residual
     else:
         residual = hidden_states
-        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)
         # hidden_states = self.input_layernorm(hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
             hidden_states=hidden_states,
@@ -108,7 +122,7 @@ def GemmaDecoderLayer_fast_forward(
 
         # Fully Connected
         residual = hidden_states
-        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)
         # hidden_states = self.post_attention_layernorm(hidden_states)
         hidden_states = self.mlp(hidden_states)
         hidden_states = residual + hidden_states
@@ -137,15 +151,18 @@ def GemmaModel_fast_forward_inference(
 ):
     # Fix out of bounds tokenization
     input_ids = input_ids[:,:self.max_seq_length]
+    out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda"")
 
     hidden_states = self.embed_tokens(input_ids)
-    hidden_states *= math_sqrt(self.config.hidden_size)
+    # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32
+    # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32
+    hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)
 
     next_decoder_cache = []
     for idx, decoder_layer in enumerate(self.layers):
         # Self Attention
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)
         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
             decoder_layer.self_attn,
             hidden_states,
@@ -156,13 +173,13 @@ def GemmaModel_fast_forward_inference(
 
         # Fully Connected
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)
         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)
         hidden_states += residual
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)
+    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)
 
     return BaseModelOutputWithPast(
         last_hidden_state = hidden_states,
@@ -173,91 +190,54 @@ def GemmaModel_fast_forward_inference(
 pass
 
 
-def GemmaForCausalLM_fast_forward(
-    self,
-    input_ids: torch.LongTensor = None,
-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-    *args, **kwargs,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-
-    if causal_mask is None and past_key_values is None:
-        causal_mask = xformers.attn_bias.LowerTriangularMask()
-
-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-    output_hidden_states = (
-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-    )
-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+# Follows line by line https://github.com/google-deepmind/gemma/blob/main/gemma/positional_embeddings.py#L45
+# Formulates cos and sin differently from Llama!
+class GemmaFixedRotaryEmbedding(torch.nn.Module):
+    # Fixes https://github.com/huggingface/transformers/pull/28837
+    # https://github.com/microsoft/DeepSpeed/issues/4932
+    # The precision of RoPE buffers is not correct, so we cast to int64.
+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
+        super().__init__()
+        self.dim = dim
+        self.max_position_embeddings = max_position_embeddings
+        self.base = base
+
+        # Build here to make `torch.jit.trace` work.
+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())
+    pass
 
-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-    self.model._has_no_labels = labels is None
+    def _set_cos_sin_cache(self, seq_len, device, dtype):
+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
+        # in FP32. They are applied (multiplied) in FP32 as well.
+        self.max_seq_len_cached = seq_len
 
-    if past_key_values is not None and \
-        hasattr(self.model.layers[0].self_attn, ""paged_attention""):
-        outputs = GemmaModel_fast_forward_inference(
-            self.model,
-            input_ids,
-            past_key_values,
-        )
-    else:
-        outputs = self.model(
-            input_ids=input_ids,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
+        # The difference is we do division explicity instead of t * (1/x) ie we do t/x.
+        freq_exponents = (2.0 / self.dim) * (
+            torch.arange(self.dim // 2, dtype = torch.int64, device = ""cpu"").float()
         )
+        timescale = self.base**freq_exponents
+        positions = torch.arange(self.max_seq_len_cached, device = ""cpu"", dtype = torch.int64).float()
+        radians_new = positions[..., None] / timescale[None, None, :]
+        radians_new = radians_new.squeeze(0)
+
+        emb = torch.cat((radians_new, radians_new), dim = -1)
+        # We must do RoPE in float32!
+        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)
+        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)
+        self.register_buffer(""cos_cached"", cos, persistent = False)
+        self.register_buffer(""sin_cached"", sin, persistent = False)
     pass
 
-    hidden_states = outputs[0]
-    bsz, q_len, hd = hidden_states.shape
-    if bsz == 1 and q_len == 1:
-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
-        logits = logits.unsqueeze(0).unsqueeze(0)
-    else:
-        logits = self.lm_head(hidden_states)
-    pass
+    def forward(self, x, position_ids=None, seq_len=None):
+        # x: [bs, num_attention_heads, seq_len, head_size]
+        if seq_len > self.max_seq_len_cached:
+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
-    loss = None
-    if labels is not None:
-        shift_logits = logits
-        if not hasattr(self, ""extra_ignored_labels""):
-            # Fixes https://github.com/unslothai/unsloth/issues/10
-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
-        pass
-        
-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
-        loss = fast_cross_entropy_loss(
-            logits = shift_logits,
-            labels = shift_labels,
+        return (
+            self.cos_cached[:seq_len].to(dtype=x.dtype),
+            self.sin_cached[:seq_len].to(dtype=x.dtype),
         )
     pass
-
-    if not return_dict:
-        output = (logits,) + outputs[1:]
-        return (loss,) + output if loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=loss,
-        logits=logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
-    )
 pass
 
 
@@ -270,7 +250,7 @@ class FastGemmaModel(FastLlamaModel):
         GemmaFlashAttention2.forward = LlamaAttention_fast_forward
         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward
         GemmaModel          .forward = LlamaModel_fast_forward
-        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward
+        GemmaForCausalLM    .forward = CausalLM_fast_forward(GemmaModel_fast_forward_inference)
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
@@ -278,7 +258,7 @@ class FastGemmaModel(FastLlamaModel):
         # https://github.com/huggingface/transformers/pull/27931
         # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
         import transformers.models.gemma.modeling_gemma
-        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = LlamaRotaryEmbedding
+        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = GemmaFixedRotaryEmbedding
         return
     pass
 
@@ -329,13 +309,14 @@ class FastGemmaModel(FastLlamaModel):
                 pass
             pass
             # Downcast RoPE embedding to correct data type
-            if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
-                and (module.cos_cached.dtype != correct_dtype):
-
-                module.cos_cached = module.cos_cached.to(correct_dtype)
-                module.sin_cached = module.sin_cached.to(correct_dtype)
-                pass
-            pass
+            # RoPE must be done in float32 for Gemma
+            # if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
+            #     and (module.cos_cached.dtype != correct_dtype):
+
+            #     module.cos_cached = module.cos_cached.to(correct_dtype)
+            #     module.sin_cached = module.sin_cached.to(correct_dtype)
+            #     pass
+            # pass
         pass
 
         # Add 1 to weight
@@ -358,8 +339,8 @@ class FastGemmaModel(FastLlamaModel):
                 # Must be in float32
                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36
                 # module = module.to(torch.float32)
-                # Don't convert to float32 since error analysis shows it makes it worse!!
-                module.weight += 1.0 # return output * (1 + self.weight)
+                # Leave + 1 to Triton kernel itself
+                # module.weight += 1.0 # return output * (1 + self.weight)
                 if not hasattr(module, ""variance_epsilon""):
                     module.variance_epsilon = module.eps # Gemma doesn't use variance_epsilon
         pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2055264..3f281a0 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -208,6 +208,7 @@ def LlamaAttention_fast_forward_inference(
 pass
 
 
+torch_nn_functional_silu = torch.nn.functional.silu
 def fast_swiglu_inference(self, X):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
@@ -217,7 +218,7 @@ def fast_swiglu_inference(self, X):
 
     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
-    gate = torch.nn.functional.silu(gate, inplace = True)
+    gate = torch_nn_functional_silu(gate, inplace = True)
     gate *= up
 
     # X = self.down_proj(gate)
@@ -509,7 +510,8 @@ def LlamaModel_fast_forward(
         inputs_embeds = self.embed_tokens(input_ids)
 
     # Mormalized from Gemma
-    if self.config.model_type == ""gemma"":
+    IS_GEMMA = self.config.model_type == ""gemma""
+    if IS_GEMMA:
         inputs_requires_grad = inputs_embeds.requires_grad
         if not inputs_embeds.is_leaf:
             inputs_embeds = inputs_embeds.detach()
@@ -517,7 +519,12 @@ def LlamaModel_fast_forward(
         elif inputs_requires_grad:
             inputs_embeds.requires_grad_(False)
         pass
-        inputs_embeds *= math_sqrt(self.config.hidden_size)
+        # Match Gemma exactly by casting to bfloat16 / float16
+        # inputs_embeds *= math_sqrt(self.config.hidden_size)
+        # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32
+        # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32
+        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)
+        # inputs_embeds *= math_sqrt(self.config.hidden_size)
         if inputs_requires_grad: inputs_embeds.requires_grad_(True)
     pass
 
@@ -619,7 +626,7 @@ def LlamaModel_fast_forward(
             all_self_attns += (layer_outputs[1],)
     pass
     
-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)
+    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
 
     # add hidden states from the last decoder layer
     if output_hidden_states:
@@ -681,91 +688,94 @@ def LlamaModel_fast_forward_inference(
 pass
 
 
-def LlamaForCausalLM_fast_forward(
-    self,
-    input_ids: torch.LongTensor = None,
-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
-    attention_mask: Optional[torch.Tensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[List[torch.FloatTensor]] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-    *args, **kwargs,
-) -> Union[Tuple, CausalLMOutputWithPast]:
+def CausalLM_fast_forward(fast_forward_inference):
+    def _CausalLM_fast_forward(
+        self,
+        input_ids: torch.LongTensor = None,
+        causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        *args, **kwargs,
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
+
+        if causal_mask is None and past_key_values is None:
+            causal_mask = xformers.attn_bias.LowerTriangularMask()
+
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-    if causal_mask is None and past_key_values is None:
-        causal_mask = xformers.attn_bias.LowerTriangularMask()
+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+        self.model._has_no_labels = labels is None
 
-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-    output_hidden_states = (
-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-    )
-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+        if past_key_values is not None and \
+            hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+            outputs = fast_forward_inference(
+                self.model,
+                input_ids,
+                past_key_values,
+            )
+        else:
+            outputs = self.model(
+                input_ids=input_ids,
+                causal_mask=causal_mask,
+                attention_mask=attention_mask,
+                position_ids=position_ids,
+                past_key_values=past_key_values,
+                inputs_embeds=inputs_embeds,
+                use_cache=use_cache,
+                output_attentions=output_attentions,
+                output_hidden_states=output_hidden_states,
+                return_dict=return_dict,
+            )
+        pass
 
-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-    self.model._has_no_labels = labels is None
+        hidden_states = outputs[0]
+        bsz, q_len, hd = hidden_states.shape
+        if bsz == 1 and q_len == 1:
+            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
+            logits = logits.unsqueeze(0).unsqueeze(0)
+        else:
+            logits = self.lm_head(hidden_states)
+        pass
 
-    if past_key_values is not None and \
-        hasattr(self.model.layers[0].self_attn, ""paged_attention""):
-        outputs = LlamaModel_fast_forward_inference(
-            self.model,
-            input_ids,
-            past_key_values,
-        )
-    else:
-        outputs = self.model(
-            input_ids=input_ids,
-            causal_mask=causal_mask,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
-    pass
+        loss = None
+        if labels is not None:
+            shift_logits = logits
+            if not hasattr(self, ""extra_ignored_labels""):
+                # Fixes https://github.com/unslothai/unsloth/issues/10
+                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
+            pass
+            
+            shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
+            loss = fast_cross_entropy_loss(
+                logits = shift_logits,
+                labels = shift_labels,
+            )
+        pass
 
-    hidden_states = outputs[0]
-    bsz, q_len, hd = hidden_states.shape
-    if bsz == 1 and q_len == 1:
-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
-        logits = logits.unsqueeze(0).unsqueeze(0)
-    else:
-        logits = self.lm_head(hidden_states)
-    pass
+        if not return_dict:
+            output = (logits,) + outputs[1:]
+            return (loss,) + output if loss is not None else output
 
-    loss = None
-    if labels is not None:
-        shift_logits = logits
-        if not hasattr(self, ""extra_ignored_labels""):
-            # Fixes https://github.com/unslothai/unsloth/issues/10
-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
-        pass
-        
-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
-        loss = fast_cross_entropy_loss(
-            logits = shift_logits,
-            labels = shift_labels,
+        return CausalLMOutputWithPast(
+            loss=loss,
+            logits=logits,
+            past_key_values=outputs.past_key_values,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
         )
     pass
-
-    if not return_dict:
-        output = (logits,) + outputs[1:]
-        return (loss,) + output if loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=loss,
-        logits=logits,
-        past_key_values=outputs.past_key_values,
-        hidden_states=outputs.hidden_states,
-        attentions=outputs.attentions,
-    )
+    return _CausalLM_fast_forward
 pass
 
 
@@ -880,7 +890,7 @@ class FastLlamaModel:
         LlamaFlashAttention2.forward = LlamaAttention_fast_forward
         LlamaDecoderLayer   .forward = LlamaDecoderLayer_fast_forward
         LlamaModel          .forward = LlamaModel_fast_forward
-        LlamaForCausalLM    .forward = LlamaForCausalLM_fast_forward
+        LlamaForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
 
         # Solves https://github.com/unslothai/unsloth/issues/168
diff --git a/unsloth/save.py b/unsloth/save.py
index 51ddeb3..5c1bceb 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -369,6 +369,7 @@ def unsloth_save_model(
 
     # Switch to our fast saving modules if it's a slow PC!
     n_cpus = psutil.cpu_count(logical = False)
+    if n_cpus is None: n_cpus = psutil.cpu_count()
     if n_cpus is None: n_cpus = 1
 
     if safe_serialization is None:
"
"diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index 3230cdc..9c032b2 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -16,5 +16,5 @@ from .loader  import FastLanguageModel, FastVisionModel
 from .llama   import FastLlamaModel
 from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
-from .dpo     import PatchDPOTrainer
+from .dpo     import PatchDPOTrainer, PatchKTOTrainer
 from ._utils  import is_bfloat16_supported
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 3a29352..5bc9529 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.11.10""
+__version__ = ""2024.12.1""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index e707435..5dc71f9 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -14,6 +14,7 @@
 
 __all__ = [
     ""PatchDPOTrainer"",
+    ""PatchKTOTrainer"",
 ]
 
 try:
@@ -127,4 +128,4 @@ def PatchDPOTrainer():
         pass
     pass
 pass
-
+PatchKTOTrainer = PatchDPOTrainer
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bb5c841..1bffb0c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1706,11 +1706,13 @@ class FastLlamaModel:
         spaces = re.search('\n([\s\t]{1,})', original_debug).group(0)[1:]
         front_spaces = re.match('([\s\t]{1,})', inner_training_loop).group(0)
 
+        # Cannot use \\ since it will cause a SyntaxWarning in Python 3.12
+        # Instead use chr(92) == \\
         debug_info = """"""debug_info = \\
         f""==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\n""\\
-        f""   \\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
-        f""O^O/ \\_/ \\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
-        f""\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
+        f""   {chr(92)}{chr(92)}   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\n""\\
+        f""O^O/ {chr(92)}_/ {chr(92)}    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\n""\\
+        f""{chr(92)}        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\n""\\
         f' ""-____-""     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}'
         logger.warning(debug_info)
         import subprocess, re, gc, numpy as np
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 80c1f82..aa4cc09 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -30,6 +30,7 @@ from transformers import set_seed as transformers_set_seed
 from unsloth_zoo.peft_utils import (
     get_peft_regex,
     merge_and_overwrite_lora,
+    # SKIP_QUANTIZATION_MODULES,
 )
 from triton import __version__ as triton_version
 
@@ -132,6 +133,7 @@ class FastBaseVisionModel:
                 bnb_4bit_use_double_quant = True,
                 bnb_4bit_quant_type       = ""nf4"",
                 bnb_4bit_compute_dtype    = dtype,
+                # llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
             )
         pass
 
@@ -424,5 +426,3 @@ class FastBaseVisionModel:
         return model
     pass
 pass
-
-
diff --git a/unsloth/save.py b/unsloth/save.py
index b503b2b..cf78bf5 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -45,6 +45,9 @@ __all__ = [
     ""create_huggingface_repo"",
 ]
 
+# llama.cpp specific targets - all takes 90s. Below takes 60s
+LLAMA_CPP_TARGETS = [""llama-quantize"", ""llama-export-lora"", ""llama-cli"",]
+
 # Check environments
 keynames = ""\n"" + ""\n"".join(os.environ.keys())
 IS_COLAB_ENVIRONMENT  = ""\nCOLAB_""  in keynames
@@ -494,7 +497,7 @@ def unsloth_save_model(
     elif safe_serialization and (n_cpus <= 2):
         logger.warning_once(
             f""Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\n""\
-            f""We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n""\
+            f""We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n""\
             f""To force `safe_serialization`, set it to `None` instead."",
         )
         safe_serialization = False
@@ -549,6 +552,8 @@ def unsloth_save_model(
 
     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)
 
+    print(""Unsloth: Saving model... This might take 5 minutes ..."")
+
     from tqdm import tqdm as ProgressBar
     for j, layer in enumerate(ProgressBar(internal_model.model.layers)):
         for item in LLAMA_WEIGHTS:
@@ -665,8 +670,6 @@ def unsloth_save_model(
         print()
     pass
 
-    print(""Unsloth: Saving model... This might take 5 minutes for Llama-7b..."")
-
     # Since merged, edit quantization_config
     old_config = model.config
     new_config = model.config.to_dict()
@@ -759,16 +762,36 @@ def install_llama_cpp_make_non_blocking():
     # https://github.com/ggerganov/llama.cpp/issues/7062
     # Weirdly GPU conversion for GGUF breaks??
     # env = { **os.environ, ""LLAMA_CUDA"": ""1"", }
-    n_jobs = max(int(psutil.cpu_count()*1.5), 1)
     # Force make clean
-    os.system(""make clean -C llama.cpp"")
-    full_command = [""make"", ""all"", ""-j""+str(n_jobs), ""-C"", ""llama.cpp""]
-
+    check = os.system(""make clean -C llama.cpp"")
+    IS_CMAKE = False
+    if check == 0:
+        # Uses old MAKE
+        n_jobs = max(int(psutil.cpu_count()*1.5), 1)
+        full_command = [""make"", ""all"", ""-j""+str(n_jobs), ""-C"", ""llama.cpp""]
+        IS_CMAKE = False
+    else:
+        # Uses new CMAKE
+        n_jobs = max(int(psutil.cpu_count()), 1) # Use less CPUs since 1.5x faster
+        check = os.system(""cmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON"")
+        if check != 0:
+            raise RuntimeError(f""*** Unsloth: Failed compiling llama.cpp using os.system(...) with error {check}. Please report this ASAP!"")
+        pass
+        # f""cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}"",
+        full_command = [
+            ""cmake"", ""--build"", ""llama.cpp/build"",
+            ""--config"", ""Release"",
+            ""-j""+str(n_jobs),
+            ""--clean-first"",
+            ""--target"",
+        ] + LLAMA_CPP_TARGETS
+        IS_CMAKE = True
+    pass
     # https://github.com/ggerganov/llama.cpp/issues/7062
     # Weirdly GPU conversion for GGUF breaks??
     # run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
     run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
-    return run_installer
+    return run_installer, IS_CMAKE
 pass
 
 
@@ -779,6 +802,29 @@ def install_python_non_blocking(packages = []):
 pass
 
 
+def try_execute(commands, force_complete = False):
+    for command in commands:
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
+            for line in sp.stdout:
+                line = line.decode(""utf-8"", errors = ""replace"")
+                if ""undefined reference"" in line:
+                    raise RuntimeError(f""*** Unsloth: Failed compiling llama.cpp with {line}. Please report this ASAP!"")
+                elif ""deprecated"" in line:
+                    return ""CMAKE""
+                elif ""Unknown argument"" in line:
+                    raise RuntimeError(f""*** Unsloth: Failed compiling llama.cpp with {line}. Please report this ASAP!"")
+                elif ""***"" in line:
+                    raise RuntimeError(f""*** Unsloth: Failed compiling llama.cpp with {line}. Please report this ASAP!"")
+                print(line, flush = True, end = """")
+            pass
+            if force_complete and sp.returncode is not None and sp.returncode != 0:
+                raise subprocess.CalledProcessError(sp.returncode, sp.args)
+        pass
+    pass
+    return None
+pass
+
+
 def install_llama_cpp_old(version = -10):
     # Download the 10th latest release since the latest might be broken!
     # FALLBACK mechanism
@@ -793,13 +839,13 @@ def install_llama_cpp_old(version = -10):
     # Check if the llama.cpp exists
     if os.path.exists(""llama.cpp""):
         print(
-            ""**[WARNING]** You have a llama.cpp old directory which is broken.\n""\
+            ""**[WARNING]** You have a llama.cpp directory which is broken.\n""\
             ""Unsloth will DELETE the broken directory and install a new one.\n""\
-            ""Press CTRL + C / cancel this if this is wrong. We shall wait 10 seconds.\n""
+            ""Press CTRL + C / cancel this if this is wrong. We shall wait 30 seconds.\n""
         )
         import time
-        for i in range(10):
-            print(f""**[WARNING]** Deleting llama.cpp directory... {10-i} seconds left."")
+        for i in range(30):
+            print(f""**[WARNING]** Deleting llama.cpp directory... {30-i} seconds left."")
             time.sleep(1)
         import shutil
         shutil.rmtree(""llama.cpp"", ignore_errors = True)
@@ -810,18 +856,25 @@ def install_llama_cpp_old(version = -10):
     commands = [
         ""git clone --recursive https://github.com/ggerganov/llama.cpp"",
         f""cd llama.cpp && git reset --hard {version} && git clean -df"",
+    ]
+    try_execute(commands)
+
+    # Try using MAKE
+    commands = [
         ""make clean -C llama.cpp"",
         f""make all -j{psutil.cpu_count()*2} -C llama.cpp"",
     ]
-    for command in commands:
-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
-            for line in sp.stdout:
-                line = line.decode(""utf-8"", errors = ""replace"")
-                if ""undefined reference"" in line:
-                    raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                print(line, flush = True, end = """")
-        pass
+    if try_execute(commands) == ""CMAKE"":
+        # Instead use CMAKE
+        commands = [
+            ""cmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON"",
+            f""cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}"",
+            ""cp llama.cpp/build/bin/llama-* llama.cpp"",
+            ""rm -rf llama.cpp/build"",
+        ]
+        try_execute(commands)
     pass
+
     # Check if successful
     if not os.path.exists(""llama.cpp/quantize"") and not os.path.exists(""llama.cpp/llama-quantize""):
         raise RuntimeError(
@@ -839,23 +892,27 @@ def install_llama_cpp_blocking(use_cuda = False):
 
     commands = [
         ""git clone --recursive https://github.com/ggerganov/llama.cpp"",
+        ""pip install gguf protobuf"",
+    ]
+    if os.path.exists(""llama.cpp""): return
+    try_execute(commands)
+
+    commands = [
         ""make clean -C llama.cpp"",
         # https://github.com/ggerganov/llama.cpp/issues/7062
         # Weirdly GPU conversion for GGUF breaks??
         # f""{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp"",
         f""make all -j{psutil.cpu_count()*2} -C llama.cpp"",
-        ""pip install gguf protobuf"",
     ]
-    if os.path.exists(""llama.cpp""): return
-
-    for command in commands:
-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
-            for line in sp.stdout:
-                line = line.decode(""utf-8"", errors = ""replace"")
-                if ""undefined reference"" in line:
-                    raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                print(line, flush = True, end = """")
-        pass
+    if try_execute(commands) == ""CMAKE"":
+        # Instead use CMAKE
+        commands = [
+            ""cmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON"",
+            f""cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}"",
+            ""cp llama.cpp/build/bin/llama-* llama.cpp"",
+            ""rm -rf llama.cpp/build"",
+        ]
+        try_execute(commands)
     pass
 pass
 
@@ -950,9 +1007,9 @@ def save_to_gguf(
 
     print_info = \
         f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
-        f""   \\\   /|    [0] Installing llama.cpp will take 3 minutes.\n""\
-        f""O^O/ \_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n""\
-        f""\        /    [2] Converting GGUF 16bits to {quantization_method} will take 10 minutes each.\n""\
+        f""   \\\   /|    [0] Installing llama.cpp might take 3 minutes.\n""\
+        f""O^O/ \_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n""\
+        f""\        /    [2] Converting GGUF 16bits to {quantization_method} might take 10 minutes each.\n""\
         f' ""-____-""     In total, you will have to wait at least 16 minutes.\n'
     print(print_info)
 
@@ -971,19 +1028,35 @@ def save_to_gguf(
     quantize_location = get_executable([""llama-quantize"", ""quantize""])
     convert_location  = get_executable([""convert-hf-to-gguf.py"", ""convert_hf_to_gguf.py""])
     
+    error = 0
     if quantize_location is not None and convert_location is not None:
         print(""Unsloth: llama.cpp found in the system. We shall skip installation."")
     else:
-        print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
+        print(""Unsloth: Installing llama.cpp. This might take 3 minutes..."")
         if _run_installer is not None:
+            _run_installer, IS_CMAKE = _run_installer
+
             error = _run_installer.wait()
+            # Check if successful
+            if error != 0:
+                print(f""Unsloth: llama.cpp error code = {error}."")
+                install_llama_cpp_old(-10)
+            pass
+
+            if IS_CMAKE:
+                # CMAKE needs to do some extra steps
+                print(""Unsloth: CMAKE detected. Finalizing some steps for installation."")
+
+                check = os.system(""cp llama.cpp/build/bin/llama-* llama.cpp"")
+                if check != 0: raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
+                check = os.system(""rm -rf llama.cpp/build"")
+                if check != 0: raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
+            pass
         else:
             error = 0
             install_llama_cpp_blocking()
         pass
 
-        # Check if successful. If not install 10th latest release
-
         # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize
         # and llama.cpp/main changed to llama.cpp/llama-cli
         # See https://github.com/ggerganov/llama.cpp/pull/7809
@@ -1012,11 +1085,6 @@ def save_to_gguf(
                 ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
             )
         pass
-
-        if error != 0 or quantize_location is None or convert_location is None:
-            print(f""Unsloth: llama.cpp error code = {error}."")
-            install_llama_cpp_old(-10)
-        pass
     pass
 
     # Determine maximum first_conversion state
@@ -1084,7 +1152,7 @@ def save_to_gguf(
     
     print(f""Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\n""\
           f""The output location will be {final_location}\n""\
-          ""This will take 3 minutes..."")
+          ""This might take 3 minutes..."")
 
     # We first check if tokenizer.model exists in the model_directory
     if os.path.exists(f""{model_directory}/tokenizer.model""):
@@ -1107,15 +1175,7 @@ def save_to_gguf(
             f""--outtype {first_conversion}""
     pass
 
-    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
-        for line in sp.stdout:
-            line = line.decode(""utf-8"", errors = ""replace"")
-            if ""undefined reference"" in line:
-                raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-            print(line, flush = True, end = """")
-        if sp.returncode is not None and sp.returncode != 0:
-            raise subprocess.CalledProcessError(sp.returncode, sp.args)
-    pass
+    try_execute([command,], force_complete = True)
 
     # Check if quantization succeeded!
     if not os.path.isfile(final_location):
@@ -1151,22 +1211,13 @@ def save_to_gguf(
     # Convert each type!
     for quant_method in quantization_method:
         if quant_method != first_conversion:
-            print(f""Unsloth: [2] Converting GGUF 16bit into {quant_method}. This will take 20 minutes..."")
+            print(f""Unsloth: [2] Converting GGUF 16bit into {quant_method}. This might take 20 minutes..."")
             final_location = str((Path(model_directory) / f""unsloth.{quant_method.upper()}.gguf"").absolute())
 
             command = f""./{quantize_location} {full_precision_location} ""\
                 f""{final_location} {quant_method} {n_cpus}""
             
-            # quantize uses stderr
-            with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
-                for line in sp.stdout:
-                    line = line.decode(""utf-8"", errors = ""replace"")
-                    if ""undefined reference"" in line:
-                        raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                    print(line, flush = True, end = """")
-                if sp.returncode is not None and sp.returncode != 0:
-                    raise subprocess.CalledProcessError(sp.returncode, sp.args)
-            pass
+            try_execute([command,], force_complete = True)
 
             # Check if quantization succeeded!
             if not os.path.isfile(final_location):
@@ -1629,7 +1680,7 @@ def unsloth_save_pretrained_gguf(
             git_clone = install_llama_cpp_clone_non_blocking()
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             git_clone.wait()
-            makefile  = install_llama_cpp_make_non_blocking()
+            makefile = install_llama_cpp_make_non_blocking()
             new_save_directory, old_username = unsloth_save_model(**arguments)
             python_install.wait()
         pass
@@ -1650,7 +1701,7 @@ def unsloth_save_pretrained_gguf(
                 git_clone = install_llama_cpp_clone_non_blocking()
                 python_install = install_python_non_blocking([""gguf"", ""protobuf""])
                 git_clone.wait()
-                makefile  = install_llama_cpp_make_non_blocking()
+                makefile = install_llama_cpp_make_non_blocking()
                 new_save_directory, old_username = unsloth_save_model(**arguments)
                 python_install.wait()
             pass
@@ -1807,7 +1858,7 @@ def unsloth_push_to_hub_gguf(
             git_clone = install_llama_cpp_clone_non_blocking()
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             git_clone.wait()
-            makefile  = install_llama_cpp_make_non_blocking()
+            makefile = install_llama_cpp_make_non_blocking()
             new_save_directory, old_username = unsloth_save_model(**arguments)
             python_install.wait()
         pass
@@ -1828,7 +1879,7 @@ def unsloth_push_to_hub_gguf(
                 git_clone = install_llama_cpp_clone_non_blocking()
                 python_install = install_python_non_blocking([""gguf"", ""protobuf""])
                 git_clone.wait()
-                makefile  = install_llama_cpp_make_non_blocking()
+                makefile = install_llama_cpp_make_non_blocking()
                 new_save_directory, old_username = unsloth_save_model(**arguments)
                 python_install.wait()
             pass
"
"diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index 00956ed..14fd163 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -20,11 +20,6 @@ from functools import wraps
 import trl
 import inspect
 from trl import SFTTrainer
-try:
-    from trl import SFTConfig as TrainingArguments
-except:
-    from transformers import TrainingArguments
-pass
 from . import is_bfloat16_supported
 from unsloth_zoo.training_utils import unsloth_train as _unsloth_train
 from packaging.version import Version
@@ -60,7 +55,11 @@ else:
     pass
 pass
 
-
+try:
+    from trl import SFTConfig as TrainingArguments
+except:
+    from transformers import TrainingArguments
+pass
 @dataclass
 class UnslothTrainingArguments(TrainingArguments):
     embedding_learning_rate : Optional[float] = field(
@@ -134,7 +133,7 @@ pass
 
 # From `trl>=0.13.0`, they changed how to pass several params to the trainer
 # We need to patch to make the transition smooth
-def create_backwards_compatible_trainer(trainer_class, config_class):
+def _backwards_compatible_trainer(trainer_class, config_class):
     original_init = trainer_class.__init__
     
     @wraps(original_init)
@@ -167,6 +166,7 @@ def create_backwards_compatible_trainer(trainer_class, config_class):
             }
 
             # Get parameters that exist in Config but not in TrainingArguments
+            from transformers import TrainingArguments
             moved_params = \
                 set(inspect.signature(config_class)     .parameters.keys()) - \
                 set(inspect.signature(TrainingArguments).parameters.keys())
@@ -207,14 +207,13 @@ def _patch_trl_trainer():
 
     import trl.trainer
     trl_classes = dir(trl.trainer)
-
-    non_convertable_trainer = set([""PPOv2"", ""AlignProp""])
-    trl_trainers = set(x[:-len(""Trainer"")] for x in trl_classes if x.endswith(""Trainer"")) - non_convertable_trainer
-    trl_configs  = set(x[:-len(""Config"")]  for x in trl_classes if x.endswith(""Config""))  - non_convertable_trainer
+    trl_trainers = set(x[:-len(""Trainer"")] for x in trl_classes if x.endswith(""Trainer""))
+    trl_configs  = set(x[:-len(""Config"")]  for x in trl_classes if x.endswith(""Config""))
     trl_classes = list(trl_trainers & trl_configs)
 
     for x in trl_classes:
-        exec(f""trl.{x}Trainer.__init__ = create_backwards_compatible_trainer(trl.{x}Trainer, trl.{x}Config)"", globals())
+        try:    exec(f""trl.{x}Trainer.__init__ = _backwards_compatible_trainer(trl.{x}Trainer, trl.{x}Config)"", globals())
+        except: continue
     pass
 
     trl.__UNSLOTH_BACKWARDS_COMPATIBLE__ = True
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7b31372..cecacc0 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -341,18 +341,19 @@ pass
 # =============================================
 
 
-def get_statistics():
+def _get_statistics(statistics = None):
     # We log some basic stats about which environment is being used.
     # We simply download a README.md file from HF - all data is made public.
     # This is simply so we can check if some envs are broken or not.
+    # You can disable this by commenting the below out
     try:
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         import psutil
         n_cpus = psutil.cpu_count(logical = False)
 
         keynames = ""\n"" + ""\n"".join(os.environ.keys())
-        statistics = None
-        if   ""\nCOLAB_""  in keynames and n_cpus == 1: statistics = ""colab""
+        if statistics is not None: pass
+        elif ""\nCOLAB_""  in keynames and n_cpus == 1: statistics = ""colab""
         elif ""\nCOLAB_""  in keynames: statistics = ""colabpro""
         elif ""\nKAGGLE_"" in keynames: statistics = ""kaggle""
         elif ""\nRUNPOD_"" in keynames: statistics = ""runpod""
@@ -371,7 +372,7 @@ def get_statistics():
 
             from transformers import AutoModelForCausalLM
             stats_model = AutoModelForCausalLM.from_pretrained(
-                f""unslothai/statistics-{statistics}"",
+                f""unslothai/{statistics}"",
                 force_download = True,
             )
             del stats_model
@@ -384,6 +385,29 @@ def get_statistics():
 pass
 
 
+def get_statistics():
+    # We log some basic stats about which environment is being used.
+    # We simply download a README.md file from HF - all data is made public.
+    # This is simply so we can check if some envs are broken or not.
+    # You can disable this by commenting the below out
+    _get_statistics(None)
+    try:
+        vram = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
+        if   vram <= 8 : vram = 8
+        elif vram <= 16: vram = 16
+        elif vram <= 20: vram = 20
+        elif vram <= 24: vram = 24
+        elif vram <= 40: vram = 40
+        elif vram <= 48: vram = 48
+        elif vram <= 80: vram = 80
+        else: vram = ""80+""
+        _get_statistics(f""vram-{vram}"")
+    except:
+        pass
+    pass
+pass
+
+
 def _calculate_n_gradient_checkpoints(
     n_layers : int,
     method   : Optional[Union[str, int]] = ""sqrt"",
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 9675b10..520c998 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -257,9 +257,9 @@ def get_chat_template(
         assert(""Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported."")
     pass
 
-    if tokenizer.__class__.__name__.startswith(""Gemma"") and chat_template == ""chatml"":
-        chat_template = ""gemma_chatml""
-    pass
+    # if tokenizer.__class__.__name__.startswith(""Gemma"") and chat_template == ""chatml"":
+    #     chat_template = ""gemma_chatml""
+    # pass
 
     old_padding_side = tokenizer.padding_side
 
@@ -298,8 +298,12 @@ def get_chat_template(
                 pass
             pass
 
-            logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
+            if not stop_word in token_mapping.values():
+                # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1
+                logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
+                string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
+            pass
+
             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3f281a0..6ed52a7 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -916,6 +916,7 @@ class FastLlamaModel:
         rope_scaling   = None,
         fix_tokenizer  = True,
         model_patcher  = None,
+        tokenizer_name = None,
         **kwargs,
     ):
         if model_patcher is None: model_patcher = FastLlamaModel
@@ -978,13 +979,16 @@ class FastLlamaModel:
             max_position_embeddings = max_position_embeddings,
             **kwargs,
         )
+
+        # Counteract saved tokenizers
+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         tokenizer = AutoTokenizer.from_pretrained(
-            model_name,
+            tokenizer_name,
             model_max_length = max_position_embeddings,
             padding_side     = ""right"",
             token            = token,
         )
-
+        
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = model_patcher.post_patch(model)
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 67a59c8..47b568a 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -18,7 +18,7 @@ from transformers import AutoConfig
 from transformers import __version__ as transformers_version
 from peft import PeftConfig, PeftModel
 from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER
-
+import os
 
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
 major, minor = transformers_version.split(""."")[:2]
@@ -118,6 +118,16 @@ class FastLanguageModel(FastLlamaModel):
             )
         pass
 
+        # Check if this is local model since the tokenizer gets overwritten
+        if  os.path.exists(os.path.join(old_model_name, ""tokenizer_config.json"")) and \
+            os.path.exists(os.path.join(old_model_name, ""tokenizer.json"")) and \
+            os.path.exists(os.path.join(old_model_name, ""special_tokens_map.json"")):
+
+            tokenizer_name = old_model_name
+        else:
+            tokenizer_name = None
+        pass
+
         model, tokenizer = dispatch_model.from_pretrained(
             model_name     = model_name,
             max_seq_length = max_seq_length,
@@ -128,6 +138,7 @@ class FastLanguageModel(FastLlamaModel):
             rope_scaling   = rope_scaling,
             fix_tokenizer  = fix_tokenizer,
             model_patcher  = dispatch_model,
+            tokenizer_name = tokenizer_name,
             *args, **kwargs,
         )
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 6c9d9ec..c1e39e4 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -294,6 +294,7 @@ class FastMistralModel(FastLlamaModel):
         rope_scaling   = None, # Mistral does not support RoPE scaling
         fix_tokenizer  = True,
         model_patcher  = None,
+        tokenizer_name = None,
         **kwargs,
     ):
         if model_patcher is None: model_patcher = FastMistralModel
@@ -354,8 +355,11 @@ class FastMistralModel(FastLlamaModel):
             # rope_scaling      = rope_scaling,
             **kwargs,
         )
+
+        # Counteract saved tokenizers
+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         tokenizer = AutoTokenizer.from_pretrained(
-            model_name,
+            tokenizer_name,
             model_max_length = max_position_embeddings,
             padding_side     = ""right"",
             token            = token,
diff --git a/unsloth/save.py b/unsloth/save.py
index 5c1bceb..5971d76 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -91,11 +91,13 @@ def _merge_lora(layer, name):
         else:
             dtype = W.dtype
         W = W.to(torch.float32).t()
+        # W = W.t()
 
         if A is not None:
             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))
             # W += sAB
             W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)
+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)
             # if not torch.isfinite(W).all():
             maximum_element = torch.max(W.min().abs(), W.max())
             if not torch.isfinite(maximum_element).item():
"
"diff --git a/pyproject.toml b/pyproject.toml
index 49347c8..d14a392 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -32,11 +32,17 @@ include-package-data = false
 exclude = [""images*""]
 
 [project.optional-dependencies]
+triton = [
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+]
 huggingface = [
-    ""unsloth_zoo>=2024.11.8"",
+    ""unsloth_zoo>=2024.12.4"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.46.1,<=4.46.3"",
+    ""transformers>=4.46.1,!=4.47.0"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -49,88 +55,113 @@ huggingface = [
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
     ""hf_transfer"",
+    ""unsloth[triton]"",
 ]
 cu118only = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu121only = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu118onlytorch211 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu121onlytorch211 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu118onlytorch212 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu121onlytorch212 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu118onlytorch220 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu121onlytorch220 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
 ]
 cu118onlytorch230 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
 ]
 cu121onlytorch230 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
 ]
 cu118onlytorch240 = [
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.27.post2%2Bcu118-cp312-cp312-manylinux2014_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
 ]
 cu121onlytorch240 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
 ]
 cu124onlytorch240 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 cu121onlytorch250 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
 ]
 cu124onlytorch250 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+]
+cu121onlytorch251 = [
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+]
+cu124onlytorch251 = [
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 cu118 = [
     ""unsloth[huggingface]"",
@@ -207,6 +238,16 @@ cu124-torch250 = [
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu124onlytorch250]"",
 ]
+cu121-torch251 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu121onlytorch251]"",
+]
+cu124-torch251 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu124onlytorch251]"",
+]
 kaggle = [
     ""unsloth[huggingface]"",
 ]
@@ -244,10 +285,10 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2024.11.8"",
+    ""unsloth_zoo>=2024.12.3"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.46.1,<=4.46.3"",
+    ""transformers>=4.46.1,!=4.47.0"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -263,122 +304,111 @@ colab-no-deps = [
     ""accelerate>=0.34.1"",
     ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
     ""peft>=0.7.1"",
-    ""xformers<0.0.27"",
-    ""bitsandbytes>=0.43.3"",
+    ""xformers"",
+    ""bitsandbytes>=0.46.1"",
     ""protobuf<4.0.0"",
 ]
 colab = [
     ""unsloth[cu121]"",
 ]
+flashattention = [
+    ""packaging ; platform_system == 'Linux'"",
+    ""ninja ; platform_system == 'Linux'"",
+    ""flash-attn>=2.6.3 ; platform_system == 'Linux'"",
+]
 colab-ampere = [
     ""unsloth[colab-ampere-torch220]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu118-ampere = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118only]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu121-ampere = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121only]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu118-ampere-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch211]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu121-ampere-torch211 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch211]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu118-ampere-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch220]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu121-ampere-torch220 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch220]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu118-ampere-torch230 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch230]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu121-ampere-torch230 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch230]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu118-ampere-torch240 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu118onlytorch240]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu121-ampere-torch240 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch240]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu121-ampere-torch250 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu121onlytorch250]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu124-ampere-torch240 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu124onlytorch240]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
 ]
 cu124-ampere-torch250 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.43.3"",
     ""unsloth[cu124onlytorch250]"",
-    ""packaging"",
-    ""ninja"",
-    ""flash-attn>=2.6.3"",
+    ""unsloth[flashattention]"",
+]
+cu121-ampere-torch251 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu121onlytorch251]"",
+    ""unsloth[flashattention]"",
+]
+cu124-ampere-torch251 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.43.3"",
+    ""unsloth[cu124onlytorch251]"",
+    ""unsloth[flashattention]"",
 ]
 
 [project.urls]
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a02cadd..ce644f8 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.8""
+__version__ = ""2024.12.9""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -72,7 +72,7 @@ from platform import system as platform_system
 platform_system = platform_system()
 import numpy as np
 import warnings, subprocess, re, inspect, psutil, os, math
-from packaging.version import Version
+from unsloth_zoo.utils import Version
 
 from unsloth_zoo.tokenizer_utils import (
     patch_tokenizer as _patch_tokenizer,
@@ -403,7 +403,7 @@ pass
 # Fix new Xformers versions TypeError: Multiple dispatch failed for 'torch._ops.aten.to.dtype_layout'
 accelerate_old_send_to_device = None
 accelerate_new_send_to_device = None
-if Version(xformers_version) >= Version(""0.0.27""):
+if xformers_version is not None and Version(xformers_version) >= Version(""0.0.27""):
     import accelerate.utils.operations
     if hasattr(accelerate.utils.operations, ""send_to_device"") and \
         accelerate.utils.operations.send_to_device.__name__ != ""_fixed_send_to_device"":
@@ -1086,6 +1086,14 @@ def patch_gradient_accumulation_fix(Trainer):
         ""if num_items_in_batch is not None: loss *= self.args.gradient_accumulation_steps"",
     )
     function = function.replace(""def training_step"", ""def _unsloth_training_step"", 1)
+
+    # Fix 4.47.0 issue where num_items_in_batch was removed
+    # See https://github.com/huggingface/transformers/pull/35121
+    function = function.replace(
+        ""if self.model_accepts_loss_kwargs:"",
+        ""if False:"",
+    )
+    
     exec(function, globals())
     Trainer.training_step = _unsloth_training_step
 pass
diff --git a/unsloth/models/cohere.py b/unsloth/models/cohere.py
index cbbebee..1610949 100644
--- a/unsloth/models/cohere.py
+++ b/unsloth/models/cohere.py
@@ -68,7 +68,7 @@ pass
 def CohereAttention_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
@@ -183,7 +183,7 @@ pass
 def CohereDecoderLayer_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 1d9a0c1..c654343 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -77,7 +77,7 @@ pass
 def GemmaDecoderLayer_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index e47a743..0f0a020 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -75,7 +75,7 @@ pass
 def Gemma2Attention_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
@@ -169,7 +169,7 @@ pass
 def Gemma2DecoderLayer_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
index 2229636..9466a8d 100644
--- a/unsloth/models/granite.py
+++ b/unsloth/models/granite.py
@@ -60,7 +60,7 @@ pass
 def GraniteAttention_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
@@ -171,7 +171,7 @@ pass
 def GraniteDecoderLayer_fast_forward(
     self,
     hidden_states:        torch.Tensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_value:       Optional[Tuple[torch.Tensor]] = None,
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f8fb7d9..c945149 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -66,6 +66,8 @@ except:
     from huggingface_hub.utils._token import get_token
 pass
 from triton import __version__ as triton_version
+BlockDiagonalCausalMask = xformers.attn_bias.BlockDiagonalCausalMask if xformers is not None else None
+
 
 def original_apply_qkv(self, X):
     Q = self.q_proj(X)
@@ -330,7 +332,7 @@ pass
 def LlamaAttention_fast_forward(
     self,
     hidden_states:       torch.Tensor,
-    causal_mask:         Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:         Optional[BlockDiagonalCausalMask] = None,
     attention_mask:      Optional[torch.Tensor] = None,
     position_ids:        Optional[torch.LongTensor] = None,
     past_key_value:      Optional[Tuple[torch.Tensor]] = None,
@@ -538,7 +540,7 @@ __DTYPE_MAP = {
 def LlamaModel_fast_forward(
     self,
     input_ids:            torch.LongTensor,
-    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:          Optional[BlockDiagonalCausalMask] = None,
     attention_mask:       Optional[torch.Tensor] = None,
     position_ids:         Optional[torch.LongTensor] = None,
     past_key_values:      Optional[List[torch.FloatTensor]] = None,
@@ -942,7 +944,7 @@ def CausalLM_fast_forward(fast_forward_inference):
     def _CausalLM_fast_forward(
         self,
         input_ids: torch.LongTensor = None,
-        causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+        causal_mask: Optional[BlockDiagonalCausalMask] = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 5ecd667..9c5ea5b 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -32,7 +32,7 @@ pass
 from huggingface_hub import HfFileSystem
 
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
-from packaging.version import Version
+from unsloth_zoo.utils import Version
 transformers_version = Version(transformers_version)
 SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
 SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index dda4304..d6c6946 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -40,7 +40,7 @@ pass
 def MistralAttention_fast_forward(
     self,
     hidden_states:       torch.Tensor,
-    causal_mask:         Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask:         Optional[BlockDiagonalCausalMask] = None,
     attention_mask:      Optional[torch.Tensor] = None,
     position_ids:        Optional[torch.LongTensor] = None,
     past_key_value:      Optional[Tuple[torch.Tensor]] = None,
@@ -172,7 +172,7 @@ pass
 def MistralForCausalLM_fast_forward(
     self,
     input_ids: torch.LongTensor = None,
-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    causal_mask: Optional[BlockDiagonalCausalMask] = None,
     attention_mask: Optional[torch.Tensor] = None,
     position_ids: Optional[torch.LongTensor] = None,
     past_key_values: Optional[List[torch.FloatTensor]] = None,
diff --git a/unsloth/save.py b/unsloth/save.py
index ea2d309..a63225e 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -12,6 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from unsloth_zoo.utils import Version
 from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
 from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 from peft.tuners.lora import Linear as Peft_Linear
@@ -2096,6 +2097,7 @@ pass
 
 
 from .models.loader_utils import get_model_name
+from unsloth_zoo.saving_utils import merge_and_overwrite_lora
 
 @torch.inference_mode
 def unsloth_generic_save(
@@ -2127,34 +2129,16 @@ def unsloth_generic_save(
     maximum_memory_usage : float = 0.9,
 ):
     if token is None and push_to_hub: token = get_token()
-
-    import unsloth_zoo
-    if Version(unsloth_zoo.__version__) <= Version(""2024.12.1""):
-        from unsloth_zoo.peft_utils import merge_and_overwrite_lora
-        merge_and_overwrite_lora(
-            get_model_name,
-            create_huggingface_repo,
-            model,
-            save_location        = save_directory,
-            push_to_hub          = push_to_hub,
-            token                = token,
-            upload_location      = save_directory if push_to_hub else None,
-            low_disk_space_usage = True,
-            private              = private,
-        )
-    else:
-        from unsloth_zoo.saving_utils import merge_and_overwrite_lora
-        merge_and_overwrite_lora(
-            get_model_name,
-            model,
-            save_directory       = save_directory,
-            push_to_hub          = push_to_hub,
-            private              = private,
-            token                = token,
-            low_disk_space_usage = False,
-            use_temp_file        = False,
-        )
-    pass
+    merge_and_overwrite_lora(
+        get_model_name,
+        model,
+        save_directory       = save_directory,
+        push_to_hub          = push_to_hub,
+        private              = private,
+        token                = token,
+        low_disk_space_usage = False,
+        use_temp_file        = False,
+    )
     return
 pass
 
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 27b9228..de53704 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -92,6 +92,11 @@ def clean_gpu_cache():
     else:
         torch.cuda.empty_cache()
 
+if DEVICE_TYPE == ""xpu"":
+    get_current_device = torch.xpu.current_device
+else:
+    get_current_device = torch.cuda.current_device
+
 def original_apply_qkv(self, X):
     Q = self.q_proj(X)
     K = self.k_proj(X)
@@ -1365,8 +1370,8 @@ class LlamaRotaryEmbedding(torch.nn.Module):
             self._set_cos_sin_cache(seq_len=self.current_rope_size, device=torch.device(device_idx), dtype=torch.get_default_dtype())
 
         # dummy so that patch_utils doesn't fail for now
-        self.cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
-        self.sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.cos_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
+        self.sin_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -1402,7 +1407,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
 
     def get_cached(self, seq_len = None, device_index = None):
         if device_index is None:
-            device_index = torch.cuda.current_device()
+            device_index = get_current_device()
         return self.multi_gpu_cos_cached[device_index], self.multi_gpu_sin_cached[device_index]
     pass
 
@@ -1484,8 +1489,8 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
             self._set_cos_sin_cache(seq_len=self.current_rope_size, device=torch.device(device_idx), dtype=torch.get_default_dtype())
 
         # dummy so that patch_utils doesn't fail for now
-        self.cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
-        self.sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.cos_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
+        self.sin_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -1518,7 +1523,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
 
     def get_cached(self, seq_len = None, device_index = None):
         if device_index is None:
-            device_index = torch.cuda.current_device()
+            device_index = get_current_device()
         return self.multi_gpu_cos_cached[device_index], self.multi_gpu_sin_cached[device_index]
     pass
 
@@ -1631,10 +1636,10 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
             self.multi_gpu_short_sin_cached[device_idx] = sin_cached
 
         # dummy so that patch_utils doesn't fail for now
-        self.short_cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
-        self.short_sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
-        self.long_cos_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
-        self.long_sin_cached = torch.empty(1, device=torch.cuda.current_device(), dtype=torch.get_default_dtype())
+        self.short_cos_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
+        self.short_sin_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
+        self.long_cos_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
+        self.long_sin_cached = torch.empty(1, device=get_current_device(), dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -1675,7 +1680,7 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
 
     def get_cached(self, seq_len = None, device_index = None):
         if device_index is None:
-            device_index = torch.cuda.current_device()
+            device_index = get_current_device()
         if seq_len is not None and seq_len < self.original_max_position_embeddings:
             return self.multi_gpu_short_cos_cached[device_index], self.multi_gpu_short_sin_cached[device_index]
         return self.multi_gpu_long_cos_cached[device_index], self.multi_gpu_long_sin_cached[device_index]
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index 940feb4..f8f884a 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -853,9 +853,13 @@ def save_to_gguf(
     model_dtype = ""f16"" if model_dtype == ""float16"" else ""bf16""
 
     # Convert quantization_method to list
-    quantization_method = \
-        quantization_method if type(quantization_method) is list else list(quantization_method)
-
+    if   isinstance(quantization_method, list):  pass
+    elif isinstance(quantization_method, str):   quantization_method = [ quantization_method, ]
+    elif isinstance(quantization_method, tuple): quantization_method = list(quantization_method)
+    else:
+        raise TypeError(""Unsloth: quantization_method can only be a string or a list of strings"")
+    pass
+    
     # Check if bfloat16 is supported
     if model_dtype == ""bf16"" and not torch.cuda.is_bf16_supported():
         logger.warning(
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 8cdb5e3..f4268e3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -30,7 +30,7 @@ import numpy as np
 import os
 import psutil
 
-__version__ = ""2024.3""
+__version__ = ""2024.4""
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
@@ -70,12 +70,13 @@ __all__ = [
     ""platform_system"",
     ""patch_tokenizer"",
     ""get_statistics"",
+    ""Offloaded_Gradient_Checkpointer"",
 ]
 
 
 def prepare_model_for_kbit_training(
     model                      : Any,
-    use_gradient_checkpointing : bool = True,
+    use_gradient_checkpointing : Optional = True,
     use_reentrant              : Optional[bool] = True,
 ) -> Any:
     """"""
@@ -101,9 +102,23 @@ def prepare_model_for_kbit_training(
             param.requires_grad_(False)
     pass
 
-    if use_gradient_checkpointing:
+    # Gradient checkpointing!
+    if use_gradient_checkpointing == ""offloaded"":
+
+        # Saves VRAM!
+        original_model = model
+        while hasattr(original_model, ""model""):
+            original_model._offloaded_gradient_checkpointing = True
+            original_model = original_model.model
+        pass
+        original_model._offloaded_gradient_checkpointing = True
+        
         model.gradient_checkpointing_enable()
 
+    elif use_gradient_checkpointing == True:
+        model.gradient_checkpointing_enable()
+    pass
+
     # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.
     if use_reentrant:
         if hasattr(model, ""enable_input_require_grads""):
@@ -179,6 +194,7 @@ def get_statistics():
     try:
         from huggingface_hub import hf_hub_download
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
+        import psutil
         n_cpus = psutil.cpu_count(logical = False)
 
         keynames = ""\n"" + ""\n"".join(os.environ.keys())
@@ -291,3 +307,35 @@ def prepare_n_gradient_checkpoints(
     _model._gradient_checkpointing_boundaries    = boundaries
     _model._gradient_checkpointing_use_reentrant = use_reentrant
 pass
+
+
+class Offloaded_Gradient_Checkpointer(torch.autograd.Function):
+    """"""
+    Saves VRAM by smartly offloading to RAM.
+    Tiny hit to performance, since we mask the movement via non blocking calls.
+    [TODO] Load the backward pass earlier
+    """"""
+    @staticmethod
+    @torch.cuda.amp.custom_fwd
+    def forward(ctx, forward_function, hidden_states, *args):
+        saved_hidden_states = hidden_states.to(""cpu"", non_blocking = True)
+        with torch.no_grad():
+            (output,) = forward_function(hidden_states, *args)
+        ctx.save_for_backward(saved_hidden_states)
+        ctx.forward_function = forward_function
+        ctx.args = args
+        return output
+    pass
+
+    @staticmethod
+    @torch.cuda.amp.custom_bwd
+    def backward(ctx, dY):
+        (hidden_states,) = ctx.saved_tensors
+        hidden_states = hidden_states.to(""cuda"", non_blocking = True).detach()
+        hidden_states.requires_grad = True
+        with torch.enable_grad():
+            (output,) = ctx.forward_function(hidden_states, *ctx.args)
+        torch.autograd.backward(output, dY)
+        return (None, hidden_states.grad,) + (None,)*len(ctx.args)
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index dc66059..a7ade9f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -628,19 +628,42 @@ def LlamaModel_fast_forward(
         boundaries = None
     pass
 
+    # Check checkpointing method
+    gradient_checkpointing = False
+    offloaded_gradient_checkpointing = False
+
+    if (self.gradient_checkpointing and self.training and not use_cache):
+
+        gradient_checkpointing = True
+
+        if output_attentions is False and hasattr(self, ""_offloaded_gradient_checkpointing""):
+            offloaded_gradient_checkpointing = True
+    pass
+
+    # Go through every layer!
     for idx, decoder_layer in enumerate(self.layers):
 
         if output_hidden_states: all_hidden_states += (hidden_states,)
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
-        if self.gradient_checkpointing and self.training:
+        if offloaded_gradient_checkpointing:
+            hidden_states = Offloaded_Gradient_Checkpointer.apply(
+                decoder_layer,
+                hidden_states,
+                causal_mask,
+                attention_mask,
+                position_ids,
+                past_key_values,
+                output_attentions,
+                use_cache,
+            )
 
+        elif gradient_checkpointing:
             def create_custom_forward(module):
                 def custom_forward(*inputs):
-                    # None for past_key_value
-                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)
-
+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)
                 return custom_forward
+            pass
 
             layer_outputs = torch.utils.checkpoint.checkpoint(
                 create_custom_forward(decoder_layer),
@@ -648,9 +671,11 @@ def LlamaModel_fast_forward(
                 causal_mask,
                 attention_mask,
                 position_ids,
-                use_reentrant=True,
-                preserve_rng_state=False,
+                use_reentrant = True,
+                preserve_rng_state = False,
             )
+            hidden_states = layer_outputs[0]
+
         else:
             layer_outputs = decoder_layer(
                 hidden_states,
@@ -662,9 +687,9 @@ def LlamaModel_fast_forward(
                 use_cache=use_cache,
                 padding_mask=padding_mask,
             )
+            hidden_states = layer_outputs[0]
         pass
 
-        hidden_states = layer_outputs[0]
         if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
         if output_attentions: all_self_attns += (layer_outputs[1],)
     pass
@@ -801,12 +826,12 @@ def CausalLM_fast_forward(fast_forward_inference):
 
         hidden_states = outputs[0]
         bsz, q_len, hd = hidden_states.shape
+        lm_head = self.lm_head.weight
         if bsz == 1 and q_len == 1:
-            lm_head = self.lm_head.weight
             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
             logits = logits.unsqueeze(0).unsqueeze(0)
         else:
-            logits = self.lm_head(hidden_states)
+            logits = self.lm_head(hidden_states.to(lm_head.dtype))
         pass
         logits = logits.to(self.config.torch_dtype)
 
@@ -1402,6 +1427,8 @@ class FastLlamaModel:
                     ""We shall do it for you!""
                 )
                 train_lm_head = True
+                if modules_to_save is None: modules_to_save = [""lm_head""]
+                else: modules_to_save.append(""lm_head"")
 
             elif module == ""embed_tokens"":
                 logger.warning_once(
@@ -1409,6 +1436,8 @@ class FastLlamaModel:
                     ""We shall do it for you!""
                 )
                 train_embed_tokens = True
+                if modules_to_save is None: modules_to_save = [""embed_tokens""]
+                else: modules_to_save.append(""embed_tokens"")
 
             else:
                 assert(module in accepted_modules)
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index e867cee..87f5c85 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -225,12 +225,12 @@ def MistralForCausalLM_fast_forward(
 
     hidden_states = outputs[0]
     bsz, q_len, hd = hidden_states.shape
+    lm_head = self.lm_head.weight
     if bsz == 1 and q_len == 1:
-        lm_head = self.lm_head.weight
         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
         logits = logits.unsqueeze(0).unsqueeze(0)
     else:
-        logits = self.lm_head(hidden_states)
+        logits = self.lm_head(hidden_states.to(lm_head.dtype))
     pass
     logits = logits.to(self.config.torch_dtype)
 
diff --git a/unsloth/save.py b/unsloth/save.py
index a08a744..49d88bf 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -33,9 +33,13 @@ __all__ = [
     ""patch_saving_functions"",
 ]
 
-# Check Kaggle
-IS_A_KAGGLE_ENVIRONMENT = ""KAGGLE_CONTAINER_NAME"" in os.environ
+# Check environments
+keynames = ""\n"" + ""\n"".join(os.environ.keys())
+IS_COLAB_ENVIRONMENT  = ""\nCOLAB_""  in keynames
+IS_KAGGLE_ENVIRONMENT = ""\nKAGGLE_"" in keynames
+del keynames
 
+# Weights
 LLAMA_WEIGHTS = (
     ""self_attn.q_proj"", ""self_attn.k_proj"", ""self_attn.v_proj"", ""self_attn.o_proj"",
     ""mlp.gate_proj"", ""mlp.up_proj"", ""mlp.down_proj"",
@@ -177,6 +181,9 @@ def unsloth_save_model(
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.9,
 ):
+    if token is None and ""HF_TOKEN"" in os.environ:
+        token = os.environ[""HF_TOKEN""]
+
     if commit_message is None: commit_message = """"
     if ""Unsloth"" not in commit_message:
         commit_message += "" (Trained with Unsloth)""
@@ -291,6 +298,10 @@ def unsloth_save_model(
             tags               = tags,
         )
         if tokenizer is not None:
+            # Set padding side to left for inference
+            old_padding_side = tokenizer.padding_side
+            tokenizer.padding_side = ""left""
+
             getattr(tokenizer, ""original_push_to_hub"", tokenizer.push_to_hub)\
             (
                 repo_id            = save_directory,
@@ -305,6 +316,9 @@ def unsloth_save_model(
                 commit_description = commit_description,
                 tags               = tags,
             )
+
+            # Revert back padding side
+            tokenizer.padding_side = old_padding_side
         pass
 
         if hasattr(model, ""config""):
@@ -361,7 +375,16 @@ def unsloth_save_model(
 
         if tokenizer is not None:
             print(""Unsloth: Saving tokenizer..."", end = """")
+
+            # Set padding side to left for inference
+            old_padding_side = tokenizer.padding_side
+            tokenizer.padding_side = ""left""
+
             tokenizer.save_pretrained(**tokenizer_save_settings)
+
+            # Revert back padding side
+            tokenizer.padding_side = old_padding_side
+
             print("" Done."")
         else:
             print()
@@ -449,12 +472,12 @@ def unsloth_save_model(
         os.makedirs(temporary_location)
     pass
 
-    # Check if Kaggle, since only 20GB of Disk space allowed.
-    if IS_A_KAGGLE_ENVIRONMENT:
+    # Check if Kaggle or Colab, since only 20GB of Disk space allowed.
+    if IS_KAGGLE_ENVIRONMENT or IS_COLAB_ENVIRONMENT:
         # We free up 4GB of space
         logger.warning_once(
-            ""Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\n""\
-            ""model which will save 4GB of disk space, allowing you to save on Kaggle.""
+            ""Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n""\
+            ""model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.""
         )
         _free_cached_model(internal_model)
     pass
@@ -462,7 +485,10 @@ def unsloth_save_model(
     # HF also uses a OrderedDict
     from collections import OrderedDict
     state_dict = OrderedDict()
-    state_dict[""model.embed_tokens.weight""] = internal_model.model.embed_tokens.weight.data
+
+    torch_dtype = model.config.torch_dtype
+    # Check modules to save float32 dtype
+    state_dict[""model.embed_tokens.weight""] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)
 
     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)
 
@@ -495,7 +521,8 @@ def unsloth_save_model(
     pass
 
     state_dict[""model.norm.weight""] = internal_model.model.norm.weight.data
-    state_dict[""lm_head.weight""]    = internal_model.lm_head.weight.data
+    # Check for modules_to_save float32 dtype
+    state_dict[""lm_head.weight""] = internal_model.lm_head.weight.data.to(torch_dtype)
 
     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter
     for key, value in state_dict.items():
@@ -552,7 +579,16 @@ def unsloth_save_model(
     # Save tokenizer
     if tokenizer is not None:
         print(""Unsloth: Saving tokenizer..."", end = """")
+
+        # Set padding side to left for inference
+        old_padding_side = tokenizer.padding_side
+        tokenizer.padding_side = ""left""
+
         tokenizer.save_pretrained(**tokenizer_save_settings)
+
+        # Revert back padding side
+        tokenizer.padding_side = old_padding_side
+            
         print("" Done."")
     else:
         print()
@@ -1216,7 +1252,7 @@ def unsloth_save_pretrained_gguf(
     # Non blocking install GGUF first
     if not os.path.exists(""llama.cpp""):
 
-        if IS_A_KAGGLE_ENVIRONMENT:
+        if IS_KAGGLE_ENVIRONMENT:
             # Kaggle is weird - no blocking installs, and no CUDA?
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             python_install.wait()
@@ -1237,7 +1273,7 @@ def unsloth_save_pretrained_gguf(
             makefile = None
         except:
             # Retry by recloning llama.cpp
-            if IS_A_KAGGLE_ENVIRONMENT:
+            if IS_KAGGLE_ENVIRONMENT:
                 # Kaggle is weird - no blocking installs, and no CUDA?
                 python_install = install_python_non_blocking([""gguf"", ""protobuf""])
                 python_install.wait()
@@ -1336,7 +1372,7 @@ def unsloth_push_to_hub_gguf(
     # Non blocking install GGUF first
     if not os.path.exists(""llama.cpp""):
 
-        if IS_A_KAGGLE_ENVIRONMENT:
+        if IS_KAGGLE_ENVIRONMENT:
             # Kaggle is weird - no blocking installs, and no CUDA?
             python_install = install_python_non_blocking([""gguf"", ""protobuf""])
             python_install.wait()
@@ -1357,7 +1393,7 @@ def unsloth_push_to_hub_gguf(
             makefile = None
         except:
             # Retry by recloning llama.cpp
-            if IS_A_KAGGLE_ENVIRONMENT:
+            if IS_KAGGLE_ENVIRONMENT:
                 # Kaggle is weird - no blocking installs, and no CUDA?
                 python_install = install_python_non_blocking([""gguf"", ""protobuf""])
                 python_install.wait()
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 00e937c..46de1c9 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -186,9 +186,6 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):
 pass
 
 
-global sentencepiece_model_pb2
-sentencepiece_model_pb2 = None
-
 def fix_sentencepiece_tokenizer(
     old_tokenizer,
     new_tokenizer,
@@ -197,19 +194,7 @@ def fix_sentencepiece_tokenizer(
 ):
     # From https://github.com/google/sentencepiece/issues/121
     # We need to manually edit the sentencepiece tokenizer!
-    global sentencepiece_model_pb2
-    if sentencepiece_model_pb2 is None:
-        try:
-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2
-            sentencepiece_model_pb2 = _sentencepiece_model_pb2
-        except:
-            if not os.path.exists(temporary_location):
-                os.system(f""git clone https://github.com/google/sentencepiece.git {temporary_location}"")
-                os.system(f""cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto"")
-            pass
-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2
-            sentencepiece_model_pb2 = _sentencepiece_model_pb2
-        pass
+    from transformers.utils import sentencepiece_model_pb2
 
     if not os.path.exists(temporary_location):
         os.makedirs(temporary_location)
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 8d985aa..27b9228 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -342,9 +342,9 @@ def LlamaAttention_fast_forward_inference(
         A = torch_matmul(A, Vnn, out = Qn)
     else:
         if SDPA_HAS_GQA:
-            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False, enable_gqa = True)
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = is_causal, enable_gqa = True)
         else:
-            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = is_causal)
     pass
     A = A.transpose(1, 2)
     A = A.reshape(bsz, 1, attention_size)
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 07e79b1..7070524 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1458,9 +1458,10 @@ extra_eos_tokens = None,
     ollama_eos = '\n'.join(f'PARAMETER stop ""{eos}""' for eos in ollama_eos)
 
     # Ollama modelfile
+    part = '""""""'
     modelfile = 'FROM {__FILE_LOCATION__}\n\n'\
-    'TEMPLATE """"""' + system_modelfile + input_modelfile + output_modelfile + \
-    '""""""\n\n' + ollama_eos
+    'TEMPLATE ' + part + system_modelfile + input_modelfile + output_modelfile + \
+        part + '\n\n' + ollama_eos
 
     # HF Jinja Chat template
     def process(part, which, content = ""message['content']""):
@@ -1659,6 +1660,70 @@ extra_eos_tokens = None,
 pass
 
 
+# From https://www.geeksforgeeks.org/longest-common-substring-array-strings/
+# Longest Common Substring in an Array of Strings
+def _longest_common_substring(arr):
+    n = len(arr)
+    s = arr[0]
+    l = len(s)
+    res = """"
+    for i in range(l):
+        for j in range(i + 1, l + 1):
+            stem = s[i:j]
+            k = 1
+            for k in range(1, n):
+                if stem not in arr[k]:
+                    break
+            if (k + 1 == n and len(res) < len(stem)):
+                res = stem
+    return res
+pass
+
+
+def _find_common_token_ids(component, tokenizer):
+    """"""
+    \n### User:\n\n
+    \n\n### User:\n\n
+    etc
+    we need to find the middle most repeatted part.
+    Tokenizers can tokenize newlines or spaces as 1 token!
+    """"""
+    right_text = """"
+    if   component.endswith ("" ""): right_text = "" ""
+    elif component.endswith(""\n""): right_text = ""\n""
+    left_text = """"
+    if   component.startswith ("" ""): left_text = "" ""
+    elif component.startswith(""\n""): left_text = ""\n""
+    stripped = component.strip()
+
+    # Add current pieces and also newlines
+    all_input_ids = []
+    for left in range(3):
+        for right in range(3):
+            x = left*left_text + stripped + right*right_text
+            x = tokenizer(x, add_special_tokens = False).input_ids
+            all_input_ids.append(x)
+
+            x = left*""\n"" + stripped + right*""\n""
+            x = tokenizer(x, add_special_tokens = False).input_ids
+            all_input_ids.append(x)
+        pass
+    pass
+    substring = _longest_common_substring([str(x + [0]) for x in all_input_ids])
+    substring = substring.split("", "")[:-1]
+    substring = [int(x) for x in substring]
+
+    # Also get rest of tokenized string
+    original = tokenizer(component, add_special_tokens = False).input_ids
+    # Get optional left and right
+    for j in range(len(original)):
+        if original[j : j + len(substring)] == substring: break
+    optional_left  = original[:j]
+    optional_right = original[j+len(substring):]
+    return substring, optional_left, optional_right
+pass
+
+
 def train_on_responses_only(
     trainer,
     instruction_part = None,
@@ -1685,41 +1750,87 @@ def train_on_responses_only(
         response_part    = tokenizer._unsloth_output_part
     pass
 
-    instruction_ids = tokenizer(instruction_part,  add_special_tokens = False).input_ids
-    response_ids    = tokenizer(response_part, add_special_tokens = False).input_ids
+    # Get most common tokens since tokenizers can tokenize stuff differently!
+    Q_must, Q_left, Q_right = _find_common_token_ids(instruction_part, tokenizer)
+    A_must, A_left, A_right = _find_common_token_ids(response_part,    tokenizer)
 
-    instruction_length = len(instruction_ids)
-    response_length    = len(response_ids)
-    max_length = max(instruction_length, response_length)
+    # Store some temporary stuff
+    A_first = A_must[0]
+    len_A_must = len(A_must)
+    A_left_reversed = A_left[::-1]
+    A_right_forward = A_right
+
+    Q_first = Q_must[0]
+    len_Q_must = len(Q_must)
+    Q_left_reversed = Q_left[::-1]
+    Q_right_forward = Q_right
 
     def _train_on_responses_only(examples):
         input_ids_ = examples[""input_ids""]
         all_labels = []
 
         for input_ids in input_ids_:
-
-            labels = [-100] * len(input_ids)
-            m = len(input_ids) - max_length
-            first_response    = response_ids[0]
-            first_instruction = instruction_ids[0]
+            n = len(input_ids)
+            labels = [-100] * n
+            n_minus_1 = n - 1
             j = 0
-            while j < m:
-                if input_ids[j] == first_response:
-                    if input_ids[j : j+response_length] == response_ids:
-                        j = j + response_length
-                        start = j
-                        while j < m:
-                            if input_ids[j] == first_instruction and input_ids[j : j+instruction_length] == instruction_ids:
-                                j = j + instruction_length
-                                labels[start : j] = input_ids[start : j]
-                                break
-                            elif j == (m-1):
-                                j = m
-                                labels[start:] = input_ids[start:]
-                                break
+            while j < n:
+                # Find <assistant>
+                if (input_ids[j] == A_first) and \
+                    (input_ids[j : (k := j + len_A_must)] == A_must):
+
+                    # Now backtrack to get previous optional tokens
+                    for optional_left in A_left_reversed:
+                        if j < 1: break
+                        if optional_left == input_ids[j-1]: j -= 1
+                        else: break
+                    pass
+                    # And forwards look as well
+                    for optional_right in A_right_forward:
+                        if k >= n_minus_1: break
+                        if optional_right == input_ids[k+1]: k += 1
+                        else: break
+                    pass
+                    # assistant_j = j
+                    assistant_k = k
+
+                    j = assistant_k
+                    # Given <assistant>, now find next user
+                    while j < n:
+                        # Find <user>
+                        # Also accept last final item if assistant is the last turn
+                        if (j == n_minus_1) or \
+                            ((input_ids[j] == Q_first) and \
+                             (input_ids[j : (k := j + len_Q_must)] == Q_must)):
+
+                            # Now backtrack to get previous optional tokens
+                            for optional_left in Q_left_reversed:
+                                if j < 1: break
+                                if optional_left == input_ids[j-1]: j -= 1
+                                else: break
+                            pass
+                            # And forwards look as well
+                            for optional_right in Q_right_forward:
+                                if k >= n_minus_1: break
+                                if optional_right == input_ids[k+1]: k += 1
+                                else: break
+                            pass
+                            user_j = j
+                            # Account for last item
+                            if user_j != n_minus_1:
+                                # user_k = k
+                                # j = user_k
+                                j = k
+                            else:
+                                user_j = n
+                                k = n
                             pass
-                            j += 1
+                            # Now copy input_ids to labels
+                            labels[assistant_k : user_j] = input_ids[assistant_k : user_j]
+                            # print(assistant_j, assistant_k, user_j, user_k)
+                            break
                         pass
+                        j += 1
                     pass
                 pass
                 j += 1
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 23561ed..bc29c46 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -224,7 +224,6 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
         self.base = base
         # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
         self.current_rope_size = min(4 * 8192, self.max_position_embeddings)
-        print(dim, max_position_embeddings, base)
 
         # Build here to make `torch.jit.trace` work.
         self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index da3295a..4b64c74 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1159,7 +1159,8 @@ class LlamaRotaryEmbedding(torch.nn.Module):
             # [TODO] Hack to pass in config - need to remove later
             base = config.rope_theta
             partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
-            dim = int((config.hidden_size // config.num_attention_heads))
+            dim = getattr(config, ""head_dim"", None)
+            if dim is None: dim = int((config.hidden_size // config.num_attention_heads))
             device = ""cuda""
             max_position_embeddings = config.max_position_embeddings
         pass
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index ee58ca2..5f5b4e1 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -781,12 +781,12 @@ def _create_formatter(possible_columns, final_optional_prompts, user_column_name
             columns = re.findall(r""\{(.+?)\}"", optional_prompt)
             formatter += columns
             # Must escape \n \r
-            final_prompt += optional_prompt.encode(""unicode-escape"").decode(""utf-8"")
+            final_prompt += optional_prompt.encode(""unicode-escape"").decode(""utf-8"").replace(""'"", ""\\'"").replace('""', '\\""')
         else:
             where, prompt = optional_prompt
             # Strip [[...]]
             # Must escape \n \r
-            prompt = prompt[2:-2].encode(""unicode-escape"").decode(""utf-8"")
+            prompt = prompt[2:-2].encode(""unicode-escape"").decode(""utf-8"").replace(""'"", ""\\'"").replace('""', '\\""')
             columns = re.findall(r""\{(.+?)\}"", prompt)
             x = f""__optional_{j}__""
             prompt = f""{' '*8}{x} = '{prompt}'.format({', '.join(f'{x} = {x}' for x in columns)}) if {columns[0]} else ''""
@@ -842,13 +842,13 @@ def to_sharegpt(
     def __convert_to_sharegpt__(examples):
         users      = examples[merged_column_name]
         assistants = examples[output_column_name]
-        texts = []
-        for user, assistant in zip(users, assistants):
-            texts.append([
-                {""from"" : ""user"",      ""content"" : user     },
-                {""from"" : ""assistant"", ""content"" : assistant},
-            ])
-        pass
+        texts = [
+            [
+                {""from"" : ""user"",      ""content"" : str(user)     },
+                {""from"" : ""assistant"", ""content"" : str(assistant)},
+            ] \
+            for user, assistant in zip(users, assistants)
+        ]
         return { ""conversations"" : texts, }
     pass
 
@@ -1236,7 +1236,7 @@ extra_eos_tokens = None,
             partial_system = partial_system.replace(tokenizer.bos_token, """", 1)
             system_part    = system_part   .replace(tokenizer.bos_token, """", 1)
         pass
-
+        
         partial_system = \
             ""{% if messages[0]['role'] == 'system' %}""\
                 ""{{ "" + partial_system + "" }}""\
@@ -1244,7 +1244,7 @@ extra_eos_tokens = None,
         if default_system_message is not None:
             full_system = system_part.replace(""{SYSTEM}"", default_system_message)
             if ""{SYSTEM}"" in system_part:
-                modelfile += '\nSYSTEM: ""' + default_system_message + '""'
+                modelfile += '\nSYSTEM ""' + default_system_message + '""'
             pass
             partial_system += ""{% else %}""\
                 ""{{ '"" + full_system + ""' }}""\
"
"diff --git a/pyproject.toml b/pyproject.toml
index 5b9dc8b..667901e 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.8"",
+    ""unsloth_zoo>=2025.3.9"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.8"",
+    ""unsloth_zoo>=2025.3.9"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 5bbb85d..9bcdd5c 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,14 +198,19 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.8""):
-        try:
-            os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
-        except:
+    if Version(unsloth_zoo_version) < Version(""2025.3.9""):
+        print(
+            ""Unsloth: Updating Unsloth-Zoo utilies to the latest version.\n""\
+            ""To disable this, set os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'""
+        )
+        if os.environ.get(""UNSLOTH_DISABLE_AUTO_UPDATES"", ""0"") == ""0"":
             try:
-                os.system(""pip install --upgrade --no-cache-dir --no-deps --user unsloth_zoo"")
+                os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
             except:
-                raise ImportError(""Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`"")
+                try:
+                    os.system(""pip install --upgrade --no-cache-dir --no-deps --user unsloth_zoo"")
+                except:
+                    raise ImportError(""Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`"")
     import unsloth_zoo
 except:
     raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`"")
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 25fa788..50dbe7c 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.9""
+__version__ = ""2025.3.10""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -109,6 +109,9 @@ from unsloth_zoo.compiler import (
     get_transformers_model_type,
     unsloth_compile_transformers as _unsloth_compile_transformers,
 )
+from unsloth_zoo.training_utils import (
+    prepare_model_for_training,
+)
 
 # =============================================
 # Disable some warnings which can get annoying
@@ -509,67 +512,16 @@ def prepare_model_for_kbit_training(
     use_gradient_checkpointing : Optional = True,
     use_reentrant              : Optional[bool] = True,
 ) -> Any:
-    """"""
-    Calculates where to place the gradient checkpoints given n_layers.
-    We also freeze all other layers's gradients
-
-    Args:
-        model: Any LlamaModel with layers.
-        use_gradient_checkpointing (`bool`, *optional*):
-            Default enabled. Provides memory savings by not saving all activations,
-            but only some.
-        use_reentrant (`bool`, *optional*):
-            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354
-            Optimal gradient checkpointing algorithm which will be the default in
-            future Pytorch versions.
-    """"""
-
-    # Freeze all parameters except LoRA
-    with torch.no_grad():
-        for name, param in model.named_parameters():
-            if "".lora_A."" in name or "".lora_B."" in name or "".lora_magnitude_vector"" in name:
-                param.requires_grad_(True)
-                # Also must be in float32!
-                if param.dtype != torch.float32:
-                    name = name.replace(""base_model"", ""model"", 1)
-                    layer_number = re.search(r""\.[\d]{1,}\."", name).group(0)
-                    name = name.replace(layer_number, f""[{layer_number[1:-1]}]."")
-                    name = name.replace("".weight"", """", 1)
-                    exec(f""{name}.to(torch.float32)"")
-                pass
-            else:
-                param.requires_grad_(False)
-        pass
-    pass
-
-    # Gradient checkpointing!
-    if use_gradient_checkpointing == ""unsloth"":
-
-        # Saves VRAM!
-        original_model = model
-        while hasattr(original_model, ""model""):
-            original_model._offloaded_gradient_checkpointing = True
-            original_model = original_model.model
-        pass
-        original_model._offloaded_gradient_checkpointing = True
-        
-        model.gradient_checkpointing_enable()
-
-    elif use_gradient_checkpointing == True:
-        model.gradient_checkpointing_enable()
-    pass
-
-    # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.
-    if use_reentrant:
-        if hasattr(model, ""enable_input_require_grads""):
-            model.enable_input_require_grads()
-        else:
-            def make_inputs_require_grad(module, input, output):
-                output.requires_grad_(True)
-            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
-    pass
-
-    return model
+    return prepare_model_for_training(
+        model                      = model,
+        use_gradient_checkpointing = use_gradient_checkpointing,
+        use_reentrant              = use_reentrant,
+        full_finetuning            = False,
+        train_layernorms           = False,
+        train_embedding            = False,
+        train_lm_head              = False,
+        float32_mixed_precision    = True,
+    )
 pass
 
 # =============================================
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 7062c48..445658b 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -73,6 +73,8 @@ class FastLanguageModel(FastLlamaModel):
         max_seq_length             = None,
         dtype                      = None,
         load_in_4bit               = True,
+        load_in_8bit               = False,
+        full_finetuning            = False,
         token                      = None,
         device_map                 = ""sequential"",
         rope_scaling               = None,
@@ -91,6 +93,28 @@ class FastLanguageModel(FastLlamaModel):
         disable_log_stats          = True,
         *args, **kwargs,
     ):
+        if load_in_8bit or full_finetuning:
+            return FastModel.from_pretrained(
+                model_name                 = model_name,
+                max_seq_length             = max_seq_length, # [TODO] No effect
+                dtype                      = dtype,
+                load_in_4bit               = load_in_4bit,
+                load_in_8bit               = load_in_8bit,
+                token                      = token,
+                device_map                 = device_map,
+                rope_scaling               = rope_scaling, # [TODO] No effect
+                fix_tokenizer              = fix_tokenizer, # [TODO] No effect
+                trust_remote_code          = trust_remote_code,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+                resize_model_vocab         = resize_model_vocab, # [TODO] No effect
+                revision                   = revision,
+                return_logits              = return_logits, # Return logits
+                fullgraph                  = fullgraph, # No graph breaks
+                use_exact_model_name       = use_exact_model_name,
+                *args, **kwargs,
+            )
+        pass
+
         if token is None: token = get_token()
         assert (dtype is None or dtype == torch.float16 or dtype == torch.bfloat16)
 
@@ -150,7 +174,7 @@ class FastLanguageModel(FastLlamaModel):
 
         # Old transformers versions check
         both_exist = (is_model and is_peft) and not SUPPORTS_LLAMA32
-        
+
         # New transformers need to check manually.
         if SUPPORTS_LLAMA32:
             # Check if folder exists locally
@@ -261,15 +285,31 @@ class FastLanguageModel(FastLlamaModel):
             dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
-        elif model_type == ""cohere"":
-            dispatch_model = FastCohereModel
-        elif model_type == ""granite"":
-            dispatch_model = FastGraniteModel
+        # Temporary disable optimized Cohere until errors match
+        # elif model_type == ""cohere"":
+        #     dispatch_model = FastCohereModel
+        # Temporary disable optimized Granite until errors match
+        # elif model_type == ""granite"":
+        #     dispatch_model = FastGraniteModel
         else:
-            raise NotImplementedError(
-                f""Unsloth: {model_name} not supported yet!\n""\
-                ""Maybe you're doing vision finetuning? Please use FastVisionModel instead!\n""\
-                ""Otherwise, make an issue to https://github.com/unslothai/unsloth!"",
+            return FastModel.from_pretrained(
+                model_name                 = model_name,
+                max_seq_length             = max_seq_length, # [TODO] No effect
+                dtype                      = dtype,
+                load_in_4bit               = load_in_4bit,
+                load_in_8bit               = load_in_8bit,
+                token                      = token,
+                device_map                 = device_map,
+                rope_scaling               = rope_scaling, # [TODO] No effect
+                fix_tokenizer              = fix_tokenizer, # [TODO] No effect
+                trust_remote_code          = trust_remote_code,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+                resize_model_vocab         = resize_model_vocab, # [TODO] No effect
+                revision                   = revision,
+                return_logits              = return_logits, # Return logits
+                fullgraph                  = fullgraph, # No graph breaks
+                use_exact_model_name       = use_exact_model_name,
+                *args, **kwargs,
             )
         pass
 
@@ -284,6 +324,11 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         if fast_inference:
+            import platform
+            if platform.system().lower() == 'windows':
+                print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
+                fast_inference = False
+            pass
             from unsloth_zoo.vllm_utils import (
                 patch_vllm, 
                 vllm_dynamic_quant_supported,
@@ -392,6 +437,8 @@ class FastModel(FastBaseModel):
         max_seq_length             = None, # [TODO] No effect
         dtype                      = None,
         load_in_4bit               = True,
+        load_in_8bit               = False,
+        full_finetuning            = False,
         token                      = None,
         device_map                 = ""sequential"",
         rope_scaling               = None, # [TODO] No effect
@@ -413,6 +460,21 @@ class FastModel(FastBaseModel):
         if use_gradient_checkpointing == ""unsloth"":
             patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
 
+        if full_finetuning and (load_in_4bit or load_in_8bit):
+            print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
+            load_in_4bit = False
+            load_in_8bit = False
+        pass
+
+        if load_in_4bit and load_in_8bit:
+            raise RuntimeError(""Unsloth: Can only load in 4bit or 8bit, not both!"")
+        if load_in_4bit: pass
+        elif load_in_8bit: pass
+        elif not load_in_4bit and not load_in_8bit and not full_finetuning:
+            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
+            load_in_4bit = True
+        pass
+
         old_model_name = model_name
         if not use_exact_model_name:
             model_name = get_model_name(model_name, load_in_4bit)
@@ -569,6 +631,8 @@ class FastModel(FastBaseModel):
             max_seq_length    = max_seq_length,
             dtype             = _get_dtype(dtype),
             load_in_4bit      = load_in_4bit,
+            load_in_8bit      = load_in_8bit,
+            full_finetuning   = full_finetuning,
             token             = token,
             device_map        = device_map,
             trust_remote_code = trust_remote_code,
@@ -576,6 +640,7 @@ class FastModel(FastBaseModel):
             model_types       = model_types,
             tokenizer_name    = tokenizer_name,
             auto_model        = auto_model,
+            use_gradient_checkpointing = use_gradient_checkpointing,
             *args, **kwargs,
         )
         
@@ -623,7 +688,7 @@ class FastModel(FastBaseModel):
                 trust_remote_code = trust_remote_code,
             )
             # Patch it as well!
-            model = FastBaseModel.patch_peft_model(model, use_gradient_checkpointing)
+            model = FastBaseModel.post_patch_model(model, use_gradient_checkpointing)
         pass
         return model, tokenizer
     pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index a2e609f..0011521 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -492,6 +492,18 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Qwen2-VL-72B-Instruct"",
         ""Qwen/Qwen2-VL-72B-Instruct"",
     ),
+    ""unsloth/Qwen2-VL-2B-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-2B"",
+        ""Qwen/Qwen2-VL-2B"",
+    ),
+    ""unsloth/Qwen2-VL-7B-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-7B"",
+        ""Qwen/Qwen2-VL-7B"",
+    ),
+    ""unsloth/Qwen2-VL-72B-bnb-4bit"" : (
+        ""unsloth/Qwen2-VL-72B"",
+        ""Qwen/Qwen2-VL-72B"",
+    ),
     ""unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-11B-Vision-Instruct"",
         ""meta-llama/Llama-3.2-11B-Vision-Instruct"",
@@ -626,6 +638,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/QwQ-32B"",
         ""unsloth/QwQ-32B-bnb-4bit"",
     ),
+    ""unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit"" : (
+        ""unsloth/Phi-4-mini-instruct"",
+        ""microsoft/Phi-4-mini-instruct"",
+        ""unsloth/Phi-4-mini-instruct"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index f13f7ef..cf5eb9c 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -234,6 +234,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         mixed_precision = \
         ""use_bf16 = getattr(args, 'bf16', False)\n""\
         ""use_fp16 = getattr(args, 'fp16', False)\n""\
+        ""mixed_precision_dtype = os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32')\n""\
         ""dtype = getattr(model.config, 'torch_dtype', None)\n""\
         ""if dtype is None: dtype = model.get_input_embeddings().dtype\n""\
         ""from unsloth_zoo.utils import _get_dtype\n""\
@@ -241,10 +242,14 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""float16 = dtype == torch.float16\n""\
         ""if float16 and use_bf16: raise TypeError('Unsloth: Model is in float16 precision but you want to use bfloat16 precision. Set fp16 to `True` and bf16 to `False`')\n""\
         ""if not float16 and use_fp16: raise TypeError('Unsloth: Model is in bfloat16 precision but you want to use float16 precision. Set fp16 to `False` and bf16 to `True`')\n""\
-        ""if not use_bf16 and not use_fp16:\n""\
+        ""if (not use_bf16 and not use_fp16) and mixed_precision_dtype == 'float32':\n""\
         ""    args.fp16 = float16\n""\
         ""    args.bf16 = not float16\n""\
         ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'fp16' if float16 else 'bf16'\n""
+        ""elif mixed_precision_dtype == 'bfloat16':\n""\
+        ""    args.fp16 = False\n""\
+        ""    args.bf16 = False\n""\
+        ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'no'\n""
         extra_args += mixed_precision
     pass
 
@@ -280,7 +285,12 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""bf16_full_eval = getattr(args, 'bf16_full_eval', False)\n""\
         ""if args.fp16 and bf16_full_eval: args.bf16_full_eval = False; args.fp16_full_eval = True\n""\
         ""if args.bf16 and fp16_full_eval: args.bf16_full_eval = True; args.fp16_full_eval = False\n""\
-        ""if not bf16_full_eval and not fp16_full_eval: args.bf16_full_eval = args.bf16; args.fp16_full_eval = args.fp16\n""
+        ""if os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32') == 'bfloat16':\n""\
+        ""    args.bf16_full_eval = True\n""\
+        ""    args.fp16_full_eval = False\n""\
+        ""elif not bf16_full_eval and not fp16_full_eval:\n""\
+        ""    args.bf16_full_eval = args.bf16\n""\
+        ""    args.fp16_full_eval = args.fp16\n""
         extra_args += eval_changes
     pass
 
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index ff07ef6..56da240 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -35,6 +35,7 @@ from unsloth_zoo.peft_utils import (
 from triton import __version__ as triton_version
 from unsloth_zoo.utils import _get_dtype
 from unsloth_zoo.patching_utils import patch_model_and_tokenizer
+from unsloth_zoo.training_utils import prepare_model_for_training
 import types
 import functools
 
@@ -90,12 +91,15 @@ class FastBaseModel:
         max_seq_length    = None,
         dtype             = None,
         load_in_4bit      = True,
+        load_in_8bit      = False,
+        full_finetuning   = False,
         token             = None,
         device_map        = ""sequential"",
         trust_remote_code = False,
         model_types       = None,
         tokenizer_name    = None,
         auto_model        = AutoModelForVision2Seq,
+        use_gradient_checkpointing = ""unsloth"",
         **kwargs,
     ):
         if trust_remote_code:
@@ -141,6 +145,14 @@ class FastBaseModel:
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
         bnb_config = None
+        if full_finetuning and (load_in_4bit or load_in_8bit):
+            print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
+            load_in_4bit = False
+            load_in_8bit = False
+        pass
+
+        if load_in_4bit and load_in_8bit:
+            raise RuntimeError(""Unsloth: Can only load in 4bit or 8bit, not both!"")
         if load_in_4bit:
             bnb_config = BitsAndBytesConfig(
                 load_in_4bit              = True,
@@ -149,6 +161,21 @@ class FastBaseModel:
                 bnb_4bit_compute_dtype    = dtype,
                 llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
             )
+        elif load_in_8bit:
+            bnb_config = BitsAndBytesConfig(
+                load_in_8bit              = True,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+            )
+        elif not load_in_4bit and not load_in_8bit and not full_finetuning:
+            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
+            load_in_4bit = True
+        pass
+
+        if full_finetuning:
+            if dtype == torch.bfloat16:
+                print(""Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%."")
+            else:
+                print(""Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32."")
         pass
 
         kwargs.pop(""attn_implementation"", None); # No need since we auto call it
@@ -209,18 +236,29 @@ class FastBaseModel:
         while hasattr(m, ""model""):
             m._saved_temp_tokenizer = tokenizer
             # Also set is_loaded_in_8bit to disable incorrect DDP
-            m.is_loaded_in_8bit = True
+            m.is_loaded_in_8bit = True if not full_finetuning else False
             m = m.model
         pass
         m._saved_temp_tokenizer = tokenizer
         # Also set is_loaded_in_8bit to disable incorrect DDP
-        m.is_loaded_in_8bit = True
+        m.is_loaded_in_8bit = True if not full_finetuning else False
 
         # Patch generate
         if model.generate.__name__ != ""unsloth_base_fast_generate"":
             model._old_generate = model.generate
             unsloth_base_fast_generate.__doc__ = model._old_generate.__doc__
             model.generate = types.MethodType(unsloth_base_fast_generate, model)
+
+        # Post patches
+        model = FastBaseModel.post_patch_model(
+            model,
+            use_gradient_checkpointing = use_gradient_checkpointing,
+        )
+        # Clear deleted GPU items
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        pass
         return model, tokenizer
     pass
 
@@ -299,7 +337,7 @@ class FastBaseModel:
         # Enable gradients on modules which are trainable
         requires_grad_for_gradient_checkpointing(model)
 
-        model = FastBaseModel.patch_peft_model(model, use_gradient_checkpointing)
+        model = FastBaseModel.post_patch_model(model, use_gradient_checkpointing)
 
         # Clear deleted GPU items
         for _ in range(3):
@@ -316,7 +354,7 @@ class FastBaseModel:
 
 
     @staticmethod
-    def patch_peft_model(
+    def post_patch_model(
         model,
         use_gradient_checkpointing = True,
     ):
@@ -325,11 +363,22 @@ class FastBaseModel:
                 ""Unsloth: Your model needs to call `.get_peft_model` first!""
             )
         pass
+        full_finetuning = hasattr(model.config, ""quantization_config"", None) is not None
 
-        model = prepare_model_for_kbit_training(
+        float32_mixed_precision = True
+        if _get_dtype(model.config.torch_dtype) == torch.bfloat16:
+            # Use bfloat16 precision for full finetuning
+            float32_mixed_precision = False
+
+        model = prepare_model_for_training(
             model,
             use_gradient_checkpointing = use_gradient_checkpointing,
-            use_reentrant = True,
+            use_reentrant              = True,
+            full_finetuning            = full_finetuning,
+            train_layernorms           = full_finetuning,
+            train_embedding            = full_finetuning,
+            train_lm_head              = full_finetuning,
+            float32_mixed_precision    = float32_mixed_precision,
         )
 
         from transformers.trainer import Trainer 
@@ -350,14 +399,14 @@ class FastBaseModel:
                 m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
             pass
             # Also set is_loaded_in_8bit to disable incorrect DDP
-            m.is_loaded_in_8bit = True
+            m.is_loaded_in_8bit = True if not full_finetuning else False
             m = m.model
         pass
         if hasattr(m, ""_saved_temp_tokenizer""):
             m._saved_temp_tokenizer.tokenizer.padding_side = ""right""
         pass
         # Also set is_loaded_in_8bit to disable incorrect DDP
-        m.is_loaded_in_8bit = True
+        m.is_loaded_in_8bit = True if not full_finetuning else False
 
         # Clear deleted GPU items
         for _ in range(3):
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 5bd66ba..07e79b1 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -508,6 +508,200 @@ phi3_template_eos_token = ""<|end|>""
 CHAT_TEMPLATES[""phi-3""] = (phi3_template, phi3_template_eos_token, False, phi3_ollama,)
 pass
 
+# =========================================== Llama-3.1
+""""""
+No trimming in Llama 3.1 Instruct!
+Also an extra newline for Cutting Knowledge Date
+See https://colab.research.google.com/drive/1Xpqq5xpIgO-B00MQ-UccYMwN2J8QFgBM?usp=sharing
+
+Also should be
+
+import datetime
+tokenizer.apply_chat_template(
+    messages,
+    add_generation_prompt = True,
+    tokenize = False,
+    date_string = datetime.today().strftime(""%d %B %Y"")),
+)
+""""""
+
+llama31_template = \
+""""""{{- bos_token }}
+{%- if custom_tools is defined %}
+    {%- set tools = custom_tools %}
+{%- endif %}
+{%- if not tools_in_user_message is defined %}
+    {%- set tools_in_user_message = true %}
+{%- endif %}
+{%- if not date_string is defined %}
+    {%- set date_string = ""26 July 2024"" %}
+{%- endif %}
+{%- if not tools is defined %}
+    {%- set tools = none %}
+{%- endif %}
+
+{#- This block extracts the system message, so we can slot it into the right place. #}
+{%- if messages[0]['role'] == 'system' %}
+    {%- set system_message = messages[0]['content'] %}
+    {%- set messages = messages[1:] %}
+{%- else %}
+    {%- set system_message = """" %}
+{%- endif %}
+
+{#- System message + builtin tools #}
+{{- ""<|start_header_id|>system<|end_header_id|>\n\n"" }}
+{%- if builtin_tools is defined or tools is not none %}
+    {{- ""Environment: ipython\n"" }}
+{%- endif %}
+{%- if builtin_tools is defined %}
+    {{- ""Tools: "" + builtin_tools | reject('equalto', 'code_interpreter') | join("", "") + ""\n\n""}}
+{%- endif %}
+{{- ""Cutting Knowledge Date: December 2023\n"" }}
+{{- ""Today Date: "" + date_string + ""\n\n"" }}
+{%- if tools is not none and not tools_in_user_message %}
+    {{- ""You have access to the following functions. To call a function, please respond with JSON for a function call."" }}
+    {{- 'Respond in the format {""name"": function name, ""parameters"": dictionary of argument name and its value}.' }}
+    {{- ""Do not use variables.\n\n"" }}
+    {%- for t in tools %}
+        {{- t | tojson(indent=4) }}
+        {{- ""\n\n"" }}
+    {%- endfor %}
+{%- endif %}
+{{- system_message }}
+{{- ""<|eot_id|>"" }}
+
+{#- Custom tools are passed in a user message with some extra guidance #}
+{%- if tools_in_user_message and not tools is none %}
+    {#- Extract the first user message so we can plug it in here #}
+    {%- if messages | length != 0 %}
+        {%- set first_user_message = messages[0]['content'] %}
+        {%- set messages = messages[1:] %}
+    {%- else %}
+        {{- raise_exception(""Cannot put tools in the first user message when there's no first user message!"") }}
+{%- endif %}
+    {{- '<|start_header_id|>user<|end_header_id|>\n\n' -}}
+    {{- ""Given the following functions, please respond with a JSON for a function call "" }}
+    {{- ""with its proper arguments that best answers the given prompt.\n\n"" }}
+    {{- 'Respond in the format {""name"": function name, ""parameters"": dictionary of argument name and its value}.' }}
+    {{- ""Do not use variables.\n\n"" }}
+    {%- for t in tools %}
+        {{- t | tojson(indent=4) }}
+        {{- ""\n\n"" }}
+    {%- endfor %}
+    {{- first_user_message + ""<|eot_id|>""}}
+{%- endif %}
+
+{%- for message in messages %}
+    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}
+        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] + '<|eot_id|>' }}
+    {%- elif 'tool_calls' in message %}
+        {%- if not message.tool_calls|length == 1 %}
+            {{- raise_exception(""This model only supports single tool-calls at once!"") }}
+        {%- endif %}
+        {%- set tool_call = message.tool_calls[0].function %}
+        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}
+            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
+            {{- ""<|python_tag|>"" + tool_call.name + "".call("" }}
+            {%- for arg_name, arg_val in tool_call.arguments | items %}
+                {{- arg_name + '=""' + arg_val + '""' }}
+                {%- if not loop.last %}
+                    {{- "", "" }}
+                {%- endif %}
+                {%- endfor %}
+            {{- "")"" }}
+        {%- else  %}
+            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
+            {{- '{""name"": ""' + tool_call.name + '"", ' }}
+            {{- '""parameters"": ' }}
+            {{- tool_call.arguments | tojson }}
+            {{- ""}"" }}
+        {%- endif %}
+        {%- if builtin_tools is defined %}
+            {#- This means we're in ipython mode #}
+            {{- ""<|eom_id|>"" }}
+        {%- else %}
+            {{- ""<|eot_id|>"" }}
+        {%- endif %}
+    {%- elif message.role == ""tool"" or message.role == ""ipython"" %}
+        {{- ""<|start_header_id|>ipython<|end_header_id|>\n\n"" }}
+        {%- if message.content is mapping or message.content is iterable %}
+            {{- message.content | tojson }}
+        {%- else %}
+            {{- message.content }}
+        {%- endif %}
+        {{- ""<|eot_id|>"" }}
+    {%- endif %}
+{%- endfor %}
+{%- if add_generation_prompt %}
+    {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' }}
+{%- endif %}
+""""""
+pass
+
+# Ollama from https://ollama.com/library/llama3.1 (needs updating!)
+llama31_ollama = \
+'''
+FROM {__FILE_LOCATION__}
+TEMPLATE """"""{{ if .Messages }}
+{{- if or .System .Tools }}<|start_header_id|>system<|end_header_id|>
+{{- if .System }}
+
+{{ .System }}
+{{- end }}
+{{- if .Tools }}
+
+You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.
+{{- end }}
+{{- end }}<|eot_id|>
+{{- range $i, $_ := .Messages }}
+{{- $last := eq (len (slice $.Messages $i)) 1 }}
+{{- if eq .Role ""user"" }}<|start_header_id|>user<|end_header_id|>
+{{- if and $.Tools $last }}
+
+Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.
+
+Respond in the format {""name"": function name, ""parameters"": dictionary of argument name and its value}. Do not use variables.
+
+{{ $.Tools }}
+{{- end }}
+
+{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>
+
+{{ end }}
+{{- else if eq .Role ""assistant"" }}<|start_header_id|>assistant<|end_header_id|>
+{{- if .ToolCalls }}
+
+{{- range .ToolCalls }}{""name"": ""{{ .Function.Name }}"", ""parameters"": {{ .Function.Arguments }}}{{ end }}
+{{- else }}
+
+{{ .Content }}{{ if not $last }}<|eot_id|>{{ end }}
+{{- end }}
+{{- else if eq .Role ""tool"" }}<|start_header_id|>ipython<|end_header_id|>
+
+{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>
+
+{{ end }}
+{{- end }}
+{{- end }}
+{{- else }}
+{{- if .System }}<|start_header_id|>system<|end_header_id|>
+
+{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>
+
+{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>
+
+{{ end }}{{ .Response }}{{ if .Response }}<|eot_id|>{{ end }}""""""
+PARAMETER stop ""<|start_header_id|>""
+PARAMETER stop ""<|end_header_id|>""
+PARAMETER stop ""<|eot_id|>""
+PARAMETER stop ""<|eom_id|>""
+'''
+
+llama31_template_eos_token = ""eos_token""
+CHAT_TEMPLATES[""llama-3.1""] = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
+CHAT_TEMPLATES[""llama-31""]  = (llama31_template, llama31_template_eos_token, False, llama31_ollama,)
+pass
+
 
 def get_chat_template(
     tokenizer,
@@ -680,21 +874,33 @@ def get_chat_template(
         )
     pass
 
-    # For ShareGPT role -> from and content -> value
-    chat_template = chat_template\
-        .replace(""'role'"",      ""'"" + mapping[""role""]      + ""'"")\
-        .replace(""'content'"",   ""'"" + mapping[""content""]   + ""'"")\
-        .replace(""'user'"",      ""'"" + mapping[""user""]      + ""'"")\
-        .replace(""'assistant'"", ""'"" + mapping[""assistant""] + ""'"")
-
     # Careful on Gemma
     # bos_token is a must or else losses become too high
     if IS_GEMMA and not chat_template.startswith(""{{ bos_token }}""):
         chat_template = ""{{ bos_token }}"" + chat_template
     pass
 
+    # For ShareGPT role -> from and content -> value
+    new_chat_template = chat_template\
+        .replace(""'role'"",      ""'"" + mapping[""role""]      + ""'"")\
+        .replace(""'content'"",   ""'"" + mapping[""content""]   + ""'"")\
+        .replace(""'user'"",      ""'"" + mapping[""user""]      + ""'"")\
+        .replace(""'assistant'"", ""'"" + mapping[""assistant""] + ""'"")
+
     _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)
-    tokenizer.padding_side  = old_padding_side
+    tokenizer.padding_side = old_padding_side
+
+    # If not normal HF, we add a check to make old templates work
+    if mapping != {""role"" : ""role"", ""content"" : ""content"", ""user"" : ""user"", ""assistant"" : ""assistant""}:
+        chat_template = \
+            ""{% if 'role' in messages[0] %}"" + \
+            chat_template + \
+            ""{% else %}"" + \
+            new_chat_template + \
+            ""{% endif %}""
+    else:
+        chat_template = new_chat_template
+    pass
     tokenizer.chat_template = chat_template
 
     # Also fix up other tokens
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6f1bb62..6a111c9 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1873,8 +1873,17 @@ class FastLlamaModel:
                 else: modules_to_save.append(""embed_tokens"")
 
             else:
-                assert(module in accepted_modules)
-                final_modules.append(module)
+                try:
+                    assert(module in accepted_modules)
+                    final_modules.append(module)
+                except AssertionError as e:
+                    final_modules.append(module)
+                    print(
+                        ""Unsloth: You added custom modules, but Unsloth hasn't optimized for this.\n""\
+                        ""Beware - your finetuning might be noticeably slower!""
+                    )
+                pass
+            pass
         pass
 
         # Check if we added new tokens!
@@ -2253,6 +2262,8 @@ class FastLlamaModel:
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
             internal_model._saved_temp_tokenizer.padding_side = ""left""
         pass
+
+        return model
     pass
 
 
@@ -2291,6 +2302,8 @@ class FastLlamaModel:
         if hasattr(internal_model, ""_saved_temp_tokenizer""):
             internal_model._saved_temp_tokenizer.padding_side = ""right""
         pass
+
+        return model
     pass
 pass
 
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index c67f82c..9c0bc1c 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -597,8 +597,34 @@ def fix_chat_template(tokenizer):
     if chat_template is None: return None
 
     ### 1. Check if add_generation_prompt works
+    # Check for ShareGPT style first
+    is_sharegpt = None
+    try:
+        messages = [
+            {""role"": ""user"", ""content"": ""Who are you?""},
+        ]
+        tokenizer.apply_chat_template(messages, add_generation_prompt = False, tokenize = False)
+        is_sharegpt = False
+    except:
+        try:
+            messages = [
+                {""from"": ""human"", ""value"": ""Who are you?""},
+            ]
+            tokenizer.apply_chat_template(messages, add_generation_prompt = False, tokenize = False)
+            is_sharegpt = True
+        except:
+            is_sharegpt = None
+        pass
+    pass
+
+    # Not ShareGPT or HF style - just return
+    if is_sharegpt is None: return chat_template
+
+    # Tokenize
     messages = [
-        {""role"": ""user"", ""content"": ""Who are you?""},
+        {""role"": ""user"", ""content"": ""Who are you?""} \
+        if not is_sharegpt else \
+        {""from"": ""human"", ""value"": ""Who are you?""}
     ]
     no  = tokenizer.apply_chat_template(messages, add_generation_prompt = False, tokenize = False)
     yes = tokenizer.apply_chat_template(messages, add_generation_prompt =  True, tokenize = False)
"
"diff --git a/.gitignore b/.gitignore
deleted file mode 100644
index 68bc17f..0000000
--- a/.gitignore
+++ /dev/null
@@ -1,160 +0,0 @@
-# Byte-compiled / optimized / DLL files
-__pycache__/
-*.py[cod]
-*$py.class
-
-# C extensions
-*.so
-
-# Distribution / packaging
-.Python
-build/
-develop-eggs/
-dist/
-downloads/
-eggs/
-.eggs/
-lib/
-lib64/
-parts/
-sdist/
-var/
-wheels/
-share/python-wheels/
-*.egg-info/
-.installed.cfg
-*.egg
-MANIFEST
-
-# PyInstaller
-#  Usually these files are written by a python script from a template
-#  before PyInstaller builds the exe, so as to inject date/other infos into it.
-*.manifest
-*.spec
-
-# Installer logs
-pip-log.txt
-pip-delete-this-directory.txt
-
-# Unit test / coverage reports
-htmlcov/
-.tox/
-.nox/
-.coverage
-.coverage.*
-.cache
-nosetests.xml
-coverage.xml
-*.cover
-*.py,cover
-.hypothesis/
-.pytest_cache/
-cover/
-
-# Translations
-*.mo
-*.pot
-
-# Django stuff:
-*.log
-local_settings.py
-db.sqlite3
-db.sqlite3-journal
-
-# Flask stuff:
-instance/
-.webassets-cache
-
-# Scrapy stuff:
-.scrapy
-
-# Sphinx documentation
-docs/_build/
-
-# PyBuilder
-.pybuilder/
-target/
-
-# Jupyter Notebook
-.ipynb_checkpoints
-
-# IPython
-profile_default/
-ipython_config.py
-
-# pyenv
-#   For a library or package, you might want to ignore these files since the code is
-#   intended to run in multiple environments; otherwise, check them in:
-# .python-version
-
-# pipenv
-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
-#   However, in case of collaboration, if having platform-specific dependencies or dependencies
-#   having no cross-platform support, pipenv may install dependencies that don't work, or not
-#   install all needed dependencies.
-#Pipfile.lock
-
-# poetry
-#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
-#   This is especially recommended for binary packages to ensure reproducibility, and is more
-#   commonly ignored for libraries.
-#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
-#poetry.lock
-
-# pdm
-#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
-#pdm.lock
-#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
-#   in version control.
-#   https://pdm.fming.dev/#use-with-ide
-.pdm.toml
-
-# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
-__pypackages__/
-
-# Celery stuff
-celerybeat-schedule
-celerybeat.pid
-
-# SageMath parsed files
-*.sage.py
-
-# Environments
-.env
-.venv
-env/
-venv/
-ENV/
-env.bak/
-venv.bak/
-
-# Spyder project settings
-.spyderproject
-.spyproject
-
-# Rope project settings
-.ropeproject
-
-# mkdocs documentation
-/site
-
-# mypy
-.mypy_cache/
-.dmypy.json
-dmypy.json
-
-# Pyre type checker
-.pyre/
-
-# pytype static type analyzer
-.pytype/
-
-# Cython debug symbols
-cython_debug/
-
-# PyCharm
-#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
-#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
-#  and can be added to the global gitignore or merged into this file.  For a more nuclear
-#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
-#.idea/
diff --git a/LICENSE b/LICENSE
index 261eeb9..8894f17 100644
--- a/LICENSE
+++ b/LICENSE
@@ -186,7 +186,7 @@
       same ""printed page"" as the copyright notice for easier
       identification within third-party archives.
 
-   Copyright [yyyy] [name of copyright owner]
+   Copyright [2024-] [Unsloth AI, Daniel Han-Chen & Michael Han-Chen]
 
    Licensed under the Apache License, Version 2.0 (the ""License"");
    you may not use this file except in compliance with the License.
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 7d271ed..d4ca45d 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -113,3 +113,4 @@ pass
 from .models import *
 from .save import *
 from .chat_templates import *
+from .tokenizer_utils import *
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index da46972..ec62c9b 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -15,15 +15,19 @@
 __all__ = [
     ""get_chat_template"",
     ""test_chat_templates"",
-    ""fix_sentencepiece_tokenizer"",
 ]
 
 from transformers import StoppingCriteria, StoppingCriteriaList
 from torch import LongTensor, FloatTensor
 from transformers.models.llama.modeling_llama import logger
-from .models._utils import patch_tokenizer
+from .save import patch_saving_functions
 import os
 import shutil
+from .tokenizer_utils import (
+    load_correct_tokenizer,
+    fix_sentencepiece_tokenizer,
+)
+from .models._utils import patch_tokenizer
 
 CHAT_TEMPLATES = {}
 
@@ -251,84 +255,23 @@ gemma_chatml_eos_token = (
 CHAT_TEMPLATES[""gemma_chatml""] = (gemma_chatml_template, gemma_chatml_eos_token,)
 
 
-def fix_sentencepiece_tokenizer(
-    old_tokenizer,
-    new_tokenizer,
-    token_mapping,
-    temporary_location = ""_unsloth_sentencepiece_temp"",
-):
-    # From https://github.com/google/sentencepiece/issues/121
-    # We need to manually edit the sentencepiece tokenizer!
-    try:
-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2
-    except:
-        if not os.path.exists(temporary_location):
-            os.system(""git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp"")
-            os.system(f""cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto"")
-            shutil.rmtree(temporary_location)
-        pass
-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2
-    pass
-
-    if not os.path.exists(temporary_location):
-        os.makedirs(temporary_location)
-    pass
-
-    # First save the old tokenizer
-    old_tokenizer.save_pretrained(temporary_location)
-
-    from sentencepiece import SentencePieceProcessor
-    tokenizer_file = sentencepiece_model_pb2.ModelProto()
-    tokenizer_file.ParseFromString(open(f""{temporary_location}/tokenizer.model"", ""rb"").read())
-
-    # Now save the new tokenizer
-    new_tokenizer.save_pretrained(temporary_location)
-
-    # Now correct the old tokenizer's .model file
-    for old_token, new_token in token_mapping.items():
-        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids
-        ids = ids[0]
-        if (len(ids) != 1):
-            # Skip this token!
-            print(f""Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!"")
-            continue
-        pass
-        ids = ids[0]
-        tokenizer_piece = tokenizer_file.pieces[ids]
-        assert(tokenizer_piece.piece == old_token)
-        tokenizer_piece.piece = new_token
-    pass
-
-    # And now write it
-    with open(f""{temporary_location}/tokenizer.model"", ""wb"") as file:
-        file.write(tokenizer_file.SerializeToString())
-    pass
-
-    # And load it!
-    from transformers import AutoTokenizer
-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)
-    return tokenizer
-pass
-
-
 def get_chat_template(
     tokenizer,
     chat_template = ""chatml"",
     mapping = {""role"" : ""role"", ""content"" : ""content"", ""user"" : ""user"", ""assistant"" : ""assistant""},
     map_eos_token = True,
 ):
+    assert(type(map_eos_token) is bool)
     old_tokenizer = tokenizer
 
-    if map_eos_token is False:
-        assert(""Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported."")
-    pass
-
     IS_GEMMA = False
     if tokenizer.__class__.__name__.startswith(""Gemma""):
         if chat_template == ""chatml"": chat_template = ""gemma_chatml""
         IS_GEMMA = True
     pass
 
+    # We first check if the tokenizer is a fast one. If not, we cannot convert this!
+    is_fast_tokenizer = getattr(tokenizer, ""is_fast"", False)
     old_padding_side = tokenizer.padding_side
 
     if type(chat_template) in (list, tuple,):
@@ -348,9 +291,17 @@ def get_chat_template(
 
         assert(type(stop_word) is str)
 
-        # token_mapping = {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""}
-        # For Gemma :)
-        if token_mapping is not None:
+        # Check fast tokenizer
+        if not is_fast_tokenizer:
+            logger.warning_once(
+                f""Unsloth: Not a fast tokenizer, so can't process it as of yet :(\n""\
+                ""Please log a Github issue if you want this as a new feature!\n""\
+                ""Your chat template will still work, but it won't add or edit tokens.""
+            )
+
+        elif token_mapping is not None:
+            # token_mapping = {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""}
+            # For Gemma :)
 
             string_vocab = tokenizer._tokenizer.to_str()
 
@@ -368,7 +319,7 @@ def get_chat_template(
                 pass
             pass
 
-            if not stop_word in token_mapping.values():
+            if map_eos_token and (not stop_word in token_mapping.values()):
                 # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1
                 logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
@@ -376,14 +327,19 @@ def get_chat_template(
 
             if skipped != len(token_mapping):
                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
-                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+
+                if map_eos_token:
+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+                else:
+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)
+                pass
 
                 # Must fix the sentence piece tokenizer since there's no tokenizer.model file!
                 tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)
             else:
                 pass
 
-        elif stop_word != ""eos_token"":
+        elif map_eos_token and (stop_word != ""eos_token""):
             logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
 
             # Replaces the old EOS token with a new one.
@@ -393,9 +349,14 @@ def get_chat_template(
             # This is a HACK!
             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser
             string_vocab = tokenizer._tokenizer.to_str()
-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
+            old_eos_token = tokenizer.eos_token
+            string_vocab = string_vocab.replace(old_eos_token, stop_word)
             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+
+            # Must fix the sentence piece tokenizer since there's no tokenizer.model file!
+            token_mapping = { old_eos_token : stop_word, }
+            tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)
         pass
 
     else:
@@ -433,7 +394,10 @@ def get_chat_template(
     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token
     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token
 
-    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)
+    # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)
+
+    # Patch saving functions
+    tokenizer = patch_saving_functions(tokenizer)
 
     return tokenizer#, stopping_criteria
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 6f7da0f..1989313 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -60,22 +60,16 @@ from xformers import __version__ as xformers_version
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
-    ""patch_tokenizer"",
-    ""check_tokenizer"",
     ""xformers"",
     ""xformers_attention"",
     ""xformers_version"",
     ""__version__"",
     ""HAS_FLASH_ATTENTION"",
     ""platform_system"",
+    ""patch_tokenizer"",
 ]
 
 
-IGNORED_TOKENIZER_CHECKING = frozenset((
-    ""CodeLlamaTokenizerFast"",
-    ""CodeLlamaTokenizer"",
-))
-
 def prepare_model_for_kbit_training(
     model                      : Any,
     use_gradient_checkpointing : bool = True,
@@ -144,103 +138,6 @@ def patch_tokenizer(model, tokenizer):
 pass
 
 
-def check_tokenizer(
-    model,
-    tokenizer,
-    model_name = ""unsloth/llama-2-7b-bnb-4bit"",
-    model_max_length = 4096,
-    padding_side = ""right"",
-    token = None,
-    _reload = True,
-):
-    # Checks tokenizer for out of bounds ids.
-    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha
-    # where <sep> had token id=32002.
-    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25
-    # Seems like the Fast tokenizer in Rust breaks things!
-
-    # We ignore some of them!
-    if tokenizer.__repr__().split(""("", 1)[0] in IGNORED_TOKENIZER_CHECKING:
-        return tokenizer
-    pass
-
-    max_embedding_size = model.model.embed_tokens.weight.shape[0]
-    added_tokens_fast = tokenizer.added_tokens_decoder
-    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}
-    sorted_keys = sorted(added_tokens_fast)
-    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}
-
-    for j, index in enumerate(added_tokens_fast.keys()):
-        if index >= max_embedding_size:
-            bad_indices = list(added_tokens_fast.keys  ())[j:]
-            bad_tokens  = list(added_tokens_fast.values())[j:]
-
-            if not _reload:
-                # Try removing the token
-                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
-                special_tokens = tokenizer.special_tokens_map
-                import itertools
-                special_tokens = frozenset(
-                    itertools.chain.from_iterable(
-                        [x] if type(x) is str else x for x in special_tokens.values()
-                    )
-                )
-                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]
-                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]
-
-                # Check of extra tokens can in fact we removed!
-
-                if  (len(can_be_removed1) == len(bad_tokens)) and \
-                    (len(can_be_removed2) == len(bad_tokens)):
-                    # Yes it can be fixed!
-                    for bad_token in can_be_removed1:
-                        remove_id = tokenizer._added_tokens_encoder[bad_token]
-                        del tokenizer._added_tokens_decoder[remove_id]
-                        del tokenizer._added_tokens_encoder[bad_token]
-                    pass
-                    # Confirm 1 more time!
-                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:
-                        logger.warning_once(
-                            f""Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\n""\
-                            f""Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\n""\
-                            ""We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.""
-                        )
-                        return tokenizer
-                    pass
-                pass
-
-                # :( Failure
-                raise RuntimeError(
-                    f""Unsloth tried to load `{model_name}`, but cannot succeed.\n""\
-                    f""Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\n""\
-                    f""Fix your tokenizer since it'll perform out of bounds memory accesses.""
-                )
-            pass
-            
-            # Try slow tokenizer which can fix things!
-            tokenizer = AutoTokenizer.from_pretrained(
-                model_name,
-                model_max_length = model_max_length,
-                padding_side = padding_side,
-                token = token,
-                use_fast = False,
-            )
-            return check_tokenizer(
-                model = model,
-                tokenizer = tokenizer,
-                model_name = model_name,
-                model_max_length = model_max_length,
-                padding_side = padding_side,
-                token = token,
-                _reload = False,
-            )
-            break
-        pass
-    pass
-    return tokenizer
-pass
-
-
 # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??
 # For mixed precision, we need it to be in float32 not float16.
 from peft.tuners.lora.layer import LoraLayer
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6b258db..bc558c2 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -26,6 +26,7 @@ from transformers.modeling_attn_mask_utils import (
 from ..kernels import *
 from ._utils import *
 from ._utils import __version__
+from ..tokenizer_utils import *
 if HAS_FLASH_ATTENTION:
     from flash_attn import flash_attn_func
 
@@ -1014,8 +1015,8 @@ class FastLlamaModel:
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
-        tokenizer = AutoTokenizer.from_pretrained(
-            tokenizer_name,
+        tokenizer = load_correct_tokenizer(
+            tokenizer_name    = tokenizer_name,
             model_max_length  = max_position_embeddings,
             padding_side      = ""right"",
             token             = token,
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index f650d8f..9c73266 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -362,7 +362,7 @@ class FastMistralModel(FastLlamaModel):
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
-        tokenizer = AutoTokenizer.from_pretrained(
+        tokenizer = load_correct_tokenizer(
             tokenizer_name,
             model_max_length  = max_position_embeddings,
             padding_side      = ""right"",
diff --git a/unsloth/save.py b/unsloth/save.py
index a161394..a08a744 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -276,7 +276,8 @@ def unsloth_save_model(
             old_username = None, private = private,
         )
 
-        model.original_push_to_hub(
+        getattr(model, ""original_push_to_hub"", tokenizer.push_to_hub)\
+        (
             repo_id            = save_directory,
             use_temp_dir       = use_temp_dir,
             commit_message     = commit_message,
@@ -290,7 +291,8 @@ def unsloth_save_model(
             tags               = tags,
         )
         if tokenizer is not None:
-            tokenizer.original_push_to_hub(
+            getattr(tokenizer, ""original_push_to_hub"", tokenizer.push_to_hub)\
+            (
                 repo_id            = save_directory,
                 use_temp_dir       = use_temp_dir,
                 commit_message     = commit_message,
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
new file mode 100644
index 0000000..00e937c
--- /dev/null
+++ b/unsloth/tokenizer_utils.py
@@ -0,0 +1,414 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from transformers import AutoTokenizer
+from transformers.convert_slow_tokenizer import convert_slow_tokenizer
+from transformers import PreTrainedTokenizerFast
+import re
+import os
+from transformers.models.llama.modeling_llama import logger
+
+__all__ = [
+    ""load_correct_tokenizer"",
+    ""fix_sentencepiece_tokenizer"",
+    ""check_tokenizer"",
+]
+
+
+IGNORED_TOKENIZER_CHECKING = frozenset((
+    ""CodeLlamaTokenizerFast"",
+    ""CodeLlamaTokenizer"",
+))
+
+
+def try_fix_tokenizer(tokenizer, prepend = True):
+
+    if hasattr(tokenizer, ""_tokenizer""):
+        converted_tokenizer = tokenizer._tokenizer
+    else:
+        converted_tokenizer = convert_slow_tokenizer(tokenizer)
+    pass
+
+    tokenizer_string = converted_tokenizer.to_str()
+
+    # Llama does apple. Sometimes this is wrong!!
+    prepend_text = '{""type"":""Prepend"",""prepend"":""""},'
+    if not prepend and prepend_text in tokenizer_string:
+        tokenizer_string = tokenizer_string.replace(prepend_text, """", 1)
+    pass
+
+    dir_names = dir(tokenizer)
+    # Get eos_token, bos_token etc
+    token_names = [x for x in dir_names if x.endswith(""_token"") and x.count(""_"") == 1]
+
+    for token_name in token_names:
+        token = getattr(tokenizer, token_name, None)
+        if token is None: continue
+        token_id = getattr(tokenizer, token_name + ""_id"", None)
+
+        # Locate the token's id mapping in the string
+        find_text = f'""id"":{token_id},""content"":""'
+        start = tokenizer_string.find(find_text) + len(find_text)
+        if start == -1: continue
+        end   = tokenizer_string.find('"",', start)
+
+        bad_token = tokenizer_string[start : end]
+        # Check if token is the actual same one - if not, edit it
+        if bad_token != token:
+            bad_text  = f'{find_text}{bad_token}"",'
+            good_text = f'{find_text}{token}"",'
+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)
+
+            # And replace vocab section
+            bad_text = f'""{bad_token}"":{token_id},'
+            good_text = f'""{token}"":{token_id},'
+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)
+        pass
+    pass
+
+    fixed_tokenizer = converted_tokenizer.from_str(tokenizer_string)
+    return fixed_tokenizer
+pass
+
+
+def get_sorted_dict(dictionary):
+    sorted_keys = sorted(dictionary.values())
+    inverted_dictionary = { value : key for key, value in dictionary.items() }
+
+    sorted_dictionary = {}
+    for key in sorted_keys:
+        value = inverted_dictionary[key]
+        sorted_dictionary[value] = key
+    return sorted_dictionary
+pass
+
+
+def convert_to_fast_tokenizer(
+    slow_tokenizer,
+    temporary_location = ""_unsloth_sentencepiece_temp"",
+):
+    is_fast = getattr(slow_tokenizer, ""is_fast"", False)
+    if is_fast: return slow_tokenizer
+    
+    try:
+        tokenizer_name = slow_tokenizer.__class__.__name__
+        lowered_tokenizer_name = tokenizer_name.lower()
+        if lowered_tokenizer_name.endswith(""tokenizer""):
+            class_name = lowered_tokenizer_name[:-len(""tokenizer"")]
+            FastTokenizer = eval(
+                f'__import__(f""transformers.models.{class_name}"").{tokenizer_name}Fast'
+            )
+        else:
+            FastTokenizer = PreTrainedTokenizerFast
+    except:
+        FastTokenizer = PreTrainedTokenizerFast
+    pass
+
+    # Get all arguments (bos_token, etc)
+    docs = FastTokenizer.__doc__
+    docs = docs[docs.find(""Args:""):]
+    args = re.findall(r""\n[\s]+([^\s]{1,}) \("", docs, flags = re.MULTILINE)
+    args = [x for x in args if not x.endswith(""_file"")]
+
+    # Also some missing maybe!
+    docs = PreTrainedTokenizerFast.__doc__
+    docs = docs[docs.find(""Args:""):]
+    args2 = re.findall(r""\n[\s]+([^\s]{1,}) \("", docs, flags = re.MULTILINE)
+    args2 = [x for x in args2 if not x.endswith(""_file"")]
+    args = list(set(args + args2))
+
+    kwargs = {}
+    for arg in args: kwargs[arg] = getattr(slow_tokenizer, arg, None)
+    kwargs[""tokenizer_object""] = try_fix_tokenizer(slow_tokenizer, prepend = True)
+    fast_tokenizer = FastTokenizer( **kwargs )
+
+    # Check if they're similar!
+    sorted_slow_tokenizer = get_sorted_dict(slow_tokenizer.get_vocab())
+    sorted_fast_tokenizer = get_sorted_dict(fast_tokenizer.get_vocab())
+
+    check_vocab   = (sorted_slow_tokenizer == sorted_fast_tokenizer)
+    check_special = (slow_tokenizer.all_special_tokens == fast_tokenizer.all_special_tokens)
+
+    # Failure so return slow_tokenizer
+    if not check_vocab or not check_special: return slow_tokenizer
+
+    # Now confirm if they match
+    if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):
+        # Maybe remove prepending of __apple?
+        kwargs[""tokenizer_object""] = try_fix_tokenizer(slow_tokenizer, prepend = False)
+        fast_tokenizer = FastTokenizer( **kwargs )
+        if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):
+            # Failure :(
+            return slow_tokenizer
+        pass
+    pass
+
+    # Also tokenizer.model is missing!
+    name = slow_tokenizer.name_or_path.replace(""/"", ""_"")
+    if not os.path.exists(temporary_location):
+        os.makedirs(temporary_location)
+    pass
+    new_location = f""{temporary_location}/{name}""
+    slow_tokenizer.save_pretrained(new_location)
+    fast_tokenizer.save_pretrained(new_location)
+
+    # Now load it!
+    fast_tokenizer = AutoTokenizer.from_pretrained(new_location)
+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):
+        return fast_tokenizer
+    return slow_tokenizer
+pass
+
+
+def assert_same_tokenization(slow_tokenizer, fast_tokenizer):
+    # Get eos_token, bos_token etc
+    dir_names = dir(slow_tokenizer)
+    special_tokens = list(filter(None, (
+        getattr(slow_tokenizer, x) for x in dir_names
+        if x.endswith(""_token"") and x.count(""_"") == 1
+    )))
+    all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))
+    string = ""\n"".join(all_special_tokens) + \
+        ""A quick brown fox jumps over the lazy dog!!\n\n"" + \
+        """".join(all_special_tokens)
+    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids
+pass
+
+
+global sentencepiece_model_pb2
+sentencepiece_model_pb2 = None
+
+def fix_sentencepiece_tokenizer(
+    old_tokenizer,
+    new_tokenizer,
+    token_mapping,
+    temporary_location = ""_unsloth_sentencepiece_temp"",
+):
+    # From https://github.com/google/sentencepiece/issues/121
+    # We need to manually edit the sentencepiece tokenizer!
+    global sentencepiece_model_pb2
+    if sentencepiece_model_pb2 is None:
+        try:
+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2
+            sentencepiece_model_pb2 = _sentencepiece_model_pb2
+        except:
+            if not os.path.exists(temporary_location):
+                os.system(f""git clone https://github.com/google/sentencepiece.git {temporary_location}"")
+                os.system(f""cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto"")
+            pass
+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2
+            sentencepiece_model_pb2 = _sentencepiece_model_pb2
+        pass
+
+    if not os.path.exists(temporary_location):
+        os.makedirs(temporary_location)
+    pass
+
+    # First save the old tokenizer
+    old_tokenizer.save_pretrained(temporary_location)
+
+    from sentencepiece import SentencePieceProcessor
+    tokenizer_file = sentencepiece_model_pb2.ModelProto()
+    tokenizer_file.ParseFromString(open(f""{temporary_location}/tokenizer.model"", ""rb"").read())
+
+    # Now save the new tokenizer
+    new_tokenizer.save_pretrained(temporary_location)
+
+    # Now correct the old tokenizer's .model file
+    for old_token, new_token in token_mapping.items():
+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids
+        ids = ids[0]
+        if (len(ids) != 1):
+            # Skip this token!
+            print(f""Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!"")
+            continue
+        pass
+        ids = ids[0]
+        tokenizer_piece = tokenizer_file.pieces[ids]
+        assert(tokenizer_piece.piece == old_token)
+        tokenizer_piece.piece = new_token
+    pass
+
+    # And now write it
+    with open(f""{temporary_location}/tokenizer.model"", ""wb"") as file:
+        file.write(tokenizer_file.SerializeToString())
+    pass
+
+    # And load it!
+    from transformers import AutoTokenizer
+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)
+    return tokenizer
+pass
+
+
+def load_correct_tokenizer(
+    tokenizer_name,
+    model_max_length = None,
+    padding_side = ""right"",
+    token = None,
+    trust_remote_code = False,
+):
+    slow_tokenizer = AutoTokenizer.from_pretrained(
+        tokenizer_name,
+        model_max_length  = model_max_length,
+        padding_side      = padding_side,
+        token             = token,
+        trust_remote_code = trust_remote_code,
+        use_fast          = False,
+    )
+    fast_tokenizer = AutoTokenizer.from_pretrained(
+        tokenizer_name,
+        model_max_length  = model_max_length,
+        padding_side      = padding_side,
+        token             = token,
+        trust_remote_code = trust_remote_code,
+    )
+    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token
+    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token
+    
+    # Confirm if slow and fast are equivalent!
+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):
+        return fast_tokenizer
+    else:
+        return convert_to_fast_tokenizer(slow_tokenizer)
+    pass
+pass
+
+
+def check_tokenizer(
+    model,
+    tokenizer,
+    model_name = ""unsloth/llama-2-7b-bnb-4bit"",
+    model_max_length = 4096,
+    padding_side = ""right"",
+    token = None,
+    _reload = True,
+):
+    # Checks tokenizer for out of bounds ids.
+    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha
+    # where <sep> had token id=32002.
+    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25
+    # Seems like the Fast tokenizer in Rust breaks things!
+
+    # We ignore some of them!
+    if tokenizer.__repr__().split(""("", 1)[0] in IGNORED_TOKENIZER_CHECKING:
+        return tokenizer
+    pass
+
+    max_embedding_size = model.model.embed_tokens.weight.shape[0]
+    added_tokens_fast = tokenizer.added_tokens_decoder
+    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}
+    sorted_keys = sorted(added_tokens_fast)
+    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}
+
+    for j, index in enumerate(added_tokens_fast.keys()):
+        if index >= max_embedding_size:
+            bad_indices = list(added_tokens_fast.keys  ())[j:]
+            bad_tokens  = list(added_tokens_fast.values())[j:]
+            if not _reload:
+                # Try removing the token
+                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
+                special_tokens = tokenizer.special_tokens_map
+                import itertools
+                special_tokens = frozenset(
+                    itertools.chain.from_iterable(
+                        [x] if type(x) is str else x for x in special_tokens.values()
+                    )
+                )
+                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]
+                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]
+
+                # Check of extra tokens can in fact we removed!
+                can_be_removed = \
+                    (len(can_be_removed1) == len(bad_tokens)) and \
+                    (len(can_be_removed2) == len(bad_tokens))
+
+                # Check if sep_token or other generic types
+                remove_generic = False
+                try_mapper = []
+                if not can_be_removed:
+                    names = dir(tokenizer)
+                    names = (x for x in names if x.endswith(""_token"") and x.count(""_"") == 1)
+                    generic_tokens = [(x, getattr(tokenizer, x, None)) for x in names]
+
+                    try_removal = []
+                    for token in bad_tokens:
+                        for (name_token, check_token) in generic_tokens:
+                            if check_token == token:
+                                try_removal.append(token)
+                                try_mapper.append(name_token)
+                            pass
+                        pass
+                    pass
+
+                    # Recheck!
+                    can_be_removed = (len(try_removal) == len(bad_tokens))
+                    if can_be_removed: remove_generic = True
+                    can_be_removed1 = bad_tokens
+                pass
+
+                if can_be_removed:
+                    # Yes it can be fixed!
+                    for j, bad_token in enumerate(can_be_removed1):
+                        remove_id = tokenizer._added_tokens_encoder[bad_token]
+                        del tokenizer._added_tokens_decoder[remove_id]
+                        del tokenizer._added_tokens_encoder[bad_token]
+
+                        if remove_generic and (try_removal[j] == bad_token):
+                            # Remove sep token for example
+                            setattr(tokenizer, try_mapper[j], None)
+                            setattr(tokenizer, try_mapper[j] + ""_id"", None)
+                        pass
+                    pass
+                    # Confirm 1 more time!
+                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:
+                        logger.warning_once(
+                            f""Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\n""\
+                            f""Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\n""\
+                            ""We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.""
+                        )
+                        return convert_to_fast_tokenizer(tokenizer)
+                    pass
+                pass
+
+                # :( Failure
+                raise RuntimeError(
+                    f""Unsloth tried to load `{model_name}`, but cannot succeed.\n""\
+                    f""Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\n""\
+                    f""Fix your tokenizer since it'll perform out of bounds memory accesses.""
+                )
+            pass
+            
+            # Try slow tokenizer which can fix things!
+            tokenizer = AutoTokenizer.from_pretrained(
+                model_name,
+                model_max_length = model_max_length,
+                padding_side = padding_side,
+                token = token,
+                use_fast = False,
+            )
+            return check_tokenizer(
+                model = model,
+                tokenizer = tokenizer,
+                model_name = model_name,
+                model_max_length = model_max_length,
+                padding_side = padding_side,
+                token = token,
+                _reload = False,
+            )
+            break
+        pass
+    pass
+    return convert_to_fast_tokenizer(tokenizer)
+pass
"
"diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 2ed2a68..76f0a98 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -118,7 +118,7 @@ def fast_gemv(X, W, quant_state, out = None):
     if quant_state is None: return torch.matmul(X, W, out = out)
     # For fast X @ W where seq_len == 1
     # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
-    bsz, q_len, hd = X.shape
+    _, q_len, hd = X.shape
     # assert(q_len == 1)
 
     if type(quant_state) is not list:
@@ -142,10 +142,10 @@ def fast_gemv(X, W, quant_state, out = None):
     bout = shape[0]
 
     if out is None:
-        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = ""cuda"")
-    else:
-        assert(out.shape == (bsz, 1, bout,))
-    pass
+        out = torch.empty((1, 1, bout,), dtype = dtype, device = ""cuda"")
+    # else:
+    #     assert(out.shape == (1, 1, bout,))
+    # pass
 
     n = 1
     m = shape[0]
@@ -171,15 +171,9 @@ def fast_gemv(X, W, quant_state, out = None):
     fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
         cgemm_4bit_inference_naive_bf16
 
-    ptr_W      = get_ptr(W)
-    ptr_absmax = get_ptr(absmax)
-    ptr_stats  = get_ptr(stats)
-    blocksize  = ctypes.c_int32(blocksize)
-
-    for row in range(bsz):
-        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),
-           lda, ldb, ldc, blocksize)
-    pass
+    blocksize = ctypes.c_int32(blocksize)
+    fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
+       lda, ldb, ldc, blocksize)
 
     return out
 pass
@@ -189,12 +183,11 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
 
     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)
 
-    bsz, _, in_dim = X.shape
+    bsz, q_len, in_dim = X.shape
 
     if W_quant is None:
         out = torch.matmul(X, W.t(), out = out)
-    elif bsz <= 2:
-        # Only batches of 2 are faster with Gemv
+    elif bsz == 1 and q_len == 1:
         out = fast_gemv(X, W, W_quant, out = out)
     else:
         W = fast_dequantize(W.t(), W_quant)
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 6bd8a6f..0be28da 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -44,11 +44,11 @@ def fast_geglu_inference(self, X):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
-    mlp_size = self.config.intermediate_size
-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
+    # mlp_size = self.config.intermediate_size
+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
 
-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
     gate = torch_nn_functional_gelu(gate, approximate = ""tanh"")
     gate *= up
 
@@ -58,18 +58,6 @@ def fast_geglu_inference(self, X):
 pass
 
 
-def fast_rms_layernorm_inference_gemma(self, X, out_weight):
-    XX = X.to(torch.float32)
-    variance = XX.square().mean(-1, keepdim = True)
-    variance += self.variance_epsilon
-    XX *= variance.rsqrt_()
-    out_weight[:] = self.weight
-    out_weight += 1.0
-    XX *= out_weight
-    return XX.to(X.dtype)
-pass
-
-
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590
 def GemmaDecoderLayer_fast_forward(
     self,
@@ -83,19 +71,21 @@ def GemmaDecoderLayer_fast_forward(
     padding_mask:         Optional[torch.LongTensor] = None,
     *args, **kwargs,
 ):
-    if past_key_value is not None:
-        do_prefill = not hasattr(self.self_attn, ""paged_attention"")
+    if use_cache: #past_key_value is not None:
         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda"")
 
         # Self Attention
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)
-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
-            self.self_attn,
-            hidden_states,
-            past_key_value,
-            position_ids,
-            do_prefill = do_prefill,
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
         )
         hidden_states += residual
 
@@ -107,7 +97,6 @@ def GemmaDecoderLayer_fast_forward(
     else:
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)
-        # hidden_states = self.input_layernorm(hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
             hidden_states=hidden_states,
             causal_mask=causal_mask,
@@ -123,19 +112,13 @@ def GemmaDecoderLayer_fast_forward(
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)
-        # hidden_states = self.post_attention_layernorm(hidden_states)
         hidden_states = self.mlp(hidden_states)
         hidden_states = residual + hidden_states
     pass
 
     outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
     return outputs
 pass
 
@@ -148,31 +131,37 @@ def GemmaModel_fast_forward_inference(
     self,
     input_ids,
     past_key_values,
+    position_ids,
+    attention_mask = None,
 ):
-    # Fix out of bounds tokenization
-    input_ids = input_ids[:,:self.max_seq_length]
     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda"")
-
-    hidden_states = self.embed_tokens(input_ids)
+    input_ids = input_ids[:,:self.max_seq_length]
+    hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32
     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32
     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)
 
+    bsz, q_len, hd = hidden_states.shape
+    seq_len = past_key_values[0][0].shape[-2]
+    if bsz != 1:
+        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)
+    pass
+
     next_decoder_cache = []
-    for idx, decoder_layer in enumerate(self.layers):
-        # Self Attention
+    for idx, decoder_layer in enumerate(self.model.layers):
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)
         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
             decoder_layer.self_attn,
-            hidden_states,
-            past_key_values[idx],
-            None,
+            hidden_states = hidden_states,
+            past_key_value = past_key_values[idx],
+            position_ids = position_ids,
+            attention_mask = attention_mask,
+            do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
         )
         hidden_states += residual
 
-        # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)
         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)
@@ -180,13 +169,13 @@ def GemmaModel_fast_forward_inference(
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)
+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)
 
     return BaseModelOutputWithPast(
         last_hidden_state = hidden_states,
-        past_key_values   = next_decoder_cache,
-        hidden_states     = [],
-        attentions        = [],
+        past_key_values = next_decoder_cache,
+        hidden_states = [],
+        attentions = [],
     )
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bfbd10e..dc66059 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -74,7 +74,8 @@ pass
 
 
 from math import sqrt as math_sqrt
-KV_CACHE_INCREMENT = 128 # KV Cache update size
+KV_CACHE_INCREMENT = 256 # KV Cache update size
+torch_nn_functional_softmax = torch.nn.functional.softmax
 
 def LlamaAttention_fast_forward_inference(
     self,
@@ -82,6 +83,7 @@ def LlamaAttention_fast_forward_inference(
     past_key_value: Optional[Tuple[torch.Tensor]],
     position_ids,
     do_prefill = False,
+    attention_mask = None,
 ):
     """"""
         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
@@ -138,6 +140,7 @@ def LlamaAttention_fast_forward_inference(
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda"")
         self.scalar = 1.0 / math_sqrt(self.head_dim)
+        self.half_head_dim = head_dim // 2
     elif kv_seq_len >= self.paged_attention.shape[0]:
         self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))
         self.paged_attention_K = self.paged_attention[:,0]
@@ -154,17 +157,23 @@ def LlamaAttention_fast_forward_inference(
 
     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
-    cos = self.rotary_emb.cos_cached[seq_len]
-    sin = self.rotary_emb.sin_cached[seq_len]
-    h = head_dim // 2
+    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)
+    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)
+    h = self.half_head_dim
 
     RH_Q = self.RH_Q
-    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]; RH_Q[:,:,:,h:] = Qn[:,:,:,:h]; torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h]);
-    Qn *= cos; Qn.addcmul_(RH_Q, sin);
+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    Qn *= cos
+    Qn.addcmul_(RH_Q, sin)
 
     RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda"")
-    RH_K[:,:,:,:h] = Kn[:,:,:,h:]; RH_K[:,:,:,h:] = Kn[:,:,:,:h]; torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h]);
-    Kn *= cos; Kn.addcmul_(RH_K, sin);
+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]
+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]
+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    Kn *= cos
+    Kn.addcmul_(RH_K, sin)
     
     # New KV cache
     # Kn = torch.cat([K1, Kn], dim = 2)
@@ -198,10 +207,15 @@ def LlamaAttention_fast_forward_inference(
     # pass
 
     # Attention
-    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
-    A *= self.scalar
-    A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
-    A = torch.matmul(A, Vnn, out = Qn)
+    if bsz == 1:
+        A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+        A *= self.scalar
+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
+        A = torch.matmul(A, Vnn, out = Qn)
+    else:
+        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+    pass
     A = A.transpose(1, 2)
     A = A.reshape(bsz, 1, attention_size)
     A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])
@@ -214,11 +228,11 @@ def fast_swiglu_inference(self, X):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
-    mlp_size = self.config.intermediate_size
-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
+    # mlp_size = self.config.intermediate_size
+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
 
-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])
+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])
     gate = torch_nn_functional_silu(gate, inplace = True)
     gate *= up
 
@@ -240,6 +254,24 @@ def fast_rms_layernorm_inference(self, X):
 pass
 
 
+def fast_rms_layernorm_inference_gemma(self, X, out_weight = None):
+    XX = X.to(torch.float32)
+    variance = XX.square().mean(-1, keepdim = True)
+    variance += self.variance_epsilon
+    XX *= variance.rsqrt_()
+
+    if out_weight is None:
+        out_weight = self.weight + 1.0
+    else:
+        out_weight[:] = self.weight
+        out_weight += 1.0
+    pass
+
+    XX *= out_weight
+    return XX.to(X.dtype)
+pass
+
+
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320
 def LlamaAttention_fast_forward(
     self,
@@ -375,18 +407,18 @@ def LlamaDecoderLayer_fast_forward(
             (see `past_key_values`).
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
-    if past_key_value is not None:
-        do_prefill = not hasattr(self.self_attn, ""paged_attention"")
-
-        # Self Attention
+    if use_cache:
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
-            self.self_attn,
-            hidden_states,
-            past_key_value,
-            position_ids,
-            do_prefill = do_prefill,
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
         )
         hidden_states += residual
 
@@ -418,13 +450,8 @@ def LlamaDecoderLayer_fast_forward(
     pass
 
     outputs = (hidden_states,)
-
-    if output_attentions:
-        outputs += (self_attn_weights,)
-
-    if use_cache:
-        outputs += (present_key_value,)
-
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
     return outputs
 pass
 
@@ -602,9 +629,8 @@ def LlamaModel_fast_forward(
     pass
 
     for idx, decoder_layer in enumerate(self.layers):
-        if output_hidden_states:
-            all_hidden_states += (hidden_states,)
 
+        if output_hidden_states: all_hidden_states += (hidden_states,)
         past_key_value = past_key_values[idx] if past_key_values is not None else None
 
         if self.gradient_checkpointing and self.training:
@@ -636,23 +662,24 @@ def LlamaModel_fast_forward(
                 use_cache=use_cache,
                 padding_mask=padding_mask,
             )
+        pass
 
         hidden_states = layer_outputs[0]
-
-        if use_cache:
-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
-
-        if output_attentions:
-            all_self_attns += (layer_outputs[1],)
+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+        if output_attentions: all_self_attns += (layer_outputs[1],)
     pass
-    
-    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
 
-    # add hidden states from the last decoder layer
-    if output_hidden_states:
-        all_hidden_states += (hidden_states,)
+    # Final layernorm
+    if use_cache:
+        hidden_states = (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\
+            (self.norm, hidden_states)
+    else:
+        hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
+    pass
 
+    if output_hidden_states: all_hidden_states += (hidden_states,)
     next_cache = next_decoder_cache if use_cache else None
+
     if not return_dict:
         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
     return BaseModelOutputWithPast(
@@ -665,32 +692,44 @@ pass
 
 
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
-# @torch.inference_mode
 def LlamaModel_fast_forward_inference(
     self,
     input_ids,
     past_key_values,
+    position_ids,
+    attention_mask = None,
 ):
-    # Fix out of bounds tokenization
     input_ids = input_ids[:,:self.max_seq_length]
-
-    hidden_states = self.embed_tokens(input_ids)
+    hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
+    bsz, q_len, hd = hidden_states.shape
+    seq_len = past_key_values[0][0].shape[-2]
+    if bsz != 1:
+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+            sliding_window = getattr(self.config, ""sliding_window"", None),
+        )
+    else:
+        attention_mask = None
+    pass
 
     next_decoder_cache = []
-    for idx, decoder_layer in enumerate(self.layers):
-        # Self Attention
+    for idx, decoder_layer in enumerate(self.model.layers):
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)
         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
             decoder_layer.self_attn,
-            hidden_states,
-            past_key_values[idx],
-            None,
+            hidden_states = hidden_states,
+            past_key_value = past_key_values[idx],
+            position_ids = position_ids,
+            attention_mask = attention_mask,
+            do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
         )
         hidden_states += residual
 
-        # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)
@@ -698,13 +737,13 @@ def LlamaModel_fast_forward_inference(
 
         next_decoder_cache.append(present_key_value)
     pass
-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)
+    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)
 
     return BaseModelOutputWithPast(
         last_hidden_state = hidden_states,
-        past_key_values   = next_decoder_cache,
-        hidden_states     = [],
-        attentions        = [],
+        past_key_values = next_decoder_cache,
+        hidden_states = [],
+        attentions = [],
     )
 pass
 
@@ -726,11 +765,13 @@ def CausalLM_fast_forward(fast_forward_inference):
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
 
-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+        if past_key_values is not None:
             outputs = fast_forward_inference(
-                self.model,
+                self,
                 input_ids,
                 past_key_values,
+                position_ids = position_ids,
+                attention_mask = attention_mask,
             )
         else:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index c609d2e..e867cee 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(
     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
     self.model._has_no_labels = labels is None
 
-    if past_key_values is not None and \
-        hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+    if past_key_values is not None:
         outputs = LlamaModel_fast_forward_inference(
-            self.model,
+            self,
             input_ids,
             past_key_values,
+            position_ids = position_ids,
+            attention_mask = attention_mask,
         )
     else:
         outputs = self.model(
"
"diff --git a/pyproject.toml b/pyproject.toml
index 829b35a..6777f7c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -35,7 +35,7 @@ exclude = [""images*""]
 huggingface = [
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.43.1"",
+    ""transformers>=4.43.2"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -46,7 +46,8 @@ huggingface = [
     ""trl>=0.7.9,<0.9.0"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
-    ""huggingface_hub[hf_transfer]"",
+    ""huggingface_hub"",
+    ""hf-transfer"",
 ]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
@@ -188,7 +189,7 @@ colab-ampere-torch220 = [
 colab-new = [
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.43.1"",
+    ""transformers>=4.43.2"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -196,7 +197,8 @@ colab-new = [
     ""wheel>=0.42.0"",
     ""numpy"",
     ""protobuf<4.0.0"",
-    ""huggingface_hub[hf_transfer]"",
+    ""huggingface_hub"",
+    ""hf-transfer"",
 ]
 colab-no-deps = [
     ""accelerate>=0.26.1"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 4640681..db54c9a 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -60,6 +60,10 @@ except:
                       ""We have some installation instructions on our Github page."")
 pass
 
+import os, re
+import numpy as np
+import subprocess
+
 # Hugging Face Hub faster downloads (only enable during Colab and Kaggle sessions)
 keynames = ""\n"" + ""\n"".join(os.environ.keys())
 if ""\nCOLAB_""  in keynames or ""\nKAGGLE_"" in keynames:
@@ -103,11 +107,6 @@ if Version(triton.__version__) >= Version(""3.0.0""):
     except: pass
 else: from triton.common.build import libcuda_dirs
 
-import os
-import re
-import numpy as np
-import subprocess
-
 try:
     cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32
     libcuda_dirs()
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 5a267a4..994f97a 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -122,7 +122,8 @@ pass
 # =============================================
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
 import torch
-if Version(torch.__version__) < Version(""2.4.0""):
+torch_version = torch.__version__
+if Version(torch_version) < Version(""2.4.0""):
     torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
     torch_amp_custom_bwd = torch.cuda.amp.custom_bwd
 else:
@@ -134,37 +135,47 @@ pass
 # =============================================
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 import bitsandbytes as bnb
-from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
+from transformers.utils.import_utils import _is_package_available
 
 major_version, minor_version = torch.cuda.get_device_capability()
 SUPPORTS_BFLOAT16 = False
 
 if major_version >= 8:
     SUPPORTS_BFLOAT16 = True
-    try:
-        from flash_attn import flash_attn_func
+    if _is_package_available(""flash_attn""):
         # Check for CUDA linking errors ""undefined symbol: _ZNK3c106SymIntltEl""
         try:
             from flash_attn.flash_attn_interface import flash_attn_cuda
             HAS_FLASH_ATTENTION = True
         except:
-            logger.warning_once(
+            print(
                 ""Unsloth: Your Flash Attention 2 installation seems to be broken?\n""\
                 ""A possible explanation is you have a new CUDA version which isn't\n""\
                 ""yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n""\
-                ""We shall now use Xformers instead, which gets a 0.01% performance hit.\n""\
+                ""We shall now use Xformers instead, which does not have any performance hits!\n""\
                 ""We found this negligible impact by benchmarking on 1x A100.""
             )
+
+            # Stop Flash Attention from importing!
+            import transformers.utils.import_utils
+            transformers.utils.import_utils.is_flash_attn_2_available = lambda *args, **kwargs: False
+            import transformers.utils
+            transformers.utils.is_flash_attn_2_available = lambda *args, **kwargs: False
+
             HAS_FLASH_ATTENTION = False
-    except:
+        pass
+    else:
         HAS_FLASH_ATTENTION = False
 else:
     # Tri Dao's benchmark shows xformers is faster for now.
     HAS_FLASH_ATTENTION = False
 pass
-import xformers.ops.fmha as xformers
-xformers_attention = xformers.memory_efficient_attention
+
+from transformers.models.llama.modeling_llama import logger
+
+# =============================================
+# Get Xformers
 from xformers import __version__ as xformers_version
 # Temporarily disable 0.0.27 and higher - inference issues
 if Version(xformers_version) >= Version(""0.0.27""):
@@ -182,9 +193,41 @@ if Version(xformers_version) >= Version(""0.0.27""):
     )
 pass
 
+if   Version(torch_version) < Version(""2.2.0"") and Version(xformers_version) >= Version(""0.0.24""):
+    raise ImportError(
+        f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
+        f""Please install xformers < 0.0.24 for torch = {torch_version}.""
+    )
+elif Version(torch_version) < Version(""2.3.0"") and Version(xformers_version) >= Version(""0.0.26""):
+    raise ImportError(
+        f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
+        f""Please install xformers < 0.0.26 for torch = {torch_version}.""
+    )
+elif Version(torch_version) < Version(""2.4.0"") and Version(xformers_version) >= Version(""0.0.27""):
+    raise ImportError(
+        f""Unsloth: You have torch = {torch_version} but xformers = {xformers_version}.\n""\
+        f""Please install xformers < 0.0.27 for torch = {torch_version}.""
+    )
+pass
+
+from xformers._cpp_lib import _register_extensions
+try:
+    _register_extensions() # Check if C++ modules are loaded correctly
+except Exception as error:
+    raise ImportError(
+        ""Unsloth: Xformers was not installed correctly.\n""\
+        ""Please install xformers separately first.\n""\
+        ""Then confirm if it's correctly installed by running:\n""\
+        ""python -m xformers.info\n\n""
+        ""Longer error message:\n"" + str(error)
+    )
+pass
+import xformers.ops.fmha as xformers
+xformers_attention = xformers.memory_efficient_attention
+
 # Check TRL version
 from trl import __version__ as trl_version
-if Version(xformers_version) >= Version(""0.9.0""):
+if Version(trl_version) >= Version(""0.9.0""):
     raise ImportError(
         ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
         ""then press Disconnect Runtime and then Restart it.\n""\
@@ -199,8 +242,6 @@ if Version(xformers_version) >= Version(""0.9.0""):
     )
 pass
 
-# =============================================
-
 # =============================================
 # Torch compile settings
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bc434ec..6b16a4c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -18,6 +18,7 @@ from typing import Optional, Tuple, List, Union
 from ._utils import *
 from ._utils import __version__
 from torch.nn.functional import scaled_dot_product_attention
+from transformers import __version__ as transformers_version
 from transformers.models.llama.modeling_llama import (
     logger,
     BaseModelOutputWithPast,
@@ -1281,7 +1282,7 @@ class FastLlamaModel:
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers = {transformers_version}.\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 6b83b8e..e2bfe1d 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -27,7 +27,7 @@ transformers_version = Version(transformers_version)
 SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
 SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
 SUPPORTS_GEMMA2  = transformers_version >= Version(""4.42"")
-SUPPORTS_LLAMA31 = transformers_version >= Version(""4.43.1"")
+SUPPORTS_LLAMA31 = transformers_version >= Version(""4.43.2"")
 if SUPPORTS_GEMMA:
     from .gemma  import FastGemmaModel
 if SUPPORTS_GEMMA2:
@@ -35,9 +35,14 @@ if SUPPORTS_GEMMA2:
 pass
 
 
-def _get_model_name(model_name, load_in_4bit = True):
+def __get_model_name(
+    model_name,
+    load_in_4bit = True,
+    INT_TO_FLOAT_MAPPER = None,
+    FLOAT_TO_INT_MAPPER = None,
+):
 
-    if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:
+    if not SUPPORTS_FOURBIT and model_name.lower() in INT_TO_FLOAT_MAPPER:
         model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
         logger.warning_once(
             f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
@@ -46,25 +51,71 @@ def _get_model_name(model_name, load_in_4bit = True):
             f""to obtain the latest transformers build, then restart this session.\n""\
             f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
         )
+        return model_name
     
-    elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:
+    elif not load_in_4bit and model_name.lower() in INT_TO_FLOAT_MAPPER:
         new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
         #     f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
         # )
-        model_name = new_model_name
+        return new_model_name
 
-    elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:
+    elif load_in_4bit and SUPPORTS_FOURBIT and model_name.lower() in FLOAT_TO_INT_MAPPER:
         new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
         #     f""We shall load `{new_model_name}` for 4x faster loading.""
         # )
-        model_name = new_model_name
+        return new_model_name
     pass
 
-    return model_name
+    return None
+pass
+
+
+def _get_new_mapper():
+    try:
+        import requests
+        new_mapper = ""https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/models/mapper.py""
+        with requests.get(new_mapper, timeout = 3) as new_mapper: new_mapper = new_mapper.text
+        new_mapper = new_mapper[new_mapper.find(""__INT_TO_FLOAT_MAPPER""):]
+        new_mapper = new_mapper\
+            .replace(""INT_TO_FLOAT_MAPPER"", ""NEW_INT_TO_FLOAT_MAPPER"")\
+            .replace(""FLOAT_TO_INT_MAPPER"", ""NEW_FLOAT_TO_INT_MAPPER"")
+        exec(new_mapper, globals())
+        return NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER
+    except:
+        return {}, {}
+    pass
+pass
+
+
+def _get_model_name(model_name, load_in_4bit = True):
+    new_model_name = __get_model_name(
+        model_name = model_name,
+        load_in_4bit = load_in_4bit,
+        INT_TO_FLOAT_MAPPER = INT_TO_FLOAT_MAPPER,
+        FLOAT_TO_INT_MAPPER = FLOAT_TO_INT_MAPPER,
+    )
+    if new_model_name is None and model_name.count(""/"") == 1 and model_name[0].isalnum():
+        # Try checking if a new Unsloth version allows it!
+        NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER = _get_new_mapper()
+        upgraded_model_name = __get_model_name(
+            model_name = model_name,
+            load_in_4bit = load_in_4bit,
+            INT_TO_FLOAT_MAPPER = NEW_INT_TO_FLOAT_MAPPER,
+            FLOAT_TO_INT_MAPPER = NEW_FLOAT_TO_INT_MAPPER,
+        )
+        if upgraded_model_name is not None:
+            raise NotImplementedError(
+                f""Unsloth: {model_name} is not supported in your current Unsloth version! Please update Unsloth via:\n\n""\
+                'pip uninstall unsloth -y\n'\
+                'pip install --upgrade --no-cache-dir ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""'
+            )
+        pass
+    pass
+    return new_model_name if new_model_name is not None else model_name
 pass
 
 
@@ -98,16 +149,22 @@ class FastLanguageModel(FastLlamaModel):
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         was_disabled = are_progress_bars_disabled()
         disable_progress_bars()
+
+        autoconfig_error = None
+        peft_error = None
         try:
             model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
             is_model = True
-        except:
+        except Exception as autoconfig_error:
+            autoconfig_error = str(autoconfig_error)
             is_model = False
         try:
             peft_config = PeftConfig .from_pretrained(model_name, token = token, revision = revision)
             is_peft = True
-        except:
+        except Exception as peft_error:
+            peft_error = str(peft_error)
             is_peft = False
+        pass
 
         # Cannot be both!
         if is_model and is_peft:
@@ -118,11 +175,7 @@ class FastLanguageModel(FastLlamaModel):
                 ""Please separate the LoRA and base models to 2 repos.""
             )
         elif not is_model and not is_peft:
-            raise RuntimeError(
-                f""Unsloth: `{model_name}` is not a base model or a PEFT model.\n""\
-                ""We could not locate a `config.json` or `adapter_config.json` file.\n""\
-                ""Are you certain the model name is correct? Does it actually exist?""
-            )
+            raise RuntimeError(autoconfig_error or peft_error)
         pass
 
         # Get base model for PEFT:
@@ -147,8 +200,8 @@ class FastLanguageModel(FastLlamaModel):
             if scaling_type == ""llama3"" and not SUPPORTS_LLAMA31:
                 raise ImportError(
                     f""Unsloth: Your transformers version of {transformers_version} does not support Llama 3.1.\n""\
-                    f""The minimum required version is 4.43.1\n""\
-                    f'Try `pip install --upgrade ""transformers>=4.43.1""`\n'\
+                    f""The minimum required version is 4.43.2\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 )
             dispatch_model = FastLlamaModel
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7f2c2db..a31bf7b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -280,7 +280,7 @@ pass
 
 from transformers import __version__ as transformers_version
 from transformers import PretrainedConfig
-model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"", ""granite"", ""qwen3"", ""qwen3_moe""]
+model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"", ""granite"", ""qwen3"", ""qwen3_moe"", ""falcon_h1""]
 
 for model_name in model_architectures:
     config_filepath = f""transformers.models.{model_name}.configuration_{model_name}""
diff --git a/unsloth/models/falcon_h1.py b/unsloth/models/falcon_h1.py
new file mode 100644
index 0000000..df4622d
--- /dev/null
+++ b/unsloth/models/falcon_h1.py
@@ -0,0 +1,699 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .llama import *
+import os
+from ._utils import __version__
+from unsloth_zoo.utils import Version, _get_dtype
+from .llama import (
+    LlamaRotaryEmbedding,
+    LlamaLinearScalingRotaryEmbedding,
+    _LlamaModel_fast_forward_inference,
+)
+try:
+    from transformers.models.falcon_h1.modeling_falcon_h1 import (
+        FalconH1Attention,
+        FalconH1DecoderLayer,
+        FalconH1Model,
+        FalconH1ForCausalLM,
+        FalconHybridMambaAttentionDynamicCache,
+    )
+except:
+    transformers_version = Version(transformers_version)
+    if not transformers_version >= Version(""4.50.3""): #TODO: Update when transformers is updated
+        raise ImportError(
+            f""Unsloth: Your transformers version of {transformers_version} does not support FalconH1.\n""\
+            f""The minimum required version is 4.50.3.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.50.3""`\n'\
+            f""to obtain the latest transformers build, then restart this session.""\
+        )
+    pass
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
+# For Pytorch 2.1.1
+try:
+    from transformers.models.falcon_h1.modeling_falcon_h1 import (
+        FalconH1Attention,
+    )
+except:
+    FalconH1Attention   = FalconH1Attention
+pass
+
+
+def FalconH1Attention_fast_forward(
+    self,
+    hidden_states:       torch.Tensor,
+    causal_mask:         Optional[BlockDiagonalCausalMask] = None,
+    attention_mask:      Optional[torch.Tensor] = None,
+    position_ids:        Optional[torch.LongTensor] = None,
+    past_key_value:      Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:   bool = False,
+    use_cache:           bool = False,
+    padding_mask:        Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+    *args, **kwargs,
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+    
+    # Clear inference
+    if hasattr(self, ""paged_attention""):
+        del self.paged_attention_K
+        del self.paged_attention_V
+        del self.paged_attention
+        del self.temp_QA
+        del self.temp_KV
+        del self.RH_Q
+        del self.attention
+    pass
+
+    bsz, q_len, _ = hidden_states.size()
+
+    n_heads    = self.config.num_attention_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.config.num_key_value_heads
+    head_dim   = self.head_dim
+    assert(n_kv_heads * n_groups == n_heads)
+
+    Q, K, V = self.apply_qkv(self, hidden_states)
+    Q = Q.view(bsz, q_len, n_heads,    head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    K = K.view(bsz, q_len, n_kv_heads, head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+
+    # Falcon H1 multiplies key states by a multiplier
+    K = K * self.config.key_multiplier
+
+    Q = Q.transpose(1, 2)
+    K = K.transpose(1, 2)
+
+    kv_seq_len = K.shape[-2]
+    if past_key_value is not None:
+        kv_seq_len += past_key_value[0].shape[-2]
+
+    if position_embeddings:
+        cos, sin = position_embeddings
+    else:
+        # Extend RoPE dynamically to fit in VRA
+        rotary_emb = self.rotary_emb
+        rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
+
+        if position_ids is None:
+            # Useful for LongRoPE
+            cos, sin = rotary_emb.get_cached(kv_seq_len)
+        else:
+            cos, sin = rotary_emb(V, seq_len = kv_seq_len)
+    Q, K = fast_rope_embedding(Q, K, cos, sin)
+
+    if past_key_value is not None:
+        K = torch.cat([past_key_value[0], K], dim = 2)
+        V = torch.cat([past_key_value[1], V], dim = 2)
+    pass
+    past_key_value = (K, V) if use_cache else None
+
+    # Attention module
+    if (not HAS_FLASH_ATTENTION and attention_mask is None):
+        # Xformers memory efficient attention
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+        K_M = V_M = bsz * kv_seq_len
+        Q_M = bsz * q_len
+
+        # Group query attention
+        K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+        V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+        K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+        V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+        if hidden_states.requires_grad:
+            K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)
+            V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)
+        else:
+            # Xformers does support the forward pass though
+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        pass
+
+        A = xformers_attention(Q, K, V, attn_bias = causal_mask)
+        A = A.view(bsz, q_len, n_heads, head_dim)
+
+    elif HAS_FLASH_ATTENTION and attention_mask is None:
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+        sw = kv_seq_len
+        window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)
+        A = flash_attn_func(Q, K, V, causal = True, window_size = window)
+    else:
+        # Grouped query attention
+        # if n_groups != 1:
+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+        K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
+        V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
+        # pass
+        # Must be contiguous or else results are False!
+        # https://github.com/pytorch/pytorch/issues/112577
+        Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
+        # Needs (batch_size, n_heads, seq_len, head_dim)
+        # is_casual and attention_mask must not be both set!
+        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)
+        # Go back to (batch_size, seq_len, n_heads, head_dim)
+        A = A.transpose(1, 2).contiguous()
+    pass
+
+    attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
+    attn_output = self.apply_o(self, attn_output)
+    attn_weights = None
+    return attn_output, attn_weights, past_key_value
+pass
+
+torch_matmul = torch.matmul
+def FalconH1Attention_fast_forward_inference(
+    self,
+    hidden_states:  torch.Tensor,
+    past_key_value: Optional[Tuple[torch.Tensor]],
+    position_ids,
+    do_prefill = False,
+    attention_mask = None,
+):
+    """"""
+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
+        Fast inference using KV cache.
+        QK^T can be computed in 4 chunks
+
+        [Q, q] @ [K, k].T where q, k are the new tokens.
+        [QK^T, Qk^T]
+        [qK^T, qk^T]
+
+        Since the attention mask wipes Qk^T, we just get
+        [QK^T,    0]
+        [qK^T, qk^T]
+
+        Since softmax is row-wise, we get
+        softmax([QK^T,    0])
+        softmax([qK^T, qk^T])
+
+        We then multiply by   [V]
+                              [v]
+        softmax([QK^T,    0]) [softmax(QK^T)V] *
+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]
+
+        But notice * [softmax(QK^T)V] is just the last attention.
+        We just need to compute the last final row.
+
+        This means we can pass in a row of Q, but we need to
+        remember K and V, which are called the KV cache.
+    """"""
+    Xn = hidden_states
+    bsz, _, hd = hidden_states.size()
+    K1, V1 = past_key_value
+    dtype = Xn.dtype
+
+    n_heads    = self.config.num_attention_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.config.num_key_value_heads
+    head_dim   = self.head_dim
+    # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
+    seq_len = K1.shape[-2]
+    kv_seq_len = seq_len + 1
+
+    # Prefill phase
+    # if not hasattr(self, ""paged_attention""):
+    device = hidden_states.device
+    if do_prefill:
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = device)
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
+        self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
+        
+        # Mistral Nemo 12b has weird dimensions
+        if attention_size != hidden_size:
+            self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = device)
+        else:
+            self.temp_O = self.temp_QA[1][:,:,:hidden_size]
+        pass
+        
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
+        self.scalar = 1.0 / math_sqrt(self.head_dim)
+        self.half_head_dim = head_dim // 2
+    elif kv_seq_len >= self.paged_attention.shape[0]:
+        self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.attention.resize_((bsz, n_heads, 1, self.attention.shape[-1]+KV_CACHE_INCREMENT))
+    pass
+
+    Qn = fast_linear_forward(self.q_proj, Xn, out = self.temp_QA[0])
+    Kn = fast_linear_forward(self.k_proj, Xn, out = self.temp_KV[0])
+    Kn = Kn * self.config.key_multiplier
+    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])
+    Qn = Qn.view(bsz, 1, n_heads,    head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+
+    Qn = Qn.transpose(1, 2)
+    Kn = Kn.transpose(1, 2)
+
+    # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
+    # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+
+    # Need to do it prior 2 steps before hitting full on short KV cache
+    # or else error
+    self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos = cos[position_ids].unsqueeze(1)
+    sin = sin[position_ids].unsqueeze(1)
+    h = self.half_head_dim
+
+    RH_Q = self.RH_Q
+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
+    RH_Q[:,:,:,:h].neg_() # torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    Qn *= cos
+    Qn.addcmul_(RH_Q, sin)
+
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]
+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]
+    RH_K[:,:,:,:h].neg_() #torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    Kn *= cos
+    Kn.addcmul_(RH_K, sin)
+    
+    # New KV cache
+    # Kn = torch.cat([K1, Kn], dim = 2)
+    # Vn = torch.cat([V1, Vn], dim = 2)
+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+
+    # Handle sliding windows
+    sliding_window = getattr(self.config, ""sliding_window"", None)
+    if sliding_window is not None and kv_seq_len > sliding_window:
+        # From https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L193
+        slicing_tokens = 1 - sliding_window
+        Knn = Kn[:, :, slicing_tokens:, :]#.contiguous()
+        Vnn = Vn[:, :, slicing_tokens:, :]#.contiguous()
+    else:
+        Knn, Vnn = Kn, Vn
+    pass
+
+    # Grouped query attention
+    _, _, cached_len, _ = Knn.shape
+    if bsz == 1 or not SDPA_HAS_GQA and n_groups != 1:
+        Knn = Knn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vnn = Vnn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)
+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)
+    pass
+    # else:
+    #     Knn, Vnn = Knn, Vnn
+    # pass
+
+    # Attention
+    if bsz == 1:
+        Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
+        # It seems like doing (Q * scalar) @ K is better than (Q @ K) * scalar to stop overflows
+        A = torch_matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
+        A = torch_matmul(A, Vnn, out = Qn)
+    else:
+        if SDPA_HAS_GQA:
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False, enable_gqa = True)
+        else:
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+    pass
+    A = A.transpose(1, 2)
+    A = A.reshape(bsz, 1, attention_size)
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_O)
+    return A, (Kn, Vn)
+pass
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_h1/modeling_falcon_h1.py
+def FalconH1DecoderLayer_fast_forward(
+    self,
+    hidden_states:       torch.Tensor,
+    causal_mask          = None,
+    attention_mask:      Optional[torch.Tensor] = None,
+    mamba_attention_mask:      Optional[torch.Tensor] = None,
+    position_ids:        Optional[torch.LongTensor] = None,
+    cache_position:        Optional[torch.LongTensor] = None,
+    past_key_value:      Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:   Optional[bool] = False,
+    use_cache:           Optional[bool] = False,
+    padding_mask:        Optional[torch.LongTensor] = None,
+    position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
+    *args, **kwargs,
+) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
+    """"""
+    Args:
+        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
+        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
+            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
+        output_attentions (`bool`, *optional*):
+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under
+            returned tensors for more detail.
+        use_cache (`bool`, *optional*):
+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
+            (see `past_key_values`).
+        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
+    """"""
+    if use_cache and hasattr(self, ""_flag_for_generation""):
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
+        attention_hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states       = hidden_states,
+            causal_mask         = causal_mask,
+            attention_mask      = attention_mask,
+            position_ids        = position_ids,
+            past_key_value      = past_key_value,
+            output_attentions   = output_attentions,
+            use_cache           = use_cache,
+            padding_mask        = padding_mask,
+            position_embeddings = position_embeddings,
+        )
+        attention_hidden_states = attention_hidden_states * self.attn_out_multiplier
+
+        mamba_hidden_states = self.mamba(
+            hidden_states=hidden_states,
+            cache_params=past_key_value,
+            cache_position=cache_position,
+            attention_mask=mamba_attention_mask,
+        )
+        mamba_hidden_states = mamba_hidden_states * self.ssm_out_multiplier
+
+        hidden_states = mamba_hidden_states + attention_hidden_states
+
+        hidden_states += residual
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_swiglu_inference(self.mlp, hidden_states)
+        hidden_states += residual
+    else:
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+
+        mamba_hidden_states = self.mamba(
+            hidden_states=hidden_states,
+            cache_params=past_key_value,
+            cache_position=cache_position,
+            attention_mask=mamba_attention_mask,
+        )
+        mamba_hidden_states = mamba_hidden_states * self.ssm_out_multiplier
+
+        attention_hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states       = hidden_states,
+            causal_mask         = causal_mask,
+            attention_mask      = attention_mask,
+            position_ids        = position_ids,
+            past_key_value      = past_key_value,
+            output_attentions   = output_attentions,
+            use_cache           = use_cache,
+            padding_mask        = padding_mask,
+            position_embeddings = position_embeddings,
+        )
+        attention_hidden_states = attention_hidden_states * self.attn_out_multiplier
+
+        hidden_states = mamba_hidden_states + attention_hidden_states
+
+        # residual connection after attention + Mamba
+        hidden_states = residual + hidden_states
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.pre_ff_layernorm, hidden_states)
+        hidden_states = self.feed_forward(hidden_states)
+        hidden_states = residual + hidden_states
+    pass
+
+    outputs = (hidden_states,)
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
+    return outputs
+pass
+
+def _FalconH1_fast_forward_inference(attention_fast_forward_inference=FalconH1Attention_fast_forward_inference, mlp_fast_forward_inference=fast_swiglu_inference):
+    # This makes the attention and MLP customisable.
+    # Now for models like qwen3 or cohere which use custom attention operations, we can use this function
+    def FalconH1Model_fast_forward_inference_custom(
+        self,
+        input_ids,
+        past_key_values,
+        position_ids,
+        cache_position = None,
+        attention_mask = None,
+        mamba_attention_mask = None,
+    ):
+        input_ids = input_ids[:,:self.max_seq_length]
+        bsz, q_len = input_ids.shape
+        hd = self.config.hidden_size
+        mlp_size = self.config.intermediate_size
+        gate_multiplier, down_multiplier = self.config.mlp_multipliers
+
+        X = self.model.embed_tokens(input_ids)
+        X = X * self.config.embedding_multiplier
+
+        X = X.to(_get_dtype(self.config.torch_dtype))
+        bsz, q_len, hd = X.shape
+        assert(q_len == 1)
+        # Get saved buffers to reduce memory movement
+        residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+        _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+        XX, XX2 = _XX[0], _XX[1]
+        variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = ""cuda:0"")
+        temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
+        temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
+        seq_len = past_key_values[0][0].shape[-2]
+        if bsz != 1:
+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask,
+                (bsz, q_len),
+                X,
+                seq_len,
+                sliding_window = getattr(self.config, ""sliding_window"", None),
+            )
+        else:
+            attention_mask = None
+        pass
+
+        next_decoder_cache = []
+
+        for idx, decoder_layer in enumerate(self.model.layers):
+            residual.copy_(X) # residual = X
+            X = fast_rms_layernorm_inference(
+                decoder_layer.input_layernorm,
+                X,
+                XX = XX,
+                XX2 = XX2,
+                variance = variance,
+            )
+            attention_hidden_states, present_key_value = attention_fast_forward_inference(
+                decoder_layer.self_attn,
+                hidden_states = X * decoder_layer.attention_in_multiplier,
+                past_key_value = past_key_values[idx],
+                position_ids = position_ids,
+                attention_mask = attention_mask,
+                do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
+            )
+            attention_hidden_states = attention_hidden_states * decoder_layer.attention_out_multiplier
+            mamba_hidden_states = decoder_layer.mamba(
+                hidden_states=X,
+                cache_params=present_key_value,
+                cache_position=cache_position,
+                attention_mask=mamba_attention_mask,
+            )
+            mamba_hidden_states = mamba_hidden_states * decoder_layer.ssm_out_multiplier
+            X = mamba_hidden_states + attention_hidden_states
+
+            X += residual
+
+            residual.copy_(X) # residual = X
+            X = fast_rms_layernorm_inference(
+                decoder_layer.pre_ff_layernorm,
+                X,
+                XX = XX,
+                XX2 = XX2,
+                variance = variance,
+            )
+            X = mlp_fast_forward_inference(
+                decoder_layer.feed_forward,
+                X,
+                temp_gate = temp_gate,
+                temp_up = temp_up,
+                gate_multiplier = gate_multiplier,
+                down_multiplier = down_multiplier
+            )
+            X += residual
+
+            next_decoder_cache.append(present_key_value)
+        pass
+        X = fast_rms_layernorm_inference(
+            self.model.final_layernorm,
+            X,
+            XX = XX,
+            XX2 = XX2,
+            variance = variance,
+        )
+
+        return BaseModelOutputWithPast(
+            last_hidden_state = X,
+            past_key_values = next_decoder_cache,
+            hidden_states = [],
+            attentions = [],
+        )
+    pass
+    return FalconH1Model_fast_forward_inference_custom
+
+#Separate prepare_inputs_for_generation for Hybrid FalconH1
+def _fast_prepare_inputs_for_generation(
+    self,
+    input_ids,
+    past_key_values=None,
+    attention_mask=None,
+    inputs_embeds=None,
+    cache_position=None,
+    position_ids=None,
+    use_cache=True,
+    **kwargs,):
+    # Overwitten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`
+    empty_past_kv = past_key_values is None
+    
+    # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens
+    # Exception 1: when passing input_embeds, input_ids may be missing entries
+    # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here
+    # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.
+    #              (we can't check exception 3 while compiling)
+    if not empty_past_kv:
+        if (
+            inputs_embeds is not None  # Exception 1
+            or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3
+        ):
+            input_ids = input_ids[:, -cache_position.shape[0] :]
+        elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the ""else"", a no op, is Exception 2)
+            input_ids = input_ids[:, cache_position]
+    else:
+        past_key_values = FalconHybridMambaAttentionDynamicCache(
+            self.config,
+            input_ids.shape[0],
+            self.dtype,
+            devices=[
+                self.model.layers[i].mamba.conv1d.weight.device for i in range(self.config.num_hidden_layers)
+            ],
+        )
+
+    if attention_mask is not None and position_ids is None:
+        # create position_ids on the fly for batch generation
+        position_ids = attention_mask.long().cumsum(-1) - 1
+        position_ids.masked_fill_(attention_mask == 0, 1)
+        if not empty_past_kv:
+            position_ids = position_ids[:, -input_ids.shape[1] :]
+
+    # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
+    if inputs_embeds is not None and empty_past_kv:
+        model_inputs = {""inputs_embeds"": inputs_embeds}
+    else:
+        model_inputs = {""input_ids"": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases
+
+    model_inputs.update(
+        {
+            ""position_ids"": position_ids,
+            ""past_key_values"": past_key_values,
+            ""use_cache"": use_cache,
+            ""attention_mask"": attention_mask,
+            ""logits_to_keep"": self.config.num_logits_to_keep,
+            ""cache_position"": cache_position,
+        }
+    )
+    return model_inputs
+pass
+
+
+def fix_prepare_inputs_for_generation(module):
+    # Fix prepare_inputs_for_generation
+    if hasattr(module, ""prepare_inputs_for_generation""):
+            module.prepare_inputs_for_generation = _fast_prepare_inputs_for_generation
+    pass
+pass
+
+class FastFalconH1Model(FastLlamaModel):
+
+    @staticmethod
+    def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""FalconH1"",
+            rope_module        = LlamaRotaryEmbedding,
+            scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
+            attention_module   = FalconH1Attention,
+        )
+        if init_name is not None:
+            exec(function, globals())
+            FalconH1Attention.__init__  = eval(init_name)
+        pass
+        FalconH1Attention      .forward = FalconH1Attention_fast_forward
+        FalconH1DecoderLayer   .forward = FalconH1DecoderLayer_fast_forward
+        FalconH1Model          .forward = LlamaModel_fast_forward
+        FalconH1ForCausalLM    .forward = CausalLM_fast_forward(_FalconH1_fast_forward_inference(FalconH1Attention_fast_forward_inference))
+        PeftModelForCausalLM.forward = PeftModel_fast_forward
+        fix_prepare_inputs_for_generation(FalconH1ForCausalLM)
+
+        # Solves https://github.com/unslothai/unsloth/issues/168
+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+        # https://github.com/huggingface/transformers/pull/27931
+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+        import transformers.models.falcon_h1.modeling_falcon_h1
+        transformers.models.falcon_h1.modeling_falcon_h1.FalconH1RotaryEmbedding = LlamaRotaryEmbedding
+        return
+    pass
+
+
+    @staticmethod
+    def from_pretrained(  #TODO: Change after release
+        model_name        = ""Qwen/FalconH1-7B"",
+        max_seq_length    = 4096,
+        dtype             = None,
+        load_in_4bit      = True,
+        token             = None,
+        device_map        = ""sequential"",
+        rope_scaling      = None,
+        fix_tokenizer     = True,
+        model_patcher     = None,
+        tokenizer_name    = None,
+        trust_remote_code = False,
+        **kwargs,
+    ):
+        return FastLlamaModel.from_pretrained(
+            model_name        = model_name,
+            max_seq_length    = max_seq_length,
+            dtype             = dtype,
+            load_in_4bit      = load_in_4bit,
+            token             = token,
+            device_map        = device_map,
+            rope_scaling      = rope_scaling,
+            fix_tokenizer     = fix_tokenizer,
+            model_patcher     = FastFalconH1Model,
+            tokenizer_name    = tokenizer_name,
+            trust_remote_code = trust_remote_code,
+            **kwargs,
+        )
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index c83de8f..bc46ba1 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -340,7 +340,7 @@ pass
 
 
 torch_nn_functional_silu = torch.nn.functional.silu
-def fast_swiglu_inference(self, X, temp_gate = None, temp_up = None):
+def fast_swiglu_inference(self, X, temp_gate = None, temp_up = None, gate_multiplier = None, down_multiplier = None):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
@@ -348,12 +348,21 @@ def fast_swiglu_inference(self, X, temp_gate = None, temp_up = None):
     # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
 
     gate = fast_linear_forward(self.gate_proj, X, out = temp_gate)
+    
+    if gate_multiplier is not None:
+        gate *= gate_multiplier
+    
     up   = fast_linear_forward(self.  up_proj, X, out = temp_up)
+
     gate = torch_nn_functional_silu(gate, inplace = True)
     gate *= up
 
     # X = self.down_proj(gate)
     down = fast_linear_forward(self.down_proj, gate, out = up[:,:,:hd])
+
+    if down_multiplier is not None:
+        down *= down_multiplier
+
     return down
 pass
 
@@ -716,6 +725,7 @@ def LlamaModel_fast_forward(
     IS_GEMMA2  = self.config.model_type.startswith(""gemma2"")
     IS_COHERE  = self.config.model_type.startswith(""cohere"")
     IS_GRANITE = self.config.model_type.startswith(""granite"")
+    IS_FALCON_H1 = self.config.model_type.startswith(""falcon_h1"")
 
     train_embed_tokens = self.embed_tokens.weight.requires_grad
 
@@ -786,8 +796,8 @@ def LlamaModel_fast_forward(
     pass
 
     hidden_states = inputs_embeds
-    if IS_GRANITE: #granite has embedding multiplier
-        hidden_states = self.embedding_multiplier * hidden_states
+    if IS_GRANITE or IS_FALCON_H1: #granite has embedding multiplier
+        hidden_states = self.config.embedding_multiplier * hidden_states
 
     if past_key_values is None and self.training:
         use_cache = False
@@ -942,11 +952,16 @@ def LlamaModel_fast_forward(
 
     # Final layernorm
     if use_cache:
-        hidden_states = \
-            (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\
-            (self.norm, hidden_states)
+        if IS_FALCON_H1:
+            hidden_states = fast_rms_layernorm_inference(self.final_layernorm, hidden_states)
+        else:
+            hidden_states = \
+                (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\
+                (self.norm, hidden_states)
     elif IS_COHERE:
         hidden_states = self.norm(hidden_states)
+    elif IS_FALCON_H1:
+        hidden_states = fast_rms_layernorm(self.final_layernorm, hidden_states, gemma = IS_GEMMA)
     else:
         hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
     pass
@@ -1155,6 +1170,10 @@ def CausalLM_fast_forward(fast_forward_inference):
             if not RETURN_LOGITS and HAS_CUT_CROSS_ENTROPY and labels is not None:
 
                 n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
+
+                if self.config.model_type == ""falcon_h1"":
+                    hidden_states = hidden_states * self.config.lm_head_multiplier
+
                 loss = fused_linear_cross_entropy(
                     hidden_states      = hidden_states,
                     lm_weight          = lm_head,
@@ -1188,6 +1207,8 @@ def CausalLM_fast_forward(fast_forward_inference):
             # granite: https://github.com/huggingface/transformers/blob/4d1d0f29a493098e6bc6b904b82e29cb331827f5/src/transformers/models/granite/modeling_granite.py#L1103
             # cohere: https://github.com/huggingface/transformers/blob/4d1d0f29a493098e6bc6b904b82e29cb331827f5/src/transformers/models/cohere/modeling_cohere.py#L1176
             logit_scaling = 1 / getattr(self.config, ""logits_scaling"", 1)
+        elif self.config.model_type == ""falcon_h1"":
+            logit_scaling = self.config.lm_head_multiplier
 
         if labels is not None:
             shift_logits = logits
@@ -1814,6 +1835,8 @@ class FastLlamaModel:
 
         # Check if RoPE Scaling is even allowed
         model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]
+        IS_FALCON_H1 = model_config.model_type.startswith(""falcon_h1"")
+
         has_rope_scaling = False
         try:
             with open(inspect.getfile(model_function), ""r"") as file:
@@ -1856,12 +1879,16 @@ class FastLlamaModel:
 
         bnb_config = None
         if load_in_4bit:
+            llm_int8_skip_modules =  SKIP_QUANTIZATION_MODULES.copy()
+            if IS_FALCON_H1:
+                # we cannot quantize out_proj layer due to mamba kernels: https://github.com/tiiuae/Falcon-H1/issues/13#issuecomment-2918671274
+                llm_int8_skip_modules.append(""out_proj"")
             bnb_config = BitsAndBytesConfig(
                 load_in_4bit              = True,
                 bnb_4bit_use_double_quant = True,
                 bnb_4bit_quant_type       = ""nf4"",
                 bnb_4bit_compute_dtype    = dtype,
-                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
+                llm_int8_skip_modules     = llm_int8_skip_modules,
             )
         pass
 
@@ -2607,6 +2634,7 @@ class FastLlamaModel:
         elif model_type == ""cohere"":  apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""granite"": apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""qwen3"":   apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""falcon_h1"":   apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""qwen3moe"":   apply_lora_mlp = apply_lora_mlp_swiglu
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 34bf05c..7800ec2 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -26,6 +26,7 @@ from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
 from .qwen3   import FastQwen3Model
 from .qwen3_moe import FastQwen3MoeModel
+from .falcon_h1 import FastFalconH1Model
 from .cohere  import FastCohereModel
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
@@ -56,7 +57,9 @@ SUPPORTS_LLAMA32   = transformers_version  > Version(""4.45.0"")
 SUPPORTS_GRANITE   = transformers_version >= Version(""4.46.0"")
 SUPPORTS_QWEN3     = transformers_version >= Version(""4.50.3"")
 SUPPORTS_QWEN3_MOE = transformers_version >= Version(""4.50.3"")
+SUPPORTS_FALCON_H1 = transformers_version >= Version(""4.53.0"")
 SUPPORTS_GEMMA3N   = transformers_version >= Version(""4.53.0"")
+
 if SUPPORTS_GEMMA:
     from .gemma  import FastGemmaModel
 if SUPPORTS_GEMMA2:
@@ -129,6 +132,8 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         if token is None: token = get_token()
+        if isinstance(dtype, str) and dtype in [""float16"", ""bfloat16""]:
+            dtype = getattr(torch, dtype)
         assert (dtype is None or dtype == torch.float16 or dtype == torch.bfloat16)
 
         if use_gradient_checkpointing == ""unsloth"":
@@ -313,6 +318,15 @@ class FastLanguageModel(FastLlamaModel):
                     f""to obtain the latest transformers build, then restart this session.""\
                 )
             dispatch_model = FastQwen3Model if model_type == ""qwen3"" else FastQwen3MoeModel
+        elif model_type == ""falcon_h1"":
+            dispatch_model = FastFalconH1Model
+            if not SUPPORTS_FALCON_H1:
+                raise ImportError(
+                    f""Unsloth: Your transformers version of {transformers_version} does not support FalconH1.\n""\
+                    f""The minimum required version is 4.50.3.\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.50.3""`\n'\
+                    f""to obtain the latest transformers build, then restart this session.""\
+                )
         # Temporary disable optimized Cohere until errors match
         # elif model_type == ""cohere"":
         #     dispatch_model = FastCohereModel
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ff2c872..2c16bf6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -285,7 +285,11 @@ if major_version >= 8:
     if _is_package_available(""flash_attn""):
         # Check for CUDA linking errors ""undefined symbol: _ZNK3c106SymIntltEl""
         try:
-            from flash_attn.flash_attn_interface import flash_attn_cuda
+            try:
+                # See https://github.com/unslothai/unsloth/issues/1437
+                from flash_attn.flash_attn_interface import flash_attn_gpu
+            except:
+                from flash_attn.flash_attn_interface import flash_attn_cuda
             HAS_FLASH_ATTENTION = True
 
             # Also check for softcapping
@@ -799,7 +803,7 @@ def patch_linear_scaling(
         f""from typing import Union, Optional, List, Any, Callable, Tuple\n""\
         f""from {model_filepath} import logger, ""\
         f""{model_name.title()}Attention, {model_name.title()}Config""
-    
+
     try:
         function = inspect.getsource(attention_module.__init__)
     except:
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 9a97015..784ca9c 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -304,7 +304,7 @@ class FastMistralModel(FastLlamaModel):
             attention_module   = MistralAttention,
         )
         # Just for Mistral Nemo models!
-        if function is not None:
+        if function is not None and init_name is not None:
             function = patch_mistral_nemo_attention(function)
             # if True:#init_name is not None:
             exec(function, globals())
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index eaddfa0..6770d65 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -17,6 +17,8 @@ from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
 from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 from peft.tuners.lora import Linear as Peft_Linear
 from typing import Optional, Callable, Union, List
+import sys
+import requests
 import torch
 import os
 import shutil
@@ -1613,6 +1615,112 @@ def create_ollama_modelfile(tokenizer, gguf_location):
     return modelfile
 pass
 
+def create_ollama_model(
+    username: str, 
+    model_name: str, 
+    tag: str, 
+    modelfile_path: str
+):
+    try:
+        init_check = subprocess.run(
+            ['curl', 'http://localhost:11434'], capture_output=True, text=True,  timeout=3
+        )
+        if init_check.returncode == 0:
+            print(init_check.stdout.strip())
+        else:
+            print(""Ollama Server is not Running"")
+    except subprocess.TimeoutExpired:
+        return ""Ollama Request Timeout""
+
+    process = subprocess.Popen(
+            ['ollama', 'create', f'{username}/{model_name}:{tag}', '-f', f'{modelfile_path}'],
+        stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        text=True,
+        bufsize=1,
+        universal_newlines=True
+    )
+
+    for line in iter(process.stdout.readline, ''):
+        print(line, end='')
+        sys.stdout.flush()
+
+    return_code = process.wait()
+
+    if return_code != 0:
+        print(f""\nMODEL CREATED FAILED WITH RETURN CODE {return_code}"")
+    else:
+        print(""\nMODEL CREATED SUCCESSFULLY"")
+pass
+
+
+def push_to_ollama_hub(username: str, model_name: str, tag: str):
+    try:
+        init_check = subprocess.run(
+            ['curl', 'http://localhost:11434'], capture_output=True, text=True,  timeout=3
+        )
+        if init_check.returncode == 0:
+            print(init_check.stdout.strip())
+        else:
+            print(""Ollama Server is not Running"")
+    except subprocess.TimeoutExpired:
+        return ""Ollama Request Timeout""
+
+    process = subprocess.Popen(
+            ['ollama', 'push', f'{username}/{model_name}:{tag}'],
+        stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        text=True,
+        bufsize=1,
+        universal_newlines=True
+    )
+
+    for line in iter(process.stdout.readline, ''):
+        print(line, end='')
+        sys.stdout.flush()
+
+    return_code = process.wait()
+
+    if return_code != 0:
+        print(f""\nMODEL PUBLISHED FAILED WITH RETURN CODE {return_code}"")
+    else:
+        print(""\nMODEL PUBLISHED SUCCESSFULLY"")
+
+
+def push_to_ollama(
+    tokenizer,
+    gguf_location,
+    username: str,
+    model_name: str,
+    tag: str
+):
+    model_file = create_ollama_modelfile(
+        tokenizer=tokenizer,
+        gguf_location=gguf_location
+    )
+
+    with open(f""Modelfile_{model_name}"", ""w"") as f:
+        f.write(model_file)
+        f.close()
+    
+    create_ollama_model(
+        username=username,
+        model_name=model_name,
+        tag=tag,
+        modelfile_path=f""Modelfile_{model_name}""
+    )
+
+    push_to_ollama_hub(
+        username=username,
+        model_name=model_name,
+        tag=tag
+    )
+
+    print(""Succesfully pushed to ollama"")
+
+
+
+
 
 def unsloth_save_pretrained_gguf(
     self,
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index a773b13..8ddf4d6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -54,6 +54,11 @@ __all__ = [
 ]
 
 
+IGNORED_TOKENIZER_CHECKING = frozenset((
+    ""CodeLlamaTokenizerFast"",
+    ""CodeLlamaTokenizer"",
+))
+
 def prepare_model_for_kbit_training(
     model                      : Any,
     use_gradient_checkpointing : bool = True,
@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(
             future Pytorch versions.
     """"""
 
-    # Freeze all parameters
-    for param in model.parameters():
-        param.requires_grad_(False)
+    # Freeze all parameters except LoRA
+    for name, param in model.named_parameters():
+        if "".lora_A."" in name or "".lora_B."" in name:
+            param.requires_grad_(True)
+        else:
+            param.requires_grad_(False)
+    pass
 
     if use_gradient_checkpointing:
         model.gradient_checkpointing_enable()
@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):
 pass
 
 
-IGNORED_TOKENIZER_CHECKING = frozenset((
-    ""CodeLlamaTokenizerFast"",
-    ""CodeLlamaTokenizer"",
-))
-
 def check_tokenizer(
     model,
     tokenizer,
@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init
             )
         pass
         self.loftq_init(adapter_name)
-        
+
     elif init_lora_weights:
         self.reset_lora_parameters(adapter_name, init_lora_weights)
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 1df1a3e..bc7aeb3 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -844,9 +844,11 @@ class FastLlamaModel:
         max_seq_length      = 2048, # not used anymore
         use_rslora          = False,
         init_lora_weights   = True,
-        loftq_config        = None,
+        loftq_config        = {},
         **kwargs,
     ):
+        transformers_set_seed(random_state)
+
         if isinstance(model, PeftModelForCausalLM):
             raise TypeError(
                 ""Unsloth: Your model already has LoRA adapters. No need to run this again!""
@@ -892,7 +894,7 @@ class FastLlamaModel:
                 )
             pass
 
-            if loftq_config is None:
+            if loftq_config == {}:
                 from peft import LoftQConfig
                 logger.warning_once(
                     f""Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\n""\
@@ -922,8 +924,6 @@ class FastLlamaModel:
             pass
         pass
 
-        transformers_set_seed(random_state)
-
         accepted_modules = frozenset((""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
                                       ""gate_proj"", ""up_proj"", ""down_proj"",),)
         model.config.update({""unsloth_version"" : __version__})
@@ -949,22 +949,40 @@ class FastLlamaModel:
         if not SUPPORTS_RSLORA: del arguments[""use_rslora""]
 
         lora_config = LoraConfig(**arguments)
+        model = _get_peft_model(model, lora_config)
+
+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)
+        return model
+    pass
+
+
+    @staticmethod
+    def patch_peft_model(
+        model,
+        use_gradient_checkpointing = True,
+    ):
+        if not isinstance(model, PeftModelForCausalLM):
+            raise TypeError(
+                ""Unsloth: Your model needs to call `.get_peft_model` first!""
+            )
+        pass
 
         model = prepare_model_for_kbit_training(
             model,
             use_gradient_checkpointing = use_gradient_checkpointing,
             use_reentrant = True,
         )
-        model = _get_peft_model(model, lora_config)
 
         # Fix up config for transformers uploading PEFT
-        name = model.peft_config[""default""].base_model_name_or_path
-        if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
-            name = name[:len(name) - len(""-bnb-4bit"")]
-            model.peft_config[""default""].base_model_name_or_path = name
+        for active_adapter in model.peft_config.keys():
+            name = model.peft_config[active_adapter].base_model_name_or_path
+            if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+                name = name[:len(name) - len(""-bnb-4bit"")]
+                model.peft_config[active_adapter].base_model_name_or_path = name
+            pass
+            # Add revision to enable future fast inference paths
+            model.peft_config[active_adapter].revision = f""unsloth""
         pass
-        # Add revision to enable future fast inference paths
-        model.peft_config[""default""].revision = f""unsloth""
 
         # Do patching
         n_mlp = 0
@@ -972,6 +990,13 @@ class FastLlamaModel:
         n_o   = 0
         import types
 
+        active_adapter = model.active_adapters[0] if \
+            hasattr(model, ""active_adapters"") else model.active_adapter
+
+        # Get dropout and bias
+        lora_dropout = model.peft_config[active_adapter].lora_dropout
+        bias         = model.peft_config[active_adapter].bias
+
         if lora_dropout == 0 and bias == ""none"":
             for idx, layer in enumerate(model.model.model.layers):
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index d812d36..5a7f844 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):
         device_map     = ""sequential"",
         rope_scaling   = None,
         fix_tokenizer  = True,
+        use_gradient_checkpointing = True,
         *args, **kwargs,
     ):
         old_model_name = model_name
@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):
         if is_peft:
             # Now add PEFT adapters
             model = PeftModel.from_pretrained(model, old_model_name)
+            # Patch it as well!
+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)
         pass
         return model, tokenizer
     pass
diff --git a/unsloth/save.py b/unsloth/save.py
index 543ffd8..5d3a9a8 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -221,6 +221,17 @@ def unsloth_save_model(
         save_pretrained_settings[""save_directory""] = new_save_directory
         save_directory = new_save_directory
     pass
+
+    # Tokenizer has different saving arguments
+    tokenizer_save_settings = \
+    {
+        ""save_directory""  : save_pretrained_settings[""save_directory""],
+        ""legacy_format""   : None,
+        ""filename_prefix"" : None,
+        ""push_to_hub""     : save_pretrained_settings[""push_to_hub""],
+        ""private""         : save_pretrained_settings[""private""],
+        ""token""           : save_pretrained_settings[""token""],
+    }
     
     if (save_method == ""merged_4bit"") or (save_method == ""lora"") or (
         not hasattr(model, ""model"") or \
@@ -240,7 +251,7 @@ def unsloth_save_model(
 
         if tokenizer is not None:
             print(""Unsloth: Saving tokenizer..."", end = """")
-            tokenizer.save_pretrained(**save_pretrained_settings)
+            tokenizer.save_pretrained(**tokenizer_save_settings)
             print("" Done."")
         else:
             print()
@@ -360,13 +371,34 @@ def unsloth_save_model(
 
     if tokenizer is not None:
         print(""Unsloth: Saving tokenizer..."", end = """")
-        tokenizer.save_pretrained(**save_pretrained_settings)
+        tokenizer.save_pretrained(**tokenizer_save_settings)
         print("" Done."")
     else:
         print()
 
     print(""Unsloth: Saving model... This might take 5 minutes for Llama-7b..."")
+
+    # Since merged, edit quantization_config
+    old_config = model.config
+    new_config = model.config.to_dict()
+    if ""quantization_config"" in new_config:
+        del new_config[""quantization_config""]
+    original_model = model
+    new_config = type(model.config).from_dict(new_config)
+    while hasattr(original_model, ""model""):
+        original_model = original_model.model
+        original_model.config = new_config
+    model.config = new_config
+
+    # Save!
     model.model.save_pretrained(**save_pretrained_settings)
+
+    # Revert config back
+    original_model = model
+    while hasattr(original_model, ""model""):
+        original_model = original_model.model
+        original_model.config = old_config
+    model.config = old_config
     print(""Done."")
 
     save_pretrained_settings[""state_dict""] = None
@@ -446,7 +478,7 @@ def save_to_gguf(
     elif quantization_method is None:             quantization_method = ""q8_0""
 
     if quantization_method not in ALLOWED_QUANTS.keys():
-        error = f""Unsloth: Quant method = [{quantization}] not supported. Choose from below:\n""
+        error = f""Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\n""
         for key, value in ALLOWED_QUANTS.items():
             error += f""[{key}] => {value}\n""
         raise RuntimeError(error)
@@ -456,7 +488,7 @@ def save_to_gguf(
         f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
         f""   \\\   /|    [0] Installing llama.cpp will take 3 minutes.\n""\
         f""O^O/ \_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n""\
-        f""\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\n""\
+        f""\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\n""\
         f' ""-____-""     In total, you will have to wait around 26 minutes.\n'
     print(print_info)
 
@@ -491,11 +523,11 @@ def save_to_gguf(
 
     if quantization_method != first_conversion:
         old_location = final_location
-        print(f""Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes..."")
-        final_location = f""./{model_directory}-unsloth.{quantization.upper()}.gguf""
+        print(f""Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes..."")
+        final_location = f""./{model_directory}-unsloth.{quantization_method.upper()}.gguf""
 
         command = f""./llama.cpp/quantize {old_location} ""\
-            f""{final_location} {quantization} {n_cpus}""
+            f""{final_location} {quantization_method} {n_cpus}""
         
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
             for line in sp.stdout:
@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(
 pass
 
 
+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):
+    print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+
+    # Check for username
+    if ""/"" not in save_directory:
+        from huggingface_hub import whoami
+        try: save_directory = f""{save_directory}/{whoami()['name']}""
+        except: pass
+    pass
+
+    from huggingface_hub import create_repo
+    create_repo(
+        repo_id   = save_directory,
+        token     = token,
+        repo_type = ""model"",
+        exist_ok  = True,
+    )
+
+    # Create model card
+    from huggingface_hub import ModelCard, ModelCardData
+    card_data = ModelCardData(
+        language = ""en"",
+        license  = ""apache-2.0"",
+        library  = ""unsloth"",
+        tags     = [""gguf"", ""unsloth"", ""text-generation-inference"", ""transformers"",],
+    )
+
+    content = f""\n""\
+    f""---\n""\
+    f""{ card_data.to_yaml() }\n""\
+    f""---\n""\
+    f""\n""\
+    f""# My Model Card for {file_location}\n""\
+    f""\n""\
+    f""\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\n""\
+    f""\n""
+    
+    card = ModelCard(content)
+    card.push_to_hub(save_directory, token = token)
+
+    # Now upload file
+    from huggingface_hub import HfApi
+    hf_api = HfApi(token = token)
+
+    if ""/"" in file_location:
+        uploaded_location = file_location[file_location.rfind(""/"")+1:]
+    else:
+        uploaded_location = file_location
+    pass
+
+    hf_api.upload_file(
+        path_or_fileobj = file_location,
+        path_in_repo    = uploaded_location,
+        repo_id         = save_directory,
+        repo_type       = ""model"",
+    )
+pass
+
+
 def unsloth_save_pretrained_gguf(
     self,
     save_directory       : Union[str, os.PathLike],
@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(
         Same as .save_pretrained(...) except 4bit weights are auto
         converted to float16 then converted to GGUF / llama.cpp format.
 
-        Choose for `quantization` to be:
+        Choose for `quantization_method` to be:
         ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
         ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
         ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(
     for _ in range(3):
         gc.collect()
 
-    file_location = save_to_gguf(new_save_directory, quantization, makefile)
-
-    # And save to HF
-    if push_to_hub:
-        print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
-
-        from huggingface_hub import create_repo
-        create_repo(
-            repo_id   = save_directory,
-            token     = token,
-            repo_type = ""model"",
-            exist_ok  = True,
-        )
-
-        from huggingface_hub import HfApi
-        hf_api = HfApi(token = token)
-
-        if ""/"" in file_location:
-            uploaded_location = file_location[file_location.rfind(""/"")+1:]
-        else:
-            uploaded_location = file_location
-        pass
-
-        hf_api.upload_file(
-            path_or_fileobj = file_location,
-            path_in_repo    = uploaded_location,
-            repo_id         = save_directory,
-            repo_type       = ""model"",
-        )
-    pass
+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+    model_type = self.config.model_type
+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)
 pass
 
 
@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(
         Same as .push_to_hub(...) except 4bit weights are auto
         converted to float16 then converted to GGUF / llama.cpp format.
 
-        Choose for `quantization` to be:
+        Choose for `quantization_method` to be:
         ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
         ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
         ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(
         gc.collect()
 
     python_install.wait()
-    file_location = save_to_gguf(new_save_directory, quantization, makefile)
-
-    # Save to hub
-    print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
-
-    from huggingface_hub import create_repo
-    create_repo(
-        repo_id   = save_directory,
-        private   = private,
-        token     = token,
-        repo_type = ""model"",
-        exist_ok  = True,
-    )
-
-    from huggingface_hub import HfApi
-    hf_api = HfApi(token = token)
-
-    if ""/"" in file_location:
-        uploaded_location = file_location[file_location.rfind(""/"")+1:]
-    else:
-        uploaded_location = file_location
-    pass
-
-    hf_api.upload_file(
-        path_or_fileobj = file_location,
-        path_in_repo    = uploaded_location,
-        repo_id         = save_directory,
-        repo_type       = ""model"",
-    )
+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+    model_type = self.config.model_type
+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)
 pass
 
 
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 67ad306..0a73a39 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -15,7 +15,9 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings
+from .utils import calculate_settings, MAX_FUSED_SIZE
+from transformers.models.llama.modeling_llama import logger
+
 
 @triton.jit
 def _cross_entropy_forward(logits_ptr, logits_row_stride,
@@ -145,6 +147,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
 pass
 
 
+slow_cross_entropy_loss = torch.nn.functional.cross_entropy
 def fast_cross_entropy_loss(logits, labels):
     """"""
     Arguments:
@@ -156,10 +159,25 @@ def fast_cross_entropy_loss(logits, labels):
     batch, seq_len, d = logits.shape
     assert(labels.shape == (batch, seq_len))
 
-    loss = Fast_CrossEntropyLoss.apply(
-        logits.view(batch*seq_len, d),
-        labels.view(-1),
-    )
-    n_items = torch.count_nonzero(labels != -100)
-    return loss.sum() / n_items
+    # Prelim support Qwen, Deepseek other large vocab sizes > 2^16
+    if d > MAX_FUSED_SIZE:
+        logger.warning_once(
+            f""Unsloth: Vocab size of {d} exceeds the max CUDA blocksize of {MAX_FUSED_SIZE}.\n""\
+            ""For now, Unsloth will use Pytorch's CrossEntropyLoss, which will entail a\n""\
+            ""25% increase in memory usage and be slower. Make an issue on \n""\
+            ""Unsloth's Github page if you want a faster and more memory efficient kernel!""
+        )
+        loss = slow_cross_entropy_loss(
+            logits.float().view(batch*seq_len, d), # Must cast to float32 for numerical stability
+            labels.view(-1),
+        )
+        return loss
+    else:
+        loss = Fast_CrossEntropyLoss.apply(
+            logits.view(batch*seq_len, d),
+            labels.view(-1),
+        )
+        n_items = torch.count_nonzero(labels != -100)
+        return loss.sum() / n_items
+    pass
 pass
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index adec68d..26fecf8 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -16,6 +16,7 @@ import torch
 from .utils import fast_dequantize, QUANT_STATE
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
 
+
 def get_lora_parameters(proj):
     # For DPO or disabled adapters
     base_layer = (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
@@ -104,7 +105,7 @@ class LoRA_MLP(torch.autograd.Function):
         dtype = X.dtype
 
         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)
-        g = matmul_lora(X,   upW,   upW_quant,  upA,   upB,   upS)
+        g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)
         h = swiglu_fg_kernel(e, g)
         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)
 
@@ -123,10 +124,10 @@ class LoRA_MLP(torch.autograd.Function):
     def backward(ctx, dY : torch.Tensor):
         gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, = \
             ctx.custom_saved_tensors
-        gateA, gateB, upA,upB, downA, downB, \
+        gateA, gateB, upA, upB, downA, downB, \
             X, e, g = ctx.saved_tensors
 
-        gateA, gateB, upA,upB, downA, downB = \
+        gateA, gateB, upA, upB, downA, downB = \
             gateA.t(), gateB.t(), upA.t(), upB.t(), downA.t(), downB.t()
 
         batch, seq_len, hd = X.shape
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 769669a..1c75dff 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -23,7 +23,7 @@ from transformers.models.llama.modeling_llama import logger
 from platform import system as platform_system
 platform_system = platform_system()
 
-__version__ = ""2023.12""
+__version__ = ""2024.1""
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 22179d4..ccf61ee 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -817,29 +817,59 @@ class FastLlamaModel:
         for idx, layer in enumerate(model.model.model.layers):
 
             # MLP patching
-            if  hasattr(layer.mlp.gate_proj, ""lora_A"") and \
-                hasattr(layer.mlp.  up_proj, ""lora_A"") and \
-                hasattr(layer.mlp.down_proj, ""lora_A""):
+            gate_proj = layer.mlp.gate_proj
+            up_proj   = layer.mlp.  up_proj
+            down_proj = layer.mlp.down_proj
+
+            if  hasattr(gate_proj, ""lora_A"") and \
+                hasattr(  up_proj, ""lora_A"") and \
+                hasattr(down_proj, ""lora_A"") and \
+                (gate_proj.base_layer if hasattr(gate_proj, ""base_layer"") else gate_proj).bias is None and \
+                (  up_proj.base_layer if hasattr(  up_proj, ""base_layer"") else   up_proj).bias is None and \
+                (down_proj.base_layer if hasattr(down_proj, ""base_layer"") else down_proj).bias is None:
 
                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)
                 n_mlp += 1
+            else:
+                logger.warning_once(
+                    ""Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n""\
+                    ""are not enabled or a bias term (like in Qwen) is used.""
+                )
             pass
 
             # QKV attention patching
-            if  hasattr(layer.self_attn.q_proj, ""lora_A"") and \
-                hasattr(layer.self_attn.k_proj, ""lora_A"") and \
-                hasattr(layer.self_attn.v_proj, ""lora_A""):
+            q_proj = layer.self_attn.q_proj
+            k_proj = layer.self_attn.k_proj
+            v_proj = layer.self_attn.v_proj
+            if  hasattr(q_proj, ""lora_A"") and \
+                hasattr(k_proj, ""lora_A"") and \
+                hasattr(v_proj, ""lora_A"") and \
+                (q_proj.base_layer if hasattr(q_proj, ""base_layer"") else q_proj).bias is None and \
+                (k_proj.base_layer if hasattr(k_proj, ""base_layer"") else k_proj).bias is None and \
+                (v_proj.base_layer if hasattr(v_proj, ""base_layer"") else v_proj).bias is None:
 
                 layer.self_attn.apply_qkv = apply_lora_qkv
                 n_qkv += 1
+            else:
+                logger.warning_once(
+                    ""Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n""\
+                    ""are not enabled or a bias term (like in Qwen) is used.""
+                )
             pass
 
             # O attention patching
-            if hasattr(layer.self_attn.o_proj, ""lora_A""):
+            o_proj = layer.self_attn.o_proj
+            if hasattr(o_proj, ""lora_A"") and \
+                (o_proj.base_layer if hasattr(o_proj, ""base_layer"") else o_proj).bias is None:
 
                 layer.self_attn.apply_o = apply_lora_o
                 n_o += 1
+            else:
+                logger.warning_once(
+                    ""Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n""\
+                    ""are not enabled or a bias term (like in Qwen) is used.""
+                )
             pass
         pass
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index de1583e..73e69dc 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 windows=[
-    ""unsloth_zoo>=2025.2.7"",
+    ""unsloth_zoo>=2025.3.1"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -61,7 +61,7 @@ windows=[
     ""xformers>=0.0.22.post7 ; platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2025.2.7"",
+    ""unsloth_zoo>=2025.3.1"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index caa06b0..c8f2926 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.2.6""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.1""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 273eddc..5eb9b8f 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -473,7 +473,7 @@ pass
 def matmul_lora(X, W, W_quant, A, B, s, out = None):
     dtype = X.dtype
     W = fast_dequantize(W.t(), W_quant, use_global_buffer = True)
-    
+
     if X.dim() == 3:
         batch, seq_len, d = X.shape
         X = X.view(-1, X.shape[-1])
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 4ebde13..10ba353 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.12""
+__version__ = ""2025.3.13""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index e412c3a..c450ef6 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -238,9 +238,8 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""use_fp16 = getattr(args, 'fp16', False)\n""\
         ""force_float32 = False\n""\
         ""if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '1':\n""\
-        ""    if use_bf16 or use_fp16:\n""\
-        ""        print('Unsloth: Switching to float32 training since model cannot work with float16')\n""\
-        ""        force_float32 = True\n""\
+        ""    print('Unsloth: Switching to float32 training since model cannot work with float16')\n""\
+        ""    force_float32 = True\n""\
         ""mixed_precision_dtype = os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32')\n""\
         ""dtype = getattr(model.config, 'torch_dtype', None)\n""\
         ""if dtype is None: dtype = model.get_input_embeddings().dtype\n""\
"
"diff --git a/pyproject.toml b/pyproject.toml
index 8b24a52..b538578 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.7.1"",
+    ""unsloth_zoo>=2025.7.2"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.7.1"",
+    ""unsloth_zoo>=2025.7.2"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 4da08da..e965b29 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -79,7 +79,7 @@ def get_device_count():
     elif DEVICE_TYPE == ""xpu"":
         return torch.xpu.device_count()
     else:
-        return 0
+        return 1
 pass
 
 DEVICE_COUNT : int = get_device_count()
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c6ff7b6..5076c42 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.1""
+__version__ = ""2025.7.2""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index db0e884..8ccdeba 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -796,7 +796,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif self.training and os.environ.get(""UNSLOTH_KEEP_PADDING"", ""0"") != '1':
+    elif self.training:
         attention_mask = None
         padding_mask = None
     else:
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 6225984..f559c6c 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -899,6 +899,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""google/gemma-3n-E2B"",
         ""unsloth/gemma-3n-E2B-unsloth-bnb-4bit"",
     ),
+    ""unsloth/Devstral-Small-2507-unsloth-bnb-4bit"" : (
+        ""unsloth/Devstral-Small-2507"",
+        ""mistralai/Devstral-Small-2507"",
+        ""unsloth/Devstral-Small-2507-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0217c7b..a2d4d50 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -176,7 +176,7 @@ def patch_tokenizer(model, tokenizer):
 
     if bad_pad_token:
         # Find a better pad token
-        aadded_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
         possible_pad_token = None
         n_possible_pad_tokens = 0
         for added_token in added_tokens[::-1]:
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a96b435..b2fbf0d 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1825,7 +1825,7 @@ class FastLlamaModel:
 
             # Convert to HF format
             _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)
-            model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype)
+            model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype, bnb_config)
             model.vllm_engine = llm
             model.fast_generate = model.vllm_engine.generate
             model.fast_generate_batches = functools.partial(generate_batches, model.vllm_engine)
"
"diff --git a/PARAMETERS.md b/PARAMETERS.md
deleted file mode 100644
index 94d6379..0000000
--- a/PARAMETERS.md
+++ /dev/null
@@ -1,87 +0,0 @@
-## LoraConfig Parameters
-
-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Heres a concise breakdown of key parameters:
-
-**r**
-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.
-- **Impact**:
-  - **Higher**: Retains more information, increases computational load.
-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.
-
-
-**lora_alpha**
-- **Description**: Scaling factor for the low-rank matrices' contribution.
-- **Impact**:
-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.
-  - **Lower**: Subtler effect, may require more training steps.
-
-**lora_dropout**
-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.
-- **Impact**:
-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.
-  - **Lower**: Less regularization, may speed up training, risks overfitting.
-
-**loftq_config**
-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.
-- **Impact**:
-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.
-  - **None**: LoftQ quantization is not applied.
-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.
-
-
-**use_rslora**
-- **Description**: Enables Rank-Stabilized LoRA (RSLora).
-- **Impact**:
-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).
-  - **False**: Uses the original default scaling factor `lora_alpha/r`.
-
-**gradient_accumulation_steps**
-- **Default**: 1
-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.
-- **Impact**: 
-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.
-  - **Lower**: Faster updates but may require more memory per step and can be less stable.
-
-**weight_decay**
-- **Default**: 0.01
-- **Description**: Regularization technique that applies a small penalty to the weights during training.
-- **Impact**:
-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.
-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.
-
-**learning_rate**
-- **Default**: 2e-4
-- **Description**: The rate at which the model updates its parameters during training.
-- **Impact**:
-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.
-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.
-
-## Target Modules 
-
-**q_proj (query projection)**
-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.
-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.
-
-**k_proj (key projection)**
-- **Description**: Projects the input into the key space in the attention mechanism.
-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.
-
-**v_proj (value projection)**
-- **Description**: Projects the input into the value space in the attention mechanism.
-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.
-
-**o_proj (output projection)**
-- **Description**: Projects the output of the attention mechanism back into the original space.
-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.
-
-**gate_proj (gate projection)**
-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.
-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.
-
-**up_proj (up projection)**
-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.
-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.
-
-**down_proj (down projection)**
-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.
-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.
diff --git a/README.md b/README.md
index 2c50f45..534079e 100644
--- a/README.md
+++ b/README.md
@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
 - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text
 - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language
-
+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.
 
 ##  Unsloth.ai News
 -  NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!
@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(
 
 
 ##  Performance Benchmarking
-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)
+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)
 
 | 1 A100 40GB  | Hugging Face | Flash Attention | Unsloth Open Source | [Unsloth Pro](https://unsloth.ai/pricing) |
 |--------------|--------------|-----------------|---------------------|-----------------|
@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(
 ### Conda Installation
 Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.
 ```bash
-conda create --name unsloth_env python=3.10
+conda create --name unsloth_env \
+    python=3.10 \
+    pytorch-cuda=<11.8/12.1> \
+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
+    -y
 conda activate unsloth_env
 
-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers
-
 pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""
 
-pip install --no-deps trl peft accelerate bitsandbytes
+pip install --no-deps ""trl<0.9.0"" peft accelerate bitsandbytes
 ```
 
 ### Pip Installation
@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele
 
 # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:
 pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""
-pip install --no-deps xformers trl peft accelerate bitsandbytes
+pip install --no-deps xformers ""trl<0.9.0"" peft accelerate bitsandbytes
 ```
 7. For Pytorch 2.3.0: Use the `""ampere""` path for newer RTX 30xx GPUs or higher.
 ```bash
@@ -257,7 +259,7 @@ trainer.train()
 # (1) Saving to GGUF / merging to 16bit for vLLM
 # (2) Continued training from a saved LoRA adapter
 # (3) Adding an evaluation loop / OOMs
-# (4) Cutomized chat templates
+# (4) Customized chat templates
 ```
 
 <a name=""DPO""></a>
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index fc2e1a9..ff2e909 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):
         if ""n_total_devices >"" not in inner_training_loop:
             raise RuntimeError(
                 ""Our OSS was designed for people with few GPU resources to level the playing field.\n""
-                ""The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\n""
+                ""The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\n""
                 ""We're a 2 person team, so we still have to fund our development costs - thanks!\n""
                 ""If you don't, please consider at least sponsoring us through Ko-fi! Appreciate it!"",
             )
@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):
             ""is_sagemaker_mp_enabled()"",
             ""False"",
         )
+        exec(inner_training_loop, globals())
         Trainer._inner_training_loop = _fast_inner_training_loop
 
         # Save max_seq_length
@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):
 
         # Add save modules
         patch_saving_functions(model)
+        Trainer._inner_training_loop = _fast_inner_training_loop
 
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
diff --git a/unsloth/models/qwen2.py b/unsloth/models/qwen2.py
index 115bf3e..4732728 100644
--- a/unsloth/models/qwen2.py
+++ b/unsloth/models/qwen2.py
@@ -12,9 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .llama import *
-import os
-from ._utils import __version__
+from .mistral import *
 
 from transformers.models.qwen2.modeling_qwen2 import (
     Qwen2Attention,
@@ -34,7 +32,7 @@ except:
 pass
 
 
-class FastQwen2Model(FastLlamaModel):
+class FastQwen2Model(FastMistralModel):
 
     @staticmethod
     def pre_patch():
@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):
         trust_remote_code = False,
         **kwargs,
     ):
-        return FastLlamaModel.from_pretrained(
+        return FastMistralModel.from_pretrained(
             model_name     = model_name,
             max_seq_length = max_seq_length,
             dtype          = dtype,
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bc7aeb3..55527db 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -144,7 +144,7 @@ def LlamaAttention_fast_forward_inference(
     A = torch.matmul(A, Vnn)
     A = A.transpose(1, 2)
     A = A.reshape(bsz, 1, self.hidden_size)
-    A = original_apply_o(self, A)
+    A = self.o_proj(A)
     return A, (Kn, Vn)
 pass
 
@@ -187,10 +187,9 @@ def LlamaAttention_fast_forward(
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     
     bsz, q_len, _ = hidden_states.size()
-    Q, K, V = self.apply_qkv(self, hidden_states)
 
     # Check for inference
-    if use_cache and past_key_value is not None and q_len == 1:
+    if past_key_value is not None and q_len == 1:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -206,6 +205,7 @@ def LlamaAttention_fast_forward(
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
+    Q, K, V = self.apply_qkv(self, hidden_states)
     Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)
     K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
     V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
@@ -304,11 +304,10 @@ def LlamaDecoderLayer_fast_forward(
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
     bsz, q_len, hd = hidden_states.size()
-
-    if (self.training):
+    if (past_key_value is not None and q_len == 1):
         # Self Attention
         residual = hidden_states
-        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
             hidden_states=hidden_states,
             causal_mask=causal_mask,
@@ -319,17 +318,16 @@ def LlamaDecoderLayer_fast_forward(
             use_cache=use_cache,
             padding_mask=padding_mask,
         )
-        hidden_states = residual + hidden_states
+        hidden_states += residual
 
         # Fully Connected
         residual = hidden_states
-        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
-        hidden_states = self.mlp(hidden_states)
-        hidden_states = residual + hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)
+        hidden_states += residual
     else:
-        # Self Attention
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
         hidden_states, self_attn_weights, present_key_value = self.self_attn(
             hidden_states=hidden_states,
             causal_mask=causal_mask,
@@ -340,13 +338,13 @@ def LlamaDecoderLayer_fast_forward(
             use_cache=use_cache,
             padding_mask=padding_mask,
         )
-        hidden_states += residual
+        hidden_states = residual + hidden_states
 
         # Fully Connected
         residual = hidden_states
-        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
-        hidden_states = fast_mlp_inference(self.mlp, hidden_states)
-        hidden_states += residual
+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states
     pass
 
     outputs = (hidden_states,)
@@ -445,7 +443,7 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
-    elif self.training:
+    elif True:#self.training:
         attention_mask = None
         padding_mask = None
     else:
@@ -524,10 +522,11 @@ def LlamaModel_fast_forward(
             all_self_attns += (layer_outputs[1],)
     pass
 
-    if (self.training):
-        hidden_states = fast_rms_layernorm(self.norm, hidden_states)
-    else:
+    bsz, q_len, hd = hidden_states.size()
+    if (past_key_value is not None and q_len == 1):
         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)
+    else:
+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)
     pass
 
     # add hidden states from the last decoder layer
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 2c85129..68add4b 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -47,10 +47,9 @@ def MistralAttention_fast_forward(
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     
     bsz, q_len, _ = hidden_states.size()
-    Q, K, V = self.apply_qkv(self, hidden_states)
 
     # Check for inference
-    if use_cache and past_key_value is not None and q_len == 1:
+    if past_key_value is not None and q_len == 1:
         A, past_key_value = LlamaAttention_fast_forward_inference(
             self,
             hidden_states,
@@ -66,6 +65,7 @@ def MistralAttention_fast_forward(
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
+    Q, K, V = self.apply_qkv(self, hidden_states)
     Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)
     K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
     V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
diff --git a/unsloth/save.py b/unsloth/save.py
index a3d5fe9..64342d9 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -94,8 +94,9 @@ def fast_save_pickle(shard, name):
     torch.save(
         shard,
         name,
-        pickle_module   = pickle,
-        pickle_protocol = pickle.HIGHEST_PROTOCOL,
+        # HIGHEST_PROTOCOL seems to not work with Pytorch!
+        # pickle_module   = pickle,
+        # pickle_protocol = pickle.HIGHEST_PROTOCOL,
     )
     return
 pass
@@ -783,12 +784,27 @@ def unsloth_save_pretrained_gguf(
     del arguments[""quantization_method""]
 
     # Non blocking install GGUF first
-    git_clone = install_llama_cpp_clone_non_blocking()
-    python_install = install_python_non_blocking([""gguf"", ""protobuf""])
-    git_clone.wait()
-    makefile  = install_llama_cpp_make_non_blocking()
-    new_save_directory = unsloth_save_model(**arguments)
-    python_install.wait()
+    if not os.path.exists(""llama.cpp""):
+        git_clone = install_llama_cpp_clone_non_blocking()
+        python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+        git_clone.wait()
+        makefile  = install_llama_cpp_make_non_blocking()
+        new_save_directory = unsloth_save_model(**arguments)
+        python_install.wait()
+    else:
+        try:
+            new_save_directory = unsloth_save_model(**arguments)
+            makefile = None
+        except:
+            # Retry by recloning llama.cpp
+            git_clone = install_llama_cpp_clone_non_blocking()
+            python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+            git_clone.wait()
+            makefile  = install_llama_cpp_make_non_blocking()
+            new_save_directory = unsloth_save_model(**arguments)
+            python_install.wait()
+        pass
+    pass
 
     for _ in range(3):
         gc.collect()
@@ -801,7 +817,10 @@ def unsloth_save_pretrained_gguf(
             self, save_directory, token,
             ""GGUF converted"", ""gguf"", file_location,
         )
-        print(f""Saved to https://huggingface.co/{username}/{new_save_directory.lstrip('/.')}"")
+        link = f""{username}/{new_save_directory.lstrip('/.')}"" \
+            if username not in new_save_directory else \
+            new_save_directory.lstrip('/.')
+        print(f""Saved to https://huggingface.co/{link}"")
     pass
 pass
 
@@ -863,16 +882,31 @@ def unsloth_push_to_hub_gguf(
     del arguments[""quantization_method""]
 
     # Non blocking install GGUF first
-    git_clone = install_llama_cpp_clone_non_blocking()
-    python_install = install_python_non_blocking([""gguf"", ""protobuf""])
-    git_clone.wait()
-    makefile  = install_llama_cpp_make_non_blocking()
-    new_save_directory = unsloth_save_model(**arguments)
+    if not os.path.exists(""llama.cpp""):
+        git_clone = install_llama_cpp_clone_non_blocking()
+        python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+        git_clone.wait()
+        makefile  = install_llama_cpp_make_non_blocking()
+        new_save_directory = unsloth_save_model(**arguments)
+        python_install.wait()
+    else:
+        try:
+            new_save_directory = unsloth_save_model(**arguments)
+            makefile = None
+        except:
+            # Retry by recloning llama.cpp
+            git_clone = install_llama_cpp_clone_non_blocking()
+            python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+            git_clone.wait()
+            makefile  = install_llama_cpp_make_non_blocking()
+            new_save_directory = unsloth_save_model(**arguments)
+            python_install.wait()
+        pass
+    pass
 
     for _ in range(3):
         gc.collect()
 
-    python_install.wait()
     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
 
     print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
@@ -880,7 +914,10 @@ def unsloth_push_to_hub_gguf(
         self, repo_id, token,
         ""GGUF converted"", ""gguf"", file_location,
     )
-    print(f""Saved to https://huggingface.co/{username}/{new_save_directory.lstrip('/')}"")
+    link = f""{username}/{new_save_directory.lstrip('/.')}"" \
+        if username not in new_save_directory else \
+        new_save_directory.lstrip('/.')
+    print(f""Saved to https://huggingface.co/{link}"")
 pass
 
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 8e73577..51c540c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,10 +37,10 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.6.4"",
+    ""unsloth_zoo>=2025.6.5"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
+    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
     ""datasets>=3.4.1"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -381,10 +381,10 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.6.4"",
+    ""unsloth_zoo>=2025.6.5"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
+    ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3"",
     ""datasets>=3.4.1"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 05c05ae..e03f50b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.6.5""
+__version__ = ""2025.6.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index b96302c..9b0f4e4 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -247,10 +247,10 @@ RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__move_model_to_vllm)
 
 # Edit _get_per_token_logps to handle mixed precision
 def grpo_trainer__get_per_token_logps(function_name, function):
-    if  function_name != ""_get_per_token_logps"": return function
+    if function_name != ""_get_per_token_logps"": return function
 
-    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep, calc_logprob_flag = None):
-        if os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '0' and not calc_logprob_flag:
+    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
+        if True: # os.environ.get('UNSLOTH_USE_NEW_MODEL', '0') == '0':
             return None # Unsloth efficient GRPO
         # Otherwise, calculate normally:
         if not hasattr(self, '_autocast_dtype'):
@@ -260,9 +260,13 @@ def grpo_trainer__get_per_token_logps(function_name, function):
         os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""1""
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
             # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
-            hidden_states = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
-            #logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-            return hidden_states
+            logits = model(
+                input_ids = input_ids,
+                attention_mask = attention_mask,
+                logits_to_keep = logits_to_keep + 1,
+            ).logits
+            # logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
+            return logits
             # input_ids = input_ids[:, -logits_to_keep:]
             # For transformers<=4.48, logits_to_keep argument isn't supported, so here we drop logits ourselves.
             # See https://github.com/huggingface/trl/issues/2770
@@ -331,19 +335,24 @@ def grpo_trainer_compute_loss(function_name, function):
         # per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
         # per_token_loss = -(per_token_loss - self.beta * per_token_kl)
         # loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
-        if ""old_per_token_logps"" in inputs.keys():
-            old_hidden_states = inputs[""old_per_token_logps""]
-        else:
-            old_hidden_states = None
-
+        old_hidden_states = inputs.get(""old_per_token_logps"", None)
         input_ids = input_ids[:, -logits_to_keep:]
+
+        # Get logit softcapping and logit scale
+        logit_softcapping = getattr(model.config, ""final_logit_softcapping"", 0) # Gemma
+        if logit_softcapping is None: logit_softcapping = 0
+        logit_scale_multiply = getattr(model.config, ""logit_scale"", 0) # Cohere
+        if logit_scale_multiply is None: logit_scale_multiply = 0
+        logit_scale_divide = getattr(model.config, ""logits_scaling"", 0) # Granite
+        if logit_scale_divide is None: logit_scale_divide = 0
+
+
         if per_token_logps is not None:
 
             if ref_per_token_logps is not None:
                 ref_per_token_logps = ref_per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-
             per_token_logps = per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-            
+
             loss, completion_length, mean_kl = grpo_compute_loss_slow(
                 ref_per_token_logps,
                 per_token_logps,
@@ -358,16 +367,19 @@ def grpo_trainer_compute_loss(function_name, function):
                 max_completion_length = self.args.max_completion_length,
                 delta = self.args.delta,
                 temperature = self.args.temperature,
+                logit_softcapping = logit_softcapping,
+                logit_scale_multiply = logit_scale_multiply,
+                logit_scale_divide = logit_scale_divide,
             )
         else:
             if hasattr(self.args, ""loss_type""):
                 loss, completion_length, mean_kl = grpo_accumulated_loss(
-                    self,
-                    _input_ids,
-                    logits_to_keep,
-                    completion_mask,
-                    advantages,
-                    old_hidden_states,
+                    trainer = self,
+                    input_ids = _input_ids,
+                    logits_to_keep = logits_to_keep,
+                    completion_mask = completion_mask,
+                    advantages = advantages,
+                    old_hidden_states = old_hidden_states,
                     n_chunks = self.args.unsloth_num_chunks,
                     loss_type = self.args.loss_type,
                     epsilon_low = self.epsilon_low,
@@ -375,26 +387,33 @@ def grpo_trainer_compute_loss(function_name, function):
                     max_completion_length = self.args.max_completion_length,
                     delta = self.args.delta,
                     temperature = self.args.temperature,
+                    logit_softcapping = logit_softcapping,
+                    logit_scale_multiply = logit_scale_multiply,
+                    logit_scale_divide = logit_scale_divide,
+                    attention_mask = attention_mask,
                 )
             else:
                 # to ensure backwards compatibility with trl 0.15.2 and maybe even 0.17
                 loss, completion_length, mean_kl = grpo_accumulated_loss(
-                    self,
-                    _input_ids,
-                    logits_to_keep,
-                    completion_mask,
-                    advantages,
-                    old_hidden_states,
+                    trainer = self,
+                    input_ids = _input_ids,
+                    logits_to_keep = logits_to_keep,
+                    completion_mask = completion_mask,
+                    advantages = advantages,
+                    old_hidden_states = old_hidden_states,
                     n_chunks = self.args.unsloth_num_chunks,
                     temperature = self.args.temperature,
+                    logit_softcapping = logit_softcapping,
+                    logit_scale_multiply = logit_scale_multiply,
+                    logit_scale_divide = logit_scale_divide,
+                    attention_mask = attention_mask,
                 )
-
+            pass
+        pass
         # Log the metrics
         # completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()
-
         # mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         # self._metrics[""kl""].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())
-
         if ""train"" in self._metrics:
             mode = ""eval"" if self.control.should_evaluate else ""train""
             self._metrics[mode][""completion_length""].append(completion_length.item())
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index a140ccd..b0f485d 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -720,6 +720,8 @@ class FastBaseModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = False
         pass
+        # Must disable returning hidden states in the case for GRPO
+        os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""0""
         return model
     pass
 
"
"diff --git a/.github/ISSUE_TEMPLATE/bug---issue.md b/.github/ISSUE_TEMPLATE/bug---issue.md
new file mode 100644
index 0000000..a08a6cf
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/bug---issue.md
@@ -0,0 +1,19 @@
+---
+name: Bug / Issue
+about: Bug / Issue
+title: ""[Bug]""
+labels: bug
+assignees: ''
+
+---
+
+1. Did you update? `pip install --upgrade unsloth unsloth_zoo`
+2. `Colab` or `Kaggle` or local / cloud
+3. Number GPUs used, use `nvidia-smi`
+4. Which notebook?
+5. Paste `Unsloth` printout with :sloth: sloth emoji
+6. Which trainer? `SFTTrainer`, `GRPOTrainer` etc
+7. **Minimal code to reproduce error Remove Hugging Face token!**
+
+For quick replies, got to https://discord.com/invite/unsloth.
+Have you tried https://docs.unsloth.ai/basics/errors-troubleshooting
diff --git a/.github/ISSUE_TEMPLATE/feature-request.md b/.github/ISSUE_TEMPLATE/feature-request.md
new file mode 100644
index 0000000..5ea70a8
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/feature-request.md
@@ -0,0 +1,21 @@
+---
+name: Feature Request
+about: New features, model support, ideas
+title: ""[Feature]""
+labels: feature request
+assignees: ''
+
+---
+
+For new models, have you tried:
+```python
+from unsloth import FastModel
+model, tokenizer = FastModel.from_pretrained(
+    ""microsoft/Phi-4-multimodal-instruct"",
+    trust_remote_code = True,
+)
+from transformers import AutoModelForSequenceClassification
+model, tokenizer = FastModel.from_pretrained(
+    auto_model = AutoModelForSequenceClassification,
+)
+```
diff --git a/.github/ISSUE_TEMPLATE/feature_request.md b/.github/ISSUE_TEMPLATE/feature_request.md
deleted file mode 100644
index 9da73b2..0000000
--- a/.github/ISSUE_TEMPLATE/feature_request.md
+++ /dev/null
@@ -1,34 +0,0 @@
----
-name: ""\U0001F680 Feature request""
-about: New features, model support, ideas
-title: ""[Feature]""
-labels: feature request
-assignees: ''
-
----
-
-1. For new models, have you tried:
-```python
-from unsloth import FastModel
-model, tokenizer = FastModel.from_pretrained(
-    ""microsoft/Phi-4-multimodal-instruct"",
-    trust_remote_code = True,
-)
-```
-If that doesn't work, try using the exact `AutoModel` class:
-```python
-from transformers import WhisperForConditionalGeneration
-model, tokenizer = FastModel.from_pretrained(
-    model_name = ""unsloth/whisper-large-v3"",
-    auto_model = WhisperForConditionalGeneration,
-)
-```
-For Sequence Classification / other `AutoModel` classes:
-```python
-from transformers import AutoModelForSequenceClassification
-model, tokenizer = FastModel.from_pretrained(
-    model_name = ""unsloth/whisper-large-v3"",
-    auto_model = AutoModelForSequenceClassification,
-)
-```
-2. Otherwise, ask away!
diff --git a/.github/ISSUE_TEMPLATE/other.md b/.github/ISSUE_TEMPLATE/other.md
new file mode 100644
index 0000000..b78d201
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE/other.md
@@ -0,0 +1,10 @@
+---
+name: Other
+about: Everything else
+title: ''
+labels: ''
+assignees: ''
+
+---
+
+Try asking https://discord.com/invite/unsloth for fast support!
diff --git ""a/.github/ISSUE_TEMPLATE/\342\235\223-other.md"" ""b/.github/ISSUE_TEMPLATE/\342\235\223-other.md""
deleted file mode 100644
index dbec522..0000000
--- ""a/.github/ISSUE_TEMPLATE/\342\235\223-other.md""
+++ /dev/null
@@ -1,10 +0,0 @@
----
-name: "" Other""
-about: Other
-title: ""[Question]""
-labels: ''
-assignees: ''
-
----
-
-Be specific. If you need urgent help, head to https://discord.com/invite/unsloth for help. Have you tried https://docs.unsloth.ai/basics/errors-troubleshooting or https://github.com/unslothai/unsloth/wiki ?
diff --git ""a/.github/ISSUE_TEMPLATE/\360\237\220\233-bug.md"" ""b/.github/ISSUE_TEMPLATE/\360\237\220\233-bug.md""
deleted file mode 100644
index 6e59ccb..0000000
--- ""a/.github/ISSUE_TEMPLATE/\360\237\220\233-bug.md""
+++ /dev/null
@@ -1,21 +0,0 @@
----
-name: ""\U0001F41B Bug""
-about: For bugs
-title: ""[Bug]""
-labels: bug
-assignees: ''
-
----
-
-1. **Environment**
-   - `Colab`, `Kaggle`, local / cloud machine
-   - Number GPUs used, GPU type, VRAM amount
-   - Copy paste Unsloth printout with sloth emoji
-   - Which notebook are you using?
-
-2. **Code to reproduce**
-   - Which trainer - `SFTTrainer, GRPOTrainer` etc
-   - Exact code to repro. **Remove Hugging Face token!**
-   - Expected behavior
-
-For quick replies, head to https://discord.com/invite/unsloth and ask away! have you tried https://github.com/unslothai/unsloth/wiki or https://docs.unsloth.ai/basics/errors-troubleshooting?
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 97da833..e9560cd 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -173,6 +173,94 @@ def GemmaModel_fast_forward_inference(
 pass
 
 
+def GemmaForCausalLM_fast_forward(
+    self,
+    input_ids: torch.LongTensor = None,
+    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask: Optional[torch.Tensor] = None,
+    position_ids: Optional[torch.LongTensor] = None,
+    past_key_values: Optional[List[torch.FloatTensor]] = None,
+    inputs_embeds: Optional[torch.FloatTensor] = None,
+    labels: Optional[torch.LongTensor] = None,
+    use_cache: Optional[bool] = None,
+    output_attentions: Optional[bool] = None,
+    output_hidden_states: Optional[bool] = None,
+    return_dict: Optional[bool] = None,
+    *args, **kwargs,
+) -> Union[Tuple, CausalLMOutputWithPast]:
+
+    if causal_mask is None and past_key_values is None:
+        causal_mask = xformers.attn_bias.LowerTriangularMask()
+
+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+    output_hidden_states = (
+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+    )
+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+    self.model._has_no_labels = labels is None
+
+    if past_key_values is not None and \
+        hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+        outputs = GemmaModel_fast_forward_inference(
+            self.model,
+            input_ids,
+            past_key_values,
+        )
+    else:
+        outputs = self.model(
+            input_ids=input_ids,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_values=past_key_values,
+            inputs_embeds=inputs_embeds,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+    pass
+
+    hidden_states = outputs[0]
+    bsz, q_len, hd = hidden_states.shape
+    if bsz == 1 and q_len == 1:
+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())
+        logits = logits.unsqueeze(0).unsqueeze(0)
+    else:
+        logits = self.lm_head(hidden_states)
+    pass
+
+    loss = None
+    if labels is not None:
+        shift_logits = logits
+        if not hasattr(self, ""extra_ignored_labels""):
+            # Fixes https://github.com/unslothai/unsloth/issues/10
+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda"")
+        pass
+        
+        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
+        loss = fast_cross_entropy_loss(
+            logits = shift_logits,
+            labels = shift_labels,
+        )
+    pass
+
+    if not return_dict:
+        output = (logits,) + outputs[1:]
+        return (loss,) + output if loss is not None else output
+
+    return CausalLMOutputWithPast(
+        loss=loss,
+        logits=logits,
+        past_key_values=outputs.past_key_values,
+        hidden_states=outputs.hidden_states,
+        attentions=outputs.attentions,
+    )
+pass
+
+
 class FastGemmaModel(FastLlamaModel):
 
     @staticmethod
@@ -182,7 +270,7 @@ class FastGemmaModel(FastLlamaModel):
         GemmaFlashAttention2.forward = LlamaAttention_fast_forward
         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward
         GemmaModel          .forward = LlamaModel_fast_forward
-        GemmaForCausalLM    .forward = LlamaForCausalLM_fast_forward
+        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
         # Solves https://github.com/unslothai/unsloth/issues/168
         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 02eea4b..6f7da0f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -26,6 +26,7 @@ from transformers import AutoTokenizer
 from platform import system as platform_system
 platform_system = platform_system()
 import math
+import numpy as np
 
 __version__ = ""2024.3""
 
@@ -269,3 +270,88 @@ except:
         ""Luckily, your training run will still work in the meantime!""
     )
 pass
+
+
+def _calculate_n_gradient_checkpoints(
+    n_layers : int,
+    method   : Optional[Union[str, int]] = ""sqrt"",
+) -> List[int]:
+    assert(type(n_layers) is int and n_layers > 0)
+
+    if method is None: method = ""sqrt""
+
+    if method == ""sqrt"":
+        n_checkpoints = int(n_layers**0.5)
+    elif type(method) is int and method > 0:
+        n_checkpoints = int(np.ceil(n_layers / method))
+    else:
+        raise ValueError(""method must be 'sqrt' or an int >0 and <= n_layers."")
+
+    size = n_layers // n_checkpoints
+    sizes = np.full(n_checkpoints, size, dtype = int)
+    leftovers = n_layers % n_checkpoints
+    # We append leftovers from the right
+    for k in range(leftovers):
+        sizes[n_checkpoints-1-k] += 1
+    boundaries = np.hstack((0, np.cumsum(sizes)))
+    boundaries = boundaries.tolist()
+    return boundaries
+pass
+
+
+def calculate_n_gradient_checkpoints(
+    n_layers              : int,
+    layers_per_checkpoint : Optional[Union[str, int]] = ""sqrt"",
+) -> List[int]:
+    assert(type(n_layers) is int and n_layers > 0)
+
+    if layers_per_checkpoint is None or layers_per_checkpoint == 1:
+        return None
+
+    boundaries = _calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)
+
+    assert(boundaries[0] == 0 and boundaries[-1] == n_layers)
+    assert(min(boundaries) == 0 and max(boundaries) == n_layers)
+    assert(np.diff(boundaries).min() >= 0)
+    return boundaries
+pass
+
+
+def prepare_n_gradient_checkpoints(
+    model                 : Any,
+    layers_per_checkpoint : Optional[Union[str, int]] = ""sqrt"",
+    use_reentrant         : Optional[bool] = True,
+) -> None:
+    """"""
+    Calculates where to place the gradient checkpoints given n_layers.
+
+    Args:
+        model: Any LlamaModel with layers.
+        layers_per_checkpoint (`Union[str, int]`, *optional*):
+            Can either be `sqrt` or an integer for how many layers per checkpoint you want.
+            The more, the less memory usage, but can be slower. Default is `sqrt`.
+            Choose 1 for Pytorch gradient checkpointing. 2 to wrap 2 layers in 1 module etc.
+        use_reentrant (`bool`, *optional*):
+            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354
+            Optimal gradient checkpointing algorithm `use_reentrant=False` which will
+            be the default in future Pytorch versions doesn't seem to work??
+    """"""
+    _model = None
+    if hasattr(model, ""layers""):
+        _model = model
+    elif hasattr(model, ""model""):
+        if hasattr(model.model, ""layers""):
+            _model = model.model
+    if _model is None:
+        raise TypeError(""`model` or `model.model` does not have attribute `layers`. Are you sure this is a model?"")
+    pass
+
+    if use_reentrant is False:
+        use_reentrant = True
+    pass
+
+    n_layers = len(_model.layers)
+    boundaries = calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)
+    _model._gradient_checkpointing_boundaries    = boundaries
+    _model._gradient_checkpointing_use_reentrant = use_reentrant
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 574385f..b7aae06 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -593,6 +593,13 @@ def LlamaModel_fast_forward(
     all_self_attns = () if output_attentions else None
     next_decoder_cache = () if use_cache else None
 
+    # Gradient checkpointing methods (ie sqrt)
+    if hasattr(self, ""_gradient_checkpointing_boundaries""):
+        boundaries = self._gradient_checkpointing_boundaries
+    else:
+        boundaries = None
+    pass
+    
     for idx, decoder_layer in enumerate(self.layers):
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
diff --git a/unsloth/save.py b/unsloth/save.py
index c8b4053..a161394 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -24,6 +24,7 @@ from transformers.models.llama.modeling_llama import logger
 from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters
 import subprocess
 import psutil
+import re
 
 __all__ = [
     ""print_quantization_methods"",
@@ -176,19 +177,6 @@ def unsloth_save_model(
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.9,
 ):
-    # First check for a token!
-    if push_to_hub:
-        from huggingface_hub import whoami
-        try: 
-            username = whoami(token = token)[""name""]
-        except:
-            raise RuntimeError(
-                ""Unsloth: Please supply a token!\n""\
-                ""Go to https://huggingface.co/settings/tokens""
-            )
-        pass
-    pass
-
     if commit_message is None: commit_message = """"
     if ""Unsloth"" not in commit_message:
         commit_message += "" (Trained with Unsloth)""
@@ -215,7 +203,19 @@ def unsloth_save_model(
     for deletion in (""model"", ""tokenizer"", ""save_method"", ""temporary_location"", ""maximum_memory_usage""):
         del save_pretrained_settings[deletion]
     pass
-    import re
+
+    # First check for a token!
+    if push_to_hub:
+        from huggingface_hub import whoami
+        try: 
+            username = whoami(token = token)[""name""]
+        except:
+            raise RuntimeError(
+                ""Unsloth: Please supply a token!\n""\
+                ""Go to https://huggingface.co/settings/tokens""
+            )
+        pass
+    pass
 
     assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)
 
@@ -588,7 +588,7 @@ def unsloth_save_model(
         from huggingface_hub import HfApi
         hf_api = HfApi(token = save_pretrained_settings[""token""])
 
-        print(""Unsloth: Uploading all files... Please wait!"")
+        print(""Unsloth: Uploading all files... Please wait..."")
         hf_api.upload_folder(
             folder_path = new_save_directory,
             path_in_repo = ""."",
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 59226f0..1c6a726 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -158,7 +158,7 @@ class FastLanguageModel(FastLlamaModel):
                 )
             pass
         pass
-        
+
         old_model_name = model_name
         if not use_exact_model_name:
             model_name = get_model_name(model_name, load_in_4bit)
@@ -214,7 +214,7 @@ class FastLanguageModel(FastLlamaModel):
             else:
                 # Because HfFileSystem assumes linux paths, we need to set the path with forward slashes, even on Windows.
                 files = HfFileSystem(token = token).glob(f""{model_name}/*.json"")
-                files = (os.path.split(x)[-1] for x in files)
+                files = list(os.path.split(x)[-1] for x in files)
                 if sum(x == ""adapter_config.json"" or x == ""config.json"" for x in files) >= 2:
                     both_exist = True
                 pass
@@ -239,7 +239,7 @@ class FastLanguageModel(FastLlamaModel):
                     f""This includes Llama 3.1. The minimum required version is 4.43.2\n""\
                     f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
-                ) 
+                )
             # Create a combined error message showing both failures
             combined_error = (
                 ""Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\n""
@@ -316,7 +316,7 @@ class FastLanguageModel(FastLlamaModel):
                     ""To update flash-attn, do the below:\n""\
                     '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
                 )
-            
+
             dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
@@ -383,7 +383,7 @@ class FastLanguageModel(FastLlamaModel):
                 fast_inference = False
             pass
             from unsloth_zoo.vllm_utils import (
-                patch_vllm, 
+                patch_vllm,
                 vllm_dynamic_quant_supported,
             )
             patch_vllm()
@@ -421,7 +421,7 @@ class FastLanguageModel(FastLlamaModel):
             disable_log_stats = disable_log_stats,
             *args, **kwargs,
         )
-        
+
         if resize_model_vocab is not None:
             model.resize_token_embeddings(resize_model_vocab)
         pass
@@ -598,7 +598,7 @@ class FastModel(FastBaseModel):
                 ""float16;torch.float16;torch.float16;""\
                 ""if name.endswith(('.conv')): module;""\
                 ""from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()""
-            
+
             if transformers_version < Version(""4.53.0""):
                 raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
         elif ""falcon-h1"" in lowered_model_name:
@@ -697,7 +697,7 @@ class FastModel(FastBaseModel):
                 both_exist = exist_adapter_config and exist_config
             else:
                 files = HfFileSystem(token = token).glob(f""{model_name}/*.json"")
-                files = (os.path.split(x)[-1] for x in files)
+                files = list(os.path.split(x)[-1] for x in files)
                 if sum(x == ""adapter_config.json"" or x == ""config.json"" for x in files) >= 2:
                     both_exist = True
                 pass
@@ -722,7 +722,7 @@ class FastModel(FastBaseModel):
                     f""This includes Llama 3.1. The minimum required version is 4.43.2\n""\
                     f'Try `pip install --upgrade ""transformers>=4.43.2""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
-                ) 
+                )
             # Create a combined error message showing both failures
             combined_error = (
                 ""Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\n""
@@ -738,7 +738,7 @@ class FastModel(FastBaseModel):
             model_name = peft_config.base_model_name_or_path
             if not use_exact_model_name:
                 model_name = get_model_name(model_name, load_in_4bit)
-            
+
             model_config = AutoConfig.from_pretrained(
                 model_name,
                 token = token,
@@ -854,7 +854,7 @@ class FastModel(FastBaseModel):
             use_gradient_checkpointing = use_gradient_checkpointing,
             supports_sdpa     = supports_sdpa,
             whisper_language  = whisper_language,
-            whisper_task      = whisper_task,            
+            whisper_task      = whisper_task,
             *args, **kwargs,
         )
 
"
"diff --git a/README.md b/README.md
index 4c12713..d843158 100644
--- a/README.md
+++ b/README.md
@@ -39,6 +39,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 - Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.
 
 ##  Unsloth.ai News
+-  NEW! [Gemma-2-2b](https://colab.research.google.com/drive/1weTpKOjBZxZJ5PQ-Ql8i6ptAY2x-FWVA?usp=sharing) now supported! Gemma-2-9b and Gemma-2-27b are alrady supported!
 -  NEW! [Llama 3.1 8b, 70b](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing) both Base and Instruct now supported
 -  NEW! [Mistral Nemo-12b](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing) both Base and Instruct now supported
 -  NEW! [Gemma-2-9b](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing) and Gemma-2-27b now supported
diff --git a/pyproject.toml b/pyproject.toml
index 6777f7c..e711325 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -171,7 +171,7 @@ colab-ampere-torch211 = [
     ""unsloth[cu121onlytorch211]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 colab-torch220 = [
     ""unsloth[huggingface]"",
@@ -184,7 +184,7 @@ colab-ampere-torch220 = [
     ""unsloth[cu121onlytorch220]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 colab-new = [
     ""packaging"",
@@ -215,7 +215,7 @@ colab-ampere = [
     ""unsloth[colab-ampere-torch220]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu118-ampere = [
     ""unsloth[huggingface]"",
@@ -223,7 +223,7 @@ cu118-ampere = [
     ""unsloth[cu118only]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu121-ampere = [
     ""unsloth[huggingface]"",
@@ -231,7 +231,7 @@ cu121-ampere = [
     ""unsloth[cu121only]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu118-ampere-torch211 = [
     ""unsloth[huggingface]"",
@@ -239,7 +239,7 @@ cu118-ampere-torch211 = [
     ""unsloth[cu118onlytorch211]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu121-ampere-torch211 = [
     ""unsloth[huggingface]"",
@@ -247,7 +247,7 @@ cu121-ampere-torch211 = [
     ""unsloth[cu121onlytorch211]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu118-ampere-torch220 = [
     ""unsloth[huggingface]"",
@@ -255,7 +255,7 @@ cu118-ampere-torch220 = [
     ""unsloth[cu118onlytorch220]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu121-ampere-torch220 = [
     ""unsloth[huggingface]"",
@@ -263,7 +263,7 @@ cu121-ampere-torch220 = [
     ""unsloth[cu121onlytorch220]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu118-ampere-torch230 = [
     ""unsloth[huggingface]"",
@@ -271,7 +271,7 @@ cu118-ampere-torch230 = [
     ""unsloth[cu118onlytorch230]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 cu121-ampere-torch230 = [
     ""unsloth[huggingface]"",
@@ -279,7 +279,7 @@ cu121-ampere-torch230 = [
     ""unsloth[cu121onlytorch230]"",
     ""packaging"",
     ""ninja"",
-    ""flash-attn"",
+    ""flash-attn>=2.6.3"",
 ]
 
 [project.urls]
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 994f97a..fe3aa90 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -21,6 +21,7 @@ __all__ = [
     ""xformers_version"",
     ""__version__"",
     ""HAS_FLASH_ATTENTION"",
+    ""HAS_FLASH_ATTENTION_SOFTCAPPING"",
     ""PRE_CHECK"",
     ""platform_system"",
     ""patch_tokenizer"",
@@ -140,6 +141,8 @@ from transformers.utils.import_utils import _is_package_available
 
 major_version, minor_version = torch.cuda.get_device_capability()
 SUPPORTS_BFLOAT16 = False
+HAS_FLASH_ATTENTION = False
+HAS_FLASH_ATTENTION_SOFTCAPPING = False
 
 if major_version >= 8:
     SUPPORTS_BFLOAT16 = True
@@ -148,6 +151,17 @@ if major_version >= 8:
         try:
             from flash_attn.flash_attn_interface import flash_attn_cuda
             HAS_FLASH_ATTENTION = True
+
+            # Also check for softcapping
+            from flash_attn import __version__ as flash_attn_version
+            HAS_FLASH_ATTENTION_SOFTCAPPING = Version(flash_attn_version) >= Version(""2.6.3"")
+            if not HAS_FLASH_ATTENTION_SOFTCAPPING:
+                print(
+                    ""Unsloth: If you want to finetune Gemma 2, upgrade flash-attn to version 2.6.3 or higher!\n""\
+                    ""Newer versions support faster and less memory usage kernels for Gemma 2's attention softcapping!\n""\
+                    ""To update flash-attn, do the below:\n""\
+                    '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
+                )
         except:
             print(
                 ""Unsloth: Your Flash Attention 2 installation seems to be broken?\n""\
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 0d21c47..1cbaf5b 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -56,6 +56,8 @@ except:
     Gemma2FlashAttention2 = Gemma2Attention
 pass
 
+if HAS_FLASH_ATTENTION_SOFTCAPPING:
+    from flash_attn import flash_attn_func
 
 # [TODO] We must randomnly use torch.compile?
 # I checked the gradients and formulas and I'm sure it's correct.
@@ -126,8 +128,36 @@ def Gemma2Attention_fast_forward(
         V = torch.cat([past_key_value[1], V], dim = 2)
     pass
     past_key_value = (K, V) if use_cache else None
-    
-    A = slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, kv_seq_len)
+
+    # Only enable if the attention_mask is True
+    has_sliding_window = type(causal_mask) is bool and causal_mask is True
+    if HAS_FLASH_ATTENTION_SOFTCAPPING and attention_mask is None:
+        window = (-1, -1)
+        if has_sliding_window:
+            sw = getattr(self.config, ""sliding_window"", None)
+            sw = kv_seq_len if (sw is None or sw == ""null"") else sw
+            window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)
+        pass
+
+        # FA uses 1 / sqrt for softmax_scale!
+        if not hasattr(self, ""_flash_attention_softmax_scale""):
+            self._flash_attention_softmax_scale = 1.0 / (self.config.query_pre_attn_scalar**0.5)
+        pass
+
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+        A = flash_attn_func(
+            Q, K, V,
+            causal = True,
+            softcap = self.config.attn_logit_softcapping,
+            softmax_scale = self._flash_attention_softmax_scale,
+            window_size = window,
+        )
+        A = A.reshape(bsz, q_len, n_heads*head_dim)
+    else:
+        A = slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, kv_seq_len)
+    pass
     A = self.apply_o(self, A)
     return A, None, past_key_value
 pass
@@ -205,6 +235,8 @@ pass
 from math import sqrt as math_sqrt
 KV_CACHE_INCREMENT = 256 # KV Cache update size
 torch_nn_functional_softmax = torch.nn.functional.softmax
+torch_matmul = torch.matmul
+torch_tanh   = torch.tanh
 
 def Gemma2Attention_fast_forward_inference(
     self,
@@ -322,13 +354,13 @@ def Gemma2Attention_fast_forward_inference(
     # if bsz == 1:
     Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
     # It seems like doing (Q * scalar) @ K is better than (Q @ K) * scalar to stop overflows
-    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+    A = torch_matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
     # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
 
-    A *= self.reciprocal_t; torch.tanh(A, out = A); A *= self.t;  # Logit softcapping
+    A *= self.reciprocal_t; torch_tanh(A, out = A); A *= self.t;  # Logit softcapping
 
     A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
-    A = torch.matmul(A, Vnn, out = Qn)
+    A = torch_matmul(A, Vnn, out = Qn)
     # else:
     #     A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
     # pass
@@ -359,19 +391,24 @@ def Gemma2Model_fast_forward_inference(
     bsz, q_len, hd = hidden_states.shape
     seq_len = past_key_values[0][0].shape[-2]
     if bsz != 1:
-        SWA = _prepare_4d_causal_attention_mask_for_sdpa(
-            attention_mask,
-            (bsz, q_len),
-            hidden_states,
-            seq_len,
-            sliding_window = self.config.sliding_window,
-        )
-        GA = _prepare_4d_causal_attention_mask_for_sdpa(
-            attention_mask,
-            (bsz, q_len),
-            hidden_states,
-            seq_len,
-        )
+        if HAS_FLASH_ATTENTION_SOFTCAPPING:
+            SWA = True
+            GA  = False
+        else:
+            SWA = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask,
+                (bsz, q_len),
+                hidden_states,
+                seq_len,
+                sliding_window = self.config.sliding_window,
+            )
+            GA = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask,
+                (bsz, q_len),
+                hidden_states,
+                seq_len,
+            )
+        pass
     else:
         SWA = attention_mask
         GA  = attention_mask
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 496a37e..b5244ed 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -682,23 +682,28 @@ def LlamaModel_fast_forward(
 
     # Gemma2 has alternating SWA and global attn
     if IS_GEMMA2 and not hasattr(self, ""SWA_mask""):
-        n = self.config.max_position_embeddings
-        # masked_fill is making stuff slower!
-        # self. GA_mask = create_boolean_mask(n = n, sliding_window = 0)
-        # self.SWA_mask = create_boolean_mask(n = n, sliding_window = self.config.sliding_window)
-        from transformers.modeling_attn_mask_utils import AttentionMaskConverter
-        self.SWA_mask = AttentionMaskConverter(
-            is_causal = True,
-            sliding_window = self.config.sliding_window,
-        )\
-            .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
-            .squeeze(0).squeeze(0)
-
-        self.GA_mask = AttentionMaskConverter(
-            is_causal = True,
-        )\
-            .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
-            .squeeze(0).squeeze(0)
+        if HAS_FLASH_ATTENTION_SOFTCAPPING:
+            self.SWA_mask = True
+            self.GA_mask  = False
+        else:
+            n = self.config.max_position_embeddings
+            # masked_fill is making stuff slower!
+            # self. GA_mask = create_boolean_mask(n = n, sliding_window = 0)
+            # self.SWA_mask = create_boolean_mask(n = n, sliding_window = self.config.sliding_window)
+            from transformers.modeling_attn_mask_utils import AttentionMaskConverter
+            self.SWA_mask = AttentionMaskConverter(
+                is_causal = True,
+                sliding_window = self.config.sliding_window,
+            )\
+                .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
+                .squeeze(0).squeeze(0)
+
+            self.GA_mask = AttentionMaskConverter(
+                is_causal = True,
+            )\
+                .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
+                .squeeze(0).squeeze(0)
+        pass
     pass
 
     # Go through every layer!
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index f22e81e..47152d6 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -12,6 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from ._utils import is_bfloat16_supported, HAS_FLASH_ATTENTION, HAS_FLASH_ATTENTION_SOFTCAPPING
 from .llama import FastLlamaModel, logger
 from .mistral import FastMistralModel
 from .qwen2 import FastQwen2Model
@@ -42,6 +43,7 @@ def __get_model_name(
     FLOAT_TO_INT_MAPPER = None,
 ):
 
+    model_name = str(model_name)
     if not SUPPORTS_FOURBIT and model_name.lower() in INT_TO_FLOAT_MAPPER:
         model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
         logger.warning_once(
@@ -232,6 +234,21 @@ class FastLanguageModel(FastLlamaModel):
                     f'Try `pip install --upgrade ""transformers>=4.42.3""`\n'\
                     f""to obtain the latest transformers build, then restart this session.""\
                 )
+            # Also check for softcapping support in flash-attn which is faster!
+            if is_bfloat16_supported() and not HAS_FLASH_ATTENTION:
+                print(
+                    ""Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n""\
+                    ""To install flash-attn, do the below:\n""\
+                    '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
+                )
+            elif HAS_FLASH_ATTENTION and not HAS_FLASH_ATTENTION_SOFTCAPPING:
+                print(
+                    ""Unsloth: If you want to finetune Gemma 2, upgrade flash-attn to version 2.6.3 or higher!\n""\
+                    ""Newer versions support faster and less memory usage kernels for Gemma 2's attention softcapping!\n""\
+                    ""To update flash-attn, do the below:\n""\
+                    '\npip install --no-deps --upgrade ""flash-attn>=2.6.3""'
+                )
+            
             dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 462555f..57ba676 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -241,6 +241,14 @@ __INT_TO_FLOAT_MAPPER = \
     ""unsloth/Mistral-Large-Instruct-2407-bnb-4bit"" : (
         ""mistralai/Mistral-Large-Instruct-2407"",
     ),
+    ""unsloth/gemma-2-2b-bnb-4bit"" : (
+        ""unsloth/gemma-2-2b"",
+        ""google/gemma-2-2b"",
+    ),
+    ""unsloth/gemma-2-2b-it-bnb-4bit"" : (
+        ""unsloth/gemma-2-2b-it"",
+        ""google/gemma-2-2b-it"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 19902a3..c3baac2 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -65,8 +65,7 @@ try:
     libcuda_dirs()
 except:
     warnings.warn(
-        ""CUDA is not linked properly.\n""\
-        ""We shall run `ldconfig /usr/lib64-nvidia` to try to fix it.""
+        ""Running `ldconfig /usr/lib64-nvidia` to link CUDA.""\
     )
     os.system(""ldconfig /usr/lib64-nvidia"")
     importlib.reload(bnb)
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 2cf3acb..ec34880 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -41,12 +41,13 @@ def _rms_layernorm_forward(
     r += row_idx * r_row_stride
 
     X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)
-    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)
+    W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)
 
     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols
-    inv_var = 1 / tl.sqrt(row_var + eps)
+    inv_var = 1.0 / tl.sqrt(row_var + eps)
     tl.store(r, inv_var)
     normed = X_row * inv_var
+    normed = normed.to(W_row.dtype) # Exact copy from HF
     output = normed * W_row
     tl.store(Y + col_offsets, output, mask = mask)
 pass
diff --git a/unsloth/kernels/swiglu.py b/unsloth/kernels/swiglu.py
index 037dcda..4e9b7ba 100644
--- a/unsloth/kernels/swiglu.py
+++ b/unsloth/kernels/swiglu.py
@@ -25,10 +25,11 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
     mask = offsets < n_elements
 
     e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)
-    g_row = tl.load(g + offsets, mask = mask, other = 0).to(tl.float32)
+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)
 
     # f = e * sigmoid(e)
     f_row = e_row / (1 + tl.exp(-e_row))
+    f_row = f_row.to(g_row.dtype) # Exact copy from HF
     # h = f * g
     h_row = f_row * g_row
 
@@ -53,12 +54,13 @@ def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
     mask = offsets < n_elements
 
-    DW_row = tl.load(DW + offsets, mask = mask, other = 0).to(tl.float32)
-    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)
-    g_row  = tl.load(g  + offsets, mask = mask, other = 0).to(tl.float32)
+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)
+    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)
+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)
 
     # f = e * sigmoid(e)
-    se_row = 1 / (1 + tl.exp(-e_row))
+    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))
+    se_row = se_row.to(e_row.dtype) # Exact copy from HF
     # f = e * se
     f_row = e_row * se_row
     # h = f * g
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index c88ade6..35b6de1 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -14,9 +14,7 @@
 
 import torch
 from typing import Union, Optional, List, Any, Callable
-import numpy as np
 import warnings
-import gc
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f370c5b..309625d 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -15,7 +15,6 @@
 import torch
 from typing import Optional, Tuple, List, Union
 from torch.nn.functional import scaled_dot_product_attention
-from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.models.llama.modeling_llama import (
     logger,
     BaseModelOutputWithPast,
@@ -46,16 +45,13 @@ except:
     LlamaFlashAttention2 = LlamaAttention
 pass
 
-from peft import PeftModelForCausalLM
-import gc
-import peft
-import bitsandbytes as bnb
-import numpy as np
-import types
-
 from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig
 from transformers import set_seed as transformers_set_seed
 from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
+from peft import PeftModelForCausalLM
+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
+from peft.tuners.lora import Linear4bit as Peft_Linear4bit
+from ..save import patch_saving_functions
 
 
 def original_apply_qkv(self, X):
@@ -110,18 +106,15 @@ def LlamaAttention_fast_forward_inference(
     bsz, _, _ = hidden_states.size()
     K1, V1 = past_key_value
 
-    Wq = self.q_proj.weight
-    Wk = self.k_proj.weight
-    Wv = self.v_proj.weight
-    Wo = self.o_proj.weight
-
     n_heads    = self.num_heads
     n_groups   = self.num_key_value_groups
     n_kv_heads = self.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
-    Qn, Kn, Vn = original_apply_qkv(self, Xn)
+    Qn = self.q_proj(Xn)
+    Kn = self.k_proj(Xn)
+    Vn = self.v_proj(Xn)
     Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)
     Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
     Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
@@ -156,6 +149,28 @@ def LlamaAttention_fast_forward_inference(
 pass
 
 
+torch_silu = torch.nn.functional.silu
+def fast_mlp_inference(self, X):
+    gate = self.gate_proj(X)
+    up   = self.up_proj(X)
+    gate = torch_silu(gate, inplace = True)
+    gate *= up
+    X = self.down_proj(gate)
+    return X
+pass
+
+
+def fast_rms_layernorm_inference(self, X):
+    X = X.to(torch.float32)
+    variance = X.square().mean(-1, keepdim = True)
+    variance += self.variance_epsilon
+    X *= variance.rsqrt_()
+    X = X.to(residual.dtype)
+    X *= self.weight
+    return X
+pass
+
+
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320
 def LlamaAttention_fast_forward(
     self,
@@ -287,28 +302,51 @@ def LlamaDecoderLayer_fast_forward(
             (see `past_key_values`).
         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
     """"""
-    residual = hidden_states
-
-    hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+    bsz, q_len, hd = hidden_states.size()
+
+    if (self.training):
+        # Self Attention
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+        hidden_states = residual + hidden_states
 
-    # Self Attention
-    hidden_states, self_attn_weights, present_key_value = self.self_attn(
-        hidden_states=hidden_states,
-        causal_mask=causal_mask,
-        attention_mask=attention_mask,
-        position_ids=position_ids,
-        past_key_value=past_key_value,
-        output_attentions=output_attentions,
-        use_cache=use_cache,
-        padding_mask=padding_mask,
-    )
-    hidden_states = residual + hidden_states
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states
+    else:
+        # Self Attention
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+        hidden_states += residual
 
-    # Fully Connected
-    residual = hidden_states
-    hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
-    hidden_states = self.mlp(hidden_states)
-    hidden_states = residual + hidden_states
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)
+        hidden_states += residual
+    pass
 
     outputs = (hidden_states,)
 
@@ -378,6 +416,7 @@ def LlamaModel_fast_forward(
     if past_key_values is not None:
         past_key_values_length = past_key_values[0][0].shape[2]
         seq_length_with_past = seq_length_with_past + past_key_values_length
+    pass
 
     # We already handle KV cache position_ids ourselves.
     if (past_key_values_length != 0):
@@ -391,10 +430,12 @@ def LlamaModel_fast_forward(
         position_ids = position_ids.view(-1, seq_length).to(torch.int32)#.long()
     else:
         position_ids = None
+    pass
 
     if position_ids is not None:
         if position_ids.shape[0] != batch_size:
             position_ids = position_ids.repeat((batch_size, 1))
+    pass
 
     # embed positions
     if inputs_embeds is None:
@@ -403,19 +444,22 @@ def LlamaModel_fast_forward(
     # Ignore attention_mask
     if attention_mask is None:
         padding_mask = None
+    elif self.training:
+        attention_mask = None
+        padding_mask = None
     else:
         if 0 in attention_mask:
             padding_mask = attention_mask
         else:
             padding_mask = None
 
+        from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
         attention_mask = _prepare_4d_causal_attention_mask(
             attention_mask,
             (batch_size, seq_length),
             inputs_embeds,
             past_key_values_length,
-            sliding_window = None if not hasattr(self.config, ""sliding_window"") else \
-                self.config.sliding_window,
+            sliding_window = getattr(self.config, ""sliding_window""),
         )
     pass
 
@@ -479,7 +523,11 @@ def LlamaModel_fast_forward(
             all_self_attns += (layer_outputs[1],)
     pass
 
-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)
+    if (self.training):
+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)
+    else:
+        hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)
+    pass
 
     # add hidden states from the last decoder layer
     if output_hidden_states:
@@ -665,6 +713,7 @@ class FastLlamaModel:
                 bnb_4bit_quant_type       = ""nf4"",
                 bnb_4bit_compute_dtype    = dtype,
             )
+        pass
 
         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12
         # RoPE Scaling's max_position_embeddings must be updated
@@ -714,6 +763,7 @@ class FastLlamaModel:
                 token = token,
             )
         pass
+        patch_saving_functions(tokenizer)
 
         # Fix up config for transformers uploading PEFT
         name = model.config._name_or_path
@@ -721,6 +771,7 @@ class FastLlamaModel:
             name = name[:len(name) - len(""-bnb-4bit"")]
             model.config.update({""_name_or_path"" : name})
         pass
+
         # Log Unsloth version for future fastpaths for inference
         model.config.update({""unsloth_version"" : __version__})
 
@@ -751,7 +802,7 @@ class FastLlamaModel:
         correct_dtype = lm_head.weight.dtype
 
         for name, module in model.named_modules():
-            if isinstance(module, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):
+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
                 weight = module.weight
                 quant_state = weight.quant_state
 
@@ -766,8 +817,10 @@ class FastLlamaModel:
         pass
 
         # Clear deleted GPU items
-        gc.collect()
-        torch.cuda.empty_cache()
+        import gc
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
         return model
     pass
 
@@ -782,11 +835,26 @@ class FastLlamaModel:
         lora_dropout = 0,
         bias = ""none"",
         layers_to_transform = None,
+        layers_pattern = None,
         use_gradient_checkpointing = True,
         random_state = 3407,
         max_seq_length = 2048, # not used anymore
+        use_rslora = False,
+        init_lora_weights = True,
+        loftq_config = None,
         **kwargs,
     ):
+        if isinstance(model, PeftModelForCausalLM):
+            raise TypeError(
+                ""Unsloth: Your model already has LoRA adapters. No need to run this again!""
+            )
+        pass
+
+        import inspect
+        signature = str(inspect.signature(LoraConfig))
+        SUPPORTS_LOFTQ  = ""loftq_config"" in signature
+        SUPPORTS_RSLORA = ""use_rslora""   in signature
+
         assert(max_seq_length <= model.max_seq_length)
 
         if lora_dropout != 0:
@@ -794,11 +862,61 @@ class FastLlamaModel:
                 f""Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = {lora_dropout}.\n""\
                 f""Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.""
             )
+        pass
+
         if bias != ""none"":
             logger.warning_once(
                 f""Unsloth: bias = `none` is supported for fast patching. You are using bias = {bias}.\n""\
                 f""Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.""
             )
+        pass
+
+        if not (type(init_lora_weights) is bool or \
+            init_lora_weights == ""gaussian"" or init_lora_weights == ""loftq""):
+            raise ValueError(
+                'Unsloth: `init_lora_weights` must be either [True, False, ""gaussian"", ""loftq""].'
+            )
+        pass
+
+        if init_lora_weights == ""loftq"":
+
+            if not SUPPORTS_LOFTQ:
+                import peft
+                raise RuntimeError(
+                    f""Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\n""\
+                    ""Please install PEFT 0.7.2 or higher.\n""\
+                    ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
+                )
+            pass
+
+            if loftq_config is None:
+                from peft import LoftQConfig
+                logger.warning_once(
+                    f""Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\n""\
+                    f""We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.""
+                )
+                loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)
+            pass
+            
+            if hasattr(model.config, ""quantization_config""):
+                raise ValueError(
+                    ""Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\n""\
+                    ""Reload your model without any quantization by setting `load_in_4bit = False`.""
+                )
+            pass
+        pass
+
+        assert(type(use_rslora) is bool)
+        if use_rslora:
+            if not SUPPORTS_RSLORA:
+                import peft
+                raise RuntimeError(
+                    f""Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\n""\
+                    ""Please install PEFT 0.7.2 or higher.\n""\
+                    ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
+                )
+            pass
+        pass
 
         transformers_set_seed(random_state)
 
@@ -810,16 +928,23 @@ class FastLlamaModel:
         pass
 
         # Get LoRA
-        lora_config = LoraConfig(
-            r              = r,
-            lora_alpha     = lora_alpha,
-            target_modules = target_modules,
-            lora_dropout   = lora_dropout,
-            bias           = bias,
-            task_type      = TaskType.CAUSAL_LM,
+        arguments = dict(
+            r                   = r,
+            lora_alpha          = lora_alpha,
+            target_modules      = target_modules,
+            lora_dropout        = lora_dropout,
+            bias                = bias,
+            task_type           = TaskType.CAUSAL_LM,
             layers_to_transform = layers_to_transform,
+            init_lora_weights   = init_lora_weights,
+            loftq_config        = loftq_config,
+            use_rslora          = use_rslora,
             **kwargs,
         )
+        if not SUPPORTS_LOFTQ:  del arguments[""loftq_config""]
+        if not SUPPORTS_RSLORA: del arguments[""use_rslora""]
+
+        lora_config = LoraConfig(**arguments)
 
         model = prepare_model_for_kbit_training(
             model,
@@ -828,10 +953,21 @@ class FastLlamaModel:
         )
         model = _get_peft_model(model, lora_config)
 
+        # Fix up config for transformers uploading PEFT
+        name = model.peft_config[""default""].base_model_name_or_path
+        if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+            name = name[:len(name) - len(""-bnb-4bit"")]
+            model.peft_config[""default""].base_model_name_or_path = name
+        pass
+        # Add revision to enable future fast inference paths
+        model.peft_config[""default""].revision = f""unsloth""
+
         # Do patching
         n_mlp = 0
         n_qkv = 0
         n_o   = 0
+        import types
+
         if lora_dropout == 0 and bias == ""none"":
             for idx, layer in enumerate(model.model.model.layers):
 
@@ -897,6 +1033,7 @@ class FastLlamaModel:
             f""Unsloth {__version__} patched {len(model.model.model.layers)} layers with ""\
             f""{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers."",
         )
+        patch_saving_functions(model)
 
         # Patch cross entropy loss labels
         # Fixes https://github.com/unslothai/unsloth/issues/10
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 48200c3..b4ad3aa 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -16,16 +16,9 @@ from .llama import FastLlamaModel, logger
 from .mistral import FastMistralModel
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
+from peft import PeftConfig, PeftModel
+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER
 
-FOURBIT_MAPPER = \
-{
-    ""unsloth/mistral-7b-bnb-4bit""    : ""unsloth/mistral-7b"",
-    ""unsloth/llama-2-7b-bnb-4bit""    : ""unsloth/llama-2-7b"",
-    ""unsloth/llama-2-13b-bnb-4bit""   : ""unsloth/llama-13-7b"",
-    ""unsloth/codellama-34b-bnb-4bit"" : ""codellama/CodeLlama-34b-hf"",
-    ""unsloth/zephyr-sft-bnb-4bit""    : ""unsloth/zephyr-sft"",
-    ""unsloth/tinyllama-bnb-4bit""     : ""unsloth/tinyllama"",
-}
 
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
 major, minor = transformers_version.split(""."")[:2]
@@ -34,6 +27,39 @@ SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)
 del major, minor
 
 
+def _get_model_name(model_name, load_in_4bit = True):
+
+    if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:
+        model_name = INT_TO_FLOAT_MAPPER[model_name]
+        logger.warning_once(
+            f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
+            f""4bit loading.\nThe minimum required version is 4.37.\n""\
+            f'Try `pip install ""git+https://github.com/huggingface/transformers.git""`\n'\
+            f""to obtain the latest transformers build, then restart this session.\n""\
+            f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
+        )
+    
+    elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:
+        new_model_name = INT_TO_FLOAT_MAPPER[model_name]
+        logger.warning_once(
+            f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
+            f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
+        )
+        model_name = new_model_name
+
+    elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:
+        new_model_name = FLOAT_TO_INT_MAPPER[model_name]
+        logger.warning_once(
+            f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
+            f""We shall load `{new_model_name}` for 4x faster loading.""
+        )
+        model_name = new_model_name
+    pass
+
+    return model_name
+pass
+
+
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
@@ -47,25 +73,27 @@ class FastLanguageModel(FastLlamaModel):
         fix_tokenizer = True,
         *args, **kwargs,
     ):
-        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:
-            model_name = FOURBIT_MAPPER[model_name]
-            logger.warning_once(
-                f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
-                f""4bit loading.\nThe minimum required version is 4.37.\n""\
-                f'Try `pip install ""git+https://github.com/huggingface/transformers.git""`\n'\
-                f""to obtain the latest transformers build, then restart this session.\n""\
-                f""For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).""
-            )
-        elif not load_in_4bit and model_name in FOURBIT_MAPPER:
-            new_model_name = FOURBIT_MAPPER[model_name]
-            logger.warning_once(
-                f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
-                f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
-            )
-            model_name = new_model_name
+        old_model_name = model_name
+        model_name = _get_model_name(model_name, load_in_4bit)
+
+        # First check if it's a normal model via AutoConfig
+        is_peft = False
+        try:
+            model_config = AutoConfig.from_pretrained(model_name, token = token)
+            is_peft = False
+        except:
+            try:
+                # Most likely a PEFT model
+                peft_config = PeftConfig.from_pretrained(model_name, token = token)
+            except:
+                raise RuntimeError(f""Unsloth: `{model_name}` is not a full model or a PEFT model."")
+            
+            # Check base model again for PEFT
+            model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
+            model_config = AutoConfig.from_pretrained(model_name, token = token)
+            is_peft = True
         pass
 
-        model_config = AutoConfig.from_pretrained(model_name)
         model_type = model_config.model_type
 
         if   model_type == ""llama"":   dispatch_model = FastLlamaModel
@@ -75,8 +103,9 @@ class FastLanguageModel(FastLlamaModel):
                 f""Unsloth: {model_name} not supported yet!\n""\
                 ""Make an issue to https://github.com/unslothai/unsloth!"",
             )
+        pass
 
-        return dispatch_model.from_pretrained(
+        model, tokenizer = dispatch_model.from_pretrained(
             model_name = model_name,
             max_seq_length = max_seq_length,
             dtype = dtype,
@@ -87,5 +116,30 @@ class FastLanguageModel(FastLlamaModel):
             fix_tokenizer = fix_tokenizer,
             *args, **kwargs,
         )
+
+        if load_in_4bit:
+            # Fix up bitsandbytes config
+            quantization_config = \
+            {
+                # Sometimes torch_dtype is not a string!!
+                ""bnb_4bit_compute_dtype""           : model.config.to_dict()[""torch_dtype""],
+                ""bnb_4bit_quant_type""              : ""nf4"",
+                ""bnb_4bit_use_double_quant""        : True,
+                ""llm_int8_enable_fp32_cpu_offload"" : False,
+                ""llm_int8_has_fp16_weight""         : False,
+                ""llm_int8_skip_modules""            : ""null"",
+                ""llm_int8_threshold""               : 6.0,
+                ""load_in_4bit""                     : True,
+                ""load_in_8bit""                     : False,
+                ""quant_method""                     : ""bitsandbytes"",
+            }
+            model.config.update({""quantization_config"" : quantization_config})
+        pass
+
+        if is_peft:
+            # Now add PEFT adapters
+            model = PeftModel.from_pretrained(model, old_model_name)
+        pass
+        return model, tokenizer
     pass
 pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
new file mode 100644
index 0000000..124eaf7
--- /dev/null
+++ b/unsloth/models/mapper.py
@@ -0,0 +1,56 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+__all__ = [
+    ""INT_TO_FLOAT_MAPPER"",
+    ""FLOAT_TO_INT_MAPPER"",
+]
+
+__INT_TO_FLOAT_MAPPER = \
+{
+    ""unsloth/mistral-7b-bnb-4bit""    : (
+        ""unsloth/mistral-7b"",
+        ""mistralai/Mistral-7B-v0.1"",
+    ),
+    ""unsloth/llama-2-7b-bnb-4bit""    : (
+        ""unsloth/llama-2-7b"",
+        ""meta-llama/Llama-2-7b-hf"",
+    ),
+    ""unsloth/llama-2-13b-bnb-4bit""   : (
+        ""unsloth/llama-13-7b"",
+        ""meta-llama/Llama-2-13b-hf"",
+    ),
+    ""unsloth/codellama-34b-bnb-4bit"" : (
+        ""codellama/CodeLlama-34b-hf"",
+    ),
+    ""unsloth/zephyr-sft-bnb-4bit""    : (
+        ""unsloth/zephyr-sft"",
+        ""alignment-handbook/zephyr-7b-sft-full"",
+    ),
+    ""unsloth/tinyllama-bnb-4bit""     : (
+        ""unsloth/tinyllama"",
+        ""TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"",
+    ),
+}
+
+INT_TO_FLOAT_MAPPER = {}
+FLOAT_TO_INT_MAPPER = {}
+
+for key, values in __INT_TO_FLOAT_MAPPER.items():
+    INT_TO_FLOAT_MAPPER[key] = values[0]
+
+    for value in values:
+        FLOAT_TO_INT_MAPPER[value] = key
+    pass
+pass
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 01b9dae..17da5cc 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -343,6 +343,7 @@ class FastMistralModel(FastLlamaModel):
                 token = token,
             )
         pass
+        patch_saving_functions(tokenizer)
 
         # Fix up config for transformers uploading PEFT
         name = model.config._name_or_path
@@ -350,6 +351,7 @@ class FastMistralModel(FastLlamaModel):
             name = name[:len(name) - len(""-bnb-4bit"")]
             model.config.update({""_name_or_path"" : name})
         pass
+        
         # Log Unsloth version for future fastpaths for inference
         model.config.update({""unsloth_version"" : __version__})
         
diff --git a/unsloth/save.py b/unsloth/save.py
index f94a1b8..29175a1 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -12,24 +12,26 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from peft import PeftModelForCausalLM
-from collections import OrderedDict
-import bitsandbytes as bnb
-import peft
-import gc
-import os
-from tqdm import tqdm as ProgressBar
-import shutil
-from typing import Optional, Callable, Union
+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
+from peft.tuners.lora import Linear4bit as Peft_Linear4bit
+from typing import Optional, Callable, Union, List
 import torch
+import os
+import pickle
+import gc
 from transformers.models.llama.modeling_llama import logger
 from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters
+import subprocess
+import psutil
 
 __all__ = [
+    ""print_quantization_methods"",
     ""unsloth_save_model"",
-    #""colab_quantize_to_gguf"",
+    ""save_to_gguf"",
+    ""patch_saving_functions"",
 ]
 
+
 LLAMA_WEIGHTS = (
     ""self_attn.q_proj"", ""self_attn.k_proj"", ""self_attn.v_proj"", ""self_attn.o_proj"",
     ""mlp.gate_proj"", ""mlp.up_proj"", ""mlp.down_proj"",
@@ -41,25 +43,36 @@ LLAMA_LAYERNORMS = (
 # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html
 ALLOWED_QUANTS = \
 {
-    ""q2_k""   : ""Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors."",
-    ""q3_k_l"" : ""Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
-    ""q3_k_m"" : ""Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
-    ""q3_k_s"" : ""Uses Q3_K for all tensors"",
-    ""q4_0""   : ""Original quant method, 4-bit."",
-    ""q4_1""   : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
-    ""q4_k_m"" : ""Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K"",
-    ""q4_k_s"" : ""Uses Q4_K for all tensors"",
-    ""q5_0""   : ""Higher accuracy, higher resource usage and slower inference."",
-    ""q5_1""   : ""Even higher accuracy, resource usage and slower inference."",
-    ""q5_k_m"" : ""Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K"",
-    ""q5_k_s"" : ""Uses Q5_K for all tensors"",
-    ""q6_k""   : ""Uses Q8_K for all tensors"",
-    ""q8_0""   : ""Almost indistinguishable from float16. High resource use and slow. Not recommended for most users."",
+    ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
+    ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
+    ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
+    ""f32""     : ""Not recommended. Retains 100% accuracy, but super slow and memory hungry."",
+    ""f16""     : ""Fastest conversion + retains 100% accuracy. Slow and memory hungry."",
+    ""q8_0""    : ""Fast conversion. High resource use, but generally acceptable."",
+    ""q4_k_m""  : ""Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K"",
+    ""q5_k_m""  : ""Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K"",
+    ""q2_k""    : ""Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors."",
+    ""q3_k_l""  : ""Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+    ""q3_k_m""  : ""Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+    ""q3_k_s""  : ""Uses Q3_K for all tensors"",
+    ""q4_0""    : ""Original quant method, 4-bit."",
+    ""q4_1""    : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
+    ""q4_k_s""  : ""Uses Q4_K for all tensors"",
+    ""q5_0""    : ""Higher accuracy, higher resource usage and slower inference."",
+    ""q5_1""    : ""Even higher accuracy, resource usage and slower inference."",
+    ""q5_k_s""  : ""Uses Q5_K for all tensors"",
+    ""q6_k""    : ""Uses Q8_K for all tensors"",
 }
 
+def print_quantization_methods():
+    for key, value in ALLOWED_QUANTS.items():
+        print(f'""{key}""  ==> {value}')
+    pass
+pass
+
 
 def _merge_lora(layer, name):
-    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):
+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):
         # Is LoRA so we need to merge!
         W, quant_state, A, B, s = get_lora_parameters(layer)
         dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]
@@ -75,100 +88,362 @@ def _merge_lora(layer, name):
 pass
 
 
+def fast_save_pickle(shard, name):
+    # Use this if # CPUs is <= 2
+    print(f""Unsloth: Saving {name}..."")
+    torch.save(
+        shard,
+        name,
+        pickle_module = pickle,
+        pickle_protocol = pickle.HIGHEST_PROTOCOL,
+    )
+    return
+pass
+
+
 @torch.inference_mode
 def unsloth_save_model(
     model,
     tokenizer,
-    save_directory: Union[str, os.PathLike],
-    is_main_process: bool = True,
-    state_dict: Optional[dict] = None,
-    save_function: Callable = torch.save,
-    push_to_hub: bool = False,
-    max_shard_size: Union[int, str] = ""7GB"",
-    safe_serialization: bool = True,
-    variant: Optional[str] = None,
-    token: Optional[Union[str, bool]] = None,
-    save_peft_format: bool = True,
-    temporary_location = ""_unsloth_temporary_saved_buffers"",
-    **kwargs,
+    save_directory       : Union[str, os.PathLike],
+    save_method          : str = ""lora"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    push_to_hub          : bool = False,
+    token                : Optional[Union[str, bool]] = None,
+    is_main_process      : bool = True,
+    state_dict           : Optional[dict] = None,
+    save_function        : Callable = torch.save,
+    max_shard_size       : Union[int, str] = ""5GB"",
+    safe_serialization   : bool = True,
+    variant              : Optional[str] = None,
+    save_peft_format     : bool = True,
+
+    # Push to hub
+    use_temp_dir         : Optional[bool] = None,
+    commit_message       : Optional[str] = None,
+    private              : Optional[bool] = None,
+    create_pr            : bool = False,
+    revision             : str = None,
+    commit_description   : str = None,
+    tags                 : List[str] = None,
+
+    # Our functions
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.9,
 ):
-    logger.warning_once(
-        ""Unsloth: `unsloth_save_model` is still in development mode.\n""\
-        ""If anything errors or breaks, please file a ticket on Github.\n""\
-        ""Also, if you used this successfully, please tell us on Discord!""
-    )
+    save_pretrained_settings = dict(locals())
+    for deletion in (""model"", ""tokenizer"", ""save_method"", ""temporary_location"", ""maximum_memory_usage""):
+        del save_pretrained_settings[deletion]
+    pass
+    import re
+
+    assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)
+
+    # Clean memory up first
+    for _ in range(3):
+        torch.cuda.empty_cache()
+        gc.collect()
+    pass
+
+    save_method = save_method.lower().replace("" "", ""_"")
+    if save_method != ""lora"" and save_method != ""merged_16bit"" and save_method != ""merged_4bit"":
+        raise RuntimeError(
+            ""Unsloth: You must select one of 3 options when saving models:\n""\
+            '""lora""         ==> This is the fastest and easiet. Just saves LoRA modules.\n'\
+            '""merged_16bit"" ==> This merges LoRA weights and saves to float16. Needed for llama.cpp / GGUF.\n'\
+            '""merged_4bit""  ==> This merges LoRA weights and saves to 4bit. Useful for DPO / inference.'
+        )
+    pass
+
+    if save_method == ""merged_4bit"":
+        print(""Unsloth: Merging 4bit and LoRA weights to 4bit..."")
+        print(""This might take 5 minutes..."")
+        model = model.merge_and_unload()
+        print(""Done."")
+    pass
+
+    if tags is not None:
+        assert(isinstance(tags, (list, tuple)))
+        tags = list(tags) + [""unsloth"",]
+    else:
+        tags = [""unsloth"",]
+    pass
+    save_pretrained_settings[""tags""] = tags
+
+    if (save_method == ""lora"") and push_to_hub:
+        if token is None:
+            raise RuntimeError(
+                ""Unsloth: Pushing to HF requires a token. Pass `token = 'hf_....'`\n""\
+                ""Go to https://huggingface.co/settings/tokens.""
+            )
+        pass
+
+        model.push_to_hub(
+            repo_id            = save_directory,
+            use_temp_dir       = use_temp_dir,
+            commit_message     = commit_message,
+            private            = private,
+            token              = token,
+            max_shard_size     = max_shard_size,
+            create_pr          = create_pr,
+            safe_serialization = safe_serialization,
+            revision           = revision,
+            commit_description = commit_description,
+            tags               = tags,
+        )
+        if tokenizer is not None:
+            tokenizer.push_to_hub(
+                repo_id            = save_directory,
+                use_temp_dir       = use_temp_dir,
+                commit_message     = commit_message,
+                private            = private,
+                token              = token,
+                max_shard_size     = max_shard_size,
+                create_pr          = create_pr,
+                safe_serialization = safe_serialization,
+                revision           = revision,
+                commit_description = commit_description,
+                tags               = tags,
+            )
+        pass
+        return save_directory
+    pass
+
+    # If push_to_hub, we must remove the .../ part of a repo
+    if push_to_hub and ""/"" in save_directory:
+
+        new_save_directory = save_directory[save_directory.find(""/""):]
+
+        logger.warning_once(
+            f""Unsloth: You are pushing to hub, but you passed your HF username.\n""\
+            f""We shall truncate {save_directory} to {new_save_directory}""
+        )
+
+        save_pretrained_settings[""save_directory""] = new_save_directory
+        save_directory = new_save_directory
+    pass
+    
+    if (save_method == ""merged_4bit"") or (save_method == ""lora"") or (
+        not hasattr(model, ""model"") or \
+        not hasattr(model.model, ""model"") or \
+        not hasattr(model.model.model, ""layers"")
+    ):
+        # Do general saving
+        
+        # Edit save_pretrained_settings
+        # [TODO] _create_repo has errors due to **kwargs getting accepted
+        for deletion in \
+            (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",):
+            del save_pretrained_settings[deletion]
+        pass
+        if hasattr(model, ""add_model_tags""):
+            model.add_model_tags([""unsloth"",])
+
+        if tokenizer is not None:
+            print(""Unsloth: Saving tokenizer..."", end = """")
+            tokenizer.save_pretrained(**save_pretrained_settings)
+            print("" Done."")
+        else:
+            print()
 
+        print(""Unsloth: Saving model..."", end = """")
+        if save_method != ""lora"": print("" This might take 10 minutes for Llama-7b..."", end = """")
+
+        model.save_pretrained(**save_pretrained_settings)
+        print("" Done."")
+        return save_directory
+    pass
+
+    print(""Unsloth: Merging 4bit and LoRA weights to 16bit..."")
+
+    # Determine max RAM usage minus sharding
+    max_ram = psutil.virtual_memory().available
+    sharded_ram_usage = 5 * 1024 * 1024 * 1024
+    if type(max_shard_size) is str:
+        gb_found = re.match(""([0-9]{1,})[\s]{0,}GB"", max_shard_size, flags = re.IGNORECASE)
+        mb_found = re.match(""([0-9]{1,})[\s]{0,}MB"", max_shard_size, flags = re.IGNORECASE)
+        if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024
+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 
+    elif type(max_shard_size) is int:
+        sharded_ram_usage = sharded_ram_usage
+    pass
+
+    # Switch to our fast saving modules if it's a slow PC!
+    n_cpus = psutil.cpu_count(logical = False)
+
+    if safe_serialization is None:
+        safe_serialization = True
+        save_pretrained_settings[""safe_serialization""] = safe_serialization
+
+    elif safe_serialization and (n_cpus <= 2):
+        logger.warning_once(
+            f""Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\n""\
+            f""We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n""\
+            f""To force `safe_serialization`, set it to None instead."",
+        )
+        safe_serialization = False
+        save_function = fast_save_pickle
+        save_pretrained_settings[""safe_serialization""] = safe_serialization
+        save_pretrained_settings[""save_function""]      = save_function
+    pass
+
+    # Only safe_serialization uses more RAM
+    if safe_serialization:
+        max_ram -= sharded_ram_usage
+    else:
+        max_ram -= sharded_ram_usage*0.25 # Uses much less
+    pass
+
+    max_ram = int(max(0, max_ram) * maximum_memory_usage)
+    print(f""Unsloth: Will use up to ""\
+          f""{round(max_ram/1024/1024/1024, 2)} out of ""\
+          f""{round(psutil.virtual_memory().total/1024/1024/1024, 2)} RAM for saving."")
+
+    # Max directory for disk saving
     if not os.path.exists(temporary_location):
         os.makedirs(temporary_location)
     pass
 
-    assert(hasattr(model, ""model""))
-    assert(hasattr(model.model, ""model""))
-    assert(hasattr(model.model.model, ""layers""))
-
     # HF also uses a OrderedDict
+    from collections import OrderedDict
     state_dict = OrderedDict()
-    state_dict[""model.embed_tokens.weight""] = model.model.model.embed_tokens.weight
+    state_dict[""model.embed_tokens.weight""] = model.model.model.embed_tokens.weight.data
 
-    print(""Unsloth: Merging 4bit and LoRA weights to 16bit..."")
+    max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)
+
+    from tqdm import tqdm as ProgressBar
     for j, layer in enumerate(ProgressBar(model.model.model.layers)):
         for item in LLAMA_WEIGHTS:
             proj = eval(f""layer.{item}"")
             name = f""model.layers.{j}.{item}.weight""
             W = _merge_lora(proj, name)
-            filename = os.path.join(temporary_location, f""{name}.pt"")
-            torch.save(W, filename)
-            state_dict[name] = torch.load(filename, map_location = ""cpu"", mmap = True)
+
+            if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:
+                # Save to GPU memory
+                state_dict[name] = W
+            # elif (max_ram - W.nbytes) > 0:
+            #     # Save to CPU memory
+            #     logger.warning_once(f""We will save to RAM and not VRAM now."")
+            #     state_dict[name] = W.to(""cpu"", non_blocking = True)
+            #     max_ram = max(max_ram - W.nbytes, 0)
+            else:
+                # Save to Disk
+                logger.warning_once(f""We will save to Disk and not RAM now."")
+                filename = os.path.join(temporary_location, f""{name}.pt"")
+                torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)
+                state_dict[name] = torch.load(filename, map_location = ""cpu"", mmap = True)
         pass
         for item in LLAMA_LAYERNORMS:
-            state_dict[f""model.layers.{j}.{item}.weight""] = eval(f""layer.{item}.weight"")
+            state_dict[f""model.layers.{j}.{item}.weight""] = eval(f""layer.{item}.weight.data"")
         pass
     pass
 
-    state_dict[""model.norm.weight""] = model.model.model.norm.weight
-    state_dict[""lm_head.weight""]    = model.model.lm_head.weight
-
-    print(""Unsloth: Saving tokenizer..."")
-    tokenizer.save_pretrained(
-        save_directory = save_directory,
-        is_main_process = is_main_process,
-        state_dict = state_dict,
-        save_function = save_function,
-        push_to_hub = push_to_hub,
-        max_shard_size = max_shard_size,
-        safe_serialization = safe_serialization,
-        variant = variant,
-        token = token,
-        save_peft_format = save_peft_format,
-    )
+    state_dict[""model.norm.weight""] = model.model.model.norm.weight.data
+    state_dict[""lm_head.weight""]    = model.model.lm_head.weight.data
 
-    print(""Unsloth: Saving model. This will take 5 minutes for Llama-7b..."")
-    model.model.save_pretrained(
-        save_directory = save_directory,
-        is_main_process = is_main_process,
-        state_dict = state_dict,
-        save_function = save_function,
-        push_to_hub = push_to_hub,
-        max_shard_size = max_shard_size,
-        safe_serialization = safe_serialization,
-        variant = variant,
-        token = token,
-        save_peft_format = save_peft_format,
-    )
+    # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter
+    for key, value in state_dict.items():
+        if hasattr(value, ""data""): state_dict[key] = value = value.data
+        if type(value) is not torch.Tensor:
+            logger.warning_once(f""Unsloth: {key} is not a Tensor but a {type(value)}."")
+        pass
+    pass
+
+    # Edit save_pretrained_settings
+    # [TODO] _create_repo has errors due to **kwargs getting accepted
+    save_pretrained_settings[""state_dict""] = state_dict
+    for deletion in \
+        (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",):
+        del save_pretrained_settings[deletion]
+    pass
+    if hasattr(model, ""add_model_tags""):
+        model.add_model_tags([""unsloth"",])
+
+    if tokenizer is not None:
+        print(""Unsloth: Saving tokenizer..."", end = """")
+        tokenizer.save_pretrained(**save_pretrained_settings)
+        print("" Done."")
+    else:
+        print()
+
+    print(""Unsloth: Saving model... This might take 5 minutes for Llama-7b..."")
+    model.model.save_pretrained(**save_pretrained_settings)
+    print(""Done."")
+
+    save_pretrained_settings[""state_dict""] = None
+
+    # for j, (key, value) in enumerate(state_dict.items()):
+    #     state_dict[key] = None
+    #     if j % 10 == 0:
+    #         torch.cuda.empty_cache()
+    #         gc.collect()
+    #     pass
+    # pass
+    # state_dict = None
+    # del state_dict
+    # torch.cuda.empty_cache()
+    # gc.collect()
 
     # Remove temporary location
+    import shutil
     shutil.rmtree(temporary_location)
+
+    # for _ in range(3):
+    #     torch.cuda.empty_cache()
+    #     gc.collect()
+    return save_directory
 pass
 
 
-""""""
-def _colab_quantize_to_gguf(save_directory, quantization_method = ""q4_k_m""):
+def install_llama_cpp_clone_non_blocking():
+    full_command = [""git"", ""clone"", ""https://github.com/ggerganov/llama.cpp""]
+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
+    return run_installer
+pass
 
-    logger.warning_once(
-        ""Unsloth: `colab_quantize_to_gguf` is still in development mode.\n""\
-        ""If anything errors or breaks, please file a ticket on Github.\n""\
-        ""Also, if you used this successfully, please tell us on Discord!""
-    )
+
+def install_llama_cpp_make_non_blocking():
+    env = { **os.environ, ""LLAMA_CUBLAS"": ""1"", }
+    n_jobs = max(int(psutil.cpu_count()*1.5), 1)
+    full_command = [""make"", ""-j"", str(n_jobs), ""-C"", ""llama.cpp""]
+    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
+    return run_installer
+pass
+
+
+def install_python_non_blocking(packages = []):
+    full_command = [""pip"", ""install""] + packages
+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)
+    return run_installer
+pass
+
+
+def install_llama_cpp_blocking():
+    commands = [
+        ""git clone https://github.com/ggerganov/llama.cpp"",
+        f""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}"",
+        ""pip install gguf protobuf"",
+    ]
+    if os.path.exists(""llama.cpp""): return
+    for command in commands:
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
+            for line in sp.stdout:
+                print(line.decode(""utf-8""), flush = True, end = """")
+        pass
+    pass
+pass
+
+
+def save_to_gguf(
+    model_directory     : str = ""unsloth_finetuned_model"",
+    quantization_method : str = ""fast_quantized"",
+    _run_installer = None, # Non blocking install of llama.cpp
+):
+    from transformers.models.llama.modeling_llama import logger
+
+    if   quantization_method == ""not_quantized"":  quantization_method = ""f16""
+    elif quantization_method == ""fast_quantized"": quantization_method = ""q8_0""
+    elif quantization_method == ""quantized"":      quantization_method = ""q4_k_m""
+    elif quantization_method is None:             quantization_method = ""q8_0""
 
     if quantization_method not in ALLOWED_QUANTS.keys():
         error = f""Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\n""
@@ -181,27 +456,409 @@ def _colab_quantize_to_gguf(save_directory, quantization_method = ""q4_k_m""):
         f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
         f""   \\\   /|    [0] Installing llama.cpp will take 3 minutes.\n""\
         f""O^O/ \_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n""\
-        f""\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n""\
+        f""\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\n""\
         f' ""-____-""     In total, you will have to wait around 26 minutes.\n'
     print(print_info)
 
-    if not os.path.exists(""llama.cpp""):
-        print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
-        !git clone https://github.com/ggerganov/llama.cpp
-        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j
-        !pip install gguf protobuf
+    print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
+    if _run_installer is not None:
+        _run_installer.wait()
+    else:
+        install_llama_cpp_blocking()
+    pass
+
+    print(""Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes..."")
+    first_conversion = ""f16""
+    if   quantization_method == ""f32"":  first_conversion = ""f32""
+    elif quantization_method == ""f16"":  first_conversion = ""f16""
+    elif quantization_method == ""q8_0"": first_conversion = ""q8_0""
+
+    n_cpus = psutil.cpu_count()*2
+    # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
+    
+    final_location = f""./{model_directory}-unsloth.{first_conversion.upper()}.gguf""
+
+    command = f""python llama.cpp/convert.py {model_directory} ""\
+        f""--outfile {final_location} ""\
+        f""--outtype {first_conversion} --concurrency {n_cpus}""
+
+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
+        for line in sp.stdout:
+            print(line.decode(""utf-8""), flush = True, end = """")
+    pass
+
+    print(f""Unsloth: Conversion completed! Output location: {final_location}"")
+
+    if quantization_method != first_conversion:
+        old_location = final_location
+        print(f""Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes..."")
+        final_location = f""./{model_directory}-unsloth.{quantization_method.upper()}.gguf""
+
+        command = f""./llama.cpp/quantize {old_location} ""\
+            f""{final_location} {quantization_method} {n_cpus}""
+        
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
+            for line in sp.stdout:
+                print(line.decode(""utf-8""), flush = True, end = """")
+        pass
+        print(f""Unsloth: Conversion completed! Output location: {final_location}"")
+    pass
+
+    return final_location
+pass
+
+
+def unsloth_save_pretrained_merged(
+    self,
+    save_directory       : Union[str, os.PathLike],
+    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    push_to_hub          : bool = False,
+    token                : Optional[Union[str, bool]] = None,
+    is_main_process      : bool = True,
+    state_dict           : Optional[dict] = None,
+    save_function        : Callable = torch.save,
+    max_shard_size       : Union[int, str] = ""5GB"",
+    safe_serialization   : bool = True,
+    variant              : Optional[str] = None,
+    save_peft_format     : bool = True,
+    tags                 : List[str] = None,
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.85,   
+):
+    """"""
+        Same as .save_pretrained(...) except 4bit weights are auto
+        converted to float16 with as few overhead as possible.
+
+        Choose for `save_method` to be either:
+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.
+    """"""
+    arguments = dict(locals())
+    arguments[""model""]     = self
+    arguments[""tokenizer""] = None
+    del arguments[""self""]
+    unsloth_save_model(**arguments)
+    for _ in range(3):
+        gc.collect()
+pass
+
+
+def unsloth_push_to_hub_merged(
+    self,
+    repo_id              : str,
+    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    use_temp_dir         : Optional[bool] = None,
+    commit_message       : Optional[str] = None,
+    private              : Optional[bool] = None,
+    token                : Union[bool, str, None] = None,
+    max_shard_size       : Union[int, str, None] = ""5GB"",
+    create_pr            : bool = False,
+    safe_serialization   : bool = True,
+    revision             : str = None,
+    commit_description   : str = None,
+    tags                 : Optional[List[str]] = None,
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.85,
+):
+    """"""
+        Same as .push_to_hub(...) except 4bit weights are auto
+        converted to float16 with as few overhead as possible.
+
+        Choose for `save_method` to be either:
+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.
+    """"""
+    arguments = dict(locals())
+    arguments[""model""]          = self
+    arguments[""tokenizer""]      = None
+    arguments[""save_directory""] = repo_id
+    arguments[""push_to_hub""]    = True
+    del arguments[""self""]
+    del arguments[""repo_id""]
+    unsloth_save_model(**arguments)
+    for _ in range(3):
+        gc.collect()
+pass
+
+
+def unsloth_save_pretrained_gguf(
+    self,
+    save_directory       : Union[str, os.PathLike],
+    tokenizer            = None,
+    quantization_method  : str = ""fast_quantized"",
+    push_to_hub          : bool = False,
+    token                : Optional[Union[str, bool]] = None,
+    is_main_process      : bool = True,
+    state_dict           : Optional[dict] = None,
+    save_function        : Callable = torch.save,
+    max_shard_size       : Union[int, str] = ""5GB"",
+    safe_serialization   : bool = True,
+    variant              : Optional[str] = None,
+    save_peft_format     : bool = True,
+    tags                 : List[str] = None,
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.85,   
+):
+    """"""
+        Same as .save_pretrained(...) except 4bit weights are auto
+        converted to float16 then converted to GGUF / llama.cpp format.
+
+        Choose for `quantization_method` to be:
+        ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
+        ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
+        ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
+        ""f32""     : ""Not recommended. Retains 100% accuracy, but super slow and memory hungry."",
+        ""f16""     : ""Fastest conversion + retains 100% accuracy. Slow and memory hungry."",
+        ""q8_0""    : ""Fast conversion. High resource use, but generally acceptable."",
+        ""q4_k_m""  : ""Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K"",
+        ""q5_k_m""  : ""Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K"",
+        ""q2_k""    : ""Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors."",
+        ""q3_k_l""  : ""Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+        ""q3_k_m""  : ""Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+        ""q3_k_s""  : ""Uses Q3_K for all tensors"",
+        ""q4_0""    : ""Original quant method, 4-bit."",
+        ""q4_1""    : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
+        ""q4_k_s""  : ""Uses Q4_K for all tensors"",
+        ""q5_0""    : ""Higher accuracy, higher resource usage and slower inference."",
+        ""q5_1""    : ""Even higher accuracy, resource usage and slower inference."",
+        ""q5_k_s""  : ""Uses Q5_K for all tensors"",
+        ""q6_k""    : ""Uses Q8_K for all tensors"",
+    """"""
+    if tokenizer is None:
+        raise ValueError(""Unsloth: Saving to GGUF must have a tokenizer."")
+
+    arguments = dict(locals())
+    arguments[""model""]       = self
+    arguments[""tokenizer""]   = tokenizer
+    arguments[""push_to_hub""] = False # We save ourselves
+    arguments[""save_method""] = ""merged_16bit"" # Must be 16bit
+    del arguments[""self""]
+    del arguments[""quantization_method""]
+
+    # Non blocking install GGUF first
+    git_clone = install_llama_cpp_clone_non_blocking()
+    python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+    git_clone.wait()
+    makefile  = install_llama_cpp_make_non_blocking()
+    new_save_directory = unsloth_save_model(**arguments)
+    python_install.wait()
+
+    for _ in range(3):
+        gc.collect()
+
+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+
+    # And save to HF
+    if push_to_hub:
+        print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+
+        from huggingface_hub import create_repo
+        create_repo(
+            repo_id   = save_directory,
+            token     = token,
+            repo_type = ""model"",
+            exist_ok  = True,
+        )
+
+        from huggingface_hub import HfApi
+        hf_api = HfApi(token = token)
+
+        if ""/"" in file_location:
+            uploaded_location = file_location[file_location.rfind(""/"")+1:]
+        else:
+            uploaded_location = file_location
+        pass
+
+        hf_api.upload_file(
+            path_or_fileobj = file_location,
+            path_in_repo    = uploaded_location,
+            repo_id         = save_directory,
+            repo_type       = ""model"",
+        )
     pass
+pass
+
+
+def unsloth_push_to_hub_gguf(
+    self,
+    repo_id              : str,
+    tokenizer            = None,
+    quantization_method  : str = ""fast_quantized"",
+    use_temp_dir         : Optional[bool] = None,
+    commit_message       : Optional[str] = None,
+    private              : Optional[bool] = None,
+    token                : Union[bool, str, None] = None,
+    max_shard_size       : Union[int, str, None] = ""5GB"",
+    create_pr            : bool = False,
+    safe_serialization   : bool = True,
+    revision             : str = None,
+    commit_description   : str = None,
+    tags                 : Optional[List[str]] = None,
+    temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage : float = 0.85,
+):
+    """"""
+        Same as .push_to_hub(...) except 4bit weights are auto
+        converted to float16 then converted to GGUF / llama.cpp format.
+
+        Choose for `quantization_method` to be:
+        ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
+        ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
+        ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
+        ""f32""     : ""Not recommended. Retains 100% accuracy, but super slow and memory hungry."",
+        ""f16""     : ""Fastest conversion + retains 100% accuracy. Slow and memory hungry."",
+        ""q8_0""    : ""Fast conversion. High resource use, but generally acceptable."",
+        ""q4_k_m""  : ""Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K"",
+        ""q5_k_m""  : ""Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K"",
+        ""q2_k""    : ""Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors."",
+        ""q3_k_l""  : ""Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+        ""q3_k_m""  : ""Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K"",
+        ""q3_k_s""  : ""Uses Q3_K for all tensors"",
+        ""q4_0""    : ""Original quant method, 4-bit."",
+        ""q4_1""    : ""Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models."",
+        ""q4_k_s""  : ""Uses Q4_K for all tensors"",
+        ""q5_0""    : ""Higher accuracy, higher resource usage and slower inference."",
+        ""q5_1""    : ""Even higher accuracy, resource usage and slower inference."",
+        ""q5_k_s""  : ""Uses Q5_K for all tensors"",
+        ""q6_k""    : ""Uses Q8_K for all tensors"",
+    """"""
+    if tokenizer is None:
+        raise ValueError(""Unsloth: Saving to GGUF must have a tokenizer."")
 
-    print(""Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes..."")
-    !python llama.cpp/convert.py {save_directory} \
-        --outfile {save_directory}-unsloth.gguf \
-        --outtype f16
+    arguments = dict(locals())
+    arguments[""model""]          = self
+    arguments[""tokenizer""]      = tokenizer
+    arguments[""save_directory""] = repo_id
+    arguments[""push_to_hub""]    = False # We save ourselves
+    arguments[""save_method""]    = ""merged_16bit"" # Must be 16bit
+    del arguments[""self""]
+    del arguments[""repo_id""]
+    del arguments[""quantization_method""]
 
-    print(""Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes..."")
-    final_location = f""./{save_directory}-{quantization_method}-unsloth.gguf""
-    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \
-        {final_location} {quantization_method}
+    # Non blocking install GGUF first
+    git_clone = install_llama_cpp_clone_non_blocking()
+    python_install = install_python_non_blocking([""gguf"", ""protobuf""])
+    git_clone.wait()
+    makefile  = install_llama_cpp_make_non_blocking()
+    new_save_directory = unsloth_save_model(**arguments)
 
-    print(f""Unsloth: Output location: {final_location}"")
+    for _ in range(3):
+        gc.collect()
+
+    python_install.wait()
+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+
+    # Save to hub
+    print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+
+    from huggingface_hub import create_repo
+    create_repo(
+        repo_id   = save_directory,
+        private   = private,
+        token     = token,
+        repo_type = ""model"",
+        exist_ok  = True,
+    )
+
+    from huggingface_hub import HfApi
+    hf_api = HfApi(token = token)
+
+    if ""/"" in file_location:
+        uploaded_location = file_location[file_location.rfind(""/"")+1:]
+    else:
+        uploaded_location = file_location
+    pass
+
+    hf_api.upload_file(
+        path_or_fileobj = file_location,
+        path_in_repo    = uploaded_location,
+        repo_id         = save_directory,
+        repo_type       = ""model"",
+    )
+pass
+
+
+def patch_saving_functions(model):
+    import inspect
+    import re
+    import types
+    from typing import Callable, Optional, Union, List
+
+    if hasattr(model, ""_original_push_to_hub""): return
+
+    original_push_to_hub = model.push_to_hub
+    signature = str(inspect.signature(original_push_to_hub)).replace(""NoneType"", ""None"")
+    signature = signature[1:]
+    signature = re.sub(""<function save at .+?>"", ""torch.save"", signature)
+    docs = original_push_to_hub.__doc__.encode(""utf-8"").decode(""utf-8"")
+    model._original_push_to_hub = original_push_to_hub
+
+    push_to_hub_text = f'''def unsloth_push_to_hub(self, {signature}:
+    """"""
+    {docs}
+    """"""
+    arguments = dict(locals())
+    del arguments[""self""]
+    if ""tags"" in arguments and arguments[""tags""] is not None:
+        assert(isinstance(arguments[""tags""], (list, tuple)))
+        arguments[""tags""] = list(arguments[""tags""]) + [""unsloth"",]
+    elif ""tags"" in arguments:
+        arguments[""tags""] = [""unsloth"",]
+    elif hasattr(self, ""add_model_tags""):
+        self.add_model_tags([""unsloth"",])
+    try:
+        return self._original_push_to_hub(**arguments)
+    except:
+        del arguments[""tags""]
+        return self._original_push_to_hub(**arguments)
+    pass
+    '''
+    exec(push_to_hub_text, globals())
+    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)
+
+    if hasattr(model, ""add_model_tags""):
+        model.add_model_tags([""unsloth"",])
+
+    if hasattr(model, ""config""):
+        # Counteract tokenizers
+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)
+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)
+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)
+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)
+    else:
+        model.push_to_hub_merged     = model.push_to_hub
+        model.save_pretrained_merged = model.save_pretrained
+        model.push_to_hub_gguf       = model.push_to_hub
+        model.save_pretrained_gguf   = model.save_pretrained
+    pass
+
+    original_model = model
+    while hasattr(original_model, ""model""):
+        original_model = original_model.model
+        if hasattr(original_model, ""_original_push_to_hub""): continue
+        
+        original_model._original_push_to_hub = original_model.push_to_hub
+        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)
+
+        if hasattr(original_model, ""add_model_tags""):
+            original_model.add_model_tags([""unsloth"",])
+
+        if hasattr(original_model, ""config""):
+            # Counteract tokenizers
+            original_model.push_to_hub_merged     = \
+                types.MethodType(unsloth_push_to_hub_merged,     original_model)
+
+            original_model.save_pretrained_merged = \
+                types.MethodType(unsloth_save_pretrained_merged, original_model)
+
+            original_model.push_to_hub_gguf       = \
+                types.MethodType(unsloth_push_to_hub_gguf,       original_model)
+
+            original_model.save_pretrained_gguf   = \
+                types.MethodType(unsloth_save_pretrained_gguf,   original_model)
+        pass
+    pass
+    return
 pass
-""""""
"
"diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 18f7720..38258e4 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -261,7 +261,10 @@ def grpo_trainer__get_per_token_logps(function_name, function):
         os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""1""
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
             # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
+            print(""input_ids Unsloth 264"", input_ids.shape)
+            print(""logits_to_keep Unsloth 264"", logits_to_keep)
             hidden_states = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
+            print(""hidden_states Unsloth 264"", hidden_states.shape)
             #logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
             return hidden_states
             # input_ids = input_ids[:, -logits_to_keep:]
@@ -315,8 +318,13 @@ def grpo_trainer_compute_loss(function_name, function):
         logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens
         _input_ids = input_ids
         _logits_to_keep = logits_to_keep
+        print(""prompt_mask Unsloth 320"", prompt_mask.shape)
+        print(""completion_mask Unsloth 320"", completion_mask.shape)
+        print(""input_ids Unsloth 320"", input_ids.shape)
+        print(""logits_to_keep Unsloth 320"", logits_to_keep)
 
         per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
+        print(""per_token_logps Unsloth 320"", per_token_logps.shape)
 
         # Compute the KL divergence between the model and the reference model
         # _prepare_inputs doesn't return reference log probs anymore. We need to calculate it ourselves.
@@ -324,26 +332,36 @@ def grpo_trainer_compute_loss(function_name, function):
         if self.beta != 0.0:
             with torch.inference_mode(), model.disable_adapter():
                 ref_per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
+                print(""ref_per_token_logps Unsloth 320"", ref_per_token_logps.shape)
         else:
             ref_per_token_logps = None
         # per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
         # x - x.detach() allows for preserving gradients from x
         advantages = inputs[""advantages""]
+        print(""advantages Unsloth 320"", advantages.shape)
         # per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
         # per_token_loss = -(per_token_loss - self.beta * per_token_kl)
         # loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         if ""old_per_token_logps"" in inputs.keys():
             old_hidden_states = inputs[""old_per_token_logps""]
+            print(""old_hidden_states Unsloth 320"", old_hidden_states.shape)
         else:
             old_hidden_states = None
 
+        print(""input_ids Unsloth 320"", input_ids.shape)
+        print(""logits_to_keep Unsloth 320"", logits_to_keep)
         input_ids = input_ids[:, -logits_to_keep:]
+        print(""input_ids Unsloth 320"", input_ids.shape)
         if per_token_logps is not None:
 
             if ref_per_token_logps is not None:
+                print(""ref_per_token_logps Unsloth 320"", ref_per_token_logps.shape)
                 ref_per_token_logps = ref_per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-
+                print(""ref_per_token_logps Unsloth 320"", ref_per_token_logps.shape)
+            
+            print(""per_token_logps Unsloth 320"", per_token_logps.shape)
             per_token_logps = per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
+            print(""per_token_logps Unsloth 320"", per_token_logps.shape)
             
             loss, completion_length, mean_kl = grpo_compute_loss_slow(
                 ref_per_token_logps,
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b7aae06..6b258db 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(
     else:
         boundaries = None
     pass
-    
+
     for idx, decoder_layer in enumerate(self.layers):
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
@@ -1601,7 +1601,11 @@ class FastLlamaModel:
         pass
 
         # Also check if lm_head / embeddings are trained
-        lm_head = getattr(model, ""model"", model).lm_head.weight
+        internal_model = model
+        while not hasattr(internal_model, ""lm_head""):
+            internal_model = internal_model.model
+        pass
+        lm_head = internal_model.lm_head.weight
         device_type = lm_head.device.type
         dtype = model.config.torch_dtype
 
"
"diff --git a/unsloth/kernels/layernorm.py b/unsloth/kernels/layernorm.py
index 11f82b8..ffcc5cc 100644
--- a/unsloth/kernels/layernorm.py
+++ b/unsloth/kernels/layernorm.py
@@ -49,7 +49,8 @@ def layernorm_forward(
     b_row = tl.load(b + col_offsets, mask = mask, other = 0).to(tl.float32)
 
     mean_X  = tl.sum(X_row,   axis = 0) / n_cols
-    XX      = X_row - mean_X
+    # (X[0] - mean) == -mean so we need to mask it out
+    XX = tl.where(mask, X_row - mean_X, 0)
     row_var = tl.sum(XX * XX, axis = 0) / n_cols
     inv_var = tl.math.rsqrt(row_var + eps)
     tl.store (r, inv_var)
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index 5d3a9a8..c266fdf 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -208,6 +208,15 @@ def unsloth_save_model(
         return save_directory
     pass
 
+    # Update model tag
+    username = """"
+    if push_to_hub:
+        username = upload_to_huggingface(
+            model, save_directory, token,
+            ""finetuned"", ""trl"", file_location = None,
+        )
+    pass
+
     # If push_to_hub, we must remove the .../ part of a repo
     if push_to_hub and ""/"" in save_directory:
 
@@ -331,6 +340,7 @@ def unsloth_save_model(
             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:
                 # Save to GPU memory
                 state_dict[name] = W
+            # [TODO] Saving to RAM seems to leak memory???
             # elif (max_ram - W.nbytes) > 0:
             #     # Save to CPU memory
             #     logger.warning_once(f""We will save to RAM and not VRAM now."")
@@ -401,27 +411,32 @@ def unsloth_save_model(
     model.config = old_config
     print(""Done."")
 
+    # Print location
+    if push_to_hub:
+        print(f""Saved to https://huggingface.co/{username}/{save_directory.lstrip('/')}"")
+    pass
+
     save_pretrained_settings[""state_dict""] = None
 
-    # for j, (key, value) in enumerate(state_dict.items()):
-    #     state_dict[key] = None
-    #     if j % 10 == 0:
-    #         torch.cuda.empty_cache()
-    #         gc.collect()
-    #     pass
-    # pass
-    # state_dict = None
-    # del state_dict
-    # torch.cuda.empty_cache()
-    # gc.collect()
+    for j, (key, value) in enumerate(state_dict.items()):
+        state_dict[key] = None
+        if j % 10 == 0:
+            torch.cuda.empty_cache()
+            gc.collect()
+        pass
+    pass
+    state_dict = None
+    del state_dict
+    torch.cuda.empty_cache()
+    gc.collect()
 
     # Remove temporary location
     import shutil
     shutil.rmtree(temporary_location)
 
-    # for _ in range(3):
-    #     torch.cuda.empty_cache()
-    #     gc.collect()
+    for _ in range(3):
+        torch.cuda.empty_cache()
+        gc.collect()
     return save_directory
 pass
 
@@ -629,14 +644,44 @@ def unsloth_push_to_hub_merged(
 pass
 
 
-def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):
-    print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+MODEL_CARD = \
+""""""---
+base_model: {base_model}
+tags:
+- text-generation-inference
+- transformers
+- unsloth
+- {model_type}
+- {extra}
+license: apache-2.0
+language:
+- en
+---
+
+# Uploaded {method} model
+
+- **Developed by:** {username}
+- **License:** apache-2.0
+- **Finetuned from model :** {base_model}
+
+This {model_type} model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.
+
+[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)
+""""""
+
 
+def upload_to_huggingface(model, save_directory, token, method, extra = """", file_location = None):
     # Check for username
+    username = """"
     if ""/"" not in save_directory:
         from huggingface_hub import whoami
-        try: save_directory = f""{save_directory}/{whoami()['name']}""
-        except: pass
+        try: 
+            username = whoami()['name']
+            save_directory = f""{save_directory}/{username}""
+        except:
+            raise RuntimeError(f""Unsloth: {save_directory} is not a Huggingface directory."")
+    else:
+        username = save_directory.split(""/"")[0]
     pass
 
     from huggingface_hub import create_repo
@@ -648,43 +693,36 @@ def upload_gguf_to_huggingface(save_directory, file_location, token, model_type)
     )
 
     # Create model card
-    from huggingface_hub import ModelCard, ModelCardData
-    card_data = ModelCardData(
-        language = ""en"",
-        license  = ""apache-2.0"",
-        library  = ""unsloth"",
-        tags     = [""gguf"", ""unsloth"", ""text-generation-inference"", ""transformers"",],
+    from huggingface_hub import ModelCard
+    content = MODEL_CARD.format(
+        username   = username,
+        base_model = model.config._name_or_path,
+        model_type = model.config.model_type,
+        method     = """",
+        extra      = extra,
     )
-
-    content = f""\n""\
-    f""---\n""\
-    f""{ card_data.to_yaml() }\n""\
-    f""---\n""\
-    f""\n""\
-    f""# My Model Card for {file_location}\n""\
-    f""\n""\
-    f""\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\n""\
-    f""\n""
-    
     card = ModelCard(content)
     card.push_to_hub(save_directory, token = token)
 
-    # Now upload file
-    from huggingface_hub import HfApi
-    hf_api = HfApi(token = token)
+    if file_location is not None:
+        # Now upload file
+        from huggingface_hub import HfApi
+        hf_api = HfApi(token = token)
 
-    if ""/"" in file_location:
-        uploaded_location = file_location[file_location.rfind(""/"")+1:]
-    else:
-        uploaded_location = file_location
-    pass
+        if ""/"" in file_location:
+            uploaded_location = file_location[file_location.rfind(""/"")+1:]
+        else:
+            uploaded_location = file_location
+        pass
 
-    hf_api.upload_file(
-        path_or_fileobj = file_location,
-        path_in_repo    = uploaded_location,
-        repo_id         = save_directory,
-        repo_type       = ""model"",
-    )
+        hf_api.upload_file(
+            path_or_fileobj = file_location,
+            path_in_repo    = uploaded_location,
+            repo_id         = save_directory,
+            repo_type       = ""model"",
+        )
+    pass
+    return username
 pass
 
 
@@ -754,8 +792,15 @@ def unsloth_save_pretrained_gguf(
         gc.collect()
 
     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
-    model_type = self.config.model_type
-    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)
+
+    if push_to_hub:
+        print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+        username = upload_to_huggingface(
+            self, model, new_save_directory, token,
+            ""GGUF converted"", ""gguf"", file_location,
+        )
+        print(f""Saved to https://huggingface.co/{username}/{new_save_directory.lstrip('/')}"")
+    pass
 pass
 
 
@@ -827,8 +872,13 @@ def unsloth_push_to_hub_gguf(
 
     python_install.wait()
     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
-    model_type = self.config.model_type
-    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)
+
+    print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
+    username = upload_to_huggingface(
+        self, model, new_save_directory, token,
+        ""GGUF converted"", ""gguf"", file_location,
+    )
+    print(f""Saved to https://huggingface.co/{username}/{new_save_directory.lstrip('/')}"")
 pass
 
 
"
"diff --git a/.github/ISSUE_TEMPLATE/documentation.md b/.github/ISSUE_TEMPLATE/documentation.md
deleted file mode 100644
index 03891ed..0000000
--- a/.github/ISSUE_TEMPLATE/documentation.md
+++ /dev/null
@@ -1,20 +0,0 @@
----
-name: Documentation
-about: Documentation request
-title: ""[DOCUMENTATION]""
-labels: documentation
-assignees: ''
-
----
-
-**Is your feature request related to a problem? Please describe.**
-A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
-
-**Describe the solution you'd like**
-A clear and concise description of what you want to happen.
-
-**Describe alternatives you've considered**
-A clear and concise description of any alternative solutions or features you've considered.
-
-**Additional context**
-Add any other context or screenshots about the feature request here.
diff --git a/.github/ISSUE_TEMPLATE/documentation_request.md b/.github/ISSUE_TEMPLATE/documentation_request.md
index c9fa21f..062962d 100644
--- a/.github/ISSUE_TEMPLATE/documentation_request.md
+++ b/.github/ISSUE_TEMPLATE/documentation_request.md
@@ -1,8 +1,8 @@
 ---
 name: Documentation request
-about: Report incorrect or needed documentation to improve CUTLASS
+about: Report incorrect or needed documentation to improve unsloth!
 title: ""[DOC]""
-labels: ""? - Needs Triage, documentation""
+labels: documentation
 assignees: ''
 
 ---
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index d347cd1..fcba2eb 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -25,11 +25,6 @@ from unsloth_zoo.loss_utils import (
 )
 
 
-@triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
-    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
-})
-@triton.jit
 def _cross_entropy_forward(
     logits_ptr        ,
     logits_row_stride ,
@@ -95,13 +90,15 @@ def _cross_entropy_forward(
     tl.store(logsumexp_ptr, logsumexp)
     tl.store(loss_ptr, loss)
 pass
+_cross_entropy_forward = triton.jit(_cross_entropy_forward)
+_cross_entropy_forward = triton.heuristics(
+    {
+        ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+        ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
+    }
+)(_cross_entropy_forward)
 
 
-@triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
-    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
-})
-@triton.jit
 def _chunked_cross_entropy_forward(
     logits_ptr        ,
     logits_row_stride ,
@@ -177,13 +174,15 @@ def _chunked_cross_entropy_forward(
     pass
     tl.store(logsumexp_ptr, logsumexp)
 pass
+_chunked_cross_entropy_forward = triton.jit(_chunked_cross_entropy_forward)
+_chunked_cross_entropy_forward = triton.heuristics(
+    {
+        ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+        ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
+    }
+)(_chunked_cross_entropy_forward)
 
 
-@triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
-    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
-})
-@triton.jit
 def _cross_entropy_backward(
     logits_ptr        ,
     logits_row_stride ,
@@ -264,10 +263,16 @@ def _cross_entropy_backward(
     # If y == 0: dC/dx = 0 ==> we already masked it to be = 0, so dloss = 0.
     tl.store(logits_ptr + col_offsets, dloss * y, mask = mask)
 pass
+_cross_entropy_backward = triton.jit(_cross_entropy_backward)
+_cross_entropy_backward = triton.heuristics(
+    {
+        ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+        ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
+    }
+)(_cross_entropy_backward)
 
 
 MAX_FUSED_SIZE = 65536 # 2**16
-
 class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
     def forward(ctx, logits, labels, logit_softcapping : float = 0, logit_scaling : float = 0):
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index b74d636..6310f7f 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -53,8 +53,6 @@ def _rms_layernorm_forward(
 pass
 
 
-@triton.heuristics({""GEMMA"": lambda args: bool(args[""GEMMA""]),})
-@triton.jit
 def _rms_layernorm_backward(
     dY, dY_row_stride,
     dX, dX_row_stride,
@@ -97,6 +95,12 @@ def _rms_layernorm_backward(
     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)
     tl.store(dX + col_offsets, output, mask = mask)
 pass
+_rms_layernorm_backward = triton.jit(_rms_layernorm_backward)
+_rms_layernorm_backward = triton.heuristics(
+    {
+        ""GEMMA"": lambda args: bool(args[""GEMMA""]),
+    }
+)(_rms_layernorm_backward)
 
 
 @triton.jit
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index 7fe15d0..88b9cca 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -18,8 +18,6 @@ import torch
 from .utils import calculate_settings
 ROPE_GROUP_SIZE : int = 4
 
-@triton.heuristics({""BACKWARD_PASS"": lambda args: bool(args[""BACKWARD_PASS""]),})
-@triton.jit
 def _rope_embedding(
     Q,     Q_row_stride,
     cos, cos_row_stride,
@@ -69,6 +67,12 @@ def _rope_embedding(
         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)
     pass
 pass
+_rope_embedding = triton.jit(_rope_embedding)
+_rope_embedding = triton.heuristics(
+    {
+        ""BACKWARD_PASS"": lambda args: bool(args[""BACKWARD_PASS""]),
+    }
+)(_rope_embedding)
 
 
 class Fast_RoPE_Embedding(torch.autograd.Function):
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 7b6da3e..ee4235c 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -23,7 +23,6 @@ __all__ = [
     ""apply_chat_template"",
 
     ""test_construct_chat_template"",
-    ""create_ollama_modelfile"",
 ]
 
 from transformers import StoppingCriteria, StoppingCriteriaList
@@ -1079,14 +1078,29 @@ extra_eos_tokens = None,
         )
     pass
 
+    # Check tokenizer types
+    tokenizer_name = tokenizer.name_or_path.lower()
+    if tokenizer_name.startswith((""unsloth/llama-3-8b-instruct"", ""unsloth/llama-3-70b-instruct"")):
+        # Add <|eot_id|>
+        extra_eos_tokens.append(""<|eot_id|>"")
+    elif (""<|eot_id|>"" in extra_eos_tokens or ""<|eot_id|>"" in chat_template) and \
+        tokenizer_name.startswith((""unsloth/llama-3-8b"", ""unsloth/llama-3-70b"")):
+        # Warn
+        logger.warning(
+            ""Unsloth: Base llama-3 models did not train <|eot_id|>.\n""\
+            ""Please use the instruct version or use <|end_of_text|>""
+        )
+    pass
+    extra_eos_tokens = list(set(extra_eos_tokens))
+
     count_eos = 0
     for eos in extra_eos_tokens:
-        count_eos += len(re.findall(r""{OUTPUT}"" + eos.encode(""unicode-escape"").decode(""utf-8""), chat_template))
+        count_eos += len(re.findall(r""{OUTPUT}"" + re.escape(eos), chat_template))
     pass
     if count_eos == 0:
         logger.warning(""Unsloth: We automatically added an EOS token to stop endless generations."")
         eos = extra_eos_tokens[0]
-        chat_template = re.sub(r""{OUTPUT}"", r""{OUTPUT}"" + eos.encode(""unicode-escape"").decode(""utf-8""), chat_template)
+        chat_template = re.sub(r""{OUTPUT}"", r""{OUTPUT}"" + eos, chat_template)
     pass
 
     # O(N^2) search finding 2 repeatted pieces of text
@@ -1151,7 +1165,9 @@ extra_eos_tokens = None,
     # Check bos_token is in system prompt
     ollama_system = system_part
     has_bos_token = False
+    always_bos_token = False
     if tokenizer(""A"").input_ids[0] == getattr(tokenizer, ""bos_token_id"", None):
+        always_bos_token = True
         if ollama_system.startswith(tokenizer.bos_token):
             has_bos_token = True
             ollama_system = ollama_system[len(tokenizer.bos_token):]
@@ -1166,11 +1182,6 @@ extra_eos_tokens = None,
     input_modelfile  = ""{{ if .Prompt }}"" + input_part .replace(""{INPUT}"",  ""{{ .Prompt }}"") + ""{{ end }}""
     output_modelfile = output_part.replace(""{OUTPUT}"", ""{{ .Response }}"")
 
-    # Check if EOS token is at the end of the output
-    if not output_modelfile.endswith(tuple(extra_eos_tokens)):
-        output_modelfile += ""{__EOS_TOKEN__}""
-    pass
-
     # Ollama EOS
     ollama_eos = get_ollama_eos_tokens(tokenizer, extra_eos_tokens)
     ollama_eos = '\n'.join(f'PARAMETER stop ""{eos}""' for eos in ollama_eos)
@@ -1215,10 +1226,7 @@ extra_eos_tokens = None,
         partial_system = process(system_part, ""{SYSTEM}"", ""messages[0]['content']"")
         partial_system = partial_system.replace(""{SYSTEM}"", """")
 
-        # If {SYSTEM} is non existent, simply just use the content
-        if ""{SYSTEM}"" not in partial_system:
-            partial_system = ""messages[0]['content']""
-        else:
+        if ""{SYSTEM}"" in partial_system:
             if default_system_message is None:
                 raise RuntimeError(""Unsloth: Please specify a default system message!"")
         pass
@@ -1226,21 +1234,22 @@ extra_eos_tokens = None,
         # Separate the BOS
         if has_bos_token:
             partial_system = partial_system.replace(tokenizer.bos_token, """", 1)
+            system_part    = system_part   .replace(tokenizer.bos_token, """", 1)
         pass
-
+        
         partial_system = \
             ""{% if messages[0]['role'] == 'system' %}""\
                 ""{{ "" + partial_system + "" }}""\
                 ""{% set loop_messages = messages[1:] %}""
         if default_system_message is not None:
             full_system = system_part.replace(""{SYSTEM}"", default_system_message)
+            if ""{SYSTEM}"" in system_part:
+                modelfile += '\nSYSTEM: ""' + default_system_message + '""'
+            pass
             partial_system += ""{% else %}""\
                 ""{{ '"" + full_system + ""' }}""\
                 ""{% set loop_messages = messages %}""\
             ""{% endif %}""
-
-            # Add to modelfile
-            modelfile += '\nSYSTEM ""' + full_system + '""'
         else:
             partial_system += ""{% endif %}""
         pass
@@ -1251,6 +1260,22 @@ extra_eos_tokens = None,
             jinja_template = ""{{ bos_token }}"" + jinja_template
     pass
 
+    # Check if system part is the same!
+    jinja_template = re.sub(
+        r""\{\% if messages\[0\]\['role'\] \=\= 'system' \%\}\{\{ '(.+?)' \}\}""\
+        r""\{\% set loop\_messages \= messages\[1\:\] \%\}""\
+        r""\{\% else \%\}\{\{ '\1' \}\}\{\% set loop\_messages \= messages \%\}\{\% endif \%\}""\
+        r""\{\% for message in loop\_messages \%\}"",
+        r""{{ '\1' }}{% for message in messages %}"",
+        jinja_template, flags = re.MULTILINE | re.DOTALL,
+    )
+
+    # Check jinja tempate for bos
+    if always_bos_token:
+        if not jinja_template.startswith(""{{ bos_token }}""):
+            jinja_template = ""{{ bos_token }}"" + jinja_template
+    pass
+    
     return modelfile, jinja_template
 pass
 
@@ -1260,7 +1285,7 @@ def test_construct_chat_template():
     from transformers import AutoTokenizer
     tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B-Instruct"", token = token)
 
-    template = """"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
+    chat_template = """"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
 
 {SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>
 
@@ -1277,7 +1302,11 @@ def test_construct_chat_template():
       
     extra_eos_tokens = None
 
-    modelfile, jinja_template = construct_chat_template(template, default_system_message, extra_eos_tokens)
+    modelfile, jinja_template = construct_chat_template(
+        tokenizer = tokenizer,
+        chat_template = chat_template,
+        extra_eos_tokens = extra_eos_tokens,
+    )
 
     messages = [
         {""role"": ""system"", ""content"": ""You are an assistant""},
@@ -1291,7 +1320,6 @@ def test_construct_chat_template():
 
     tokenizer.chat_template = jinja_template
     new_output = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
-
     assert(correct_output == new_output)
     pass
 pass
@@ -1344,43 +1372,6 @@ extra_eos_tokens = None,
 pass
 
 
-def create_ollama_modelfile(tokenizer, gguf_location):
-    """"""
-        Creates an Ollama Modelfile.
-        Use ollama.create(model = ""new_ollama_model"", modelfile = modelfile)
-    """"""
-    modelfile = getattr(tokenizer, ""_ollama_modelfile"", None)
-    if modelfile is None:
-        raise RuntimeError(
-            ""Unsloth: Tokenizer does not have a `ollama_modelfile` attribute.\n""\
-            ""Please use get_chat_template(...).""
-        )
-    pass
-
-    system_message = getattr(tokenizer, ""_system_message"", None)
-    if system_message is None:
-        __SYSTEM_MESSAGE__ = """"
-    else:
-        __SYSTEM_MESSAGE__ = f'SYSTEM """"""{system_message}""""""'
-    pass
-
-    modelfile = modelfile\
-        .replace(""{{"", ""@#"")\
-        .replace(""}}"", ""@#"")\
-        .format(
-            __FILE_LOCATION__  = gguf_location,
-            __SYSTEM_MESSAGE__ = __SYSTEM_MESSAGE__,
-            __EOS_TOKEN__      = tokenizer.eos_token,
-        )\
-        .replace(""@#"", ""{{"")\
-        .replace(""@#"", ""}}"")\
-        .rstrip()
-    pass
-
-    return modelfile
-pass
-
-
 def create_stopping_criteria(tokenizer, stop_word = ""eos_token""):
     class StoppingCriteriaSub(StoppingCriteria):
         __slots__ = ""stop_token"", ""single_match"", ""length"",
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 5ef7583..4b40065 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -47,9 +47,11 @@ __INT_TO_FLOAT_MAPPER = \
         ""TinyLlama/TinyLlama-1.1B-Chat-v1.0"",
     ),
     ""unsloth/mistral-7b-instruct-v0.1-bnb-4bit"" : (
+        ""unsloth/mistral-7b-instruct-v0.1"",
         ""mistralai/Mistral-7B-Instruct-v0.1"",
     ),
     ""unsloth/mistral-7b-instruct-v0.2-bnb-4bit"" : (
+        ""unsloth/mistral-7b-instruct-v0.2"",
         ""mistralai/Mistral-7B-Instruct-v0.2"",
     ),
     ""unsloth/llama-2-7b-chat-bnb-4bit"" : (
diff --git a/unsloth/save.py b/unsloth/save.py
index f8f884a..9163c6d 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -891,10 +891,10 @@ def save_to_gguf(
     # Map quant methods
     new_quantization_method = []
     for quant_method in quantization_method:
-        if   quant_method == ""not_quantized"":  quantization_method = model_dtype
-        elif quant_method == ""fast_quantized"": quantization_method = ""q8_0""
-        elif quant_method == ""quantized"":      quantization_method = ""q4_k_m""
-        elif quant_method is None:             quantization_method = ""q8_0""
+        if   quant_method == ""not_quantized"":  quant_method = model_dtype
+        elif quant_method == ""fast_quantized"": quant_method = ""q8_0""
+        elif quant_method == ""quantized"":      quant_method = ""q4_k_m""
+        elif quant_method is None:             quant_method = ""q8_0""
 
         # Check if wrong method
         if quant_method not in ALLOWED_QUANTS.keys():
@@ -978,6 +978,11 @@ def save_to_gguf(
         pass
     pass
 
+    # If only q8_0:
+    if len(quantization_method) == 1 and quantization_method[0] == ""q8_0"":
+        strength = 0
+    pass
+
     if   strength >= 3: first_conversion = ""f32""
     elif strength >= 2: first_conversion = ""f16""
     elif strength >= 1: first_conversion = ""bf16""
@@ -1008,7 +1013,7 @@ def save_to_gguf(
     n_cpus *= 2
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
     
-    final_location = f""./{model_directory}-unsloth.{first_conversion.upper()}.gguf""
+    final_location = f""./{model_directory}/unsloth.{first_conversion.upper()}.gguf""
 
     print(f""Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\n""\
           f""The output location will be {final_location}\n""\
@@ -1072,12 +1077,12 @@ def save_to_gguf(
 
     full_precision_location = final_location
 
-    all_saved_locations = []
+    all_saved_locations = [full_precision_location,]
     # Convert each type!
     for quant_method in quantization_method:
         if quant_method != first_conversion:
             print(f""Unsloth: [2] Converting GGUF 16bit into {quant_method}. This will take 20 minutes..."")
-            final_location = f""./{model_directory}-unsloth.{quant_method.upper()}.gguf""
+            final_location = f""./{model_directory}/unsloth.{quant_method.upper()}.gguf""
 
             command = f""./{quantize_location} {full_precision_location} ""\
                 f""{final_location} {quant_method} {n_cpus}""
@@ -1365,6 +1370,29 @@ def fix_tokenizer_bos_token(tokenizer):
 pass
 
 
+def create_ollama_modelfile(tokenizer, gguf_location):
+    """"""
+        Creates an Ollama Modelfile.
+        Use ollama.create(model = ""new_ollama_model"", modelfile = modelfile)
+    """"""
+    modelfile = getattr(tokenizer, ""_ollama_modelfile"", None)
+    if modelfile is None: return None
+
+    modelfile = modelfile\
+        .replace(""{{"", ""@#"")\
+        .replace(""}}"", ""@#"")\
+        .format(
+            __FILE_LOCATION__  = gguf_location,
+        )\
+        .replace(""@#"", ""{{"")\
+        .replace(""@#"", ""}}"")\
+        .rstrip()
+    pass
+
+    return modelfile
+pass
+
+
 def unsloth_save_pretrained_gguf(
     self,
     save_directory       : Union[str, os.PathLike],
@@ -1500,10 +1528,21 @@ def unsloth_save_pretrained_gguf(
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
+    # Save Ollama modelfile
+    modelfile = create_ollama_modelfile(tokenizer, all_file_locations[0])
+    modelfile_location = None
+    if modelfile is not None:
+        modelfile_location = os.path.join(new_save_directory, ""Modelfile"")
+        with open(modelfile_location, ""w"") as file:
+            file.write(modelfile)
+        pass
+        print(f""Unsloth: Saved Ollama Modelfile to {modelfile_location}"")
+    pass
+
     if fix_bos_token:
         logger.warning(
             f""Unsloth: ##### The current model auto adds a BOS token.\n""\
-            ""Unsloth: ##### We removed in GGUF's chat template for you.""
+            ""Unsloth: ##### We removed it in GGUF's chat template for you.""
         )
     pass
 
@@ -1520,6 +1559,15 @@ def unsloth_save_pretrained_gguf(
                 new_save_directory.lstrip('/.')
             print(f""Saved GGUF to https://huggingface.co/{link}"")
         pass
+
+        # Save modelfile
+        if modelfile_location is not None:
+            username = upload_to_huggingface(
+                self, save_directory, token,
+                ""GGUF converted"", ""gguf"", modelfile_location, old_username, private,
+            )
+            print(f""Saved Ollama Modelfile to https://huggingface.co/{link}"")
+        pass
     pass
 pass
 
@@ -1654,6 +1702,17 @@ def unsloth_push_to_hub_gguf(
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
+    # Save Ollama modelfile
+    modelfile = create_ollama_modelfile(tokenizer, all_file_locations[0])
+    modelfile_location = None
+    if modelfile is not None:
+        modelfile_location = os.path.join(new_save_directory, ""Modelfile"")
+        with open(modelfile_location, ""w"") as file:
+            file.write(modelfile)
+        pass
+        print(f""Unsloth: Saved Ollama Modelfile to {modelfile_location}"")
+    pass
+
     for file_location in all_file_locations:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
         username = upload_to_huggingface(
@@ -1667,10 +1726,19 @@ def unsloth_push_to_hub_gguf(
         print(f""Saved GGUF to https://huggingface.co/{link}"")
     pass
 
+    # Save modelfile
+    if modelfile_location is not None:
+        username = upload_to_huggingface(
+            self, repo_id, token,
+            ""GGUF converted"", ""gguf"", modelfile_location, old_username, private,
+        )
+        print(f""Saved Ollama Modelfile to https://huggingface.co/{link}"")
+    pass
+
     if fix_bos_token:
         logger.warning(
             f""Unsloth: ##### The current model auto adds a BOS token.\n""\
-            ""Unsloth: ##### We removed in GGUF's chat template for you.""
+            ""Unsloth: ##### We removed it in GGUF's chat template for you.""
         )
     pass
 pass
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index b8473e6..24e8002 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -19,17 +19,22 @@ from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh
 from transformers.models.llama.modeling_llama import logger
 
 
-@triton.heuristics({""DO_SOFTCAPPING"": lambda args: args[""DO_SOFTCAPPING""],})
+@triton.heuristics({
+    ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
+    ""DO_LOGIT_SCALING"": lambda args: args[""DO_LOGIT_SCALING""],
+})
 @triton.jit
 def _cross_entropy_forward(
     logits_ptr, logits_row_stride,
     loss_ptr,
     logsumexp_ptr,
     labels_ptr,
-    VOCAB_SIZE     : tl.constexpr,
-    BLOCK_SIZE     : tl.constexpr,
-    DO_SOFTCAPPING : tl.constexpr,
-    SOFTCAP        : tl.constexpr,
+    VOCAB_SIZE      : tl.constexpr,
+    BLOCK_SIZE      : tl.constexpr,
+    DO_SOFTCAPPING  : tl.constexpr,
+    SOFTCAP         : tl.constexpr,
+    DO_LOGIT_SCALING: tl.constexpr,
+    LOGIT_SCALE     : tl.constexpr,
 ):
     """"""
         Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]
@@ -62,8 +67,11 @@ def _cross_entropy_forward(
 
     label_idx = tl.load(labels_ptr).to(tl.int32)
     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+
+    # Go logit scaling for Cohere: t * x
+    if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
+    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
 
     logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
@@ -71,8 +79,10 @@ def _cross_entropy_forward(
 
     if label_idx != -100:
         x = tl.load(logits_ptr + label_idx)
+        # Go logit scaling for Cohere: t * x
+        if DO_LOGIT_SCALING: x = LOGIT_SCALE * x
         # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-        if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)
+        if DO_SOFTCAPPING:   x = SOFTCAP * triton_tanh(x / SOFTCAP)
         loss = logsumexp - x.to(tl.float32)
     else:
         loss = 0.0
@@ -81,18 +91,23 @@ def _cross_entropy_forward(
 pass
 
 
-@triton.heuristics({""DO_SOFTCAPPING"": lambda args: args[""DO_SOFTCAPPING""],})
+@triton.heuristics({
+    ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
+    ""DO_LOGIT_SCALING"": lambda args: args[""DO_LOGIT_SCALING""],
+})
 @triton.jit
 def _chunked_cross_entropy_forward(
     logits_ptr, logits_row_stride,
     loss_ptr,
     logsumexp_ptr,
     labels_ptr,
-    VOCAB_SIZE     : tl.constexpr,
-    N_CHUNKS       : tl.constexpr,
-    BLOCK_SIZE     : tl.constexpr,
-    DO_SOFTCAPPING : tl.constexpr,
-    SOFTCAP        : tl.constexpr,
+    VOCAB_SIZE      : tl.constexpr,
+    N_CHUNKS        : tl.constexpr,
+    BLOCK_SIZE      : tl.constexpr,
+    DO_SOFTCAPPING  : tl.constexpr,
+    SOFTCAP         : tl.constexpr,
+    DO_LOGIT_SCALING: tl.constexpr,
+    LOGIT_SCALE     : tl.constexpr,
 ):
     """"""
         256K vocab divided in 4 chunks
@@ -130,8 +145,11 @@ def _chunked_cross_entropy_forward(
 
     label_idx = tl.load(labels_ptr).to(tl.int32)
     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+
+    # Go logit scaling for Cohere: t * x
+    if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
+    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
 
     logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
@@ -142,8 +160,10 @@ def _chunked_cross_entropy_forward(
         # Do the -x separately
         if label_idx != -100:
             x = tl.load(logits_ptr + label_idx).to(tl.float32)
+            # Go logit scaling for Cohere: t * x
+            if DO_LOGIT_SCALING: x = LOGIT_SCALE * x
             # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-            if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)
+            if DO_SOFTCAPPING:   x = SOFTCAP * triton_tanh(x / SOFTCAP)
             loss = -1.0 * x.to(tl.float32)
         else:
             loss = 0.0
@@ -153,17 +173,22 @@ def _chunked_cross_entropy_forward(
 pass
 
 
-@triton.heuristics({""DO_SOFTCAPPING"": lambda args: args[""DO_SOFTCAPPING""],})
+@triton.heuristics({
+    ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
+    ""DO_LOGIT_SCALING"": lambda args: args[""DO_LOGIT_SCALING""],
+})
 @triton.jit
 def _cross_entropy_backward(
     logits_ptr, logits_row_stride,
     dloss_ptr,   dloss_row_stride,
     logsumexp_ptr,
     labels_ptr,
-    VOCAB_SIZE     : tl.constexpr,
-    BLOCK_SIZE     : tl.constexpr,
-    DO_SOFTCAPPING : tl.constexpr,
-    SOFTCAP        : tl.constexpr,
+    VOCAB_SIZE      : tl.constexpr,
+    BLOCK_SIZE      : tl.constexpr,
+    DO_SOFTCAPPING  : tl.constexpr,
+    SOFTCAP         : tl.constexpr,
+    DO_LOGIT_SCALING: tl.constexpr,
+    LOGIT_SCALE     : tl.constexpr,
 ):
     """"""
         CE_i = -y log(P) = y * (log[sum(exp(x))] - x)
@@ -195,6 +220,13 @@ def _cross_entropy_backward(
         dloss = 0.0
 
     x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
+
+    # Do logit scaling for Cohere
+    if DO_LOGIT_SCALING:
+        # d/dx [s * x] = s
+        x = x * LOGIT_SCALE
+    pass
+
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
     if DO_SOFTCAPPING:
         # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
@@ -210,6 +242,11 @@ def _cross_entropy_backward(
         y,       # exp(x - logsumexp)
     )
 
+    if DO_LOGIT_SCALING:
+        # d/dx [s * x] = s
+        y = y * LOGIT_SCALE
+    pass
+
     if DO_SOFTCAPPING:
         # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
         y = y * (1.0 - partial*partial)
@@ -224,14 +261,15 @@ MAX_FUSED_SIZE = 65536 # 2**16
 
 class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
-    def forward(ctx, logits, labels, logit_softcapping = 0):
+    def forward(ctx, logits, labels, logit_softcapping = 0, logit_scaling = 0):
         n_rows, vocab_size = logits.shape
 
         div, mod = divmod(vocab_size, MAX_FUSED_SIZE)
         n_chunks = div + (mod != 0)
         losses = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
 
-        DO_SOFTCAPPING = (logit_softcapping != 0)
+        DO_SOFTCAPPING   = (logit_softcapping != 0)
+        DO_LOGIT_SCALING = (logit_scaling != 0)
 
         if n_chunks == 1:
             # For small vocabs <= 65336 like Llama, Mistral
@@ -243,11 +281,13 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
                 losses,
                 logsumexp,
                 labels,
-                VOCAB_SIZE     = vocab_size,
-                BLOCK_SIZE     = BLOCK_SIZE,
-                DO_SOFTCAPPING = DO_SOFTCAPPING,
-                SOFTCAP        = logit_softcapping,
-                num_warps      = num_warps,
+                VOCAB_SIZE       = vocab_size,
+                BLOCK_SIZE       = BLOCK_SIZE,
+                DO_SOFTCAPPING   = DO_SOFTCAPPING,
+                SOFTCAP          = logit_softcapping,
+                DO_LOGIT_SCALING = DO_LOGIT_SCALING,
+                LOGIT_SCALE      = logit_scaling,
+                num_warps        = num_warps,
             )
         else:
             # For large vocabs > 65336 like Gemma 256K
@@ -258,12 +298,14 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
                 losses,
                 logsumexp,
                 labels,
-                VOCAB_SIZE     = vocab_size,
-                N_CHUNKS       = n_chunks,
-                BLOCK_SIZE     = MAX_FUSED_SIZE,
-                DO_SOFTCAPPING = DO_SOFTCAPPING,
-                SOFTCAP        = logit_softcapping,
-                num_warps      = 32,
+                VOCAB_SIZE       = vocab_size,
+                N_CHUNKS         = n_chunks,
+                BLOCK_SIZE       = MAX_FUSED_SIZE,
+                DO_SOFTCAPPING   = DO_SOFTCAPPING,
+                SOFTCAP          = logit_softcapping,
+                DO_LOGIT_SCALING = DO_LOGIT_SCALING,
+                LOGIT_SCALE      = logit_scaling,
+                num_warps        = 32,
             )
             # logsumexp(chunked_logsumexp) - x
             # Do the -x separately
@@ -275,6 +317,8 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
         ctx.save_for_backward(logits, logsumexp, labels)
         ctx.DO_SOFTCAPPING    = DO_SOFTCAPPING
         ctx.logit_softcapping = logit_softcapping
+        ctx.DO_LOGIT_SCALING  = DO_LOGIT_SCALING
+        ctx.logit_scaling     = logit_scaling
         return losses
     pass
 
@@ -292,19 +336,26 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
             dlosses, dlosses.stride(0),
             logsumexp,
             labels,
-            VOCAB_SIZE     = vocab_size,
-            BLOCK_SIZE     = BLOCK_SIZE,
-            DO_SOFTCAPPING = ctx.DO_SOFTCAPPING,
-            SOFTCAP        = ctx.logit_softcapping,
+            VOCAB_SIZE       = vocab_size,
+            BLOCK_SIZE       = BLOCK_SIZE,
+            DO_SOFTCAPPING   = ctx.DO_SOFTCAPPING,
+            SOFTCAP          = ctx.logit_softcapping,
+            DO_LOGIT_SCALING = ctx.DO_LOGIT_SCALING,
+            LOGIT_SCALE      = ctx.logit_scaling,
             num_warps      = 8,
         )
-        return logits, None, None,
+        return logits, None, None, None,
     pass
 pass
 
 
 @torch._disable_dynamo
-def fast_cross_entropy_loss(logits, labels, logit_softcapping = 0):
+def fast_cross_entropy_loss(
+    logits,
+    labels,
+    logit_softcapping = 0,
+    logit_scaling = 0,
+):
     """"""
     Arguments:
         logits: (batch, seq_len, vocab_size)
@@ -319,6 +370,7 @@ def fast_cross_entropy_loss(logits, labels, logit_softcapping = 0):
         logits.view(batch*seq_len, d),
         labels.view(-1),
         logit_softcapping,
+        logit_scaling,
     )
     n_items = torch.count_nonzero(labels != -100)
     return loss.sum() / n_items
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index 8f41017..2177b43 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -68,7 +68,8 @@ class LoRA_MLP(torch.autograd.Function):
                 gateW, gateW_quant, gateA, gateB, gateS,
                   upW,   upW_quant, upA,   upB,   upS,
                 downW, downW_quant, downA, downB, downS,
-                _forward_function, _backward_function,):
+                _forward_function, _backward_function,
+                inplace = True,):
         dtype = X.dtype
 
         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)
@@ -84,6 +85,7 @@ class LoRA_MLP(torch.autograd.Function):
         )
         ctx.save_for_backward(gateA, gateB, upA, upB, downA, downB,
                               X, e, g)
+        ctx.inplace = inplace
         return i
     pass
 
@@ -131,7 +133,7 @@ class LoRA_MLP(torch.autograd.Function):
         # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)
         # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)
         upW = fast_dequantize(upW.t(), upW_quant)
-        dX = torch.matmul(df, upW.t(), out = X)
+        dX = torch.matmul(df, upW.t(), out = X if ctx.inplace else None)
         del upW
         dX += df @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())
 
@@ -147,13 +149,13 @@ class LoRA_MLP(torch.autograd.Function):
             None, None, d_gateA.t(), d_gateB.t(), None, \
             None, None,   d_upA.t(),   d_upB.t(), None, \
             None, None, d_downA.t(), d_downB.t(), None, \
-            None, None, # _backward and _forward
+            None, None, None, # _backward and _forward and inplace
     pass
 pass
 
 
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
-def apply_lora_mlp_swiglu(self, X):
+def apply_lora_mlp_swiglu(self, X, inplace = True):
     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
@@ -161,13 +163,14 @@ def apply_lora_mlp_swiglu(self, X):
                          gateW, gateW_quant, gateA, gateB, gateS,
                          upW,     upW_quant, upA,   upB,   upS,
                          downW, downW_quant, downA, downB, downS,
-                         swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel,)
+                         swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel,
+                         inplace,)
     return out
 pass
 
 
 from .geglu import geglu_exact_forward_kernel, geglu_exact_backward_kernel
-def apply_lora_mlp_geglu_exact(self, X):
+def apply_lora_mlp_geglu_exact(self, X, inplace = True):
     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
@@ -175,7 +178,8 @@ def apply_lora_mlp_geglu_exact(self, X):
                          gateW, gateW_quant, gateA, gateB, gateS,
                          upW,     upW_quant, upA,   upB,   upS,
                          downW, downW_quant, downA, downB, downS,
-                         geglu_exact_forward_kernel, geglu_exact_backward_kernel,)
+                         geglu_exact_forward_kernel, geglu_exact_backward_kernel,
+                         inplace,)
     return out
 pass
 
@@ -229,7 +233,8 @@ class LoRA_QKV(torch.autograd.Function):
     def forward(ctx, X : torch.Tensor,
                 QW, QW_quant, QA, QB, QS,
                 KW, KW_quant, KA, KB, KS,
-                VW, VW_quant, VA, VB, VS,):
+                VW, VW_quant, VA, VB, VS,
+                inplace = True):
         dtype = X.dtype
 
         Q = matmul_lora(X, QW, QW_quant, QA, QB, QS)
@@ -242,6 +247,7 @@ class LoRA_QKV(torch.autograd.Function):
             VW, VW_quant, VS,
         )
         ctx.save_for_backward(X, QA, QB, KA, KB, VA, VB,)
+        ctx.inplace = inplace
         return Q, K, V
     pass
 
@@ -286,7 +292,7 @@ class LoRA_QKV(torch.autograd.Function):
         # Combine derivatives to find dX
         # dQ
         QW = fast_dequantize(QW.t(), QW_quant)
-        dX = torch.matmul(dQ, QW.t(), out = X)
+        dX = torch.matmul(dQ, QW.t(), out = X if ctx.inplace else None)
         del QW
         dX += (dQ @ QB.to(dtype).t() @ (QS * QA.to(dtype).t()))
 
@@ -308,12 +314,13 @@ class LoRA_QKV(torch.autograd.Function):
         return dX.view(batch, seq_len, hd), \
             None, None, d_QA.t(), d_QB.t(), None, \
             None, None, d_KA.t(), d_KB.t(), None, \
-            None, None, d_VA.t(), d_VB.t(), None
+            None, None, d_VA.t(), d_VB.t(), None, \
+            None,
     pass
 pass
 
 
-def apply_lora_qkv(self, X):
+def apply_lora_qkv(self, X, inplace = True):
     QW, QW_quant, QA, QB, QS = get_lora_parameters(self.q_proj)
     KW, KW_quant, KA, KB, KS = get_lora_parameters(self.k_proj)
     VW, VW_quant, VA, VB, VS = get_lora_parameters(self.v_proj)
@@ -321,6 +328,7 @@ def apply_lora_qkv(self, X):
         QW, QW_quant, QA, QB, QS,
         KW, KW_quant, KA, KB, KS,
         VW, VW_quant, VA, VB, VS,
+        inplace,
     )
     return Q, K, V
 pass
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index f26e596..ac5beb5 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -186,7 +186,9 @@ pass
 
 def fast_rms_layernorm(layernorm, X, gemma = False):
     W   = layernorm.weight
-    eps = layernorm.variance_epsilon
+    eps = layernorm.variance_epsilon if \
+        hasattr(layernorm, ""variance_epsilon"") \
+        else layernorm.eps
     out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)
     return out
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 1c48e8e..ea9a0c5 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -295,7 +295,7 @@ if Version(xformers_version) >= Version(""0.0.27""):
             send_to_device,
         ).replace(""def send_to_device"", ""def _fixed_send_to_device"")
         exec(send_to_device)
-        accelerate.utils.operations.send_to_device = _fixed_send_to_device
+        # accelerate.utils.operations.send_to_device = _fixed_send_to_device
     pass
 pass
 # =============================================
diff --git a/unsloth/models/cohere.py b/unsloth/models/cohere.py
new file mode 100644
index 0000000..aa0bcb5
--- /dev/null
+++ b/unsloth/models/cohere.py
@@ -0,0 +1,473 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .llama import *
+from ._utils import __version__
+try:
+    from transformers.models.cohere.modeling_cohere import (
+        CohereAttention,
+        CohereDecoderLayer,
+        CohereModel,
+        CohereForCausalLM,
+        CohereRotaryEmbedding,
+        apply_rotary_pos_emb,
+        repeat_kv,
+    )
+except:
+    from packaging.version import Version
+    transformers_version = Version(transformers_version)
+    if not transformers_version >= Version(""4.42""):
+        raise ImportError(
+            f""Unsloth: Your transformers version of {transformers_version} does not support Cohere.\n""\
+            f""The minimum required version is 4.42.3.\n""\
+            f'Try `pip install --upgrade ""transformers>=4.42.3""`\n'\
+            f""to obtain the latest transformers build, then restart this session.""\
+        )
+    pass
+pass
+
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
+# For Pytorch 2.1.1
+try:
+    from transformers.models.cohere.modeling_cohere import (
+        CohereSdpaAttention,
+        CohereFlashAttention2,
+    )
+except:
+    CohereSdpaAttention   = CohereAttention
+    CohereFlashAttention2 = CohereAttention
+pass
+
+
+def fast_layernorm_inference(self, X, out_weight = None):
+    XX = X.to(torch.float32, copy = True)
+    XX -= X.mean(-1, keepdim = True)
+    variance = XX.square().mean(-1, keepdim = True)
+    variance += self.variance_epsilon
+    XX *= variance.rsqrt_()
+    out_weight[:] = self.weight
+    XX *= out_weight
+    return XX.to(X.dtype)
+pass
+
+
+# QK norm in Cohere
+def CohereAttention_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    bool = False,
+    use_cache:            bool = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    *args, **kwargs,
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+    
+    # Clear inference
+    if hasattr(self, ""paged_attention""):
+        del self.paged_attention_K
+        del self.paged_attention_V
+        del self.paged_attention
+        del self.temp_QA
+        del self.temp_KV
+        del self.RH_Q
+        del self.attention
+        del self.q_norm_out_weight
+        del self.k_norm_out_weight
+    pass
+
+    bsz, q_len, _ = hidden_states.size()
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    assert(n_kv_heads * n_groups == n_heads)
+
+    Q, K, V = self.apply_qkv(self, hidden_states)
+    Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)
+    K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+    V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)
+    if self.use_qk_norm:
+        Q = fast_layernorm_compiled(self.q_norm, Q)
+        K = fast_layernorm_compiled(self.k_norm, K)
+    pass
+
+    kv_seq_len = K.shape[-2]
+    if past_key_value is not None:
+        kv_seq_len += past_key_value[0].shape[-2]
+
+    if position_ids is None:
+        cos = self.rotary_emb.cos_cached
+        sin = self.rotary_emb.sin_cached
+        Q, K = fast_rope_embedding(Q, K, cos, sin)
+    else:
+        cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
+        Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
+    pass
+
+    if past_key_value is not None:
+        K = torch.cat([past_key_value[0], K], dim = 2)
+        V = torch.cat([past_key_value[1], V], dim = 2)
+    pass
+    past_key_value = (K, V) if use_cache else None
+
+    # Attention module
+    if (not HAS_FLASH_ATTENTION and attention_mask is None):
+        # Xformers memory efficient attention
+        # Also has Flash Attention v2 dispatching
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+
+        # Group query attention
+        if n_groups != 1:
+            K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+            V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)
+            K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+            V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)
+            if hidden_states.requires_grad:
+                K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)
+                V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)
+            else:
+                Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
+        pass
+        A = xformers_attention(Q, K, V, attn_bias = causal_mask)
+        A = A.view(bsz, q_len, n_heads, head_dim)
+
+    elif HAS_FLASH_ATTENTION and attention_mask is None:
+        Q = Q.transpose(1, 2)
+        K = K.transpose(1, 2)
+        V = V.transpose(1, 2)
+        A = flash_attn_func(Q, K, V, causal = True)
+    else:
+        # Grouped query attention
+        if n_groups != 1:
+            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)
+            K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)
+            V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)
+        pass
+        # Must be contiguous or else results are False!
+        # https://github.com/pytorch/pytorch/issues/112577
+        Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
+        # Needs (batch_size, n_heads, seq_len, head_dim)
+        # is_casual and attention_mask must not be both set!
+        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)
+        # Go back to (batch_size, seq_len, n_heads, head_dim)
+        A = A.transpose(1, 2).contiguous()
+    pass
+    attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
+    attn_output = self.apply_o(self, attn_output)
+    attn_weights = None
+    return attn_output, attn_weights, past_key_value
+pass
+
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590
+def CohereDecoderLayer_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    Optional[bool] = False,
+    use_cache:            Optional[bool] = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    *args, **kwargs,
+):
+    if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = ""cuda:0"")
+
+        # Self Attention
+        residual = hidden_states
+        hidden_states = fast_layernorm_inference(self.input_layernorm, hidden_states, out_weight)
+        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+
+        # Fully Connected
+        hidden_states_mlp = fast_swiglu_inference(self.mlp, hidden_states)
+        residual += hidden_states_attention
+        residual += hidden_states_mlp
+        hidden_states = residual
+    else:
+        residual = hidden_states
+        hidden_states = fast_layernorm_compiled(self.input_layernorm, hidden_states)
+        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+
+        # Fully Connected
+        hidden_states_mlp = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states_attention + hidden_states_mlp
+    pass
+
+    outputs = (hidden_states,)
+    if output_attentions: outputs += (self_attn_weights,)
+    if use_cache: outputs += (present_key_value,)
+    return outputs
+pass
+
+
+from math import sqrt as math_sqrt
+KV_CACHE_INCREMENT = 256 # KV Cache update size
+torch_nn_functional_softmax = torch.nn.functional.softmax
+torch_matmul = torch.matmul
+
+def CohereAttention_fast_forward_inference(
+    self,
+    hidden_states:  torch.Tensor,
+    past_key_value: Optional[Tuple[torch.Tensor]],
+    position_ids,
+    do_prefill = False,
+    attention_mask = None,
+):
+    Xn = hidden_states
+    bsz, _, hd = hidden_states.size()
+    K1, V1 = past_key_value
+    dtype = Xn.dtype
+
+    n_heads    = self.num_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.num_key_value_heads
+    head_dim   = self.head_dim
+    attention_size = n_heads*head_dim
+    # assert(n_kv_heads * n_groups == n_heads)
+    seq_len = K1.shape[-2]
+    kv_seq_len = seq_len + 1
+
+    # Prefill phase
+    # if not hasattr(self, ""paged_attention""):
+    if do_prefill:
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = ""cuda:0"")
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
+        self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+        
+        # Mistral Nemo 12b has weird dimensions
+        if attention_size != self.hidden_size:
+            self.temp_O = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        else:
+            self.temp_O = self.temp_QA[1][:,:,:self.hidden_size]
+        pass
+        
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
+        self.scalar = 1.0 / math_sqrt(self.head_dim)
+        self.half_head_dim = head_dim // 2
+        # Cohere has QK layernorms
+        if self.use_qk_norm:
+            self.q_norm_out_weight = torch.empty(self.q_norm.weight.shape, dtype = torch.float32, device = ""cuda:0"")
+            self.k_norm_out_weight = torch.empty(self.k_norm.weight.shape, dtype = torch.float32, device = ""cuda:0"")
+        else:
+            self.q_norm_out_weight = None
+            self.k_norm_out_weight = None
+        pass
+    elif kv_seq_len >= self.paged_attention.shape[0]:
+        self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.attention.resize_((bsz, n_heads, 1, self.attention.shape[-1]+KV_CACHE_INCREMENT))
+    pass
+
+    Qn = fast_linear_forward(self.q_proj, Xn, out = self.temp_QA[0])
+    Kn = fast_linear_forward(self.k_proj, Xn, out = self.temp_KV[0])
+    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])
+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)
+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+    if self.use_qk_norm:
+        Q = fast_layernorm_inference(self.q_norm, Q, self.q_norm_out_weight)
+        K = fast_layernorm_inference(self.k_norm, K, self.k_norm_out_weight)
+    pass
+
+    # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
+    # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos = cos[position_ids].unsqueeze(1)
+    sin = sin[position_ids].unsqueeze(1)
+    h = self.half_head_dim
+
+    RH_Q = self.RH_Q
+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    Qn *= cos
+    Qn.addcmul_(RH_Q, sin)
+
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]
+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]
+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    Kn *= cos
+    Kn.addcmul_(RH_K, sin)
+    
+    # New KV cache
+    # Kn = torch.cat([K1, Kn], dim = 2)
+    # Vn = torch.cat([V1, Vn], dim = 2)
+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+
+    # Handle sliding windows
+    sliding_window = getattr(self.config, ""sliding_window"", None)
+    if sliding_window is not None and kv_seq_len > sliding_window:
+        # From https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L193
+        slicing_tokens = 1 - sliding_window
+        Knn = Kn[:, :, slicing_tokens:, :]#.contiguous()
+        Vnn = Vn[:, :, slicing_tokens:, :]#.contiguous()
+    else:
+        Knn, Vnn = Kn, Vn
+    pass
+
+    # Grouped query attention
+    _, _, cached_len, _ = Knn.shape
+    if n_groups != 1:
+        Knn = Knn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vnn = Vnn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)
+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)
+    pass
+    # else:
+    #     Knn, Vnn = Knn, Vnn
+    # pass
+
+    # Attention
+    if bsz == 1:
+        Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
+        # It seems like doing (Q * scalar) @ K is better than (Q @ K) * scalar to stop overflows
+        A = torch_matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
+        A = torch_matmul(A, Vnn, out = Qn)
+    else:
+        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+    pass
+    A = A.transpose(1, 2)
+    A = A.reshape(bsz, 1, attention_size)
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_O)
+    return A, (Kn, Vn)
+pass
+
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
+# @torch.inference_mode
+def CohereModel_fast_forward_inference(
+    self,
+    input_ids,
+    past_key_values,
+    position_ids,
+    attention_mask = None,
+):
+    out_weight = torch.empty_like(self.model.layers[0].input_layernorm.weight, dtype = torch.float32, device = ""cuda:0"")
+    input_ids = input_ids[:,:self.max_seq_length]
+    hidden_states = self.model.embed_tokens(input_ids)
+    hidden_states = hidden_states.to(self.config.torch_dtype)
+    bsz, q_len, hd = hidden_states.shape
+    seq_len = past_key_values[0][0].shape[-2]
+    if bsz != 1:
+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+            attention_mask,
+            (bsz, q_len),
+            hidden_states,
+            seq_len,
+            sliding_window = getattr(self.config, ""sliding_window"", None),
+        )
+    else:
+        attention_mask = None
+    pass
+
+    next_decoder_cache = []
+    for idx, decoder_layer in enumerate(self.model.layers):
+        residual = hidden_states
+        hidden_states = fast_layernorm_inference(decoder_layer.input_layernorm, hidden_states, out_weight)
+        hidden_states_attention, present_key_value = CohereAttention_fast_forward_inference(
+            decoder_layer.self_attn,
+            hidden_states = hidden_states,
+            past_key_value = past_key_values[idx],
+            position_ids = position_ids,
+            attention_mask = attention_mask,
+            do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
+        )
+
+        hidden_states_mlp = fast_swiglu_inference(self.mlp, hidden_states)
+        residual += hidden_states_attention
+        residual += hidden_states_mlp
+        hidden_states = residual
+
+        next_decoder_cache.append(present_key_value)
+    pass
+    hidden_states = fast_layernorm_inference(self.model.norm, hidden_states, out_weight)
+
+    return BaseModelOutputWithPast(
+        last_hidden_state = hidden_states,
+        past_key_values = next_decoder_cache,
+        hidden_states = [],
+        attentions = [],
+    )
+pass
+
+
+class FastCohereModel(FastLlamaModel):
+
+    @staticmethod
+    def pre_patch():
+        init_name, function = patch_linear_scaling(
+            model_name         = ""cohere"",
+            rope_module        = LlamaRotaryEmbedding,
+            scaled_rope_module = LlamaLinearScalingRotaryEmbedding,
+            attention_module   = CohereAttention,
+        )
+        if init_name is not None:
+            exec(function, globals())
+            CohereAttention.__init__  = eval(init_name)
+        pass
+        CohereAttention      .forward = CohereAttention_fast_forward
+        CohereSdpaAttention  .forward = CohereAttention_fast_forward
+        CohereFlashAttention2.forward = CohereAttention_fast_forward
+        CohereDecoderLayer   .forward = CohereDecoderLayer_fast_forward
+        CohereModel          .forward = LlamaModel_fast_forward
+        CohereForCausalLM    .forward = CausalLM_fast_forward(CohereModel_fast_forward_inference)
+        PeftModelForCausalLM .forward = PeftModelForCausalLM_fast_forward
+        fix_prepare_inputs_for_generation(CohereForCausalLM)
+        
+        import transformers.models.cohere.modeling_cohere
+        transformers.models.cohere.modeling_cohere.CohereRotaryEmbedding = LlamaRotaryEmbedding
+        return
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index f62f0f1..5ccf906 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -305,6 +305,20 @@ def fast_rms_layernorm_inference_gemma(self, X, out_weight = None):
 pass
 
 
+# Normal layernorm with mean removal
+@torch.compile(fullgraph = False, dynamic = True, options = torch_compile_options)
+def fast_layernorm_compiled(layernorm, X):
+    old_dtype = X.dtype
+    X = X.float()
+    mean = X.mean(-1, keepdim = True)
+    Xbar = X - mean
+    X = Xbar * torch.rsqrt(Xbar.square().mean(-1, keepdim = True) + \
+        layernorm.variance_epsilon) * \
+        layernorm.weight.float()
+    return X.to(old_dtype)
+pass
+
+
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320
 def LlamaAttention_fast_forward(
     self,
@@ -495,6 +509,16 @@ def LlamaDecoderLayer_fast_forward(
 pass
 
 
+# https://github.com/unslothai/unsloth/issues/404#issuecomment-2323473452
+__DTYPE_MAP = {
+    ""float32"": torch.float32,
+    torch.float32: torch.float32,
+    ""float16"": torch.float16,
+    torch.float16: torch.float16,
+    ""bfloat16"": torch.bfloat16,
+    torch.bfloat16: torch.bfloat16,
+}
+
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
 def LlamaModel_fast_forward(
     self,
@@ -576,11 +600,18 @@ def LlamaModel_fast_forward(
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
-    inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
+    # inputs_embeds = inputs_embeds.to(self.config.torch_dtype)
+    torch_dtype = __DTYPE_MAP.get(self.config.torch_dtype, None)
+    if torch_dtype is not None:
+        inputs_embeds = inputs_embeds.to(torch_dtype)
+    else:
+        raise TypeError(""Unsloth: torch_dtype for models is not bfloat16, float16 or float32!"")
+    pass
 
     # Normalized from Gemma
     IS_GEMMA  = self.config.model_type.startswith(""gemma"")
     IS_GEMMA2 = self.config.model_type.startswith(""gemma2"")
+    IS_COHERE = self.config.model_type.startswith(""cohere"")
     train_embed_tokens = self.embed_tokens.weight.requires_grad
 
     if IS_GEMMA:
@@ -786,8 +817,11 @@ def LlamaModel_fast_forward(
 
     # Final layernorm
     if use_cache:
-        hidden_states = (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\
+        hidden_states = \
+            (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\
             (self.norm, hidden_states)
+    elif IS_COHERE:
+        hidden_states = fast_layernorm_compiled(self.norm, hidden_states)
     else:
         hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
     pass
@@ -877,6 +911,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
+        num_logits_to_keep: Optional[int] = 0,
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
         
@@ -925,6 +960,7 @@ def CausalLM_fast_forward(fast_forward_inference):
 
         loss = None
         logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
+        logit_scaling     = getattr(self.config, ""logit_scale"", 0)
         if labels is not None:
             shift_logits = logits
             if not hasattr(self, ""extra_ignored_labels""):
@@ -937,16 +973,26 @@ def CausalLM_fast_forward(fast_forward_inference):
                 logits = shift_logits,
                 labels = shift_labels,
                 logit_softcapping = logit_softcapping,
+                logit_scaling     = logit_scaling,
             )
-        elif logit_softcapping != 0:
-            if logits.requires_grad:
-                logits = (1.0 / logit_softcapping) * logits
-                logits = torch.tanh(logits)
-                logits = logit_softcapping * logits
-            else:
-                logits *= (1.0 / logit_softcapping)
-                torch.tanh(logits, out = logits)
-                logits *= logit_softcapping
+        else:
+            if logit_scaling != 0:
+                if logits.requires_grad:
+                    logits = logit_scaling * logits
+                else:
+                    logits *= logit_scaling
+                pass
+            pass
+            if logit_softcapping != 0:
+                if logits.requires_grad:
+                    logits = (1.0 / logit_softcapping) * logits
+                    logits = torch.tanh(logits)
+                    logits = logit_softcapping * logits
+                else:
+                    logits *= (1.0 / logit_softcapping)
+                    torch.tanh(logits, out = logits)
+                    logits *= logit_softcapping
+                pass
             pass
         pass
 
@@ -978,6 +1024,7 @@ def PeftModelForCausalLM_fast_forward(
     output_hidden_states=None,
     return_dict=None,
     task_ids=None,
+    num_logits_to_keep=0,
     **kwargs,
 ):
     return self.base_model(
@@ -989,6 +1036,7 @@ def PeftModelForCausalLM_fast_forward(
         output_attentions=output_attentions,
         output_hidden_states=output_hidden_states,
         return_dict=return_dict,
+        num_logits_to_keep=num_logits_to_keep,
         **kwargs,
     )
 pass
@@ -2181,6 +2229,7 @@ class FastLlamaModel:
         elif model_type == ""qwen2"":   apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu_approx
         elif model_type == ""gemma2"":  apply_lora_mlp = apply_lora_mlp_geglu_approx
+        elif model_type == ""cohere"":  apply_lora_mlp = apply_lora_mlp_swiglu
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
         pass
@@ -2240,6 +2289,14 @@ class FastLlamaModel:
         lora_dropout = model.peft_config[active_adapter].lora_dropout
         bias         = model.peft_config[active_adapter].bias
 
+        # We also do not inplace edit QKV for Cohere!
+        from functools import partial
+        _apply_lora_mlp = \
+            partial(apply_lora_mlp, inplace = False) \
+            if model_type == ""cohere"" else \
+            apply_lora_mlp
+        pass
+
         if lora_dropout == 0 and bias == ""none"":
             for idx, layer in enumerate(model.model.model.layers):
 
@@ -2259,7 +2316,7 @@ class FastLlamaModel:
                     (len(getattr(down_proj, ""lora_magnitude_vector"", []) or []) == 0):
 
                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
-                    layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)
+                    layer.mlp.forward = types.MethodType(_apply_lora_mlp, layer.mlp)
                     n_mlp += 1
                 else:
                     logger.warning_once(
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index e1f17ac..13710ee 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -13,9 +13,10 @@
 # limitations under the License.
 
 from ._utils import is_bfloat16_supported, HAS_FLASH_ATTENTION, HAS_FLASH_ATTENTION_SOFTCAPPING
-from .llama import FastLlamaModel, logger
+from .llama   import FastLlamaModel, logger
 from .mistral import FastMistralModel
-from .qwen2 import FastQwen2Model
+from .qwen2   import FastQwen2Model
+from .cohere  import FastCohereModel
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
 from peft import PeftConfig, PeftModel
@@ -278,6 +279,8 @@ class FastLanguageModel(FastLlamaModel):
             dispatch_model = FastGemma2Model
         elif model_type == ""qwen2"":
             dispatch_model = FastQwen2Model
+        elif model_type == ""cohere"":
+            dispatch_model = FastCohereModel
         else:
             raise NotImplementedError(
                 f""Unsloth: {model_name} not supported yet!\n""\
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 3f49c96..bff7f02 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -227,6 +227,7 @@ __INT_TO_FLOAT_MAPPER = \
         ""meta-llama/Meta-Llama-3.1-8B-Instruct"",
     ),
     ""unsloth/Meta-Llama-3.1-70B-bnb-4bit"" : (
+        ""unsloth/Meta-Llama-3.1-70B"",
         ""meta-llama/Meta-Llama-3.1-70B"",
     ),
     ""unsloth/Meta-Llama-3.1-405B-bnb-4bit"" : (
@@ -236,6 +237,7 @@ __INT_TO_FLOAT_MAPPER = \
         ""meta-llama/Meta-Llama-3.1-405B-Instruct"",
     ),
     ""unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit"" : (
+        ""unsloth/Meta-Llama-3.1-70B-Instruct"",
         ""meta-llama/Meta-Llama-3.1-70B-Instruct"",
     ),
     ""unsloth/Mistral-Large-Instruct-2407-bnb-4bit"" : (
@@ -253,6 +255,27 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Phi-3.5-mini-instruct"",
         ""microsoft/Phi-3.5-mini-instruct"",
     ),
+    ""unsloth/c4ai-command-r-08-2024-bnb-4bit"" : (
+        ""CohereForAI/c4ai-command-r-08-2024"",
+    ),
+    ""unsloth/c4ai-command-r-plus-08-2024-bnb-4bit"" : (
+        ""CohereForAI/c4ai-command-r-plus-08-2024"",
+    ),
+    ""unsloth/Llama-3.1-Storm-8B-bnb-4bit"" : (
+        ""unsloth/Llama-3.1-Storm-8B"",
+        ""akjindal53244/Llama-3.1-Storm-8B"",
+    ),
+    ""unsloth/Hermes-3-Llama-3.1-8B-bnb-4bit"" : (
+        ""unsloth/Hermes-3-Llama-3.1-8B"",
+        ""NousResearch/Hermes-3-Llama-3.1-8B"",
+    ),
+    ""unsloth/Hermes-3-Llama-3.1-70B-bnb-4bit"" : (
+        ""unsloth/Hermes-3-Llama-3.1-70B"",
+        ""NousResearch/Hermes-3-Llama-3.1-70B"",
+    ),
+    ""unsloth/Hermes-3-Llama-3.1-405B-bnb-4bit"" : (
+        ""NousResearch/Hermes-3-Llama-3.1-405B"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index c8e00be..45616ca 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -14,8 +14,13 @@
 
 from dataclasses import dataclass, field
 from typing import Optional
-from transformers import TrainingArguments
+
 from trl import SFTTrainer
+try:
+    from trl import SFTConfig as TrainingArguments
+except:
+    from transformers import TrainingArguments
+pass
 from . import is_bfloat16_supported
 
 __all__ = [
"
"diff --git a/tests/saving/test_unsloth_save.py b/tests/saving/test_unsloth_save.py
new file mode 100644
index 0000000..14e8af4
--- /dev/null
+++ b/tests/saving/test_unsloth_save.py
@@ -0,0 +1,169 @@
+import json
+import os
+import shutil
+import tempfile
+import pytest
+
+from unsloth import FastLanguageModel, FastModel
+
+model_to_test = [
+    # Text Models
+    ""unsloth/tinyllama"",
+    ""unsloth/tinyllama-bnb-4bit"",
+    ""unsloth/Qwen2.5-0.5B-Instruct"",
+    ""unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit"",
+    ""unsloth/Phi-4-mini-instruct"",
+    ""unsloth/Phi-4-mini-instruct-bnb-4bit"",
+    ""unsloth/Qwen2.5-0.5B"",
+    # Vision Models
+    ""unsloth/gemma-3-1b-it"",
+    ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
+    ""unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit""
+]
+
+# Variables
+save_file_sizes = {}
+save_file_sizes[""merged_16bit""] = {}
+save_file_sizes[""merged_4bit""] = {}
+
+tokenizer_files = [
+    ""tokenizer_config.json"",
+    ""special_tokens_map.json"",
+]
+
+@pytest.fixture(scope=""session"", params=model_to_test)
+def loaded_model_tokenizer(request):
+    model_name = request.param
+    print(""Loading model and tokenizer..."")
+
+    model, tokenizer = FastModel.from_pretrained(
+        model_name, # use small model
+        max_seq_length=128,
+        dtype=None,
+        load_in_4bit=True,
+    )
+
+    # Apply LoRA
+    model = FastModel.get_peft_model(
+        model,
+        r=16,
+        target_modules=[""q_proj"", ""k_proj"", ""v_proj"", ""o_proj""],
+        lora_alpha=16,
+        use_gradient_checkpointing=""unsloth"",
+    )
+
+    return model, tokenizer
+
+@pytest.fixture(scope=""session"")
+def model(loaded_model_tokenizer):
+    return loaded_model_tokenizer[0]
+
+@pytest.fixture(scope=""session"")
+def tokenizer(loaded_model_tokenizer):
+    return loaded_model_tokenizer[1]
+
+@pytest.fixture
+def temp_save_dir():
+    dir = tempfile.mkdtemp()
+    print(f""Temporary directory created at: {dir}"")
+    yield dir
+    print(f""Temporary directory deleted: {dir}"")
+    shutil.rmtree(dir)
+
+
+def delete_quantization_config(model):
+    # Since merged, edit quantization_config
+    old_config = model.config
+    new_config = model.config.to_dict()
+    if ""quantization_config"" in new_config:
+        del new_config[""quantization_config""]
+    original_model = model
+    new_config = type(model.config).from_dict(new_config)
+    while hasattr(original_model, ""model""):
+        original_model = original_model.model
+        original_model.config = new_config
+    model.config = new_config
+
+def test_save_merged_16bit(model, tokenizer, temp_save_dir: str):
+    save_path = os.path.join(temp_save_dir, ""unsloth_merged_16bit"", model.config._name_or_path.replace(""/"", ""_""))
+
+    model.save_pretrained_merged(
+        save_path,
+        tokenizer=tokenizer,
+        save_method=""merged_16bit""
+    )
+
+    # Check model files
+    assert os.path.isdir(save_path), f""Directory {save_path} does not exist.""
+    assert os.path.isfile(os.path.join(save_path, ""config.json"")), ""config.json not found.""
+
+    weight_files = [f for f in os.listdir(save_path) if f.endswith("".bin"") or f.endswith("".safetensors"")]
+    assert len(weight_files) > 0, ""No weight files found in the save directory.""
+
+    # Check tokenizer files
+    for file in tokenizer_files:
+        assert os.path.isfile(os.path.join(save_path, file)), f""{file} not found in the save directory.""
+
+    # Check config to see if it is 16bit by checking for quantization config
+    config_path = os.path.join(save_path, ""config.json"")
+    with open(config_path, ""r"") as f:
+        config = json.load(f)
+
+    assert ""quantization_config"" not in config, ""Quantization config not found in the model config.""
+
+    # Store the size of the model files
+    total_size = sum(os.path.getsize(os.path.join(save_path, f)) for f in weight_files)
+    save_file_sizes[""merged_16bit""][model.config._name_or_path] = total_size
+    print(f""Total size of merged_16bit files: {total_size} bytes"")
+
+    # Test loading the model from the saved path
+    loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(
+        save_path,
+        max_seq_length=128,
+        dtype=None,
+        load_in_4bit=True,
+    )
+
+def test_save_merged_4bit(model, tokenizer, temp_save_dir: str):
+    save_path = os.path.join(temp_save_dir, ""unsloth_merged_4bit"", model.config._name_or_path.replace(""/"", ""_""))
+
+    model.save_pretrained_merged(
+        save_path,
+        tokenizer=tokenizer,
+        save_method=""merged_4bit_forced""
+    )
+
+    # Check model files
+    assert os.path.isdir(save_path), f""Directory {save_path} does not exist.""
+    assert os.path.isfile(os.path.join(save_path, ""config.json"")), ""config.json not found.""
+
+    weight_files = [f for f in os.listdir(save_path) if f.endswith("".bin"") or f.endswith("".safetensors"")]
+    assert len(weight_files) > 0, ""No weight files found in the save directory.""
+
+    # Check tokenizer files
+    for file in tokenizer_files:
+        assert os.path.isfile(os.path.join(save_path, file)), f""{file} not found in the save directory.""
+
+    # Store the size of the model files
+    total_size = sum(os.path.getsize(os.path.join(save_path, f)) for f in weight_files)
+    save_file_sizes[""merged_4bit""][model.config._name_or_path] = total_size
+
+    print(f""Total size of merged_4bit files: {total_size} bytes"")
+
+    assert total_size < save_file_sizes[""merged_16bit""][model.config._name_or_path], ""Merged 4bit files are larger than merged 16bit files.""
+
+    # Check config to see if it is 4bit
+    config_path = os.path.join(save_path, ""config.json"")
+    with open(config_path, ""r"") as f:
+        config = json.load(f)
+
+    assert ""quantization_config"" in config, ""Quantization config not found in the model config.""
+
+    # Test loading the model from the saved path
+    loaded_model, loaded_tokenizer = FastModel.from_pretrained(
+        save_path,
+        max_seq_length=128,
+        dtype=None,
+        load_in_4bit=True,
+    )
+
diff --git a/unsloth/save.py b/unsloth/save.py
index b8da9c0..e3eece6 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2301,6 +2301,17 @@ def unsloth_generic_save(
     maximum_memory_usage : float = 0.9,
 ):
     if token is None and push_to_hub: token = get_token()
+
+    if save_method == ""merged_4bit"":
+        raise RuntimeError(
+            ""Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\n""\
+            ""to merge to GGUF or others later on. I suggest you to do this as a final step\n""\
+            ""if you're planning to do multiple saves.\n""\
+            ""If you are certain, change `save_method` to `merged_4bit_forced`.""
+        )
+    elif save_method == ""merged_4bit_forced"":
+        save_method = ""merged_4bit""
+    
     merge_and_overwrite_lora(
         get_model_name,
         model                = model,
@@ -2309,6 +2320,7 @@ def unsloth_generic_save(
         push_to_hub          = push_to_hub,
         private              = private,
         token                = token,
+        save_method          = save_method,
         output_dtype         = None,
         low_disk_space_usage = True,
         use_temp_file        = False,
"
"diff --git a/README.md b/README.md
index 6bff98c..f658e6c 100644
--- a/README.md
+++ b/README.md
@@ -212,6 +212,9 @@ For **advanced installation instructions** or if you see weird errors during ins
 - Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
 - We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
 - We're in Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
+- If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.
+
+> unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.
 
 ```python
 from unsloth import FastLanguageModel 
diff --git a/pyproject.toml b/pyproject.toml
index ce33015..bf4c995 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -148,20 +148,20 @@ cu124onlytorch250 = [
     ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post2-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 cu121onlytorch251 = [
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
 ]
 cu124onlytorch251 = [
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
-    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.28.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu124/xformers-0.0.29.post1-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 cu118 = [
     ""unsloth[huggingface]"",
diff --git a/unsloth-cli.py b/unsloth-cli.py
index ddb0ac8..b7613f9 100644
--- a/unsloth-cli.py
+++ b/unsloth-cli.py
@@ -30,11 +30,14 @@ Happy fine-tuning!
 """"""
 
 import argparse
+import os
+
 
 def run(args):
     import torch
     from unsloth import FastLanguageModel
     from datasets import load_dataset
+    from transformers.utils import strtobool
     from trl import SFTTrainer
     from transformers import TrainingArguments
     from unsloth import is_bfloat16_supported
@@ -86,8 +89,13 @@ def run(args):
             texts.append(text)
         return {""text"": texts}
 
-    # Load and format dataset
-    dataset = load_dataset(args.dataset, split=""train"")
+    use_modelscope = strtobool(os.environ.get('UNSLOTH_USE_MODELSCOPE', 'False'))
+    if use_modelscope:
+        from modelscope import MsDataset
+        dataset = MsDataset.load(args.dataset, split=""train"")
+    else:
+        # Load and format dataset
+        dataset = load_dataset(args.dataset, split=""train"")
     dataset = dataset.map(formatting_prompts_func, batched=True)
     print(""Data is formatted and ready!"")
 
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 980425e..bbeded9 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -17,16 +17,6 @@ from packaging.version import Version
 import os, re, subprocess, inspect
 import numpy as np
 
-# # Define a list of modules to check
-# MODULES_TO_CHECK = [""bitsandbytes""]
-
-# # Check if any of the modules in the list have been imported
-# for module in MODULES_TO_CHECK:
-#     if module in sys.modules:
-#         raise ImportError(f""Unsloth: Please import Unsloth before {module}."")
-#     pass
-# pass
-
 # Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
@@ -55,7 +45,12 @@ else:
 pass
 
 # Reduce VRAM usage by reducing fragmentation
-os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""expandable_segments:True,roundup_power2_divisions:[64:128,256:64,>:32]""
+# And optimize pinning of memory
+os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = \
+    ""expandable_segments:True,""\
+    ""roundup_power2_divisions:[32:256,64:128,256:64,>:32],""\
+    ""pinned_use_cuda_host_register:True,""\
+    ""pinned_num_register_threads:8""
 
 # Hugging Face Hub faster downloads
 if ""HF_HUB_ENABLE_HF_TRANSFER"" not in os.environ:
@@ -89,6 +84,36 @@ elif (major_torch == 2) and (minor_torch < 2):
     del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
 pass
 
+# Fix Xformers performance issues since 0.0.25
+import importlib.util
+from pathlib import Path
+from importlib.metadata import version as importlib_version
+from packaging.version import Version
+try:
+    xformers_version = importlib_version(""xformers"")
+    if Version(xformers_version) < Version(""0.0.29""):
+        xformers_location = importlib.util.find_spec(""xformers"").origin
+        xformers_location = os.path.split(xformers_location)[0]
+        cutlass = Path(xformers_location) / ""ops"" / ""fmha"" / ""cutlass.py""
+
+        if cutlass.exists():
+            with open(cutlass, ""r+"") as f:
+                text = f.read()
+                # See https://github.com/facebookresearch/xformers/issues/1176#issuecomment-2545829591
+                if ""num_splits_key=-1,"" in text:
+                    text = text.replace(""num_splits_key=-1,"", ""num_splits_key=None,"")
+                    f.seek(0)
+                    f.write(text)
+                    f.truncate()
+                    print(""Unsloth: Patching Xformers to fix some performance issues."")
+                pass
+            pass
+        pass
+    pass
+except:
+    pass
+pass
+
 # Torch 2.4 has including_emulation
 major_version, minor_version = torch.cuda.get_device_capability()
 SUPPORTS_BFLOAT16 = (major_version >= 8)
@@ -166,9 +191,18 @@ pass
 
 # Check for unsloth_zoo
 try:
+    unsloth_zoo_version = importlib_version(""unsloth_zoo"")
+    if Version(unsloth_zoo_version) < Version(""2025.1.1""):
+        try:
+            os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
+        except:
+            try:
+                os.system(""pip install --upgrade --no-cache-dir --no-deps --user unsloth_zoo"")
+            except:
+                raise ImportError(""Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`"")
     import unsloth_zoo
 except:
-    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth-zoo`"")
+    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`"")
 pass
 
 from .models import *
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index d347cd1..fcba2eb 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -25,11 +25,6 @@ from unsloth_zoo.loss_utils import (
 )
 
 
-@triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
-    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
-})
-@triton.jit
 def _cross_entropy_forward(
     logits_ptr        ,
     logits_row_stride ,
@@ -95,13 +90,15 @@ def _cross_entropy_forward(
     tl.store(logsumexp_ptr, logsumexp)
     tl.store(loss_ptr, loss)
 pass
+_cross_entropy_forward = triton.jit(_cross_entropy_forward)
+_cross_entropy_forward = triton.heuristics(
+    {
+        ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+        ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
+    }
+)(_cross_entropy_forward)
 
 
-@triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
-    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
-})
-@triton.jit
 def _chunked_cross_entropy_forward(
     logits_ptr        ,
     logits_row_stride ,
@@ -177,13 +174,15 @@ def _chunked_cross_entropy_forward(
     pass
     tl.store(logsumexp_ptr, logsumexp)
 pass
+_chunked_cross_entropy_forward = triton.jit(_chunked_cross_entropy_forward)
+_chunked_cross_entropy_forward = triton.heuristics(
+    {
+        ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+        ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
+    }
+)(_chunked_cross_entropy_forward)
 
 
-@triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
-    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
-})
-@triton.jit
 def _cross_entropy_backward(
     logits_ptr        ,
     logits_row_stride ,
@@ -264,10 +263,16 @@ def _cross_entropy_backward(
     # If y == 0: dC/dx = 0 ==> we already masked it to be = 0, so dloss = 0.
     tl.store(logits_ptr + col_offsets, dloss * y, mask = mask)
 pass
+_cross_entropy_backward = triton.jit(_cross_entropy_backward)
+_cross_entropy_backward = triton.heuristics(
+    {
+        ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+        ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
+    }
+)(_cross_entropy_backward)
 
 
 MAX_FUSED_SIZE = 65536 # 2**16
-
 class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
     def forward(ctx, logits, labels, logit_softcapping : float = 0, logit_scaling : float = 0):
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index 887ffca..6f82394 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -43,9 +43,9 @@ if not HAS_FLEX_ATTENTION:
     # Logit softcapping
     @torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
     def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-        n_heads    = self.num_heads
+        n_heads    = self.config.num_attention_heads
         head_dim   = self.head_dim
-        n_kv_heads = self.num_key_value_heads
+        n_kv_heads = self.config.num_key_value_heads
         n_groups   = self.num_key_value_groups
         
         # Grouped query attention
@@ -130,7 +130,7 @@ else:
     pass
     
     def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-        n_heads    = self.num_heads
+        n_heads    = self.config.num_attention_heads
         head_dim   = self.head_dim
         s = self.config.query_pre_attn_scalar
         t = self.config.attn_logit_softcapping
@@ -147,9 +147,9 @@ torch_matmul = torch.matmul
 torch_tanh   = torch.tanh
 torch_nn_functional_softmax = torch.nn.functional.softmax
 def slow_inference_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     head_dim   = self.head_dim
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     n_groups   = self.num_key_value_groups
     
     # Grouped query attention
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index b74d636..6310f7f 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -53,8 +53,6 @@ def _rms_layernorm_forward(
 pass
 
 
-@triton.heuristics({""GEMMA"": lambda args: bool(args[""GEMMA""]),})
-@triton.jit
 def _rms_layernorm_backward(
     dY, dY_row_stride,
     dX, dX_row_stride,
@@ -97,6 +95,12 @@ def _rms_layernorm_backward(
     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)
     tl.store(dX + col_offsets, output, mask = mask)
 pass
+_rms_layernorm_backward = triton.jit(_rms_layernorm_backward)
+_rms_layernorm_backward = triton.heuristics(
+    {
+        ""GEMMA"": lambda args: bool(args[""GEMMA""]),
+    }
+)(_rms_layernorm_backward)
 
 
 @triton.jit
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index 7fe15d0..88b9cca 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -18,8 +18,6 @@ import torch
 from .utils import calculate_settings
 ROPE_GROUP_SIZE : int = 4
 
-@triton.heuristics({""BACKWARD_PASS"": lambda args: bool(args[""BACKWARD_PASS""]),})
-@triton.jit
 def _rope_embedding(
     Q,     Q_row_stride,
     cos, cos_row_stride,
@@ -69,6 +67,12 @@ def _rope_embedding(
         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)
     pass
 pass
+_rope_embedding = triton.jit(_rope_embedding)
+_rope_embedding = triton.heuristics(
+    {
+        ""BACKWARD_PASS"": lambda args: bool(args[""BACKWARD_PASS""]),
+    }
+)(_rope_embedding)
 
 
 class Fast_RoPE_Embedding(torch.autograd.Function):
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 3cb6ffb..86adc0e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,9 +12,12 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.12.12""
+__version__ = ""2025.1.1""
 
 __all__ = [
+    ""SUPPORTS_BFLOAT16"",
+    ""is_bfloat16_supported"",
+
     ""prepare_model_for_kbit_training"",
     ""xformers"",
     ""xformers_attention"",
@@ -30,7 +33,6 @@ __all__ = [
     ""offload_to_disk"",
     ""offload_input_embeddings"",
     ""offload_output_embeddings"",
-    ""is_bfloat16_supported"",
     ""unsloth_offloaded_gradient_checkpoint"",
     ""torch_compile_options"",
     ""patch_linear_scaling"",
@@ -58,7 +60,6 @@ __all__ = [
     ""fused_linear_cross_entropy"",
     ""patch_unsloth_smart_gradient_checkpointing"",
     ""unpatch_unsloth_smart_gradient_checkpointing"",
-    ""create_gradient_checkpointing_buffer"",
 
     ""patch_compiled_autograd"",
     ""process_vision_info"",
@@ -97,7 +98,6 @@ from unsloth_zoo.gradient_checkpointing import (
 
     patch_unsloth_smart_gradient_checkpointing,
     unpatch_unsloth_smart_gradient_checkpointing,
-    create_gradient_checkpointing_buffer,
 )
 from unsloth_zoo.loss_utils import (
     HAS_CUT_CROSS_ENTROPY,
@@ -556,6 +556,7 @@ def prepare_model_for_kbit_training(
             def make_inputs_require_grad(module, input, output):
                 output.requires_grad_(True)
             model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
+    pass
 
     return model
 pass
@@ -1203,8 +1204,6 @@ def unsloth_compile_transformers(
         return
     pass
 
-    if disable: return
-
     model_types = get_transformers_model_type(
         model_name        = model_name,
         token             = token,
@@ -1212,6 +1211,8 @@ def unsloth_compile_transformers(
         trust_remote_code = trust_remote_code,
     )
 
+    if disable: return
+
     for model_type in model_types:
         _unsloth_compile_transformers(
             model_type,
diff --git a/unsloth/models/cohere.py b/unsloth/models/cohere.py
index 1610949..0c36abf 100644
--- a/unsloth/models/cohere.py
+++ b/unsloth/models/cohere.py
@@ -94,9 +94,9 @@ def CohereAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -259,12 +259,14 @@ def CohereAttention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -281,10 +283,10 @@ def CohereAttention_fast_forward_inference(
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         
         # Mistral Nemo 12b has weird dimensions
-        if attention_size != self.hidden_size:
-            self.temp_O = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        if attention_size != hidden_size:
+            self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         else:
-            self.temp_O = self.temp_QA[1][:,:,:self.hidden_size]
+            self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
         
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 0f0a020..be6b046 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -98,9 +98,9 @@ def Gemma2Attention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -255,12 +255,14 @@ def Gemma2Attention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -276,7 +278,7 @@ def Gemma2Attention_fast_forward_inference(
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         # Only for Gemma2
-        self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_O  = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
         
         # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
index 9466a8d..497a357 100644
--- a/unsloth/models/granite.py
+++ b/unsloth/models/granite.py
@@ -20,7 +20,8 @@ from .llama import (
     LlamaLinearScalingRotaryEmbedding,
 )
 from .mistral import *
-
+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
+from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 try:
     from transformers.models.granite.modeling_granite import (
         GraniteAttention,
@@ -84,9 +85,9 @@ def GraniteAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -181,6 +182,11 @@ def GraniteDecoderLayer_fast_forward(
     position_embeddings:  Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
     *args, **kwargs,
 ):
+    residual_multiplier = \
+        self.residual_multiplier \
+        if hasattr(self, ""residual_multiplier"") else \
+        self.config.residual_multiplier
+
     if use_cache and hasattr(self, ""_flag_for_generation""): #past_key_value is not None:
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
@@ -196,13 +202,13 @@ def GraniteDecoderLayer_fast_forward(
             position_embeddings = position_embeddings,
             _flag_for_generation=self._flag_for_generation,
         )
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
         hidden_states = fast_swiglu_inference(self.mlp, hidden_states)
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
     else:
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
@@ -217,13 +223,13 @@ def GraniteDecoderLayer_fast_forward(
             padding_mask=padding_mask,
             position_embeddings = position_embeddings,
         )
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
         hidden_states = self.mlp(hidden_states)
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
     pass
 
     outputs = (hidden_states,)
@@ -257,12 +263,14 @@ def GraniteAttention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -278,7 +286,7 @@ def GraniteAttention_fast_forward_inference(
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         # Only for Gemma2
-        self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_O  = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
 
 
@@ -367,6 +375,10 @@ def GraniteModel_fast_forward_inference(
     hidden_states = self.model.embed_tokens(input_ids)
     hidden_states = hidden_states.to(self.config.torch_dtype)
     hidden_states *= self.model.embedding_multiplier
+    residual_multiplier = \
+        self.residual_multiplier \
+        if hasattr(self, ""residual_multiplier"") else \
+        self.config.residual_multiplier
 
     bsz, q_len, hd = hidden_states.shape
     seq_len = past_key_values[0][0].shape[-2]
@@ -398,12 +410,12 @@ def GraniteModel_fast_forward_inference(
             position_embeddings = position_embeddings,
         )
 
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)
-        hidden_states = torch.add(residual, hidden_states, alpha = self.config.residual_multiplier)
+        hidden_states = torch.add(residual, hidden_states, alpha = residual_multiplier)
 
         next_decoder_cache.append(present_key_value)
     pass
@@ -421,6 +433,18 @@ class GraniteRotaryEmbedding(LlamaRotaryEmbedding):
     def __init__(self, config):
         super().__init__(config = config)
 
+def patched_init(original_init):
+    def new_init(self, *args, **kwargs):
+        # we can use self.residual_multiplier arg in GraniteDecoderLayer_fast_forward as mentioned here
+        # https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src/transformers/models/granite/modeling_granite.py#L243
+        # The problem is, we don't have access to either the value or config in GraniteModel_fast_forward_inference
+        # So we need a way to pass this value around. It is probably better to pass on entire config just in case we need it later
+        config = kwargs.get(""config"", args[0] if args else None)
+        if config is not None:
+            self.config = config
+        original_init(self, *args, **kwargs)
+    return new_init
+
 class FastGraniteModel(FastLlamaModel):
 
     @staticmethod
@@ -435,12 +459,13 @@ class FastGraniteModel(FastLlamaModel):
             exec(function, globals())
             GraniteAttention.__init__  = eval(init_name)
         pass
-        GraniteAttention      .forward = GraniteAttention_fast_forward
-        GraniteSdpaAttention  .forward = GraniteAttention_fast_forward
-        GraniteFlashAttention2.forward = GraniteAttention_fast_forward
-        GraniteDecoderLayer   .forward = GraniteDecoderLayer_fast_forward
-        GraniteModel          .forward = LlamaModel_fast_forward
-        GraniteForCausalLM    .forward = CausalLM_fast_forward(GraniteModel_fast_forward_inference)
+        GraniteAttention      .forward  = GraniteAttention_fast_forward
+        GraniteSdpaAttention  .forward  = GraniteAttention_fast_forward
+        GraniteFlashAttention2.forward  = GraniteAttention_fast_forward
+        GraniteDecoderLayer   .forward  = GraniteDecoderLayer_fast_forward
+        GraniteModel          .forward  = LlamaModel_fast_forward
+        GraniteForCausalLM    .forward  = CausalLM_fast_forward(GraniteModel_fast_forward_inference)
+        GraniteForCausalLM    .__init__ = patched_init(GraniteForCausalLM.__init__)
         PeftModelForCausalLM .forward = PeftModelForCausalLM_fast_forward
         fix_prepare_inputs_for_generation(GraniteForCausalLM)
 
@@ -452,7 +477,7 @@ class FastGraniteModel(FastLlamaModel):
 
 
     @staticmethod
-    def post_patch(model):
+    def post_patch(model, tokenizer):
 
         # Torch.compile fails on embedding matrix??
         # Workaround randomnly fixes it for torch versions < 2.2
@@ -517,7 +542,7 @@ class FastGraniteModel(FastLlamaModel):
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()
-        return model
+        return model, tokenizer
     pass
 pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index c945149..edd3ddf 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -20,6 +20,10 @@ from ._utils import *
 from ._utils import __version__
 from torch.nn.functional import scaled_dot_product_attention
 from transformers import __version__ as transformers_version
+from unsloth_zoo.utils import Version
+transformers_version = Version(transformers_version)
+# Transformers moved rotary embeddings out of all attention layers
+IS_ATTENTION_REFACTOR = transformers_version > Version(""4.47.1"")
 from transformers.models.llama.modeling_llama import (
     logger,
     BaseModelOutputWithPast,
@@ -146,12 +150,14 @@ def LlamaAttention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -168,10 +174,10 @@ def LlamaAttention_fast_forward_inference(
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         
         # Mistral Nemo 12b has weird dimensions
-        if attention_size != self.hidden_size:
-            self.temp_O = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        if attention_size != hidden_size:
+            self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         else:
-            self.temp_O = self.temp_QA[1][:,:,:self.hidden_size]
+            self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
         
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
@@ -356,9 +362,9 @@ def LlamaAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -786,7 +792,7 @@ def LlamaModel_fast_forward(
         pass
     pass
 
-    if transformers_version > ""4.47.1"" and hasattr(self, ""rotary_emb""):
+    if IS_ATTENTION_REFACTOR and not hasattr(self.layers[0].self_attn, ""rotary_emb""):
         # Transformers main has made it mandatory to pass position_embeddings
         # https://github.com/huggingface/transformers/pull/34858
         position_embeddings = self.rotary_emb(hidden_states, position_ids, self.config.max_position_embeddings)
@@ -996,18 +1002,20 @@ def CausalLM_fast_forward(fast_forward_inference):
         lm_head = self.lm_head.weight
         logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
         logit_scaling     = getattr(self.config, ""logit_scale"", 0)
+        dtype = lm_head.dtype
 
         if bsz == 1 and q_len == 1:
-            logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
+            logits = torch.mv(lm_head, hidden_states.ravel().to(dtype))
             logits = logits.unsqueeze(0).unsqueeze(0)
         elif num_logits_to_keep != 0:
-            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
+            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(dtype))
         else:
             RETURN_LOGITS = os.environ.get(""UNSLOTH_RETURN_LOGITS"", ""0"") == ""1""
             # < 1024 Normal Unsloth uses less VRAM!
             if bsz*q_len <= 1024: RETURN_LOGITS = True
             
             if not RETURN_LOGITS and HAS_CUT_CROSS_ENTROPY and labels is not None:
+
                 n_items = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None)
                 loss = fused_linear_cross_entropy(
                     hidden_states      = hidden_states,
@@ -1029,7 +1037,7 @@ def CausalLM_fast_forward(fast_forward_inference):
                 )
                 return output
             pass
-            logits = self.lm_head(hidden_states.to(lm_head.dtype))
+            logits = self.lm_head(hidden_states.to(dtype))
         pass
 
         torch_dtype = __DTYPE_MAP.get(self.config.torch_dtype, None)
@@ -1607,6 +1615,9 @@ class FastLlamaModel:
         elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:
             logger.warning_once(""Device does not support bfloat16. Will change to float16."")
             dtype = torch.float16
+        elif dtype == torch.float16 and SUPPORTS_BFLOAT16:
+            logger.warning_once(""Device supports bfloat16 but you selected float16. Will change to bfloat16."")
+            dtype = torch.bfloat16
 
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
@@ -1879,6 +1890,13 @@ class FastLlamaModel:
             internal_model = internal_model.model
         pass
         internal_model._saved_temp_tokenizer = tokenizer
+
+        # For transformers > 4.47.1, we need to add rotary_emb to all attention layers
+        if IS_ATTENTION_REFACTOR or hasattr(model.model, ""rotary_emb""):
+            rotary_emb = model.model.rotary_emb
+            for layer in model.model.layers:
+                layer.self_attn.rotary_emb = rotary_emb
+        pass
         
         return model, tokenizer
     pass
@@ -1967,29 +1985,41 @@ class FastLlamaModel:
                 if ""embed_tokens"" in new_target_modules:
                     print(""Unsloth: Training embed_tokens in mixed precision to save VRAM"")
 
-                    dtype = model.model.model.embed_tokens.modules_to_save.default.weight.dtype
-                    model.model.model.embed_tokens.modules_to_save.default\
-                        .to(device = ""cuda:0"", dtype=(dtype if (dtype != torch.float16) else torch.float32), non_blocking = True)
-                    model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
+                    new_dtype = model.get_input_embeddings().modules_to_save.default.weight.dtype
+                    if new_dtype == torch.float16:
+                        # See https://github.com/unslothai/unsloth/pull/1200
+                        # Tesla T4 must use float32 and not float16
+                        new_dtype = torch.float32
+                    pass
+
+                    model.get_input_embeddings().modules_to_save.default\
+                        .to(device = ""cuda:0"", dtype = new_dtype, non_blocking = True)
+                    model.get_input_embeddings().modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old embed_tokens to CPU - should be disk!
-                    model.model.model.embed_tokens.original_module\
+                    model.get_input_embeddings().original_module\
                         .to(device = ""cpu"", non_blocking = True)
-                    model.model.model.embed_tokens.original_module.requires_grad_(False)
+                    model.get_input_embeddings().original_module.requires_grad_(False)
                 pass
 
                 if ""lm_head"" in new_target_modules:
                     print(""Unsloth: Training lm_head in mixed precision to save VRAM"")
 
-                    dtype = model.model.model.lm_head.modules_to_save.default.weight.dtype
-                    model.model.lm_head.modules_to_save.default\
-                        .to(device = ""cuda:0"", dtype=(dtype if (dtype != torch.float16) else torch.float32), non_blocking = True)
-                    model.model.lm_head.modules_to_save.default.requires_grad_(True)
+                    new_dtype = model.get_output_embeddings().modules_to_save.default.weight.dtype
+                    if new_dtype == torch.float16:
+                        # See https://github.com/unslothai/unsloth/pull/1200
+                        # Tesla T4 must use float32 and not float16
+                        new_dtype = torch.float32
+                    pass
+
+                    model.get_output_embeddings().modules_to_save.default\
+                        .to(device = ""cuda:0"", dtype = new_dtype, non_blocking = True)
+                    model.get_output_embeddings().modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old lm_head to CPU - should be disk!
-                    model.model.lm_head.original_module\
+                    model.get_output_embeddings().original_module\
                         .to(device = ""cpu"", non_blocking = True)
-                    model.model.lm_head.original_module.requires_grad_(False)
+                    model.get_output_embeddings().original_module.requires_grad_(False)
                 pass
 
                 return model
@@ -2216,25 +2246,36 @@ class FastLlamaModel:
 
         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)
 
-        # Now patch lm_head and embed_tokens
         if train_embed_tokens:
             print(""Unsloth: Training embed_tokens in mixed precision to save VRAM"")
-            assert(hasattr(model.model.model.embed_tokens, ""modules_to_save""))
+            assert(hasattr(model.get_input_embeddings(), ""modules_to_save""))
 
-            dtype = model.model.model.embed_tokens.modules_to_save.default.weight.dtype
-            model.model.model.embed_tokens.modules_to_save.default\
-                .to(device = ""cuda:0"", dtype=(dtype if (dtype != torch.float16) else torch.float32), non_blocking = True)
-            model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
+            new_dtype = model.get_input_embeddings().modules_to_save.default.weight.dtype
+            if new_dtype == torch.float16:
+                # See https://github.com/unslothai/unsloth/pull/1200
+                # Tesla T4 must use float32 and not float16
+                new_dtype = torch.float32
+            pass
+
+            model.get_input_embeddings().modules_to_save.default\
+                .to(device = ""cuda:0"", dtype = new_dtype, non_blocking = True)
+            model.get_input_embeddings().modules_to_save.default.requires_grad_(True)
         pass
 
         if train_lm_head:
             print(""Unsloth: Training lm_head in mixed precision to save VRAM"")
-            assert(hasattr(model.model.lm_head, ""modules_to_save""))
+            assert(hasattr(model.get_output_embeddings(), ""modules_to_save""))
+
+            new_dtype = model.get_output_embeddings().modules_to_save.default.weight.dtype
+            if new_dtype == torch.float16:
+                # See https://github.com/unslothai/unsloth/pull/1200
+                # Tesla T4 must use float32 and not float16
+                new_dtype = torch.float32
+            pass
 
-            dtype = model.model.lm_head.modules_to_save.default.weight.dtype
-            model.model.lm_head.modules_to_save.default\
-                .to(device = ""cuda:0"", dtype=(dtype if (dtype != torch.float16) else torch.float32), non_blocking = True)
-            model.model.lm_head.modules_to_save.default.requires_grad_(True)
+            model.get_output_embeddings().modules_to_save.default\
+                .to(device = ""cuda:0"", dtype = new_dtype, non_blocking = True)
+            model.get_output_embeddings().modules_to_save.default.requires_grad_(True)
         pass
 
         # Patch tokenizer to pad to the right
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 113c4fb..e9caad0 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -31,8 +31,17 @@ except:
 pass
 from huggingface_hub import HfFileSystem
 
+# [TODO] Move USE_MODELSCOPE to utils
+USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
+if USE_MODELSCOPE:
+    import importlib
+    if importlib.util.find_spec(""modelscope"") is None:
+        raise ImportError(f'You are using the modelscope hub, please install modelscope by `pip install modelscope -U`')
+    pass
+pass
+
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
-from unsloth_zoo.utils import Version
+from unsloth_zoo.utils import Version, _get_dtype
 transformers_version = Version(transformers_version)
 SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
 SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
@@ -47,28 +56,11 @@ if SUPPORTS_GEMMA2:
 pass
 import torch
 
-def _get_dtype(dtype):
-    __DTYPE_MAP = {
-        ""float32"": torch.float32,
-        torch.float32: torch.float32,
-        ""float16"": torch.float16,
-        torch.float16: torch.float16,
-        ""bfloat16"": torch.bfloat16,
-        torch.bfloat16: torch.bfloat16,
-    }
-    if   dtype is None or dtype == None: return None
-    elif dtype in __DTYPE_MAP: return __DTYPE_MAP[dtype]
-    else:
-        print(f""Unsloth: {dtype} is not recognized, so we'll default to None"")
-        return None
-    pass
-pass
-
 
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
-        model_name                 = ""unsloth/llama-3-8b-bnb-4bit"",
+        model_name                 = ""unsloth/Llama-3.2-1B-Instruct"",
         max_seq_length             = None,
         dtype                      = None,
         load_in_4bit               = True,
@@ -80,12 +72,19 @@ class FastLanguageModel(FastLlamaModel):
         use_gradient_checkpointing = ""unsloth"",
         resize_model_vocab         = None,
         revision                   = None,
+        use_exact_model_name       = False,
         *args, **kwargs,
     ):
         if token is None: token = get_token()
         
         old_model_name = model_name
-        model_name = get_model_name(model_name, load_in_4bit)
+        if not use_exact_model_name:
+            model_name = get_model_name(model_name, load_in_4bit)
+
+        if USE_MODELSCOPE and not os.path.exists(model_name):
+            from modelscope import snapshot_download
+            model_name = snapshot_download(model_name)
+        pass
 
         # First check if it's a normal model via AutoConfig
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
@@ -165,7 +164,9 @@ class FastLanguageModel(FastLlamaModel):
         # Get base model for PEFT:
         if is_peft:
             # Check base model again for PEFT
-            model_name = get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
+            model_name = peft_config.base_model_name_or_path
+            if not use_exact_model_name:
+                model_name = get_model_name(model_name, load_in_4bit)
             model_config = AutoConfig.from_pretrained(
                 model_name,
                 token = token,
@@ -354,6 +355,7 @@ class FastVisionModel(FastBaseVisionModel):
         revision                   = None,
         return_logits              = False, # Return logits
         fullgraph                  = True, # No graph breaks
+        use_exact_model_name       = False,
         *args, **kwargs,
     ):
         if token is None: token = get_token()
@@ -361,10 +363,16 @@ class FastVisionModel(FastBaseVisionModel):
         patch_compiled_autograd()
         patch_compiling_bitsandbytes()
         if use_gradient_checkpointing == ""unsloth"":
-            patch_unsloth_smart_gradient_checkpointing()
+            patch_unsloth_smart_gradient_checkpointing(dtype = dtype)
         
         old_model_name = model_name
-        model_name = get_model_name(model_name, load_in_4bit)
+        if not use_exact_model_name:
+            model_name = get_model_name(model_name, load_in_4bit)
+
+        if USE_MODELSCOPE and not os.path.exists(model_name):
+            from modelscope import snapshot_download
+            model_name = snapshot_download(model_name)
+        pass
 
         # First check if it's a normal model via AutoConfig
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
@@ -410,7 +418,7 @@ class FastVisionModel(FastBaseVisionModel):
                 exist_config         = os.path.exists(os.path.join(model_name, ""config.json""))
                 both_exist = exist_adapter_config and exist_config
             else:
-                files = HfFileSystem(token = token).glob(os.path.join(model_name, ""*.json""))
+                files = HfFileSystem(token = token).glob(f""{model_name}/*.json"")
                 files = (os.path.split(x)[-1] for x in files)
                 if sum(x == ""adapter_config.json"" or x == ""config.json"" for x in files) >= 2:
                     both_exist = True
@@ -443,7 +451,10 @@ class FastVisionModel(FastBaseVisionModel):
         # Get base model for PEFT:
         if is_peft:
             # Check base model again for PEFT
-            model_name = get_model_name(peft_config.base_model_name_or_path, load_in_4bit)
+            model_name = peft_config.base_model_name_or_path
+            if not use_exact_model_name:
+                model_name = get_model_name(model_name, load_in_4bit)
+            
             model_config = AutoConfig.from_pretrained(
                 model_name,
                 token = token,
@@ -454,7 +465,10 @@ class FastVisionModel(FastBaseVisionModel):
 
         if not was_disabled: enable_progress_bars()
 
-        with contextlib.redirect_stdout(open(os.devnull, ""w"")):
+        do_logging = os.environ.get(""UNSLOTH_ENABLE_LOGGING"", ""0"") == ""1""
+        redirector = sys.stdout if do_logging else open(os.devnull, ""w"")
+
+        with contextlib.redirect_stdout(redirector):
             patch_loss_functions(torch_compile = False)
             model_types = unsloth_compile_transformers(
                 model_name              = model_name,
@@ -470,7 +484,7 @@ class FastVisionModel(FastBaseVisionModel):
                 fuse_lm_head            = True,
                 gradient_checkpointing  = True,
                 manual_replacements     = True,
-                fast_lora_forwards      = False,
+                fast_lora_forwards      = True,
                 fast_residual_stream    = False,
                 accurate_accumulation   = True,
                 epilogue_fusion         = True,
@@ -484,6 +498,7 @@ class FastVisionModel(FastBaseVisionModel):
                 return_logits           = return_logits,
             )
         pass
+        if do_logging: redirector.close()
 
         # Check if this is local model since the tokenizer gets overwritten
         if  os.path.exists(os.path.join(old_model_name, ""tokenizer_config.json"")) and \
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index d6c6946..9a97015 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -64,9 +64,9 @@ def MistralAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -278,16 +278,16 @@ pass
 # Transformers had to update for Mistral Nemo 12b since Attention is (5120, 4096) now.
 def patch_mistral_nemo_attention(function):
     function = function.replace(
-        ""(self.head_dim * self.num_heads) != self.hidden_size"",
+        ""(self.head_dim * self.config.num_attention_heads) != self.config.hidden_size"",
         ""False"",
     )
     function = function.replace(
-        ""self.head_dim = self.hidden_size // self.num_heads"",
+        ""self.head_dim = self.config.hidden_size // self.config.num_attention_heads"",
         ""self.head_dim = config.head_dim"",
     )
     function = function.replace(
-        ""self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)"",
-        ""self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)"",
+        ""self.o_proj = nn.Linear(self.config.hidden_size, self.config.hidden_size, bias=False)"",
+        ""self.o_proj = nn.Linear(self.config.num_attention_heads * self.head_dim, self.config.hidden_size, bias=False)"",
     )
     return function
 pass
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 2dc4b88..51450aa 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -30,6 +30,7 @@ from transformers import set_seed as transformers_set_seed
 from unsloth_zoo.peft_utils import (
     get_peft_regex,
     SKIP_QUANTIZATION_MODULES,
+    requires_grad_for_gradient_checkpointing,
 )
 from triton import __version__ as triton_version
 
@@ -275,6 +276,8 @@ class FastBaseVisionModel:
             use_gradient_checkpointing = use_gradient_checkpointing,
         )
         model = get_peft_model(model, lora_config)
+        # Enable gradients on modules which are trainable
+        requires_grad_for_gradient_checkpointing(model)
 
         model = FastBaseVisionModel.patch_peft_model(model, use_gradient_checkpointing)
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 70b0322..2bceca5 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ huggingface = [
     ""datasets"",
     ""sentencepiece"",
     ""accelerate"",
-    ""trl==0.7.7"",
+    ""trl"",
     ""peft"",
     ""packaging"",
     ""ninja"",
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index f8f5967..5f48f31 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -22,7 +22,7 @@ def get_lora_parameters(proj):
     base_layer = (proj.base_layer if hasattr(proj, ""base_layer"") else proj)
     W = base_layer.weight
 
-    if proj.disable_adapters or proj.merged:
+    if not hasattr(proj, ""disable_adapters"") or proj.disable_adapters or proj.merged:
         return W, QUANT_STATE(W), None, None, None
     pass
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 36ad645..c88ade6 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -116,6 +116,11 @@ def patch_tokenizer(model, tokenizer):
 pass
 
 
+IGNORED_TOKENIZER_CHECKING = frozenset((
+    ""CodeLlamaTokenizerFast"",
+    ""CodeLlamaTokenizer"",
+))
+
 def check_tokenizer(
     model,
     tokenizer,
@@ -131,6 +136,11 @@ def check_tokenizer(
     # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25
     # Seems like the Fast tokenizer in Rust breaks things!
 
+    # We ignore some of them!
+    if tokenizer.__repr__().split(""("", 1)[0] in IGNORED_TOKENIZER_CHECKING:
+        return tokenizer
+    pass
+
     max_embedding_size = model.model.embed_tokens.weight.shape[0]
     added_tokens_fast = tokenizer.added_tokens_decoder
     added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 73c78ba..f370c5b 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -714,6 +714,16 @@ class FastLlamaModel:
                 token = token,
             )
         pass
+
+        # Fix up config for transformers uploading PEFT
+        name = model.config._name_or_path
+        if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+            name = name[:len(name) - len(""-bnb-4bit"")]
+            model.config.update({""_name_or_path"" : name})
+        pass
+        # Log Unsloth version for future fastpaths for inference
+        model.config.update({""unsloth_version"" : __version__})
+
         return model, tokenizer
     pass
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index e48a982..01b9dae 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -343,6 +343,16 @@ class FastMistralModel(FastLlamaModel):
                 token = token,
             )
         pass
+
+        # Fix up config for transformers uploading PEFT
+        name = model.config._name_or_path
+        if name.startswith(""unsloth/"") and name.endswith(""-bnb-4bit""):
+            name = name[:len(name) - len(""-bnb-4bit"")]
+            model.config.update({""_name_or_path"" : name})
+        pass
+        # Log Unsloth version for future fastpaths for inference
+        model.config.update({""unsloth_version"" : __version__})
+        
         return model, tokenizer
     pass
 pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index e60881f..282aa99 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -248,7 +248,7 @@ model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"", ""granite""
 for model_name in model_architectures:
     config_filepath = f""transformers.models.{model_name}.configuration_{model_name}""
     model_filepath = f""transformers.models.{model_name}.modeling_{model_name}""
-    config_filename = f""{model_name.title()}Config""
+    config_filename = f""{model_name.title().replace('_','')}Config"" # qwen3 arch folder is qwen3_moe but config is Qwen3Config. Need to remove underscore(_) for now
     exec(f""from {config_filepath} import {config_filename}"", globals())
 
     try:
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 722b50d..9c218d3 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2487,6 +2487,8 @@ class FastLlamaModel:
         elif model_type == ""gemma2"":  apply_lora_mlp = apply_lora_mlp_geglu_approx
         elif model_type == ""cohere"":  apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""granite"": apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""qwen3"":   apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""qwen3moe"":   apply_lora_mlp = apply_lora_mlp_swiglu
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
         pass
diff --git a/unsloth/models/qwen3.py b/unsloth/models/qwen3.py
index aa99aa8..9d658cf 100644
--- a/unsloth/models/qwen3.py
+++ b/unsloth/models/qwen3.py
@@ -216,7 +216,7 @@ class FastQwen3Model(FastLlamaModel):
         # https://github.com/huggingface/transformers/pull/27931
         # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
         import transformers.models.qwen3.modeling_qwen3
-        transformers.models.Qwen3.modeling_qwen3.Qwen3RotaryEmbedding = LlamaRotaryEmbedding
+        transformers.models.qwen3.modeling_qwen3.Qwen3RotaryEmbedding = LlamaRotaryEmbedding
         return
     pass
 
"
"diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index 99db55c..fb3fc5c 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -20,5 +20,5 @@ from .qwen3     import FastQwen3Model
 from .qwen3_moe import FastQwen3MoeModel
 from .granite   import FastGraniteModel
 from .dpo       import PatchDPOTrainer, PatchKTOTrainer
-from ._utils    import is_bfloat16_supported, __version__
-from .rl        import PatchFastRL, vLLMSamplingParams
+from ._utils import is_bfloat16_supported, is_vLLM_available, __version__
+from .rl        import PatchFastRL, vLLMSamplingParams
\ No newline at end of file
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 82a1d02..4b9e2cc 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -17,6 +17,7 @@ __version__ = ""2025.5.1""
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
     ""is_bfloat16_supported"",
+    ""is_vLLM_available"",
 
     ""prepare_model_for_kbit_training"",
     ""xformers"",
@@ -800,6 +801,9 @@ def is_bfloat16_supported():
     return SUPPORTS_BFLOAT16
 pass
 
+def is_vLLM_available():
+    return _is_package_available(""vllm"")
+pass
 
 # Patches models to add RoPE Scaling
 def patch_linear_scaling(
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2145571..207540a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1661,9 +1661,8 @@ class FastLlamaModel:
             )
         pass
         if fast_inference:
-            import platform
-            if platform.system().lower() == 'windows':
-                print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
+            if not is_vLLM_available():
+                print(""Unsloth: vLLM is not installed! Will use Unsloth inference!"")
                 fast_inference = False
             major_version, minor_version = torch.cuda.get_device_capability()
             if major_version < 7:
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 7e90447..c93c5d2 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -14,6 +14,7 @@
 
 from ._utils import (
     is_bfloat16_supported,
+    is_vLLM_available,
     HAS_FLASH_ATTENTION,
     HAS_FLASH_ATTENTION_SOFTCAPPING,
     USE_MODELSCOPE,
@@ -351,9 +352,8 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         if fast_inference:
-            import platform
-            if platform.system().lower() == 'windows':
-                print(""Unsloth: vLLM does not work in Windows! Will use Unsloth inference!"")
+            if not is_vLLM_available():
+                print(""Unsloth: vLLM is not installed! Will use Unsloth inference!"")
                 fast_inference = False
             pass
             from unsloth_zoo.vllm_utils import (
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index fdaf7b4..f3fef87 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -55,18 +55,14 @@ if ""HF_HUB_ENABLE_HF_TRANSFER"" not in os.environ:
     os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 pass
 
-# XET is slower in Colab - investigate why
-keynames = ""\n"" + ""\n"".join(os.environ.keys())
-if ""HF_XET_HIGH_PERFORMANCE"" not in os.environ:
-    os.environ[""HF_XET_HIGH_PERFORMANCE""] = ""1""
-pass
-# Disable XET cache sine it eats too much space
-if ""HF_XET_CHUNK_CACHE_SIZE_BYTES"" not in os.environ:
-    os.environ[""HF_XET_CHUNK_CACHE_SIZE_BYTES""] = ""0""
-pass
-if ""\nCOLAB_"" in keynames:
-    os.environ[""HF_XET_RECONSTRUCT_WRITE_SEQUENTIALLY""] = ""0""
-pass
+# Disable XET Cache for now
+os.environ[""HF_XET_HIGH_PERFORMANCE""] = ""1""
+os.environ[""HF_XET_CHUNK_CACHE_SIZE_BYTES""] = ""0""
+os.environ[""HF_XET_RECONSTRUCT_WRITE_SEQUENTIALLY""] = ""0""
+os.environ[""HF_XET_NUM_CONCURRENT_RANGE_GETS""] = ""64""
+# More verbose HF Hub info
+if os.environ.get(""UNSLOTH_ENABLE_LOGGING"", ""0"") == ""1"":
+    os.environ[""HF_HUB_VERBOSITY""] = ""info""
 
 # Log Unsloth is being used
 os.environ[""UNSLOTH_IS_PRESENT""] = ""1""
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 602f7ee..76eefc3 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -183,6 +183,8 @@ except:
 try:
     from transformers.generation.utils import logger as transformers_generation_utils_logger
     transformers_generation_utils_logger.addFilter(HideLoggingMessage(""Setting `pad_token_id` to `eos_token_id`""))
+    # ""You have set `compile_config`
+    transformers_generation_utils_logger.addFilter(HideLoggingMessage(""compile_config""))
     del transformers_generation_utils_logger
 except:
     pass
"
"diff --git a/pyproject.toml b/pyproject.toml
index 51c3037..49347c8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -36,7 +36,7 @@ huggingface = [
     ""unsloth_zoo>=2024.11.8"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.46.1"",
+    ""transformers>=4.46.1,<=4.46.3"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -247,7 +247,7 @@ colab-new = [
     ""unsloth_zoo>=2024.11.8"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.46.1"",
+    ""transformers>=4.46.1,<=4.46.3"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3e67b4e..f8fb7d9 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1054,7 +1054,6 @@ def CausalLM_fast_forward(fast_forward_inference):
                 self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
             pass
             shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
-            print(kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None))
             loss = fast_cross_entropy_loss(
                 logits = shift_logits,
                 labels = shift_labels,
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index bbeded9..d460432 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -48,9 +48,11 @@ pass
 # And optimize pinning of memory
 os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = \
     ""expandable_segments:True,""\
-    ""roundup_power2_divisions:[32:256,64:128,256:64,>:32],""\
-    ""pinned_use_cuda_host_register:True,""\
-    ""pinned_num_register_threads:8""
+    ""roundup_power2_divisions:[32:256,64:128,256:64,>:32]""
+
+# [TODO] Check why some GPUs don't work
+#    ""pinned_use_cuda_host_register:True,""\
+#    ""pinned_num_register_threads:8""
 
 # Hugging Face Hub faster downloads
 if ""HF_HUB_ENABLE_HF_TRANSFER"" not in os.environ:
"
"diff --git a/README.md b/README.md
index 05977ba..c666f2d 100644
--- a/README.md
+++ b/README.md
@@ -22,7 +22,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 
 | Unsloth supports | Free Notebooks | Performance | Memory use |
 |-----------|---------|--------|----------|
-| **Llama 3 (8B)**      | [ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
+| **Llama 3.1 (8B)**      | [ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |
 | **Mistral Nemo (12B)** | [ Start for free](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)               | 2x faster | 60% less |
 | **Gemma 2 (9B)**      | [ Start for free](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)               | 2x faster | 63% less |
 | **Phi-3 (mini)** | [ Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |
@@ -32,13 +32,14 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 | **DPO Zephyr**     | [ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |
 | **TinyLlama**  | [ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |
 
-- **Kaggle Notebooks** for [Llama 3 (8B)](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
+- **Kaggle Notebooks** for [Llama 3.1 (8B)](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
 - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral v0.3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
 - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text
 - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language
 - Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.
 
 ##  Unsloth.ai News
+-  NEW! [Llama 3.1 8b, 70b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) both Base and Instruct now supported
 -  NEW! [Mistral Nemo-12b](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing) both Base and Instruct now supported
 -  NEW! [Gemma-2-9b](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing) and Gemma-2-27b now supported
 -  UPDATE! [Phi-3 mini](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) model updated. [Phi-3 Medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) 2x faster finetuning.
diff --git a/pyproject.toml b/pyproject.toml
index 29b3557..829b35a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -35,7 +35,7 @@ exclude = [""images*""]
 huggingface = [
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.42.3"",
+    ""transformers>=4.43.1"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -188,7 +188,7 @@ colab-ampere-torch220 = [
 colab-new = [
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.42.3"",
+    ""transformers>=4.43.1"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 466a5fe..b021e89 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -33,6 +33,7 @@ __all__ = [
     ""unsloth_offloaded_gradient_checkpoint"",
     ""torch_compile_options"",
     ""patch_linear_scaling"",
+    ""patch_llama_rope_scaling"",
     ""check_nvidia"",
     ""create_boolean_mask"",
     ""torch_amp_custom_fwd"",
@@ -332,7 +333,13 @@ def patch_tokenizer(model, tokenizer):
         Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!
         Fixes https://github.com/unslothai/unsloth/issues/5
     """"""
-    possible_reserved_tokens = (""<|reserved"", ""<|placeholder"", ""[control"")
+    possible_reserved_tokens = (
+        ""<|reserved"",                # Llama-3
+        ""<|placeholder"",             # Phi-3
+        ""[control"",                  # Forgot where lol
+        ""<pad>"",                     # Mistral Nemo
+        ""<|finetune_right_pad_id|>"", # Llama-3.1
+    )
 
     if model is not None:
         model.config.update({""unsloth_version"" : __version__})
@@ -745,7 +752,7 @@ def patch_linear_scaling(
     fix_rope_function = """"""
     if getattr(self.config, ""rope_scaling"", None) is None:
         self.rotary_emb = {rope_function}(
-            self.head_dim,
+            dim = self.head_dim,
             max_position_embeddings=self.max_position_embeddings,
             base=self.rope_theta,
         )
@@ -754,7 +761,7 @@ def patch_linear_scaling(
         scaling_factor = self.config.rope_scaling[""factor""]
         if scaling_type == ""linear"":
             self.rotary_emb = {scaled_rope_function}(
-                self.head_dim,
+                dim = self.head_dim,
                 max_position_embeddings=self.max_position_embeddings,
                 scaling_factor=scaling_factor,
                 base=self.rope_theta,
@@ -779,6 +786,91 @@ def patch_linear_scaling(
 pass
 
 
+# Patches for Llama-3 LlamaExtendedRotaryEmbedding
+def patch_llama_rope_scaling(
+    model_name = ""llama"",
+    rope_module = None,
+    scaled_rope_module = None,
+    extended_rope_module = None,
+    attention_module = None,
+):
+    assert(\
+        rope_module is not None and \
+        scaled_rope_module is not None and \
+        extended_rope_module is not None
+    )
+    assert(attention_module is not None)
+
+    rope_name = rope_module.__name__
+    scaled_rope_name = scaled_rope_module.__name__
+    model_filepath = f""transformers.models.{model_name}.modeling_{model_name}""
+    exec_code = \
+        f""import torch.nn as nn\n""\
+        f""from typing import Union, Optional, List, Any, Callable, Tuple\n""\
+        f""from {model_filepath} import logger, ""\
+        f""{model_name.title()}Attention, {model_name.title()}Config""
+
+    try:
+        function = inspect.getsource(attention_module.__init__)
+    except:
+        # Most likely already patched!
+        return None, None
+    where = function.find(""def"")
+    function = function.split(""\n"")
+    function = ""\n"".join(x[where:] for x in function)
+    init_name = f""{model_name.title()}Attention__init__""
+    function = function.replace(""def __init__"", f""def {init_name}"")
+    function = function.replace(
+        ""super().__init__()"",
+        f""super({model_name.title()}Attention, self).__init__()"",
+    )
+    fix_rope_function = """"""
+    if getattr(self.config, ""rope_scaling"", None) is None:
+        self.rotary_emb = {rope_function}(
+            dim = self.head_dim,
+            max_position_embeddings=self.max_position_embeddings,
+            base=self.rope_theta,
+        )
+    else:
+        scaling_type1 = self.config.rope_scaling.get(""type"", None)
+        scaling_type2 = self.config.rope_scaling.get(""rope_type"", None)
+        scaling_type = scaling_type1 if scaling_type1 is not None else scaling_type2
+        scaling_factor = self.config.rope_scaling.get(""factor"")
+
+        if scaling_type == ""linear"":
+            self.rotary_emb = {scaled_rope_function}(
+                dim = self.head_dim,
+                max_position_embeddings=self.max_position_embeddings,
+                scaling_factor=scaling_factor,
+                base=self.rope_theta,
+            )
+        elif scaling_type == ""llama3"":
+            self.rotary_emb = {extended_rope_function}(
+                dim = self.head_dim,
+                max_position_embeddings=self.max_position_embeddings,
+                base=self.rope_theta,
+            )
+        else:
+            raise ValueError(f""Unknown RoPE scaling type {{scaling_type}}"")
+    pass
+    """"""
+    fix_rope_function = fix_rope_function.format(
+        rope_function          = rope_module.__name__,
+        scaled_rope_function   = scaled_rope_module.__name__,
+        extended_rope_function = extended_rope_module.__name__,
+    )
+    rotary_emb = re.findall(
+        ""self.rotary_emb = .+?\)"", function,
+        flags = re.DOTALL | re.MULTILINE,
+    )
+    if len(rotary_emb) == 0: return None, function
+    rotary_emb = rotary_emb[0]
+    function = function.replace(rotary_emb, fix_rope_function, 1)
+    function = exec_code + ""\n\n"" + function
+    return init_name, function
+pass
+
+
 def check_nvidia():
     # Unsloth doesn't work yet on AMD devices - we're working on it!
     output = np.array([0,])
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index ce89ad3..e3f1e61 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -205,8 +205,11 @@ class GemmaFixedRotaryEmbedding(torch.nn.Module):
     # Fixes https://github.com/huggingface/transformers/pull/28837
     # https://github.com/microsoft/DeepSpeed/issues/4932
     # The precision of RoPE buffers is not correct, so we cast to int64.
-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
+    def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None,
+        config = None, # [TODO] Hack to pass in config - need to remove later
+    ):
         super().__init__()
+        if config is not None: return # [TODO] Hack to pass in config - need to remove later
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
@@ -264,9 +267,11 @@ class GemmaFixedLinearScalingRotaryEmbedding(GemmaFixedRotaryEmbedding):
     # Fixes https://github.com/huggingface/transformers/pull/28837
     # https://github.com/microsoft/DeepSpeed/issues/4932
     # The precision of RoPE buffers is not correct, so we cast to int64.
-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):
+    def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0,
+        config = None, # [TODO] Hack to pass in config - need to remove later
+    ):
         self.scaling_factor = scaling_factor
-        super().__init__(dim, max_position_embeddings, base, device)
+        super().__init__(dim = dim, max_position_embeddings = max_position_embeddings, base = base, device = device, config = config)
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index ff51b90..338ae0a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -977,8 +977,19 @@ class LlamaRotaryEmbedding(torch.nn.Module):
     # Fixes https://github.com/huggingface/transformers/pull/28837
     # https://github.com/microsoft/DeepSpeed/issues/4932
     # The precision of RoPE buffers is not correct, so we cast to int64.
-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
+    def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None,
+        config = None, # [TODO] Hack to pass in config - need to remove later
+    ):
         super().__init__()
+        if config is not None:
+            # [TODO] Hack to pass in config - need to remove later
+            base = config.rope_theta
+            partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
+            dim = int((config.hidden_size // config.num_attention_heads))
+            device = ""cuda""
+            max_position_embeddings = config.max_position_embeddings
+        pass
+
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
@@ -1030,9 +1041,11 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
     # Fixes https://github.com/huggingface/transformers/pull/28837
     # https://github.com/microsoft/DeepSpeed/issues/4932
     # The precision of RoPE buffers is not correct, so we cast to int64.
-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):
+    def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0,
+        config = None, # [TODO] Hack to pass in config - need to remove later
+    ):
         self.scaling_factor = scaling_factor
-        super().__init__(dim, max_position_embeddings, base, device)
+        super().__init__(dim = dim, max_position_embeddings = max_position_embeddings, base = base, device = device, config = config)
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
@@ -1052,6 +1065,99 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
 pass
 
 
+# See https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/rotary_embedding.py#L736
+# For Llama 3.1
+class LlamaExtendedRotaryEmbedding(torch.nn.Module):
+    def __init__(self, dim = None, max_position_embeddings=2048, base=10000, device=None,
+        config = None, # [TODO] Hack to pass in config - need to remove later
+    ):
+        super().__init__()
+        if config is not None:
+            # [TODO] Hack to pass in config - need to remove later
+            base = config.rope_theta
+            partial_rotary_factor = config.partial_rotary_factor if hasattr(config, ""partial_rotary_factor"") else 1.0
+            dim = int((config.hidden_size // config.num_attention_heads))
+            device = ""cuda""
+            max_position_embeddings = config.max_position_embeddings
+        pass
+
+        self.dim = dim
+        self.max_position_embeddings = max_position_embeddings
+        self.base = base
+        # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
+        self.current_rope_size = min(4 * 8192, self.max_position_embeddings)
+
+        # Normal Llama-3 RoPE
+        inv_freq = 1.0 / (
+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=""cpu"").float() / self.dim)
+        )
+        inv_freq = self.apply_scaling(inv_freq)
+        self.register_buffer(""inv_freq"", inv_freq, persistent = False)
+
+        # Build here to make `torch.jit.trace` work.
+        self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
+    pass
+
+    def _set_cos_sin_cache(self, seq_len, device, dtype):
+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
+        # in FP32. They are applied (multiplied) in FP32 as well.
+        self.current_rope_size = seq_len
+        
+        t = torch.arange(self.current_rope_size, device=""cpu"", dtype=torch.int64).float()
+
+        freqs = torch.outer(t, self.inv_freq)
+        # Different from paper, but it uses a different permutation in order to obtain the same calculation
+        emb = torch.cat((freqs, freqs), dim=-1)
+        self.register_buffer(""cos_cached"", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+        self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
+    pass
+
+    # From https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/api/model.py#L41
+    def apply_scaling(self, freqs: torch.Tensor):
+        # Values obtained from grid search
+        scale_factor = 8
+        low_freq_factor = 1
+        high_freq_factor = 4
+        old_context_len = 8192  # original llama3 length
+
+        low_freq_wavelen = old_context_len / low_freq_factor
+        high_freq_wavelen = old_context_len / high_freq_factor
+        new_freqs = []
+        for freq in freqs:
+            wavelen = 2 * math.pi / freq
+            if wavelen < high_freq_wavelen:
+                new_freqs.append(freq)
+            elif wavelen > low_freq_wavelen:
+                new_freqs.append(freq / scale_factor)
+            else:
+                assert low_freq_wavelen != high_freq_wavelen
+                smooth = (old_context_len / wavelen - low_freq_factor) / (
+                    high_freq_factor - low_freq_factor
+                )
+                new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)
+        return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)
+    pass
+
+    def forward(self, x, position_ids=None, seq_len=None):
+        # x: [bs, num_attention_heads, seq_len, head_size]
+        if seq_len > self.current_rope_size:
+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
+
+        return (
+            self.cos_cached[:seq_len].to(dtype = x.dtype),
+            self.sin_cached[:seq_len].to(dtype = x.dtype),
+        )
+    pass
+
+    def extend_rope_embedding(self, x, seq_len):
+        if seq_len <= self.current_rope_size: return
+        # Iteratively grow by increments of 8192
+        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
+    pass
+pass
+
+
 def _wrap_fast_inference(generate, device_type, dtype, model):
     # Wraps inference with bfloat16 / float16
     @torch.inference_mode
@@ -1108,6 +1214,17 @@ class FastLlamaModel:
 
     @staticmethod
     def pre_patch():
+        init_name, function = patch_llama_rope_scaling(
+            model_name           = ""llama"",
+            rope_module          = LlamaRotaryEmbedding,
+            scaled_rope_module   = LlamaLinearScalingRotaryEmbedding,
+            extended_rope_module = LlamaExtendedRotaryEmbedding,
+            attention_module     = LlamaAttention,
+        )
+        if init_name is not None:
+            exec(function, globals())
+            LlamaAttention.__init__  = eval(init_name)
+        pass
         LlamaAttention      .forward = LlamaAttention_fast_forward
         LlamaSdpaAttention  .forward = LlamaAttention_fast_forward
         LlamaFlashAttention2.forward = LlamaAttention_fast_forward
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 0f17059..ece8af2 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -27,6 +27,7 @@ transformers_version = Version(transformers_version)
 SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
 SUPPORTS_GEMMA   = transformers_version >= Version(""4.38"")
 SUPPORTS_GEMMA2  = transformers_version >= Version(""4.42"")
+SUPPORTS_LLAMA31 = transformers_version >= Version(""4.43.1"")
 if SUPPORTS_GEMMA:
     from .gemma  import FastGemmaModel
 if SUPPORTS_GEMMA2:
@@ -130,7 +131,19 @@ class FastLanguageModel(FastLlamaModel):
 
         model_type = model_config.model_type
 
-        if   model_type == ""llama"":   dispatch_model = FastLlamaModel
+        if   model_type == ""llama"":
+            scaling_type1 = model_config.rope_scaling.get(""type"", None)
+            scaling_type2 = model_config.rope_scaling.get(""rope_type"", None)
+            scaling_type = scaling_type1 if scaling_type1 is not None else scaling_type2
+
+            if scaling_type == ""llama3"" and not SUPPORTS_LLAMA31:
+                raise ImportError(
+                    f""Unsloth: Your transformers version of {transformers_version} does not support Llama 3.1.\n""\
+                    f""The minimum required version is 4.43.1\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.43.1""`\n'\
+                    f""to obtain the latest transformers build, then restart this session.""\
+                )
+            dispatch_model = FastLlamaModel
         elif model_type == ""mistral"": dispatch_model = FastMistralModel
         elif model_type == ""gemma"":
             if not SUPPORTS_GEMMA:
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 38cbdbe..fc13c94 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -218,6 +218,22 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/Mistral-Nemo-Base-2407"",
         ""mistralai/Mistral-Nemo-Base-2407"",
     ),
+    ""unsloth/Meta-Llama-3.1-8B-bnb-4bit"" : (
+        ""unsloth/Meta-Llama-3.1-8B"",
+        ""meta-llama/Meta-Llama-3.1-8B"",
+    ),
+    ""unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit"" : (
+        ""unsloth/Meta-Llama-3.1-8B-Instruct"",
+        ""meta-llama/Meta-Llama-3.1-8B-Instruct"",
+    ),
+    ""unsloth/Meta-Llama-3.1-70B-bnb-4bit"" : (
+        ""unsloth/Meta-Llama-3.1-70B"",
+        ""meta-llama/Meta-Llama-3.1-70B"",
+    ),
+    ""unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit"" : (
+        ""unsloth/Meta-Llama-3.1-70B-Instruct"",
+        ""meta-llama/Meta-Llama-3.1-70B-Instruct"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
"
"diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index ae01469..45b8ca6 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -168,7 +168,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         trainer = eval(f""trl.trainer.{trainer_file}"")
     except Exception as error:
         return
-    
+
     # Get SFTTrainer and SFTConfig names
     name   = [x for x in dir(trainer) if x.endswith(""Trainer"") and x != ""Trainer"" and trainer_file.split(""_"")[0] in x.lower()]
     config = [x for x in dir(trainer) if x.endswith(""Config"")  and x != ""Config""  and trainer_file.split(""_"")[0] in x.lower()]
@@ -484,7 +484,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""dataloader_persistent_workers"" : True, # Keeps dataloader in RAM
         ""dataloader_prefetch_factor""    : 2,
         ""dataloader_pin_memory""         : True,
-        ""dataloader_num_workers""        : 0, # Default is 0 means 1
+        ""dataloader_num_workers""        : 1,
     }
     for k, v in replacements.items():
         x = f""{k}( = [^,\n]{{1,}})?,\n""
@@ -565,7 +565,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     pass
 
     # Check GRPO num_generations mismatch
-    if ""per_device_train_batch_size"" in call_args and ""num_generations"" in call_args: 
+    if ""per_device_train_batch_size"" in call_args and ""num_generations"" in call_args:
         check_num_generations = \
         ""if (per_device_train_batch_size // num_generations) * num_generations != per_device_train_batch_size:\n""\
         ""    print('Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\\n""\
@@ -576,7 +576,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     pass
 
     # Check temperature must not be <= 0. Also stop if >= 10
-    if ""temperature"" in call_args: 
+    if ""temperature"" in call_args:
         check_temperature = \
         ""if temperature <= 0:\n""\
         ""    raise MathError('Unsloth: Please set a positive non-zero temperature since your results will be wrong.')\n""\
@@ -625,7 +625,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     if ""SamplingParams"" in old_RLTrainer_source:
         RL_pre = RL_pre + ""\n"" + inspect.getsource(vLLMSamplingParams)
     pass
-    
+
     # Selective log softmax
     selective_log_softmax_code = inspect.getsource(selective_log_softmax)
 
@@ -651,12 +651,12 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
 
         selective_log_softmax_code = selective_log_softmax_code,
     )
-    
+
     if RLTrainer_name == ""SFTTrainer"":
         original_text = 'self._signature_columns = [""input_ids"", ""attention_mask"", ""completion_mask""]'
         new_text = 'self._signature_columns = [""input_ids"", ""attention_mask"", ""completion_mask"",""labels""]'
         RLTrainer_source = RLTrainer_source.replace(original_text, new_text)
-        
+
     # Remove multiple doc strings
     if __RLConfig_doc__ != """" and RLTrainer_source.count(__RLTrainer_doc__) == 2:
         RLTrainer_source = RLTrainer_source.replace(__RLTrainer_doc__, """", 1)
@@ -673,12 +673,12 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         imports,
         overwrite = False,
     )
-    
+
     # Patch Trainer
     exec(f""trl.{RLTrainer_name} = created_module.Unsloth{RLTrainer_name}"", locals(), globals())
     exec(f""trl.trainer.{RLTrainer_name} = created_module.Unsloth{RLTrainer_name}"", locals(), globals())
     exec(f""trl.trainer.{trainer_file}.{RLTrainer_name} = created_module.Unsloth{RLTrainer_name}"", locals(), globals())
-    
+
     # Patch Config
     exec(f""trl.{RLConfig_name} = created_module.Unsloth{RLConfig_name}"", locals(), globals())
     exec(f""trl.trainer.{RLConfig_name} = created_module.Unsloth{RLConfig_name}"", locals(), globals())
@@ -754,7 +754,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             new_vllm_part,
             flags = re.MULTILINE | re.DOTALL,
         )
-        
+
         if len(sampling_params) == 1:
             sampling_params = sampling_params[0]
             # Fix guided_decoding
@@ -768,7 +768,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             sampling_params = \
                 "" ""*12 + ""self.llm = model.vllm_engine; self._last_loaded_step = 0; "" + \
                 sampling_params # Add spaces
-            
+
             # count the indentation of last line of sampling_params.
             last_line = sampling_params.split(""\n"")[-1]
             last_prev_line = sampling_params.split(""\n"")[-2]
@@ -844,7 +844,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             r"""",
             source,
         )
-        
+
         # Replace self.llm.generate and self.llm.chat
         lora_name = trainer_file + ""_lora_model""
         source = re.sub(
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index 5652e15..107b553 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -888,11 +888,14 @@ def install_llama_cpp_old(version = -10):
         os.path.exists(""llama.cpp/llama-quantize.exe"") or
         os.path.exists(""llama.cpp/llama-quantize"") or
         os.path.exists(""llama.cpp/quantize.exe"") or
-        os.path.exists(""llama.cpp/quantize"")
+        os.path.exists(""llama.cpp/quantize"") or
+        os.path.exists(""llama.cpp/build/bin/llama-quantize"") or
+        os.path.exists(""llama.cpp/build/bin/quantize"")
     ):
         raise RuntimeError(
             ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
-            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
+            ""We've also double checked the building directory under 'llama.cpp/build/bin/'.\n""\
+            ""But we expect this file to exist! Check if the file exists under llama.cpp and investigate the building process of llama.cpp (make/cmake)!""
         )
     pass
 pass
@@ -1082,10 +1085,15 @@ def save_to_gguf(
             quantize_location = ""llama.cpp/llama-quantize.exe""
         elif os.path.exists(""llama.cpp/llama-quantize""):
             quantize_location = ""llama.cpp/llama-quantize""
+        elif os.path.exists(""llama.cpp/build/bin/llama-quantize""):
+            quantize_location = ""llama.cpp/build/bin/llama-quantize""
+        elif os.path.exists(""llama.cpp/build/bin/quantize""):
+            quantize_location = ""llama.cpp/build/bin/quantize""
         else:
             raise RuntimeError(
-                ""Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\n""\
-                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
+                ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
+                ""We've also double checked the building directory under 'llama.cpp/build/bin/'.\n""\
+                ""But we expect this file to exist! Check if the file exists under llama.cpp and investigate the building process of llama.cpp (make/cmake)!""
             )
         pass
 
"
"diff --git a/pyproject.toml b/pyproject.toml
index 96aa069..14797c8 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -39,7 +39,7 @@ triton = [
     ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2025.2.6"",
+    ""unsloth_zoo>=2025.2.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -50,7 +50,7 @@ huggingface = [
     ""wheel>=0.42.0"",
     ""numpy"",
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
@@ -348,7 +348,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.2.6"",
+    ""unsloth_zoo>=2025.2.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -366,7 +366,7 @@ colab-new = [
 ]
 colab-no-deps = [
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
     ""peft>=0.7.1"",
     ""xformers"",
     ""bitsandbytes>=0.46.1"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 3820245..e1259af 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -143,6 +143,11 @@ transformers_training_args_logger.addFilter(HideLoggingMessage(""The speedups""))
 transformers_training_args_logger.addFilter(HideLoggingMessage(""torch.distributed""))
 del transformers_training_args_logger
 
+# No label_names provided for model class
+from transformers.trainer import logger as transformers_trainer_logger
+transformers_trainer_logger.addFilter(HideLoggingMessage(""No label_names""))
+del transformers_trainer_logger
+
 # Using the default loss: `ForCausalLMLoss`.
 try:
     from transformers.modeling_utils import logger as transformers_modeling_utils_logger
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 909dfc3..3e0717a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -725,8 +725,9 @@ def LlamaModel_fast_forward(
             past_key_values_length,
             sliding_window = getattr(self.config, ""sliding_window"", None),
         )
-        if attention_mask is not None:
-            attention_mask = attention_mask.to(torch.bool)
+        # Must NOT convert to bool - weirdly this causes stuff to error out!
+        # if attention_mask is not None:
+        #     attention_mask = attention_mask.to(torch.bool)
     pass
 
     hidden_states = inputs_embeds
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 23b3117..06ae821 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -164,10 +164,11 @@ RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__prepare_inputs)
 # Remove _move_model_to_vllm
 def grpo_trainer__move_model_to_vllm(function_name, function):
     if  function_name != ""_move_model_to_vllm"": return function
+    
+    def _move_model_to_vllm(self, *args, **kwargs): return None
 
-    # .*? matches first match. .+? matches final match.
-    replacement = ""def _move_model_to_vllm(self, *args, **kwargs): return None\n""
-    return "" ""*function.find(""def"") + replacement
+    function = inspect.getsource(_move_model_to_vllm)
+    return function
 pass
 RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__move_model_to_vllm)
 
"
"diff --git a/README.md b/README.md
index 412e3ff..fc28260 100644
--- a/README.md
+++ b/README.md
@@ -91,13 +91,11 @@ Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA
 conda create --name unsloth_env python=3.10
 conda activate unsloth_env
 
-conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia
+conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers
 
-conda install xformers -c xformers
+pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""
 
-pip install bitsandbytes
-
-pip install ""unsloth[conda] @ git+https://github.com/unslothai/unsloth.git""
+pip install --no-deps trl peft accelerate bitsandbytes
 ```
 
 ### Pip Installation
@@ -144,6 +142,22 @@ pip install ""unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/u
 ```bash
 pip install --upgrade pip
 ```
+6. For Pytorch 2.2.1:
+```bash
+# RTX 3090, 4090 Ampere GPUs:
+pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""
+pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes
+
+# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:
+pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""
+pip install --no-deps xformers trl peft accelerate bitsandbytes
+```
+7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.
+```bash
+nvcc
+python -m xformers.info
+python -m bitsandbytes
+```
 
 ##  Documentation
 - Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 0cc76cd..40da487 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -18,9 +18,9 @@ import importlib
 # Currently only supports 1 GPU, or else seg faults will occur.
 if ""CUDA_VISIBLE_DEVICES"" in os.environ:
     devices = os.environ[""CUDA_VISIBLE_DEVICES""]
-    # check if there are multiple cuda devices set in env
+    # Check if there are multiple cuda devices set in env
     if not devices.isdigit():
-        first_id = devices.split(',')[0]
+        first_id = devices.split("","")[0]
         warnings.warn(
             f""Unsloth: 'CUDA_VISIBLE_DEVICES' is currently {devices} \n""\
             ""Multiple CUDA devices detected but we require a single device.\n""\
@@ -33,20 +33,29 @@ else:
     os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
 pass
 
+# Reduce VRAM usage by reducing fragmentation
+os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""expandable_segments:True""
+
 try:
     import torch
 except:
     raise ImportError(""Pytorch is not installed. Go to https://pytorch.org/.\n""\
                       ""We have some installation instructions on our Github page."")
 
-# We support torch 2.1 and 2.1.1
+# We support Pytorch 2
 # Fixes https://github.com/unslothai/unsloth/issues/38
 torch_version = torch.__version__.split(""."")
 major_torch, minor_torch = torch_version[0], torch_version[1]
 major_torch, minor_torch = int(major_torch), int(minor_torch)
-if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):
-    raise ImportError(""Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\n""\
+if (major_torch < 2):
+    raise ImportError(""Unsloth only supports Pytorch 2 for now. Please update your Pytorch to 2.1.\n""\
                       ""We have some installation instructions on our Github page."")
+elif (major_torch == 2) and (minor_torch < 2):
+    # Disable expandable_segments
+    del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
+    # Must reimport Pytorch!
+    importlib.reload(torch)
+pass
 
 
 # Try loading bitsandbytes and triton
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index 99a99dc..b32d75e 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -17,6 +17,7 @@ import triton.language as tl
 import torch
 from .utils import calculate_settings
 
+ROPE_GROUP_SIZE = 4
 
 @triton.heuristics({""BACKWARD_PASS"": lambda args: args[""BACKWARD_PASS""],})
 @triton.jit
@@ -24,9 +25,11 @@ def _rope_embedding(
     Q,     Q_row_stride,
     cos, cos_row_stride,
     sin, sin_row_stride,
-    seqlen, head_dim, group_size, n_heads,
-    BACKWARD_PASS: tl.constexpr,
-    BLOCK_SIZE : tl.constexpr,
+    seqlen,
+    head_dim      : tl.constexpr,
+    n_heads       : tl.constexpr,
+    BACKWARD_PASS : tl.constexpr,
+    BLOCK_SIZE    : tl.constexpr,
 ):
     """"""
         Calculates the RoPE Embedding quickly
@@ -49,16 +52,18 @@ def _rope_embedding(
         sin1 = -sin1
     pass
 
-    head_start = group_head_position * group_size
-    head_end = tl.math.min((head_start + group_size), n_heads)
+    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8
+    head_start = group_head_position * ROPE_GROUP_SIZE
+    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)
 
-    for i in range(head_start, head_end):
-        offs_q1 = row_position * Q_row_stride + i * head_dim + col_offsets
-        offs_q2 = row_position * Q_row_stride + i * head_dim + col_offsets + half_head_dim
+    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)
+    for k in range(head_start, head_end):
+        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets
+        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim
 
         # For Gemma - sometimes RoPE must be done in float32 and not bfloat16
-        Q1   = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)
-        Q2   = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)
+        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)
+        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)
 
         tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)
         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)
@@ -78,21 +83,24 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         # [TODO] Changing blocksize to head_dim//2 seems to have
         # some concurrency / un-deterministic issues.
         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)
-        group_size = 4 # 4 or 8, too large group_size can hurt performance.
-        n_groups = triton.cdiv(n_heads, group_size)
+        
+        # group_size = 4 # 4 or 8, too large group_size can hurt performance.
+        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)
+        n_groups = div + (mod != 0)
 
-        grid = (n_rows, n_groups, )
-        _rope_embedding[grid](
+        _rope_embedding[(n_rows, n_groups, )](
               Q,   Q.stride(0),
             cos, cos.stride(0),
             sin, sin.stride(0),
-            seq_len, head_dim, group_size, n_heads,
+            seq_len,
+            head_dim, n_heads,
             BACKWARD_PASS = False,
             BLOCK_SIZE = BLOCK_SIZE,
             num_warps  = num_warps,
         )
         ctx.BLOCK_SIZE = BLOCK_SIZE
         ctx.num_warps  = num_warps
+        ctx.n_groups = n_groups
         ctx.cos = cos
         ctx.sin = sin
         return Q.view(batch, seq_len, n_heads, head_dim)
@@ -108,15 +116,11 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         cos = ctx.cos
         sin = ctx.sin
 
-        group_size = 4 # 4 or 8, too large group_size can hurt performance.
-        n_groups = triton.cdiv(n_heads, group_size)
-
-        grid = (n_rows, n_groups, )
-        _rope_embedding[grid](
+        _rope_embedding[(n_rows, ctx.n_groups, )](
             dY,  dY .stride(0),
             cos, cos.stride(0),
             sin, sin.stride(0),
-            seq_len, head_dim, group_size, n_heads,
+            seq_len, head_dim, n_heads,
             BACKWARD_PASS = True,
             BLOCK_SIZE = ctx.BLOCK_SIZE,
             num_warps  = ctx.num_warps,
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 95a032b..02eea4b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -18,6 +18,8 @@ import warnings
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""huggingface_hub"")
 warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""subprocess"")
+warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""transformers"")
+warnings.filterwarnings(action = ""ignore"", category = FutureWarning, module = ""accelerate"")
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
 from transformers import AutoTokenizer
diff --git a/unsloth/save.py b/unsloth/save.py
index dd0dc26..05ff748 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -593,16 +593,17 @@ def install_llama_cpp_old(version = -10):
     pass
 
     # Clone a specific commit
+    # Also don't use the GPU!
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
         f""cd llama.cpp && git reset --hard {version} && git clean -df && ""\
-        f""make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}"",
+        f""make clean make all -j{psutil.cpu_count()*2}"",
         ""pip install gguf protobuf"",
     ]
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
             for line in sp.stdout:
-                print(line.decode(""utf-8""), flush = True, end = """")
+                print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         pass
     pass
     # Check if successful
@@ -625,12 +626,55 @@ def install_llama_cpp_blocking():
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
             for line in sp.stdout:
-                print(line.decode(""utf-8""), flush = True, end = """")
+                print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         pass
     pass
 pass
 
 
+def _fix_gemma_gguf():
+    # Fixes Gemma saving to GGUF to float32 instead of float16!
+    with open(""llama.cpp/convert-hf-to-gguf.py"", ""rb"") as file:
+        text = file.read()
+    pass
+
+    gemma_start = text.find(b""class GemmaModel(Model):"")
+    if gemma_start == -1: return
+
+    gemma_end   = text.find(b""self.gguf_writer.add_tensor(new_name, data)"", gemma_start)
+    if gemma_end == -1: return
+
+    gemma_text = text[gemma_start : gemma_end]
+    bad_text = \
+b""""""         data = data.astype(np.float32)
+
+            # if f16 desired, convert any float32 2-dim weight tensors to float16
+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith("".weight"") and n_dims == 2:
+                data = data.astype(np.float16)""""""
+    good_text = \
+b""""""         # if f32 desired, convert any float16 to float32
+            if self.ftype == 0 and data_dtype == np.float16:
+                data = data.astype(np.float32)
+
+            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32
+            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:
+                data = data.astype(np.float32)
+
+            # if f16 desired, convert any float32 2-dim weight tensors to float16
+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith("".weight"") and n_dims == 2:
+                data = data.astype(np.float16)""""""
+    find_bad = gemma_text.find(bad_text)
+    if find_bad == -1: return
+
+    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]
+    text = text[:gemma_start] + gemma_text + text[gemma_end:]
+
+    with open(""llama.cpp/convert-hf-to-gguf.py"", ""w+b"") as file:
+        file.write(text)
+    pass
+pass
+
+
 def save_to_gguf(
     model_type           : str,
     model_directory      : str = ""unsloth_finetuned_model"",
@@ -686,7 +730,10 @@ def save_to_gguf(
         install_llama_cpp_blocking()
     pass
     # Check if successful. If not install 10th latest release
-    if error != 0 or not os.path.exists(""llama.cpp/quantize""): install_llama_cpp_old(-10)
+    if error != 0 or not os.path.exists(""llama.cpp/quantize""):
+        print(f""Unsloth: llama.cpp error code = {error}."")
+        install_llama_cpp_old(-10)
+    pass
 
     if   quantization_method == ""f32"":  first_conversion = ""f32""
     elif quantization_method == ""f16"":  first_conversion = ""f16""
@@ -723,6 +770,9 @@ def save_to_gguf(
             f""--outfile {final_location} --vocab-type hfft ""\
             f""--outtype {first_conversion} --concurrency {n_cpus}""
     else:
+        # Need to fix convert-hf-to-gguf.py for some models!
+        _fix_gemma_gguf()
+
         command = f""python llama.cpp/convert-hf-to-gguf.py {model_directory} ""\
             f""--outfile {final_location} ""\
             f""--outtype {first_conversion}""
@@ -730,7 +780,7 @@ def save_to_gguf(
 
     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:
         for line in sp.stdout:
-            print(line.decode(""utf-8""), flush = True, end = """")
+            print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         if sp.returncode is not None and sp.returncode != 0:
             raise subprocess.CalledProcessError(sp.returncode, sp.args)
     pass
@@ -760,7 +810,7 @@ def save_to_gguf(
         # quantize uses stderr
         with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:
             for line in sp.stderr:
-                print(line.decode(""utf-8""), flush = True, end = """")
+                print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
             if sp.returncode is not None and sp.returncode != 0:
                 raise subprocess.CalledProcessError(sp.returncode, sp.args)
         pass
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index feb550b..0490484 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -60,12 +60,6 @@ except:
                       ""We have some installation instructions on our Github page."")
 pass
 
-# Fix up is_bf16_supported https://github.com/unslothai/unsloth/issues/504
-major_version, minor_version = torch.cuda.get_device_capability()
-SUPPORTS_BFLOAT16 = (major_version >= 8)
-def is_bf16_supported(): return SUPPORTS_BFLOAT16
-torch.cuda.is_bf16_supported = is_bf16_supported
-
 # We support Pytorch 2
 # Fixes https://github.com/unslothai/unsloth/issues/38
 torch_version = torch.__version__.split(""."")
@@ -79,6 +73,19 @@ elif (major_torch == 2) and (minor_torch < 2):
     del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
 pass
 
+# Torch 2.5 has including_emulation
+major_version, minor_version = torch.cuda.get_device_capability()
+SUPPORTS_BFLOAT16 = (major_version >= 8)
+
+if (major_torch == 2) and (minor_torch >= 5): 
+    old_is_bf16_supported = torch.cuda.is_bf16_supported
+    def is_bf16_supported(including_emulation = False):
+        return old_is_bf16_supported(including_emulation)
+    torch.cuda.is_bf16_supported = is_bf16_supported
+else:
+    def is_bf16_supported(): SUPPORTS_BFLOAT16
+    torch.cuda.is_bf16_supported = is_bf16_supported
+pass
 
 # Try loading bitsandbytes and triton
 import bitsandbytes as bnb
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index ebea02a..c2de979 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -17,24 +17,32 @@ from .rms_layernorm import fast_rms_layernorm
 from .rope_embedding import fast_rope_embedding, inplace_rope_embedding
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
 from .geglu import (
-	geglu_exact_forward_kernel,
-	geglu_exact_backward_kernel,
-	geglu_approx_forward_kernel,
-	geglu_approx_backward_kernel,
+    geglu_exact_forward_kernel,
+    geglu_exact_backward_kernel,
+    geglu_approx_forward_kernel,
+    geglu_approx_backward_kernel,
 )
 from .fast_lora import (
-	get_lora_parameters,
-	get_lora_parameters_bias,
-	apply_lora_mlp_swiglu,
-	apply_lora_mlp_geglu_exact,
-	apply_lora_mlp_geglu_approx,
-	apply_lora_qkv,
-	apply_lora_o,
+    get_lora_parameters,
+    get_lora_parameters_bias,
+    apply_lora_mlp_swiglu,
+    apply_lora_mlp_geglu_exact,
+    apply_lora_mlp_geglu_approx,
+    apply_lora_qkv,
+    apply_lora_o,
 )
 from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora
 
+from .flex_attention import HAS_FLEX_ATTENTION, slow_attention_softcapping
+
+if HAS_FLEX_ATTENTION:
+    from .flex_attention import (
+        FLEX_ATTENTION_PADDING,
+    )
+pass
+
 try:
-	print("" Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+    print("" Unsloth: Will patch your computer to enable 2x faster free finetuning."")
 except:
-	print(""Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+    print(""Unsloth: Will patch your computer to enable 2x faster free finetuning."")
 pass
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index dc1ad26..6074a51 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings, MAX_FUSED_SIZE
+from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh
 from transformers.models.llama.modeling_llama import logger
 
 
@@ -63,7 +63,7 @@ def _cross_entropy_forward(
     label_idx = tl.load(labels_ptr).to(tl.int32)
     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING: logits = SOFTCAP * tl.math.tanh(logits / SOFTCAP)
+    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
 
     logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
@@ -72,7 +72,7 @@ def _cross_entropy_forward(
     if label_idx != -100:
         x = tl.load(logits_ptr + label_idx)
         # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-        if DO_SOFTCAPPING: x = SOFTCAP * tl.math.tanh(x / SOFTCAP)
+        if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)
         loss = logsumexp - x.to(tl.float32)
     else:
         loss = 0.0
@@ -131,7 +131,7 @@ def _chunked_cross_entropy_forward(
     label_idx = tl.load(labels_ptr).to(tl.int32)
     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf""))
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING: logits = SOFTCAP * tl.math.tanh(logits / SOFTCAP)
+    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
 
     logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
@@ -143,7 +143,7 @@ def _chunked_cross_entropy_forward(
         if label_idx != -100:
             x = tl.load(logits_ptr + label_idx).to(tl.float32)
             # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-            if DO_SOFTCAPPING: x = SOFTCAP * tl.math.tanh(x / SOFTCAP)
+            if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)
             loss = -1.0 * x.to(tl.float32)
         else:
             loss = 0.0
@@ -198,7 +198,7 @@ def _cross_entropy_backward(
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
     if DO_SOFTCAPPING:
         # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
-        partial = tl.math.tanh(x / SOFTCAP)
+        partial = triton_tanh(x / SOFTCAP)
         x = SOFTCAP * partial
     pass
 
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
new file mode 100644
index 0000000..1eb2486
--- /dev/null
+++ b/unsloth/kernels/flex_attention.py
@@ -0,0 +1,77 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import torch
+from functools import lru_cache
+from transformers.models.llama.modeling_llama import logger
+
+torch_compile_options = {
+    ""epilogue_fusion""   : True,
+    ""max_autotune""      : True,
+    ""shape_padding""     : True,
+    ""trace.enabled""     : False, # Output Triton kernel outputs!
+    ""triton.cudagraphs"" : False,
+}
+
+# Flex Attention supported from torch 2.5 onwards only
+import torch.nn.attention
+if hasattr(torch.nn.attention, ""flex_attention""):
+    import torch.nn.attention.flex_attention
+    from torch.nn.attention.flex_attention import flex_attention
+    from torch.nn.attention.flex_attention import create_block_mask
+    FLEX_ATTENTION_PADDING = getattr(
+        torch.nn.attention.flex_attention,
+        ""_DEFAULT_SPARSE_BLOCK_SIZE"",
+        1,
+    )
+    flex_attention = torch.compile(flex_attention, dynamic = False)
+    HAS_FLEX_ATTENTION = True
+else:
+    HAS_FLEX_ATTENTION = False
+pass
+
+# Logit softcapping
+@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
+def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
+    n_heads    = self.num_heads
+    head_dim   = self.head_dim
+    n_kv_heads = self.num_key_value_heads
+    n_groups   = self.num_key_value_groups
+    
+    # Grouped query attention
+    K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+    V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+    K = K.reshape(bsz, n_heads, q_len, head_dim)
+    V = V.reshape(bsz, n_heads, q_len, head_dim)
+
+    # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
+    # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
+    # We default to using the config file itself
+    # s = self.config.hidden_size // self.config.num_attention_heads
+    s = self.config.query_pre_attn_scalar
+    t = self.config.attn_logit_softcapping
+
+    Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
+    A = torch.matmul(Q, K.transpose(2, 3))
+    A = t * torch.tanh(A / t) # Logit softcapping
+    A += causal_mask[:q_len, :q_len]
+    # Much slower in torch compile!
+    # A.masked_fill_(causal_mask[:q_len, :q_len], -float(""inf""))
+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
+    A = torch.matmul(A, V)
+    A = A.transpose(1, 2).contiguous()
+    A = A.reshape(bsz, q_len, n_heads*head_dim)
+    return A
+pass
+
diff --git a/unsloth/kernels/geglu.py b/unsloth/kernels/geglu.py
index 006e8c0..9fedae7 100644
--- a/unsloth/kernels/geglu.py
+++ b/unsloth/kernels/geglu.py
@@ -15,7 +15,7 @@
 import triton
 import triton.language as tl
 import torch
-from .utils import calculate_settings
+from .utils import calculate_settings, triton_tanh
 
 
 @triton.jit
@@ -119,7 +119,7 @@ def _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
     g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)
 
     f_row = 0.5 * e_row * (
-        tl.math.tanh(s * e_row * (1.0 + 0.044715 * e_row * e_row)) \
+        triton_tanh(s * e_row * (1.0 + 0.044715 * e_row * e_row)) \
         + 1.0
     )
     f_row = f_row.to(g_row.dtype) # Exact copy from HF
@@ -168,7 +168,7 @@ def _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
     s = 0.7978845608028654 # math.sqrt(2 / math.pi)
     a = s * e_row # a = sqrt(2 / pi) * x
     b = a * 0.044715 * e_row * e_row # b = a * 0.044715 * x^2
-    T = 1.0 + tl.math.tanh(a + b)
+    T = 1.0 + triton_tanh(a + b)
     T2 = 0.5 * T
     # Q = 0.5 * -T * (T - 2.0) * (a + 3.0 * b)
     Q2 = -T2 * (T - 2.0) * (a + 3.0 * b) 
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index b32d75e..2934ac4 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -16,7 +16,6 @@ import triton
 import triton.language as tl
 import torch
 from .utils import calculate_settings
-
 ROPE_GROUP_SIZE = 4
 
 @triton.heuristics({""BACKWARD_PASS"": lambda args: args[""BACKWARD_PASS""],})
@@ -36,6 +35,7 @@ def _rope_embedding(
         RoPE is Q * cos + rotate_half(Q) * sin
         See our blog post for more info
     """"""
+    ROPE_GROUP_SIZE = 4
     row_position  = tl.program_id(0)
     group_head_position = tl.program_id(1)
     col_offsets  = tl.arange(0, BLOCK_SIZE)
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 4b78900..23be372 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -28,6 +28,18 @@ else:
 pass
 
 
+# tl.math.tanh now is libdevice.tanh
+from packaging.version import Version
+import triton
+if Version(triton.__version__) >= Version(""3.0.0""):
+    from triton.language.extra import libdevice
+    triton_tanh = libdevice.tanh
+else:
+    import triton.language as tl
+    triton_tanh = tl.math.tanh
+pass
+
+
 def calculate_settings(n):
     BLOCK_SIZE = next_power_of_2(n)
     if BLOCK_SIZE > MAX_FUSED_SIZE:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index b224e85..025daec 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -145,7 +145,15 @@ from xformers import __version__ as xformers_version
 # Temporarily disable 0.0.27 and higher - inference issues
 if Version(xformers_version) >= Version(""0.0.27""):
     raise ImportError(
-        f""Unsloth: Your xformers version of {xformers_version} is too new.\n""\
+        ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
+        ""then press Disconnect Runtime and then Restart it.\n""\
+        ""\n""\
+        ""%%capture\n""
+        ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
+        '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
+        '!pip install --no-deps ""xformers<0.0.27"" ""trl<0.9.0"" peft accelerate bitsandbytes\n'\
+        '\n'\
+        f""Otherwise in local machines, your xformers version of {xformers_version} is too new.\n""\
         'Please downgrade xformers via `pip install --force-reinstall ""xformers<0.0.27""'
     )
 pass
@@ -154,7 +162,15 @@ pass
 from trl import __version__ as trl_version
 if Version(xformers_version) >= Version(""0.9.0""):
     raise ImportError(
-        f""Unsloth: Your TRL version of {trl_version} is too new.\n""\
+        ""Unsloth: If you are in Colab, we updated the top cell install instructions - please change it to below ""\
+        ""then press Disconnect Runtime and then Restart it.\n""\
+        ""\n""\
+        ""%%capture\n""
+        ""# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n""
+        '!pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""\n'
+        '!pip install --no-deps ""xformers<0.0.27"" ""trl<0.9.0"" peft accelerate bitsandbytes\n'\
+        '\n'\
+        f""Otherwise in local machines, your TRL version of {trl_version} is too new.\n""\
         'Please downgrade TRL via `pip install --force-reinstall ""trl<0.9.0""'
     )
 pass
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 9c055ff..0d21c47 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -70,52 +70,6 @@ def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
 pass
 
 
-# Flex Attention in torch 2.5 and higher
-# try:
-#     from torch.nn.attention._flex_attention import _flex_attention
-#     from functools import lru_cache
-#     @lru_cache
-#     def create_block_mask_from_score_mod(score_mod, B, H, M, N):
-#         SPARSE_BLOCK = 128
-#         block_mask = _create_block_mask(score_mod, B, H, M, N, device = ""cuda:0"")
-#         return block_mask
-
-
-# Logit softcapping
-@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
-def gemma2_attention(Q, K, V, causal_mask, self, bsz, q_len):
-    n_heads    = self.num_heads
-    head_dim   = self.head_dim
-    n_kv_heads = self.num_key_value_heads
-    n_groups   = self.num_key_value_groups
-    
-    # Grouped query attention
-    K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-    V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-    K = K.reshape(bsz, n_heads, q_len, head_dim)
-    V = V.reshape(bsz, n_heads, q_len, head_dim)
-
-    # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
-    # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
-    # We default to using the config file itself
-    # s = self.config.hidden_size // self.config.num_attention_heads
-    s = self.config.query_pre_attn_scalar
-    t = self.config.attn_logit_softcapping
-
-    Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
-    A = torch.matmul(Q, K.transpose(2, 3))
-    A = t * torch.tanh(A / t) # Logit softcapping
-    A += causal_mask[:q_len, :q_len]
-    # Much slower in torch compile!
-    # A.masked_fill_(causal_mask[:q_len, :q_len], -float(""inf""))
-    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
-    A = torch.matmul(A, V)
-    A = A.transpose(1, 2).contiguous()
-    A = A.reshape(bsz, q_len, n_heads*head_dim)
-    return A
-pass
-
-
 # Logit softcapping
 def Gemma2Attention_fast_forward(
     self,
@@ -172,8 +126,8 @@ def Gemma2Attention_fast_forward(
         V = torch.cat([past_key_value[1], V], dim = 2)
     pass
     past_key_value = (K, V) if use_cache else None
-
-    A = gemma2_attention(Q, K, V, causal_mask, self, bsz, kv_seq_len)
+    
+    A = slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, kv_seq_len)
     A = self.apply_o(self, A)
     return A, None, past_key_value
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2d888b8..9bea364 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -662,6 +662,12 @@ def LlamaModel_fast_forward(
             offloaded_gradient_checkpointing = True
     pass
 
+    # Check for Flex Attention
+    # if IS_GEMMA2 and HAS_FLEX_ATTENTION:
+    #     if not (seq_length % FLEX_ATTENTION_PADDING == 0):
+    #     USE_FLEX_ATTENTION = True
+
+
     # Gemma2 has alternating SWA and global attn
     if IS_GEMMA2 and not hasattr(self, ""SWA_mask""):
         n = self.config.max_position_embeddings
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 31b3ab6..38cbdbe 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -210,6 +210,14 @@ __INT_TO_FLOAT_MAPPER = \
     ""unsloth/Phi-3-mini-4k-instruct-v0-bnb-4bit"" : ( # Old Phi pre July
         ""unsloth/Phi-3-mini-4k-instruct-v0"",
     ),
+    ""unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"" : ( # New 12b Mistral models
+        ""unsloth/Mistral-Nemo-Instruct-2407"",
+        ""mistralai/Mistral-Nemo-Instruct-2407"",
+    ),
+    ""unsloth/Mistral-Nemo-Base-2407-bnb-4bit"" : ( # New 12b Mistral models
+        ""unsloth/Mistral-Nemo-Base-2407"",
+        ""mistralai/Mistral-Nemo-Base-2407"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
diff --git a/unsloth/save.py b/unsloth/save.py
index c8e791c..a5904ef 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -1359,6 +1359,20 @@ def upload_to_huggingface(
             uploaded_location = file_location
         pass
 
+        # find ftevent file from tensorboard and upload it
+        import glob
+        ftevent_files = glob.glob(""*out.tfevents*"", recursive = True)
+        if len(ftevent_files) > 0:
+            print(""Unsloth: Uploading tensorboard files... Please wait..."", file_location + ""*out.tfevents*"")
+            for ftevent_file in ftevent_files:
+                hf_api.upload_file(
+                    path_or_fileobj = ftevent_file,
+                    path_in_repo    = ftevent_file.replace(file_location, """"),
+                    repo_id         = save_directory,
+                    repo_type       = ""model"",
+                    commit_message  = ""(Trained with Unsloth)"",
+                )
+
         hf_api.upload_file(
             path_or_fileobj = file_location,
             path_in_repo    = uploaded_location,
"
"diff --git a/pyproject.toml b/pyproject.toml
index 506e9a7..c7368b2 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -36,9 +36,9 @@ huggingface = [
     ""transformers>=4.37.0"",
     ""datasets"",
     ""sentencepiece"",
-    ""accelerate"",
+    ""accelerate>=0.26.1"",
     ""trl>=0.7.9"",
-    ""peft"",
+    ""peft>=0.7.1"",
     ""tqdm"",
     ""psutil"",
 ]
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index b487ff9..b3a1098 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -90,6 +90,8 @@ class LoRA_MLP(torch.autograd.Function):
 
         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)
         g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)
+        # f = torch.nn.functional.silu(e)
+        # h = f * g
         h = swiglu_fg_kernel(e, g)
         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)
 
@@ -103,6 +105,7 @@ class LoRA_MLP(torch.autograd.Function):
         return i
     pass
 
+
     @staticmethod
     @torch.cuda.amp.custom_bwd
     def backward(ctx, dY : torch.Tensor):
@@ -121,11 +124,16 @@ class LoRA_MLP(torch.autograd.Function):
         g  = g .view(-1, g .shape[-1])
         dtype = X.dtype
 
-        # DW_f   = (D @ W.T * f)
-        # DW_dfg = (D @ W.T * df * g)
         DW = matmul_lora(dY, downW.t(), downW_quant, downB, downA, downS)
+        # e = e.float()
+        # se = 1.0 / (1.0 + torch.exp(-e))
+        # f = (se * e).to(dtype)
+        # h = f * g
+        # df = DW * f
+        # dg = DW * g
+        # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)
         DW, e, g = swiglu_DWf_DW_dfg_kernel(DW, e, g)
-        h, DW_f, DW_dfg = DW, e, g
+        h, df, de = DW, e, g
 
         # Down projection LoRA weights
         d_downA = h.t() @ (dY @ downB.t())
@@ -134,31 +142,29 @@ class LoRA_MLP(torch.autograd.Function):
         d_downB *= downS
 
         # Up projection LoRA weights
-        d_upA   = X.t() @ (DW_f @ upB.t())
-        d_upB   = (upA.t() @ X.t()) @ DW_f
+        d_upA   = X.t() @ (df @ upB.t())
+        d_upB   = (upA.t() @ X.t()) @ df
         d_upA  *= upS
         d_upB  *= upS
 
         # Gate projection LoRA weights
-        d_gateA = X.t() @ (DW_dfg @ gateB.t())
-        d_gateB = (gateA.t() @ X.t()) @ DW_dfg
+        d_gateA = X.t() @ (de @ gateB.t())
+        d_gateB = (gateA.t() @ X.t()) @ de
         d_gateA *= gateS
         d_gateB *= gateS
 
-        # Final derivatives to backpropagate backwards.
-        # See our blogpost for more details.
-        # (D @ W.T * f) @ U.T
+        # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)
+        # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)
+
         upW = fast_dequantize(upW.t(), upW_quant)
-        # (D @ W.T * f) @ (U.T + B.T @ A.T)
-        dX = torch.matmul(DW_f, upW.t(), out = X)
+        dX = torch.matmul(df, upW.t(), out = X)
         del upW
-        dX += DW_f @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())
+        dX += df @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())
 
-        # And add the derivative for the gate projection
         gateW = fast_dequantize(gateW.t(), gateW_quant)
-        dX += DW_dfg @ gateW.t()
+        dX += de @ gateW.t()
         del gateW
-        dX += DW_dfg @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())
+        dX += de @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())
 
         # gateW, gateW_quant, gateA, gateB, gateS,
         #  upW,    upW_quant,   upA,   upB,   upS,
@@ -172,6 +178,11 @@ pass
 
 
 def apply_lora_mlp(self, X):
+    # gate = self.gate_proj(X)
+    # up   = self.  up_proj(X)
+    # h = torch.nn.functional.silu(gate) * up
+    # down = self.down_proj(h)
+    # return down
     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
diff --git a/unsloth/kernels/swiglu.py b/unsloth/kernels/swiglu.py
index 4e9b7ba..ff6b162 100644
--- a/unsloth/kernels/swiglu.py
+++ b/unsloth/kernels/swiglu.py
@@ -28,7 +28,7 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
     g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)
 
     # f = e * sigmoid(e)
-    f_row = e_row / (1 + tl.exp(-e_row))
+    f_row = e_row * tl.sigmoid(e_row) # e_row / (1 + tl.exp(-e_row))
     f_row = f_row.to(g_row.dtype) # Exact copy from HF
     # h = f * g
     h_row = f_row * g_row
@@ -50,30 +50,43 @@ pass
 
 @triton.jit
 def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
+    """"""
+    e = e.float()
+    se = 1.0 / (1.0 + torch.exp(-e))
+    f = (se * e).to(dtype)
+    h = f * g
+    df = DW * f
+    dg = DW * g
+    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)
+    """"""
     block_idx = tl.program_id(0)
     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
     mask = offsets < n_elements
 
     DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)
-    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)
+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)
     g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)
 
-    # f = e * sigmoid(e)
-    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))
-    se_row = se_row.to(e_row.dtype) # Exact copy from HF
-    # f = e * se
-    f_row = e_row * se_row
+    # e = e.float()
+    # se = 1.0 / (1.0 + torch.exp(-e))
+    se_row = tl.sigmoid(e_row) # 1.0 / (1.0 + tl.exp(-e_row))
+    # f = (se * e).to(dtype)
+    f_row = se_row * e_row
+    f_row = f_row.to(DW_row.dtype)
     # h = f * g
-    h_row = f_row * g_row
-    # DW_f = DW * f
-    DWf_row = DW_row * f_row
-    # DW_dfg = DW * (se*(g - h) + h)
-    DW_dfg_row = DW_row * (se_row*(g_row - h_row) + h_row)
+    h_row  =  f_row * g_row
+    # df = DW * f
+    df_row = DW_row * f_row
+    # dg = DW * g
+    dg_row = DW_row * g_row
+    # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)
+    de_row = dg_row.to(tl.float32) * se_row * (1.0 + e_row * (1.0 - se_row))
+    de_row = de_row.to(DW_row.dtype)
 
     # Store derivatives in buffers
-    tl.store(DW + offsets, h_row,      mask = mask)
-    tl.store(e  + offsets, DWf_row,    mask = mask)
-    tl.store(g  + offsets, DW_dfg_row, mask = mask)
+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g
+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f
+    tl.store(g  + offsets, de_row, mask = mask) # de
 pass
 
 
diff --git a/unsloth/save.py b/unsloth/save.py
index 6c44d23..471897c 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -14,6 +14,7 @@
 
 from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
 from peft.tuners.lora import Linear4bit as Peft_Linear4bit
+from peft.tuners.lora import Linear as Peft_Linear
 from typing import Optional, Callable, Union, List
 import torch
 import os
@@ -72,11 +73,15 @@ pass
 
 
 def _merge_lora(layer, name):
-    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):
+
+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):
         # Is LoRA so we need to merge!
         W, quant_state, A, B, s = get_lora_parameters(layer)
-        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]
-        W = fast_dequantize(W, quant_state).to(torch.float32).t()
+        if quant_state is not None:
+            dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]
+            W = fast_dequantize(W, quant_state)
+        pass
+        W = W.to(torch.float32).t()
 
         if A is not None:
             sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))
@@ -84,7 +89,6 @@ def _merge_lora(layer, name):
             if not torch.isfinite(W).all():
                 raise ValueError(f""Unsloth: Merge failed.\n{name} has some elements = infinity."")
         pass
-        
         W = W.t().to(dtype)
     else:
         W = layer.weight
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 40da487..7d271ed 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -53,8 +53,6 @@ if (major_torch < 2):
 elif (major_torch == 2) and (minor_torch < 2):
     # Disable expandable_segments
     del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
-    # Must reimport Pytorch!
-    importlib.reload(torch)
 pass
 
 
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index b2fbf0d..0780527 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1548,7 +1548,7 @@ def unsloth_fast_generate(
         if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
             if kwargs[""input_ids""].shape[-1] + kwargs[""max_new_tokens""] > self.config.max_position_embeddings:
                 raise ValueError(
-                    f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {model.config.max_position_embeddings}!\n'\
+                    f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {self.config.max_position_embeddings}!\n'\
                     'You will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`.'
                 )
     pass
"
"diff --git a/unsloth-cli.py b/unsloth-cli.py
index b7613f9..86f0207 100644
--- a/unsloth-cli.py
+++ b/unsloth-cli.py
@@ -182,7 +182,7 @@ if __name__ == ""__main__"":
     lora_group = parser.add_argument_group("" LoRA Options"", ""These options are used to configure the LoRA model."")
     lora_group.add_argument('--r', type=int, default=16, help=""Rank for Lora model, default is 16.  (common values: 8, 16, 32, 64, 128)"")
     lora_group.add_argument('--lora_alpha', type=int, default=16, help=""LoRA alpha parameter, default is 16. (common values: 8, 16, 32, 64, 128)"")
-    lora_group.add_argument('--lora_dropout', type=float, default=0, help=""LoRA dropout rate, default is 0.0 which is optimized."")
+    lora_group.add_argument('--lora_dropout', type=float, default=0.0, help=""LoRA dropout rate, default is 0.0 which is optimized."")
     lora_group.add_argument('--bias', type=str, default=""none"", help=""Bias setting for LoRA"")
     lora_group.add_argument('--use_gradient_checkpointing', type=str, default=""unsloth"", help=""Use gradient checkpointing"")
     lora_group.add_argument('--random_state', type=int, default=3407, help=""Random state for reproducibility, default is 3407."")
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 0c6f6c1..3551c6c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -2220,7 +2220,7 @@ class FastLlamaModel:
         target_modules      = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
                                ""gate_proj"", ""up_proj"", ""down_proj""],
         lora_alpha          = 16,
-        lora_dropout        = 0,
+        lora_dropout        = 0.0,
         bias                = ""none"",
         layers_to_transform = None,
         layers_pattern      = None,
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 9899d60..7442f07 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -554,7 +554,7 @@ class FastBaseModel:
         r                          = 16,
         target_modules             = None,
         lora_alpha                 = 16,
-        lora_dropout               = 0,
+        lora_dropout               = 0.0,
         bias                       = ""none"",
         finetune_vision_layers     = True,
         finetune_language_layers   = True,
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 21ebbfd..c445e98 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.7.6""
+__version__ = ""2025.7.7""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -297,6 +297,12 @@ def patch_mistral_nemo_config(config):
     return config
 pass
 
+try:
+    # Some Config files use layer_type_validation
+    # for eg Gemma-2, so we must import it to stop errors.
+    from transformers.configuration_utils import layer_type_validation
+except:
+    pass
 from transformers import __version__ as transformers_version
 from transformers import PretrainedConfig
 model_architectures = [""llama"", ""mistral"", ""gemma"", ""gemma2"", ""qwen2"", ""granite"", ""qwen3"", ""qwen3_moe"", ""falcon_h1""]
"
"diff --git a/pyproject.toml b/pyproject.toml
index f8558a8..0462327 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -207,6 +207,16 @@ cu126onlytorch260 = [
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
+cu118onlytorch270 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
+]
 cu126onlytorch270 = [
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp39-cp39-manylinux_2_28_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
@@ -227,6 +237,30 @@ cu128onlytorch270 = [
     ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
     ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.30-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
+cu118onlytorch271 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl ; platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.31.post1-cp39-abi3-win_amd64.whl ; platform_system == 'Windows'"",
+]
+cu126onlytorch271 = [
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl ; platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.31.post1-cp39-abi3-win_amd64.whl ; platform_system == 'Windows'"",
+]
+cu128onlytorch271 = [
+    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl ; platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.31.post1-cp39-abi3-win_amd64.whl ; platform_system == 'Windows'"",
+]
+cu118onlytorch280 = [
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl ; platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.32.post2-cp39-abi3-win_amd64.whl ; platform_system == 'Windows'"",
+]
+cu126onlytorch280 = [
+    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl ; platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu128/xformers-0.0.32.post2-cp39-abi3-win_amd64.whl ; platform_system == 'Windows'"",
+]
+cu128onlytorch280 = [
+    ""xformers @ https://download.pytorch.org/whl/cu129/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl ; platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu129/xformers-0.0.32.post2-cp39-abi3-win_amd64.whl ; platform_system == 'Windows'"",
+]
 cu118 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.45.5"",
@@ -337,6 +371,11 @@ cu126-torch260 = [
     ""bitsandbytes>=0.45.5"",
     ""unsloth[cu126onlytorch260]"",
 ]
+cu118-torch270 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu118onlytorch270]"",
+]
 cu126-torch270 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.45.5"",
@@ -347,6 +386,36 @@ cu128-torch270 = [
     ""bitsandbytes>=0.45.5"",
     ""unsloth[cu128onlytorch270]"",
 ]
+cu118-torch271 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu118onlytorch271]"",
+]
+cu126-torch271 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu126onlytorch271]"",
+]
+cu128-torch271 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu128onlytorch271]"",
+]
+cu118-torch280 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu118onlytorch280]"",
+]
+cu126-torch280 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu126onlytorch280]"",
+]
+cu128-torch280 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu128onlytorch280]"",
+]
 kaggle = [
     ""unsloth[huggingface]"",
 ]
@@ -540,6 +609,12 @@ cu126-ampere-torch260 = [
     ""unsloth[cu126onlytorch260]"",
     ""unsloth[flashattention]"",
 ]
+cu118-ampere-torch270 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu118onlytorch270]"",
+    ""unsloth[flashattention]"",
+]
 cu126-ampere-torch270 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes>=0.45.5"",
@@ -552,7 +627,42 @@ cu128-ampere-torch270 = [
     ""unsloth[cu128onlytorch270]"",
     ""unsloth[flashattention]"",
 ]
-
+cu118-ampere-torch271 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu118onlytorch271]"",
+    ""unsloth[flashattention]"",
+]
+cu126-ampere-torch271 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu126onlytorch271]"",
+    ""unsloth[flashattention]"",
+]
+cu128-ampere-torch271 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu128onlytorch271]"",
+    ""unsloth[flashattention]"",
+]
+cu118-ampere-torch280 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu118onlytorch280]"",
+    ""unsloth[flashattention]"",
+]
+cu126-ampere-torch280 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu126onlytorch280]"",
+    ""unsloth[flashattention]"",
+]
+cu128-ampere-torch280 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes>=0.45.5"",
+    ""unsloth[cu128onlytorch280]"",
+    ""unsloth[flashattention]"",
+]
 flashattentiontorch260abiFALSEcu12x = [
     ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.9'"",
     ""flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; platform_system == 'Linux' and python_version == '3.10'"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 5d9ddbd..1055dfb 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -12,6 +12,31 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+try:
+    # Fix up AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
+    # MUST do this at the start primarily due to tensorflow causing issues
+    import google.protobuf.message_factory
+    class MessageFactory:
+        def CreatePrototype(self, *args, **kwargs): return
+        def GetMessages(self, *args, **kwargs): return
+        def GetPrototype(self, *args, **kwargs): return
+    if not hasattr(google.protobuf.message_factory, ""MessageFactory""):
+        google.protobuf.message_factory.MessageFactory = MessageFactory
+    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
+        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
+        not hasattr(google.protobuf.message_factory, ""GetMessageClass""):
+        google.protobuf.message_factory.MessageFactory = MessageFactory
+    elif hasattr(google.protobuf.message_factory, ""MessageFactory"") and \
+        not hasattr(google.protobuf.message_factory.MessageFactory, ""GetPrototype"") and \
+        hasattr(google.protobuf.message_factory, ""GetMessageClass""):
+        GetMessageClass = google.protobuf.message_factory.GetMessageClass
+        def GetPrototype(self, descriptor):
+            return GetMessageClass(descriptor)
+        google.protobuf.message_factory.MessageFactory.GetPrototype = GetPrototype
+    pass
+except:
+    pass
+
 import warnings, importlib, sys
 from packaging.version import Version
 import os, re, subprocess, inspect
diff --git a/unsloth/_auto_install.py b/unsloth/_auto_install.py
index c855939..27b23ed 100644
--- a/unsloth/_auto_install.py
+++ b/unsloth/_auto_install.py
@@ -30,7 +30,11 @@ elif v  < V('2.5.0'): x = 'cu{}{}-torch240'
 elif v  < V('2.5.1'): x = 'cu{}{}-torch250'
 elif v <= V('2.5.1'): x = 'cu{}{}-torch251'
 elif v  < V('2.7.0'): x = 'cu{}{}-torch260'
-elif v  < V('2.8.0'): x = 'cu{}{}-torch270'
+elif v  < V('2.7.9'): x = 'cu{}{}-torch270'
+elif v  < V('2.8.0'): x = 'cu{}{}-torch271'
+elif v  < V('2.8.9'): x = 'cu{}{}-torch280'
 else: raise RuntimeError(f""Torch = {v} too new!"")
+if v > V('2.6.9') and cuda not in (""11.8"", ""12.6"", ""12.8""):
+	raise RuntimeError(f""CUDA = {cuda} not supported!"")
 x = x.format(cuda.replace(""."", """"), ""-ampere"" if is_ampere else """")
 print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git""')
\ No newline at end of file
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 749becf..dd1798f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -273,6 +273,38 @@ try:
 except:
     pass
 
+# Using a slow image processor as `use_fast`
+try:
+    from transformers.processing_utils import logger as processing_utils_logger
+    processing_utils_logger.addFilter(HideLoggingMessage(""`use_fast`""))
+    del processing_utils_logger
+except:
+    pass
+
+# Using a slow image processor as `use_fast`
+try:
+    from transformers.models.auto.image_processing_auto import logger as processing_utils_logger
+    processing_utils_logger.addFilter(HideLoggingMessage(""`use_fast`""))
+    del processing_utils_logger
+except:
+    pass
+
+# `use_cache=True` is incompatible with gradient checkpointing
+try:
+    from transformers.trainer import logger as trainer_logger
+    trainer_logger.addFilter(HideLoggingMessage(""`use_cache=True`""))
+    del trainer_logger
+except:
+    pass
+
+# `use_cache=True` is incompatible with gradient checkpointing
+try:
+    from transformers.utils.generic import logger as trainer_logger
+    trainer_logger.addFilter(HideLoggingMessage(""`use_cache=True`""))
+    del trainer_logger
+except:
+    pass
+
 # Errors out on
 # Some weights of Gemma3nForConditionalGeneration were not initialized from the model checkpoint
 from transformers.modeling_utils import logger as transformers_logger
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 52b1e83..afa6b25 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -133,15 +133,18 @@ class Unsloth{RLConfig_name}({RLConfig_name}):
         default = -1,
         metadata = {{'help': 'Chunk size to reduce memory usage. -1 is most efficient.'}},
     )
+    {max_seq_length_pre}
     def __init__({RLConfig_arguments},
         vllm_sampling_params = None,
         unsloth_num_chunks = -1,
+        {max_seq_length_call}
         **kwargs,
     ):
 {RLConfig_extra_args}
         super().__init__({RLConfig_call_args}{RLConfig_kwargs})
         self.vllm_sampling_params = vllm_sampling_params
         self.unsloth_num_chunks = unsloth_num_chunks
+        {max_seq_length_post}
 pass
 
 {RLTrainer_extras}
@@ -353,9 +356,7 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
             ""            max_length = args.max_length\n""\
             ""    else:\n""\
             ""        model_max_length = getattr(model, 'max_seq_length', None)\n""\
-            ""        # print(model_max_length, 'mml1')\n""\
             ""        if model_max_length is None: model_max_length = getattr(model, 'max_length', None)\n""\
-            ""        # print(model_max_length, 'mml2')\n""\
             ""        if model_max_length is not None:\n""\
             ""            args.max_length = model_max_length\n""\
             ""            max_length = args.max_length\n""\
@@ -535,6 +536,21 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         extra_args += learning_rate_check
     pass
 
+    # Check if max_seq_length is NOT defined (max_length is now default)
+    if ""max_seq_length"" not in call_args and ""max_length"" in call_args:
+        max_seq_length_pre = \
+            """"""max_seq_length : Optional[int] = field(
+        default = None,
+        metadata = {'help': 'Maximum sequence length to truncate to.'},
+    )""""""
+        max_seq_length_call = ""max_seq_length = max_seq_length,""
+        max_seq_length_post = ""self.max_seq_length = max_seq_length""
+    else:
+        max_seq_length_pre = """"
+        max_seq_length_call = """"
+        max_seq_length_post = """"
+    pass
+
     # Add output_dir saving
     if ""output_dir"" in call_args:
         # Default checks
@@ -666,6 +682,10 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         RLTrainer_post       = RLTrainer_post,
         RL_pre               = RL_pre,
 
+        max_seq_length_pre   = max_seq_length_pre,
+        max_seq_length_call  = max_seq_length_call,
+        max_seq_length_post  = max_seq_length_post,
+
         selective_log_softmax_code = selective_log_softmax_code,
     )
 
"
"diff --git a/CODE_OF_CONDUCT.md b/CODE_OF_CONDUCT.md
new file mode 100644
index 0000000..b23f315
--- /dev/null
+++ b/CODE_OF_CONDUCT.md
@@ -0,0 +1,133 @@
+
+# Contributor Covenant Code of Conduct
+
+## Our Pledge
+
+We as members, contributors, and leaders pledge to make participation in our
+community a harassment-free experience for everyone, regardless of age, body
+size, visible or invisible disability, ethnicity, sex characteristics, gender
+identity and expression, level of experience, education, socio-economic status,
+nationality, personal appearance, race, caste, color, religion, or sexual
+identity and orientation.
+
+We pledge to act and interact in ways that contribute to an open, welcoming,
+diverse, inclusive, and healthy community.
+
+## Our Standards
+
+Examples of behavior that contributes to a positive environment for our
+community include:
+
+* Demonstrating empathy and kindness toward other people
+* Being respectful of differing opinions, viewpoints, and experiences
+* Giving and gracefully accepting constructive feedback
+* Accepting responsibility and apologizing to those affected by our mistakes,
+  and learning from the experience
+* Focusing on what is best not just for us as individuals, but for the overall
+  community
+
+Examples of unacceptable behavior include:
+
+* The use of sexualized language or imagery, and sexual attention or advances of
+  any kind
+* Trolling, insulting or derogatory comments, and personal or political attacks
+* Public or private harassment
+* Publishing others' private information, such as a physical or email address,
+  without their explicit permission
+* Other conduct which could reasonably be considered inappropriate in a
+  professional setting
+
+## Enforcement Responsibilities
+
+Community leaders are responsible for clarifying and enforcing our standards of
+acceptable behavior and will take appropriate and fair corrective action in
+response to any behavior that they deem inappropriate, threatening, offensive,
+or harmful.
+
+Community leaders have the right and responsibility to remove, edit, or reject
+comments, commits, code, wiki edits, issues, and other contributions that are
+not aligned to this Code of Conduct, and will communicate reasons for moderation
+decisions when appropriate.
+
+## Scope
+
+This Code of Conduct applies within all community spaces, and also applies when
+an individual is officially representing the community in public spaces.
+Examples of representing our community include using an official e-mail address,
+posting via an official social media account, or acting as an appointed
+representative at an online or offline event.
+
+## Enforcement
+
+Instances of abusive, harassing, or otherwise unacceptable behavior may be
+reported to the community leaders responsible for enforcement at
+feedback@huggingface.co.
+All complaints will be reviewed and investigated promptly and fairly.
+
+All community leaders are obligated to respect the privacy and security of the
+reporter of any incident.
+
+## Enforcement Guidelines
+
+Community leaders will follow these Community Impact Guidelines in determining
+the consequences for any action they deem in violation of this Code of Conduct:
+
+### 1. Correction
+
+**Community Impact**: Use of inappropriate language or other behavior deemed
+unprofessional or unwelcome in the community.
+
+**Consequence**: A private, written warning from community leaders, providing
+clarity around the nature of the violation and an explanation of why the
+behavior was inappropriate. A public apology may be requested.
+
+### 2. Warning
+
+**Community Impact**: A violation through a single incident or series of
+actions.
+
+**Consequence**: A warning with consequences for continued behavior. No
+interaction with the people involved, including unsolicited interaction with
+those enforcing the Code of Conduct, for a specified period of time. This
+includes avoiding interactions in community spaces as well as external channels
+like social media. Violating these terms may lead to a temporary or permanent
+ban.
+
+### 3. Temporary Ban
+
+**Community Impact**: A serious violation of community standards, including
+sustained inappropriate behavior.
+
+**Consequence**: A temporary ban from any sort of interaction or public
+communication with the community for a specified period of time. No public or
+private interaction with the people involved, including unsolicited interaction
+with those enforcing the Code of Conduct, is allowed during this period.
+Violating these terms may lead to a permanent ban.
+
+### 4. Permanent Ban
+
+**Community Impact**: Demonstrating a pattern of violation of community
+standards, including sustained inappropriate behavior, harassment of an
+individual, or aggression toward or disparagement of classes of individuals.
+
+**Consequence**: A permanent ban from any sort of public interaction within the
+community.
+
+## Attribution
+
+This Code of Conduct is adapted from the [Contributor Covenant][homepage],
+version 2.1, available at
+[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].
+
+Community Impact Guidelines were inspired by
+[Mozilla's code of conduct enforcement ladder][Mozilla CoC].
+
+For answers to common questions about this code of conduct, see the FAQ at
+[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
+[https://www.contributor-covenant.org/translations][translations].
+
+[homepage]: https://www.contributor-covenant.org
+[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
+[Mozilla CoC]: https://github.com/mozilla/diversity
+[FAQ]: https://www.contributor-covenant.org/faq
+[translations]: https://www.contributor-covenant.org/translations
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 58a2652..eb60a5a 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -3,27 +3,27 @@
 Thank you for not only using Unsloth but also for being interested in helping out! We value all contributions, whether they come in the form of code, ideas, support for others or just by simply spreading the word of Unsloth! 
 
 - **[Support the Community](https://github.com/unslothai/unsloth/issues)**: Answer questions, review pull requests, or assist others in discussions.
-- **Fix Bugs**: Identify and resolve issues with the existing codebase.  
-- **Submit Ideas**: Request new features or share enhancements you'd like to see.  
+- **Fix Bugs**: Identify and resolve issues with the existing codebase.
+- **Submit Ideas**: Request new features or share enhancements you'd like to see.
 - **Develop Features**: Implement new functionality or improve existing tools which can be done via PRs.
 - **[Improve Documentation](https://docs.unsloth.ai/)**: Help by creating guides, FAQs, or enhancing clarity.
 
 One of the best ways to support us is by spreading the word about Unsloth! Share how its powering your amazing projects in blog posts or social media, and inspire others to explore its potential. Even a simple star on our repo goes a long way in showing your support and helping the community grow. 
 
-## Submitting Issues  
-If you find a bug or have a feature idea, wed love to hear from you! Heres how to make your submission stand out:  
+## Submitting Issues
+If you find a bug or have a feature idea, wed love to hear from you! Heres how to make your submission stand out:
 
-### Reporting Bugs  
-1. **Search First**: Check if the issue has already been reported using GitHubs search bar under Issues.  
-2. **Details Matter**: Is this on Google Colab, Kaggle, or on another platform service? Are you using Unsloth's official notebook? Include your OS, Python version, and other relevant details. For bugs, a concise code snippet that reproduces the issue is incredibly helpful.  
+### Reporting Bugs
+1. **Search First**: Check if the issue has already been reported using GitHubs search bar under Issues.
+2. **Details Matter**: Is this on Google Colab, Kaggle, or on another platform service? Are you using Unsloth's official notebook? Include your OS, Python version, and other relevant details. For bugs, a concise code snippet that reproduces the issue is incredibly helpful.
 3. **Be Thorough**: Attach screenshots, traceback logs, or any additional information that might speed up resolution.
 
 ## Spread the Word
-Your support extends beyond code:  
-- Spread the word by writing about Unsloth in blogs or social media.  
-- Share how Unsloth powers your projects.  
-- Star our repository to show your appreciation.  
+Your support extends beyond code:
+- Spread the word by writing about Unsloth in blogs or social media.
+- Share how Unsloth powers your projects.
+- Star our repository to show your appreciation.
 
-Finally, please be mindful of our [Code of Conduct](https://github.com/unslothai/unsloth/tree/main/unsloth/CODE_OF_CONDUCT.md) to ensure a welcoming and inclusive environment for everyone.  
+Finally, please be mindful of our [Code of Conduct](https://github.com/unslothai/unsloth/blob/main/CODE_OF_CONDUCT.md) to ensure a welcoming and inclusive environment for everyone.
 
 Thank you so much for reading and we hope you have lots of fun using Unsloth! 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 2a5b71d..0c51c17 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.2.11""
+__version__ = ""2025.2.12""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 404fce3..048bee7 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -259,6 +259,7 @@ pass
 
 def assert_same_tokenization(slow_tokenizer, fast_tokenizer):
     # Get eos_token, bos_token etc
+    if not hasattr(slow_tokenizer, ""all_special_tokens""): return True
     dir_names = dir(slow_tokenizer)
     special_tokens = list(filter(None, (
         getattr(slow_tokenizer, x) for x in dir_names
@@ -503,12 +504,14 @@ def _load_correct_tokenizer(
             cache_dir         = cache_dir,
         )
     except:
-        pass
+        slow_tokenizer = None
         # print(
         #     f""Unsloth: {tokenizer_name} has no tokenizer.model file.\n""\
         #     ""Just informing you about this - this is not a critical error.""
         # )
     pass
+    # Unsure why this occurs!
+    if type(slow_tokenizer) is bool: slow_tokenizer = None
 
     fast_tokenizer = AutoTokenizer.from_pretrained(
         tokenizer_name,
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index ec62c9b..6a0be38 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -293,7 +293,7 @@ def get_chat_template(
 
         # Check fast tokenizer
         if not is_fast_tokenizer:
-            logger.warning_once(
+            print(
                 f""Unsloth: Not a fast tokenizer, so can't process it as of yet :(\n""\
                 ""Please log a Github issue if you want this as a new feature!\n""\
                 ""Your chat template will still work, but it won't add or edit tokens.""
@@ -348,11 +348,31 @@ def get_chat_template(
             # But training the lm_head and embeddings are slow!
             # This is a HACK!
             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser
+
+            old_bos_token = getattr(tokenizer, ""bos_token"", None)
+            old_eos_token = getattr(tokenizer, ""eos_token"", None)
+            old_pad_token = getattr(tokenizer, ""pad_token"", None)
+            old_unk_token = getattr(tokenizer, ""unk_token"", None)
+
             string_vocab = tokenizer._tokenizer.to_str()
-            old_eos_token = tokenizer.eos_token
-            string_vocab = string_vocab.replace(old_eos_token, stop_word)
+            # First check if new stop_word is in the tokenizer
+            if stop_word in string_vocab:
+                # We shall swap them around
+                temporary_stop_token = ""<|:__TEMP//STOP//TOKEN__:|>""
+                string_vocab = string_vocab.replace(old_eos_token, temporary_stop_token)
+                string_vocab = string_vocab.replace(stop_word, old_eos_token)
+                string_vocab = string_vocab.replace(temporary_stop_token, stop_word)
+            else:
+                string_vocab = string_vocab.replace(old_eos_token, stop_word)
+            pass
             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
-            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+            new_tokenizer = tokenizer.__class__(
+                tokenizer_object = new_tokenizer,
+                bos_token = old_bos_token,
+                eos_token = stop_word,
+                unk_token = old_unk_token,
+                pad_token = old_pad_token,
+            )
 
             # Must fix the sentence piece tokenizer since there's no tokenizer.model file!
             token_mapping = { old_eos_token : stop_word, }
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 202c692..6f70bc5 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1017,6 +1017,12 @@ class FastLlamaModel:
         trust_remote_code = False,
         **kwargs,
     ):
+        if token is None and ""HF_TOKEN"" in os.environ:
+            token = os.environ[""HF_TOKEN""]
+
+        if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
+            token = os.environ[""HUGGINGFACE_TOKEN""]
+
         if model_patcher is None: model_patcher = FastLlamaModel
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
@@ -1445,8 +1451,8 @@ class FastLlamaModel:
         for module in target_modules:
             if module == ""lm_head"":
                 logger.warning_once(
-                    ""Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.""\
-                    ""We shall do it for you!""
+                    ""Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`. ""\
+                    ""Luckily, we shall do it for you!""
                 )
                 train_lm_head = True
                 if modules_to_save is None: modules_to_save = [""lm_head""]
@@ -1454,8 +1460,8 @@ class FastLlamaModel:
 
             elif module == ""embed_tokens"":
                 logger.warning_once(
-                    ""Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.""\
-                    ""We shall do it for you!""
+                    ""Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`. ""\
+                    ""Luckily, we shall do it for you!""
                 )
                 train_embed_tokens = True
                 if modules_to_save is None: modules_to_save = [""embed_tokens""]
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 29d25f3..fa864a9 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -78,6 +78,12 @@ class FastLanguageModel(FastLlamaModel):
         use_gradient_checkpointing = True,
         *args, **kwargs,
     ):
+        if token is None and ""HF_TOKEN"" in os.environ:
+            token = os.environ[""HF_TOKEN""]
+
+        if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
+            token = os.environ[""HUGGINGFACE_TOKEN""]
+
         old_model_name = model_name
         model_name = _get_model_name(model_name, load_in_4bit)
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 3034b83..5610893 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 from .llama import *
+import os
 from ._utils import __version__
 
 from transformers.models.mistral.modeling_mistral import (
@@ -301,6 +302,12 @@ class FastMistralModel(FastLlamaModel):
         trust_remote_code = False,
         **kwargs,
     ):
+        if token is None and ""HF_TOKEN"" in os.environ:
+            token = os.environ[""HF_TOKEN""]
+
+        if token is None and ""HUGGINGFACE_TOKEN"" in os.environ:
+            token = os.environ[""HUGGINGFACE_TOKEN""]
+
         if model_patcher is None: model_patcher = FastMistralModel
         # Mistral does NOT support RoPE Scaling!
         if rope_scaling is not None:
diff --git a/unsloth/save.py b/unsloth/save.py
index d001032..655d1c5 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -327,7 +327,7 @@ def unsloth_save_model(
         if hasattr(model, ""config""):
             print(f""Saved {save_method} model to https://huggingface.co/"" + save_directory)
         pass
-        return save_directory
+        return save_directory, None
     pass
 
     # Tokenizer has different saving arguments
@@ -402,7 +402,7 @@ def unsloth_save_model(
         pass
 
         print("" Done."")
-        return save_directory
+        return save_directory, None
     pass
 
     # If push_to_hub, we must remove the .../ part of a repo
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 2640f32..1325912 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -48,7 +48,7 @@ def try_fix_tokenizer(tokenizer, prepend = True):
 
     tokenizer_string = converted_tokenizer.to_str()
 
-    # Llama does apple. Sometimes this is wrong!!
+    # Llama does _apple. Sometimes this is wrong!!
     prepend_text = '{""type"":""Prepend"",""prepend"":""""},'
     if not prepend and prepend_text in tokenizer_string:
         tokenizer_string = tokenizer_string.replace(prepend_text, """", 1)
@@ -269,15 +269,26 @@ def load_correct_tokenizer(
         cache_dir = None
     pass
 
-    slow_tokenizer = AutoTokenizer.from_pretrained(
-        tokenizer_name,
-        model_max_length  = model_max_length,
-        padding_side      = padding_side,
-        token             = token,
-        trust_remote_code = trust_remote_code,
-        use_fast          = False,
-        cache_dir         = cache_dir,
-    )
+    # Try loading the slow tokenizer. If it fails, then try Fast only
+    # Mainly to solve Deepseek models with no tokenizer.model file
+    slow_tokenizer = None
+    try:
+        slow_tokenizer = AutoTokenizer.from_pretrained(
+            tokenizer_name,
+            model_max_length  = model_max_length,
+            padding_side      = padding_side,
+            token             = token,
+            trust_remote_code = trust_remote_code,
+            use_fast          = False,
+            cache_dir         = cache_dir,
+        )
+    except:
+        print(
+            f""Unsloth: {tokenizer_name} has no tokenizer.model file.\n""\
+            ""Just informing you about this - this is not a critical error.""
+        )
+    pass
+
     fast_tokenizer = AutoTokenizer.from_pretrained(
         tokenizer_name,
         model_max_length  = model_max_length,
@@ -286,14 +297,19 @@ def load_correct_tokenizer(
         trust_remote_code = trust_remote_code,
         cache_dir         = cache_dir,
     )
-    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token
-    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token
-    
-    # Confirm if slow and fast are equivalent!
-    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):
-        return fast_tokenizer
+
+    if slow_tokenizer is not None:
+        fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token
+        fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token
+        
+        # Confirm if slow and fast are equivalent!
+        if assert_same_tokenization(slow_tokenizer, fast_tokenizer):
+            return fast_tokenizer
+        else:
+            return convert_to_fast_tokenizer(slow_tokenizer)
+        pass
     else:
-        return convert_to_fast_tokenizer(slow_tokenizer)
+        return fast_tokenizer
     pass
 pass
 
@@ -408,25 +424,37 @@ def check_tokenizer(
                 cache_dir = None
             pass
 
-            # Try slow tokenizer which can fix things!
-            tokenizer = AutoTokenizer.from_pretrained(
-                model_name,
-                model_max_length = model_max_length,
-                padding_side = padding_side,
-                token = token,
-                use_fast = False,
-                cache_dir = cache_dir,
-            )
-            return check_tokenizer(
-                model = model,
-                tokenizer = tokenizer,
-                model_name = model_name,
-                model_max_length = model_max_length,
-                padding_side = padding_side,
-                token = token,
-                _reload = False,
-            )
-            break
+            # Sometimes slow tokenizer does not work like Deepseek
+            try:
+                # Try slow tokenizer which can fix things!
+                tokenizer = AutoTokenizer.from_pretrained(
+                    model_name,
+                    model_max_length = model_max_length,
+                    padding_side = padding_side,
+                    token = token,
+                    use_fast = False,
+                    cache_dir = cache_dir,
+                )
+                return check_tokenizer(
+                    model = model,
+                    tokenizer = tokenizer,
+                    model_name = model_name,
+                    model_max_length = model_max_length,
+                    padding_side = padding_side,
+                    token = token,
+                    _reload = False,
+                )
+                break
+            except:
+                # Tokenizer has out of bounds issues and we can't
+                # load the slow tokenizer version :(
+                logger.warning_once(
+                    ""Unsloth: Tokenizer is most likely buggy, and Unsloth failed to repair it.\n""\
+                    ""It will still work, but beware of out of bounds memory accesses.\n""\
+                    ""Please file an issue on the model owner's repo about this issue.""
+                )
+                return tokenizer
+            pass
         pass
     pass
     return convert_to_fast_tokenizer(tokenizer)
"
"diff --git a/PARAMETERS.md b/PARAMETERS.md
new file mode 100644
index 0000000..94d6379
--- /dev/null
+++ b/PARAMETERS.md
@@ -0,0 +1,87 @@
+## LoraConfig Parameters
+
+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Heres a concise breakdown of key parameters:
+
+**r**
+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.
+- **Impact**:
+  - **Higher**: Retains more information, increases computational load.
+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.
+
+
+**lora_alpha**
+- **Description**: Scaling factor for the low-rank matrices' contribution.
+- **Impact**:
+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.
+  - **Lower**: Subtler effect, may require more training steps.
+
+**lora_dropout**
+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.
+- **Impact**:
+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.
+  - **Lower**: Less regularization, may speed up training, risks overfitting.
+
+**loftq_config**
+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.
+- **Impact**:
+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.
+  - **None**: LoftQ quantization is not applied.
+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.
+
+
+**use_rslora**
+- **Description**: Enables Rank-Stabilized LoRA (RSLora).
+- **Impact**:
+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).
+  - **False**: Uses the original default scaling factor `lora_alpha/r`.
+
+**gradient_accumulation_steps**
+- **Default**: 1
+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.
+- **Impact**: 
+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.
+  - **Lower**: Faster updates but may require more memory per step and can be less stable.
+
+**weight_decay**
+- **Default**: 0.01
+- **Description**: Regularization technique that applies a small penalty to the weights during training.
+- **Impact**:
+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.
+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.
+
+**learning_rate**
+- **Default**: 2e-4
+- **Description**: The rate at which the model updates its parameters during training.
+- **Impact**:
+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.
+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.
+
+## Target Modules 
+
+**q_proj (query projection)**
+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.
+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.
+
+**k_proj (key projection)**
+- **Description**: Projects the input into the key space in the attention mechanism.
+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.
+
+**v_proj (value projection)**
+- **Description**: Projects the input into the value space in the attention mechanism.
+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.
+
+**o_proj (output projection)**
+- **Description**: Projects the output of the attention mechanism back into the original space.
+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.
+
+**gate_proj (gate projection)**
+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.
+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.
+
+**up_proj (up projection)**
+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.
+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.
+
+**down_proj (down projection)**
+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.
+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index d85eca0..93960e2 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -14,8 +14,20 @@
 import os
 import warnings
 import importlib
+import sys
+from packaging.version import Version
 
-# Currently only supports 1 GPU, or else seg faults will occur.
+# Define a list of modules to check
+MODULES_TO_CHECK = [""peft"", ""bitsandbytes""]
+
+# Check if any of the modules in the list have been imported
+for module in MODULES_TO_CHECK:
+    if module in sys.modules:
+        raise ImportError(f""Unsloth: Please import Unsloth before {module}."")
+    pass
+pass
+
+# Currently only supports 1 GPU, or else seg faults will occur.    
 if ""CUDA_VISIBLE_DEVICES"" in os.environ:
     os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
     devices = os.environ[""CUDA_VISIBLE_DEVICES""]
@@ -66,8 +78,14 @@ pass
 
 # Try loading bitsandbytes and triton
 import bitsandbytes as bnb
+
 import triton
-from triton.common.build import libcuda_dirs
+libcuda_dirs = lambda: None
+if Version(triton.__version__) >= Version(""3.0.0""):
+    try: from triton.backends.nvidia.driver import libcuda_dirs
+    except: pass
+else: from triton.common.build import libcuda_dirs
+
 import os
 import re
 import numpy as np
@@ -103,8 +121,11 @@ except:
     importlib.reload(bnb)
     importlib.reload(triton)
     try:
-        import bitsandbytes as bnb
-        from triton.common.build import libcuda_dirs
+        libcuda_dirs = lambda: None
+        if Version(triton.__version__) >= Version(""3.0.0""):
+            try: from triton.backends.nvidia.driver import libcuda_dirs
+            except: pass
+        else: from triton.common.build import libcuda_dirs
         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32
         libcuda_dirs()
     except:
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 4c78232..2e3761f 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = ""./model-unsloth.F16.gguf"")
     pass
     
     for prompt in prompts:
-        command = f""./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt ""\
+        command = f""./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt ""\
             f""--check-tensors -p '{prompt}'""
 
         datas = []
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index b1fdba8..ebea02a 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -24,6 +24,7 @@ from .geglu import (
 )
 from .fast_lora import (
 	get_lora_parameters,
+	get_lora_parameters_bias,
 	apply_lora_mlp_swiglu,
 	apply_lora_mlp_geglu_exact,
 	apply_lora_mlp_geglu_approx,
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index aba44f0..8f7aea5 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -13,7 +13,13 @@
 # limitations under the License.
 
 import torch
-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora
+from .utils import (
+    fast_dequantize,
+    QUANT_STATE,
+    get_lora_parameters,
+    get_lora_parameters_bias,
+    matmul_lora,
+)
 
 
 class LoRA_MLP(torch.autograd.Function):
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 3bc091b..de1e2e5 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -33,11 +33,8 @@ del major, minor
 
 def _get_model_name(model_name, load_in_4bit = True):
 
-    # First try replacing lowercase 'b' with uppercase 'B'
-    model_name = model_name.lower()
-
     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:
-        model_name = INT_TO_FLOAT_MAPPER[model_name]
+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
         logger.warning_once(
             f""Unsloth: Your transformers version of {transformers_version} does not support native ""\
             f""4bit loading.\nThe minimum required version is 4.37.\n""\
@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):
         )
     
     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:
-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]
+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\n""\
         #     f""`load_in_4bit = False`. We shall load `{new_model_name}` instead.""
@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):
         model_name = new_model_name
 
     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:
-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]
+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]
         # logger.warning_once(
         #     f""Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\n""\
         #     f""We shall load `{new_model_name}` for 4x faster loading.""
@@ -70,17 +67,18 @@ pass
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
-        model_name     = ""unsloth/llama-3-8b-bnb-4bit"",
-        max_seq_length = None,
-        dtype          = None,
-        load_in_4bit   = True,
-        token          = None,
-        device_map     = ""sequential"",
-        rope_scaling   = None,
-        fix_tokenizer  = True,
-        trust_remote_code = False,
-        use_gradient_checkpointing = True,
-        resize_model_vocab = None,
+        model_name                 = ""unsloth/llama-3-8b-bnb-4bit"",
+        max_seq_length             = None,
+        dtype                      = None,
+        load_in_4bit               = True,
+        token                      = None,
+        device_map                 = ""sequential"",
+        rope_scaling               = None,
+        fix_tokenizer              = True,
+        trust_remote_code          = False,
+        use_gradient_checkpointing = ""unsloth"",
+        resize_model_vocab         = None,
+        revision                   = None,
         *args, **kwargs,
     ):
         if token is None and ""HF_TOKEN"" in os.environ:
@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):
         # First check if it's a normal model via AutoConfig
         is_peft = False
         try:
-            model_config = AutoConfig.from_pretrained(model_name, token = token)
+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)
             is_peft = False
         except:
             try:
                 # Most likely a PEFT model
-                peft_config = PeftConfig.from_pretrained(model_name, token = token)
+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)
             except:
                 raise RuntimeError(f""Unsloth: `{model_name}` is not a full model or a PEFT model."")
             
@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         model, tokenizer = dispatch_model.from_pretrained(
-            model_name     = model_name,
-            max_seq_length = max_seq_length,
-            dtype          = dtype,
-            load_in_4bit   = load_in_4bit,
-            token          = token,
-            device_map     = device_map,
-            rope_scaling   = rope_scaling,
-            fix_tokenizer  = fix_tokenizer,
-            model_patcher  = dispatch_model,
-            tokenizer_name = tokenizer_name,
+            model_name        = model_name,
+            max_seq_length    = max_seq_length,
+            dtype             = dtype,
+            load_in_4bit      = load_in_4bit,
+            token             = token,
+            device_map        = device_map,
+            rope_scaling      = rope_scaling,
+            fix_tokenizer     = fix_tokenizer,
+            model_patcher     = dispatch_model,
+            tokenizer_name    = tokenizer_name,
             trust_remote_code = trust_remote_code,
+            revision          = revision if not is_peft else None,
             *args, **kwargs,
         )
         
         if resize_model_vocab is not None:
             model.resize_token_embeddings(resize_model_vocab)
+        pass
 
         # In case the model supports tagging, add the unsloth tag.
         if hasattr(model, ""add_model_tags""):
@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         if is_peft:
+            # From https://github.com/huggingface/peft/issues/184
             # Now add PEFT adapters
-            model = PeftModel.from_pretrained(model, old_model_name, token = token)
+            model.enable_input_require_grads()
+            model = PeftModel.from_pretrained(
+                model,
+                old_model_name,
+                token = token,
+                revision = revision,
+                is_trainable = True,
+            )
             # Patch it as well!
             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)
         pass
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 73aa06c..5ef7583 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \
     ""unsloth/Qwen2-70B-Instruct-bnb-4bit"" : (
         ""Qwen/Qwen2-70B-Instruct"",
     ),
+    ""mistralai/Codestral-22B-v0.1"" : (
+        ""mistral-community/Codestral-22B-v0.1"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
diff --git a/unsloth/save.py b/unsloth/save.py
index 3ad2f34..cae59ca 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -22,7 +22,7 @@ import shutil
 import pickle
 import gc
 from transformers.models.llama.modeling_llama import logger
-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters
+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias
 import subprocess
 import psutil
 import re
@@ -132,9 +132,10 @@ pass
 
 def _merge_lora(layer, name):
 
+    bias = None
     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):
         # Is LoRA so we need to merge!
-        W, quant_state, A, B, s = get_lora_parameters(layer)
+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)
         if quant_state is not None:
             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]
             W = fast_dequantize(W, quant_state)
@@ -156,7 +157,7 @@ def _merge_lora(layer, name):
         W = W.t().to(dtype)
     else:
         W = layer.weight
-    return W
+    return W, bias
 pass
 
 
@@ -527,7 +528,12 @@ def unsloth_save_model(
         for item in LLAMA_WEIGHTS:
             proj = eval(f""layer.{item}"")
             name = f""model.layers.{j}.{item}.weight""
-            W = _merge_lora(proj, name)
+            W, bias = _merge_lora(proj, name)
+
+            # Bias term
+            if bias is not None:
+                state_dict[f""model.layers.{j}.{item}.bias""] = bias
+            pass
 
             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:
                 # Save to GPU memory
@@ -643,7 +649,8 @@ def unsloth_save_model(
     model.config = new_config
 
     # Save!
-
+    
+    save_pretrained_settings[""selected_adapters""] = None
     # Check if pushing to an organization
     if save_pretrained_settings[""push_to_hub""] and (username != actual_username):
         print(f""Unsloth: Saving to organization with address {new_save_directory}"")
@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):
         pass
     pass
     # Check if successful
-    if not os.path.exists(""llama.cpp/quantize""):
+    if not os.path.exists(""llama.cpp/quantize"") and not os.path.exists(""llama.cpp/llama-quantize""):
         raise RuntimeError(
             ""Unsloth: llama.cpp GGUF seems to be too buggy to install.\n""\
             ""File a report to llama.cpp's main repo since this is not an Unsloth issue.""
@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):
 pass
 
 
-def install_llama_cpp_blocking(use_cuda = True):
+def install_llama_cpp_blocking(use_cuda = False):
     # https://github.com/ggerganov/llama.cpp/issues/7062
     # Weirdly GPU conversion for GGUF breaks??
     # use_cuda = ""LLAMA_CUDA=1"" if use_cuda else """"
@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):
 pass
 
 
-def _fix_gemma_gguf():
-    # Fixes Gemma saving to GGUF to float32 instead of float16!
-    with open(""llama.cpp/convert-hf-to-gguf.py"", ""rb"") as file:
-        text = file.read()
-    pass
-
-    gemma_start = text.find(b""class GemmaModel(Model):"")
-    if gemma_start == -1: return
-
-    gemma_end   = text.find(b""self.gguf_writer.add_tensor(new_name, data)"", gemma_start)
-    if gemma_end == -1: return
-
-    gemma_text = text[gemma_start : gemma_end]
-    bad_text = \
-b""""""         data = data.astype(np.float32)
-
-            # if f16 desired, convert any float32 2-dim weight tensors to float16
-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith("".weight"") and n_dims == 2:
-                data = data.astype(np.float16)""""""
-    good_text = \
-b""""""         # if f32 desired, convert any float16 to float32
-            if self.ftype == 0 and data_dtype == np.float16:
-                data = data.astype(np.float32)
-
-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32
-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:
-                data = data.astype(np.float32)
-
-            # if f16 desired, convert any float32 2-dim weight tensors to float16
-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith("".weight"") and n_dims == 2:
-                data = data.astype(np.float16)""""""
-    find_bad = gemma_text.find(bad_text)
-    if find_bad == -1: return
-
-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]
-    text = text[:gemma_start] + gemma_text + text[gemma_end:]
-
-    with open(""llama.cpp/convert-hf-to-gguf.py"", ""w+b"") as file:
-        file.write(text)
-    pass
-pass
-
-
 def save_to_gguf(
     model_type           : str,
     model_dtype          : str,
@@ -930,7 +894,7 @@ def save_to_gguf(
 
     # Check first_conversion format
     if   first_conversion == ""f16""  : pass
-    if   first_conversion == ""bf16"" : pass
+    elif first_conversion == ""bf16"" : pass
     elif first_conversion == ""f32""  : pass
     elif first_conversion == ""q8_0"" : pass
     else:
@@ -946,8 +910,20 @@ def save_to_gguf(
         error = 0
         install_llama_cpp_blocking()
     pass
+
     # Check if successful. If not install 10th latest release
-    if error != 0 or not os.path.exists(""llama.cpp/quantize""):
+
+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize
+    # and llama.cpp/main changed to llama.cpp/llama-cli
+    # See https://github.com/ggerganov/llama.cpp/pull/7809
+    quantize_location = None
+    if os.path.exists(""llama.cpp/quantize""):
+        quantize_location = ""llama.cpp/quantize""
+    elif os.path.exists(""llama.cpp/llama-quantize""):
+        quantize_location = ""llama.cpp/llama-quantize""
+    pass
+
+    if error != 0 or quantize_location is None:
         print(f""Unsloth: llama.cpp error code = {error}."")
         install_llama_cpp_old(-10)
     pass
@@ -1017,9 +993,6 @@ def save_to_gguf(
             f""--outfile {final_location} --vocab-type {vocab_type} ""\
             f""--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab""
     else:
-        # Need to fix convert-hf-to-gguf.py for some models!
-        # _fix_gemma_gguf()
-
         command = f""python llama.cpp/convert-hf-to-gguf.py {model_directory} ""\
             f""--outfile {final_location} ""\
             f""--outtype {first_conversion}""
@@ -1065,7 +1038,7 @@ def save_to_gguf(
         print(f""Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes..."")
         final_location = f""./{model_directory}-unsloth.{quantization_method.upper()}.gguf""
 
-        command = f""./llama.cpp/quantize {old_location} ""\
+        command = f""./{quantize_location} {old_location} ""\
             f""{final_location} {quantization_method} {n_cpus}""
         
         # quantize uses stderr
@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(
     pass
 pass
 
+# Corrected function to save LoRA to a custom directory
+def save_lora_to_custom_dir(model, tokenizer, save_directory):
+    # Create the custom directory if it doesn't exist
+    os.makedirs(save_directory, exist_ok=True)
+
+    # Call the unsloth_save_model function with the custom directory
+    unsloth_save_model(
+        model,
+        tokenizer,
+        save_directory=save_directory,
+        save_method=""lora"",
+        push_to_hub=False,
+    )
+
+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub
+def unsloth_convert_lora_to_ggml_and_push_to_hub(
+    self,
+    tokenizer,
+    repo_id: str,
+    use_temp_dir: Optional[bool] = None,
+    commit_message: Optional[str] = ""Converted LoRA to GGML with Unsloth"",
+    private: Optional[bool] = None,
+    token: Union[bool, str, None] = None,
+    create_pr: bool = False,
+    revision: str = None,
+    commit_description: str = ""Convert LoRA to GGML format using Unsloth"",
+    temporary_location: str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage: float = 0.85,
+):
+    if not os.path.exists(""llama.cpp""):
+        if IS_KAGGLE_ENVIRONMENT:
+            python_install = install_python_non_blocking([""protobuf""])
+            python_install.wait()
+            install_llama_cpp_blocking(use_cuda=False)
+            makefile = None
+        else:
+            git_clone = install_llama_cpp_clone_non_blocking()
+            python_install = install_python_non_blocking([""protobuf""])
+            git_clone.wait()
+            makefile = install_llama_cpp_make_non_blocking()
+            python_install.wait()
+    else:
+        makefile = None
+
+    for _ in range(3):
+        gc.collect()
+
+    lora_directory_push = ""lora-to-ggml-push""
+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)
+
+    model_type = self.config.model_type
+    output_file = os.path.join(lora_directory_push, ""ggml-adapter-model.bin"")
+
+    print(f""Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format."")
+    print(f""The output file will be {output_file}"")
+
+    command = f""python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama""
+
+    try:
+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:
+            for line in sp.stdout:
+                print(line, end="""", flush=True)
+            for line in sp.stderr:
+                print(line, end="""", flush=True)
+            sp.wait()
+            if sp.returncode != 0:
+                raise subprocess.CalledProcessError(sp.returncode, command)
+    except subprocess.CalledProcessError as e:
+        print(f""Error: Conversion failed with return code {e.returncode}"")
+        return
+
+    print(f""Unsloth: Conversion completed! Output file: {output_file}"")
+
+    print(""Unsloth: Uploading GGML file to Hugging Face Hub..."")
+    username = upload_to_huggingface(
+        self, repo_id, token,
+        ""GGML converted LoRA"", ""ggml"", output_file, None, private,
+    )
+    link = f""{repo_id.lstrip('/')}""
+    print(""Unsloth: Done."")
+    print(f""Converted LoRA to GGML and uploaded to https://huggingface.co/{link}"")
+    print(""\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!"")
+
+def unsloth_convert_lora_to_ggml_and_save_locally(
+    self,
+    save_directory: str, # Added parameter for the folder name 
+    tokenizer, 
+    temporary_location: str = ""_unsloth_temporary_saved_buffers"",
+    maximum_memory_usage: float = 0.85,
+):
+    if not os.path.exists(""llama.cpp""):
+        if IS_KAGGLE_ENVIRONMENT:
+            python_install = install_python_non_blocking([""protobuf""])
+            python_install.wait()
+            install_llama_cpp_blocking(use_cuda=False)
+            makefile = None
+        else:
+            git_clone = install_llama_cpp_clone_non_blocking()
+            python_install = install_python_non_blocking([""protobuf""])
+            git_clone.wait()
+            makefile = install_llama_cpp_make_non_blocking()
+            python_install.wait()
+    else:
+        makefile = None
+
+    for _ in range(3):
+        gc.collect()
+
+    # Use the provided save_directory for local saving
+    save_lora_to_custom_dir(self, tokenizer, save_directory)
+
+    model_type = self.config.model_type
+    output_file = os.path.join(save_directory, ""ggml-adapter-model.bin"")
+
+    print(f""Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format."")
+    print(f""The output file will be {output_file}"")
+
+    command = f""python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama""
+
+    try:
+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:
+            for line in sp.stdout:
+                print(line, end="""", flush=True)
+            for line in sp.stderr:
+                print(line, end="""", flush=True)
+            sp.wait()
+            if sp.returncode != 0:
+                raise subprocess.CalledProcessError(sp.returncode, command)
+    except subprocess.CalledProcessError as e:
+        print(f""Error: Conversion failed with return code {e.returncode}"")
+        return
+    print(""Unsloth: Done."")
+    print(f""Unsloth: Conversion completed! Output file: {output_file}"")
+    print(""\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!"")
 
 def patch_saving_functions(model):
     import inspect
@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):
     # Add saving methods to top level model
     if hasattr(model, ""config""):
         # Counteract tokenizers
-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)
-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)
-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)
-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)
+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)
+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)
+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)
+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)
+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)
+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)
     pass
     return model
 pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index f10b2c0..395c3b7 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -232,62 +232,6 @@ llama_template = \
         ""{% endif %}""\
     ""{% endfor %}""
 pass
-    
-
-def select_correct_slow_tokenizer(
-    tokenizer_name,
-    model_max_length = None,
-    padding_side = ""right"",
-    token = None,
-    trust_remote_code = False,
-    cache_dir = ""huggingface_tokenizers_cache"",
-):
-    """"""
-    Returns 'correct' tokenizer by checking if the chat templates are
-    actually tokenized correctly.
-    """"""
-    messages = [
-        {""role"": ""user"", ""content"": ""What is 2+2?""},
-        {""role"": ""assistant"", ""content"": ""It's 4.""},
-    ]
-    
-    settings = (
-        (False, False, True,),
-        (False, True,  True,),
-        (True,  False, True,),
-        (True,  False, False,),
-    )
-
-    for (use_fast, legacy, from_slow,) in settings:
-        # Default as mentioned by Arthur from HF:
-        slow_tokenizer = AutoTokenizer.from_pretrained(
-            tokenizer_name,
-            model_max_length  = model_max_length,
-            padding_side      = padding_side,
-            token             = token,
-            trust_remote_code = trust_remote_code,
-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373
-            use_fast          = use_fast,
-            legacy            = legacy,
-            from_slow         = from_slow,
-            cache_dir         = cache_dir,
-        )
-        slow_tokenizer_chat_template = slow_tokenizer.chat_template
-
-        slow_tokenizer.chat_template = llama_template
-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))
-        slow_tokenizer.chat_template = mistral_template
-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))
-
-        # If 2 spaces seen, normally wrong!
-        if "" ""*2 not in result1 and "" ""*2 not in result2:
-            slow_tokenizer.chat_template = slow_tokenizer_chat_template
-            return slow_tokenizer
-        pass
-    pass
-    # Return fast version as default
-    return slow_tokenizer
-pass
 
 
 def assert_same_tokenization(slow_tokenizer, fast_tokenizer):
@@ -508,13 +452,17 @@ def load_correct_tokenizer(
     # Mainly to solve Deepseek models with no tokenizer.model file
     slow_tokenizer = None
     try:
-        slow_tokenizer = select_correct_slow_tokenizer(
+        slow_tokenizer = AutoTokenizer.from_pretrained(
             tokenizer_name,
-            model_max_length = model_max_length,
-            padding_side = padding_side,
-            token = token,
+            model_max_length  = model_max_length,
+            padding_side      = padding_side,
+            token             = token,
             trust_remote_code = trust_remote_code,
-            cache_dir = cache_dir,
+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373
+            use_fast          = False,
+            legacy            = False,
+            from_slow         = True,
+            cache_dir         = cache_dir,
         )
     except:
         pass
@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):
     pass
 
     # Count all the possible bad tokens
-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)
+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)
     def mapping(examples):
         input_ids = examples[""input_ids""]
         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)
@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():
 
         check_text = \
         ""\n""\
-        ""test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\n""\
+        ""test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\n""\
         ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
         ""chat_template = '' if chat_template is None else chat_template\n""\
         ""has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) ""\
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9c218d3..2b07e5f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -911,98 +911,104 @@ pass
 
 
 # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
-def LlamaModel_fast_forward_inference(
-    self,
-    input_ids,
-    past_key_values,
-    position_ids,
-    attention_mask = None,
-):
-    input_ids = input_ids[:,:self.max_seq_length]
-    bsz, q_len = input_ids.shape
-    hd = self.config.hidden_size
-    mlp_size = self.config.intermediate_size
-
-    X = self.model.embed_tokens(input_ids)
-    X = X.to(_get_dtype(self.config.torch_dtype))
-    bsz, q_len, hd = X.shape
-    assert(q_len == 1)
-    # Get saved buffers to reduce memory movement
-    residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
-    _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
-    XX, XX2 = _XX[0], _XX[1]
-    variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = ""cuda:0"")
-    temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
-    temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
-
-    seq_len = past_key_values[0][0].shape[-2]
-    if bsz != 1:
-        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
-            attention_mask,
-            (bsz, q_len),
-            X,
-            seq_len,
-            sliding_window = getattr(self.config, ""sliding_window"", None),
-        )
-    else:
-        attention_mask = None
-    pass
+def _LlamaModel_fast_forward_inference(attention_fast_forward_inference=LlamaAttention_fast_forward_inference, mlp_fast_forward_inference=fast_swiglu_inference):
+    # This makes the attention and MLP customisable.
+    # Now for models like qwen3 or cohere which use custom attention operations, we can use this function
+    def LlamaModel_fast_forward_inference_custom(
+        self,
+        input_ids,
+        past_key_values,
+        position_ids,
+        attention_mask = None,
+    ):
+        input_ids = input_ids[:,:self.max_seq_length]
+        bsz, q_len = input_ids.shape
+        hd = self.config.hidden_size
+        mlp_size = self.config.intermediate_size
+
+        X = self.model.embed_tokens(input_ids)
+        X = X.to(_get_dtype(self.config.torch_dtype))
+        bsz, q_len, hd = X.shape
+        assert(q_len == 1)
+        # Get saved buffers to reduce memory movement
+        residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+        _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+        XX, XX2 = _XX[0], _XX[1]
+        variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = ""cuda:0"")
+        temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
+        temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
+
+        seq_len = past_key_values[0][0].shape[-2]
+        if bsz != 1:
+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask,
+                (bsz, q_len),
+                X,
+                seq_len,
+                sliding_window = getattr(self.config, ""sliding_window"", None),
+            )
+        else:
+            attention_mask = None
+        pass
 
-    next_decoder_cache = []
+        next_decoder_cache = []
 
-    for idx, decoder_layer in enumerate(self.model.layers):
-        residual.copy_(X) # residual = X
-        X = fast_rms_layernorm_inference(
-            decoder_layer.input_layernorm,
-            X,
-            XX = XX,
-            XX2 = XX2,
-            variance = variance,
-        )
-        X, present_key_value = LlamaAttention_fast_forward_inference(
-            decoder_layer.self_attn,
-            hidden_states = X,
-            past_key_value = past_key_values[idx],
-            position_ids = position_ids,
-            attention_mask = attention_mask,
-            do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
-        )
-        X += residual
+        for idx, decoder_layer in enumerate(self.model.layers):
+            residual.copy_(X) # residual = X
+            X = fast_rms_layernorm_inference(
+                decoder_layer.input_layernorm,
+                X,
+                XX = XX,
+                XX2 = XX2,
+                variance = variance,
+            )
+            X, present_key_value = attention_fast_forward_inference(
+                decoder_layer.self_attn,
+                hidden_states = X,
+                past_key_value = past_key_values[idx],
+                position_ids = position_ids,
+                attention_mask = attention_mask,
+                do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
+            )
+            X += residual
+
+            residual.copy_(X) # residual = X
+            X = fast_rms_layernorm_inference(
+                decoder_layer.post_attention_layernorm,
+                X,
+                XX = XX,
+                XX2 = XX2,
+                variance = variance,
+            )
+            X = mlp_fast_forward_inference(
+                decoder_layer.mlp,
+                X,
+                temp_gate = temp_gate,
+                temp_up = temp_up,
+            )
+            X += residual
 
-        residual.copy_(X) # residual = X
+            next_decoder_cache.append(present_key_value)
+        pass
         X = fast_rms_layernorm_inference(
-            decoder_layer.post_attention_layernorm,
+            self.model.norm,
             X,
             XX = XX,
             XX2 = XX2,
             variance = variance,
         )
-        X = fast_swiglu_inference(
-            decoder_layer.mlp,
-            X,
-            temp_gate = temp_gate,
-            temp_up = temp_up,
-        )
-        X += residual
 
-        next_decoder_cache.append(present_key_value)
+        return BaseModelOutputWithPast(
+            last_hidden_state = X,
+            past_key_values = next_decoder_cache,
+            hidden_states = [],
+            attentions = [],
+        )
     pass
-    X = fast_rms_layernorm_inference(
-        self.model.norm,
-        X,
-        XX = XX,
-        XX2 = XX2,
-        variance = variance,
-    )
-
-    return BaseModelOutputWithPast(
-        last_hidden_state = X,
-        past_key_values = next_decoder_cache,
-        hidden_states = [],
-        attentions = [],
-    )
-pass
+    return LlamaModel_fast_forward_inference_custom
 
+# For ensuring backwards compatibility, we create LlamaModel_fast_forward_inference that is consumed by other models
+LlamaModel_fast_forward_inference = _LlamaModel_fast_forward_inference()
 
 def CausalLM_fast_forward(fast_forward_inference):
     def _CausalLM_fast_forward(
diff --git a/unsloth/models/qwen3.py b/unsloth/models/qwen3.py
index 3f48449..c0ceefd 100644
--- a/unsloth/models/qwen3.py
+++ b/unsloth/models/qwen3.py
@@ -18,6 +18,7 @@ from ._utils import __version__
 from .llama import (
     LlamaRotaryEmbedding,
     LlamaLinearScalingRotaryEmbedding,
+    _LlamaModel_fast_forward_inference,
 )
 try:
     from transformers.models.qwen3.modeling_qwen3 import (
@@ -37,7 +38,9 @@ except:
             f""to obtain the latest transformers build, then restart this session.""\
         )
     pass
-
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
 # For Pytorch 2.1.1
 try:
     from transformers.models.qwen3.modeling_qwen3 import (
@@ -103,17 +106,19 @@ def Qwen3Attention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
-    # Extend RoPE dynamically to fit in VRAM
-    self.rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
-
-    if position_ids is None:
-        cos = self.rotary_emb.cos_cached
-        sin = self.rotary_emb.sin_cached
-        Q, K = fast_rope_embedding(Q, K, cos, sin)
+    if position_embeddings:
+        cos, sin = position_embeddings
     else:
-        cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)
-        Q, K = inplace_rope_embedding(Q, K, cos, sin, position_ids)
-    pass
+        # Extend RoPE dynamically to fit in VRA
+        rotary_emb = self.rotary_emb
+        rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
+
+        if position_ids is None:
+            # Useful for LongRoPE
+            cos, sin = rotary_emb.get_cached(kv_seq_len)
+        else:
+            cos, sin = rotary_emb(V, seq_len = kv_seq_len)
+    Q, K = fast_rope_embedding(Q, K, cos, sin)
 
     if past_key_value is not None:
         K = torch.cat([past_key_value[0], K], dim = 2)
@@ -164,8 +169,7 @@ def Qwen3Attention_fast_forward(
         Q = Q.transpose(1, 2)
         K = K.transpose(1, 2)
         V = V.transpose(1, 2)
-        sw = getattr(self.config, ""sliding_window"", None)
-        sw = kv_seq_len if (sw is None or sw == ""null"") else sw
+        sw = kv_seq_len
         window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)
         A = flash_attn_func(Q, K, V, causal = True, window_size = window)
     else:
@@ -185,13 +189,276 @@ def Qwen3Attention_fast_forward(
         # Go back to (batch_size, seq_len, n_heads, head_dim)
         A = A.transpose(1, 2).contiguous()
     pass
-    
+
     attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
     attn_output = self.apply_o(self, attn_output)
     attn_weights = None
     return attn_output, attn_weights, past_key_value
 pass
 
+torch_matmul = torch.matmul
+def Qwen3Attention_fast_forward_inference(
+    self,
+    hidden_states:  torch.Tensor,
+    past_key_value: Optional[Tuple[torch.Tensor]],
+    position_ids,
+    do_prefill = False,
+    attention_mask = None,
+):
+    """"""
+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406
+        Fast inference using KV cache.
+        QK^T can be computed in 4 chunks
+
+        [Q, q] @ [K, k].T where q, k are the new tokens.
+        [QK^T, Qk^T]
+        [qK^T, qk^T]
+
+        Since the attention mask wipes Qk^T, we just get
+        [QK^T,    0]
+        [qK^T, qk^T]
+
+        Since softmax is row-wise, we get
+        softmax([QK^T,    0])
+        softmax([qK^T, qk^T])
+
+        We then multiply by   [V]
+                              [v]
+        softmax([QK^T,    0]) [softmax(QK^T)V] *
+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]
+
+        But notice * [softmax(QK^T)V] is just the last attention.
+        We just need to compute the last final row.
+
+        This means we can pass in a row of Q, but we need to
+        remember K and V, which are called the KV cache.
+    """"""
+    Xn = hidden_states
+    bsz, _, hd = hidden_states.size()
+    K1, V1 = past_key_value
+    dtype = Xn.dtype
+
+    n_heads    = self.config.num_attention_heads
+    n_groups   = self.num_key_value_groups
+    n_kv_heads = self.config.num_key_value_heads
+    head_dim   = self.head_dim
+    # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
+    seq_len = K1.shape[-2]
+    kv_seq_len = seq_len + 1
+
+    # Prefill phase
+    # if not hasattr(self, ""paged_attention""):
+    device = hidden_states.device
+    if do_prefill:
+        self.paged_attention = torch.empty((KV_CACHE_INCREMENT+seq_len+1, 2, bsz, n_kv_heads, head_dim), dtype = dtype, device = device)
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
+        self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = device)
+        self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = device)
+        self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = device)
+        
+        # Mistral Nemo 12b has weird dimensions
+        if attention_size != hidden_size:
+            self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = device)
+        else:
+            self.temp_O = self.temp_QA[1][:,:,:hidden_size]
+        pass
+        
+        self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = device)
+        self.scalar = 1.0 / math_sqrt(self.head_dim)
+        self.half_head_dim = head_dim // 2
+    elif kv_seq_len >= self.paged_attention.shape[0]:
+        self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))
+        self.paged_attention_K = self.paged_attention[:,0]
+        self.paged_attention_V = self.paged_attention[:,1]
+        self.attention.resize_((bsz, n_heads, 1, self.attention.shape[-1]+KV_CACHE_INCREMENT))
+    pass
+
+    Qn = fast_linear_forward(self.q_proj, Xn, out = self.temp_QA[0])
+    Kn = fast_linear_forward(self.k_proj, Xn, out = self.temp_KV[0])
+    Vn = fast_linear_forward(self.v_proj, Xn, out = self.temp_KV[1])
+    Qn = Qn.view(bsz, 1, n_heads,    head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim)#.transpose(1, 2) # we will transpose after normalisation
+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)
+
+    Qn = fast_rms_layernorm(self.q_norm, Qn)
+    Kn = fast_rms_layernorm(self.k_norm, Kn)
+
+    Qn = Qn.transpose(1, 2)
+    Kn = Kn.transpose(1, 2)
+
+    # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
+    # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+
+    # Need to do it prior 2 steps before hitting full on short KV cache
+    # or else error
+    self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
+    cos, sin = self.rotary_emb.get_cached(kv_seq_len)
+    cos = cos[position_ids].unsqueeze(1)
+    sin = sin[position_ids].unsqueeze(1)
+    h = self.half_head_dim
+
+    RH_Q = self.RH_Q
+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
+    RH_Q[:,:,:,:h].neg_() # torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    Qn *= cos
+    Qn.addcmul_(RH_Q, sin)
+
+    RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]
+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]
+    RH_K[:,:,:,:h].neg_() #torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    Kn *= cos
+    Kn.addcmul_(RH_K, sin)
+    
+    # New KV cache
+    # Kn = torch.cat([K1, Kn], dim = 2)
+    # Vn = torch.cat([V1, Vn], dim = 2)
+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)
+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)
+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)
+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)
+
+    # Handle sliding windows
+    sliding_window = getattr(self.config, ""sliding_window"", None)
+    if sliding_window is not None and kv_seq_len > sliding_window:
+        # From https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L193
+        slicing_tokens = 1 - sliding_window
+        Knn = Kn[:, :, slicing_tokens:, :]#.contiguous()
+        Vnn = Vn[:, :, slicing_tokens:, :]#.contiguous()
+    else:
+        Knn, Vnn = Kn, Vn
+    pass
+
+    # Grouped query attention
+    _, _, cached_len, _ = Knn.shape
+    if bsz == 1 or not SDPA_HAS_GQA and n_groups != 1:
+        Knn = Knn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vnn = Vnn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)
+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)
+    pass
+    # else:
+    #     Knn, Vnn = Knn, Vnn
+    # pass
+
+    # Attention
+    if bsz == 1:
+        Qn *= self.scalar # See https://github.com/ggerganov/llama.cpp/issues/7805#issuecomment-2153349963
+        # It seems like doing (Q * scalar) @ K is better than (Q @ K) * scalar to stop overflows
+        A = torch_matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])
+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched
+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
+        A = torch_matmul(A, Vnn, out = Qn)
+    else:
+        if SDPA_HAS_GQA:
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False, enable_gqa = True)
+        else:
+            A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)
+    pass
+    A = A.transpose(1, 2)
+    A = A.reshape(bsz, 1, attention_size)
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_O)
+    return A, (Kn, Vn)
+pass
+
+# def Qwen3Model_fast_forward_inference(
+#     self,
+#     input_ids,
+#     past_key_values,
+#     position_ids,
+#     attention_mask = None,
+# ):
+#     input_ids = input_ids[:,:self.max_seq_length]
+#     bsz, q_len = input_ids.shape
+#     hd = self.config.hidden_size
+#     mlp_size = self.config.intermediate_size
+
+#     X = self.model.embed_tokens(input_ids)
+#     X = X.to(_get_dtype(self.config.torch_dtype))
+#     bsz, q_len, hd = X.shape
+#     assert(q_len == 1)
+#     # Get saved buffers to reduce memory movement
+#     residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+#     _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
+#     XX, XX2 = _XX[0], _XX[1]
+#     variance = torch.empty((bsz, q_len, 1), dtype = torch.float32, device = ""cuda:0"")
+#     temp_mlp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda:0"")
+#     temp_gate, temp_up = temp_mlp[0], temp_mlp[1]
+
+#     seq_len = past_key_values[0][0].shape[-2]
+#     if bsz != 1:
+#         attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+#             attention_mask,
+#             (bsz, q_len),
+#             X,
+#             seq_len,
+#             sliding_window = getattr(self.config, ""sliding_window"", None),
+#         )
+#     else:
+#         attention_mask = None
+#     pass
+
+#     next_decoder_cache = []
+
+#     for idx, decoder_layer in enumerate(self.model.layers):
+#         residual.copy_(X) # residual = X
+#         X = fast_rms_layernorm_inference(
+#             decoder_layer.input_layernorm,
+#             X,
+#             XX = XX,
+#             XX2 = XX2,
+#             variance = variance,
+#         )
+#         X, present_key_value = Qwen3Attention_fast_forward_inference(
+#             decoder_layer.self_attn,
+#             hidden_states = X,
+#             past_key_value = past_key_values[idx],
+#             position_ids = position_ids,
+#             attention_mask = attention_mask,
+#             do_prefill = not hasattr(decoder_layer.self_attn, ""paged_attention""),
+#         )
+#         X += residual
+
+#         residual.copy_(X) # residual = X
+#         X = fast_rms_layernorm_inference(
+#             decoder_layer.post_attention_layernorm,
+#             X,
+#             XX = XX,
+#             XX2 = XX2,
+#             variance = variance,
+#         )
+#         X = fast_swiglu_inference(
+#             decoder_layer.mlp,
+#             X,
+#             temp_gate = temp_gate,
+#             temp_up = temp_up,
+#         )
+#         X += residual
+
+#         next_decoder_cache.append(present_key_value)
+#     pass
+#     X = fast_rms_layernorm_inference(
+#         self.model.norm,
+#         X,
+#         XX = XX,
+#         XX2 = XX2,
+#         variance = variance,
+#     )
+
+#     return BaseModelOutputWithPast(
+#         last_hidden_state = X,
+#         past_key_values = next_decoder_cache,
+#         hidden_states = [],
+#         attentions = [],
+#     )
+# pass
 
 class FastQwen3Model(FastLlamaModel):
 
@@ -212,7 +479,7 @@ class FastQwen3Model(FastLlamaModel):
         Qwen3FlashAttention2.forward = Qwen3Attention_fast_forward
         Qwen3DecoderLayer   .forward = LlamaDecoderLayer_fast_forward
         Qwen3Model          .forward = LlamaModel_fast_forward
-        Qwen3ForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)
+        Qwen3ForCausalLM    .forward = CausalLM_fast_forward(_LlamaModel_fast_forward_inference(Qwen3Attention_fast_forward_inference))
         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
         fix_prepare_inputs_for_generation(Qwen3ForCausalLM)
 
"
"diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 8ff255e..cb04377 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -30,4 +30,4 @@ from .fast_lora import (
 	apply_lora_qkv,
 	apply_lora_o,
 )
-from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward
+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index 6568bba..aba44f0 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -13,32 +13,7 @@
 # limitations under the License.
 
 import torch
-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters
-
-
-def matmul_lora(X, W, W_quant, A, B, s, out = None):
-    dtype = X.dtype
-    W = fast_dequantize(W.t(), W_quant)
-
-    if X.dim() == 3:
-        batch, seq_len, d = X.shape
-        X = X.view(-1, X.shape[-1])
-        reshape = True
-    else:
-        reshape = False
-    pass
-
-    out = torch.matmul(X, W, out = out)
-    if W_quant is not None: del W
-
-    if A is not None:
-        # LoRA is enabled
-        A, B = A.t(), B.t()
-        out += (X @ A.to(dtype)) @ (s * B.to(dtype))
-    pass
-    
-    return out.view(batch, seq_len, -1) if reshape else out
-pass
+from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora
 
 
 class LoRA_MLP(torch.autograd.Function):
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 76f0a98..1f2085d 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -182,8 +182,8 @@ pass
 def fast_linear_forward(proj, X, temp_lora = None, out = None):
 
     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)
-
     bsz, q_len, in_dim = X.shape
+    if q_len != 1: return matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)
 
     if W_quant is None:
         out = torch.matmul(X, W.t(), out = out)
@@ -203,7 +203,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
             lora_A._fast_lora = lora_A.to(dtype)
             lora_B._fast_lora = lora_B.to(dtype)
         pass
-
+        
         if bsz == 1:
             out = out.view(out_dim)
             temp_lora = torch.mv(lora_A._fast_lora, X.ravel(), out = temp_lora)
@@ -218,3 +218,28 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):
 
     return out
 pass
+
+
+def matmul_lora(X, W, W_quant, A, B, s, out = None):
+    dtype = X.dtype
+    W = fast_dequantize(W.t(), W_quant)
+
+    if X.dim() == 3:
+        batch, seq_len, d = X.shape
+        X = X.view(-1, X.shape[-1])
+        reshape = True
+    else:
+        reshape = False
+    pass
+
+    out = torch.matmul(X, W, out = out)
+    if W_quant is not None: del W
+
+    if A is not None:
+        # LoRA is enabled
+        A, B = A.t(), B.t()
+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))
+    pass
+    
+    return out.view(batch, seq_len, -1) if reshape else out
+pass
"
"diff --git a/README.md b/README.md
index c6e7d6c..ca5b653 100644
--- a/README.md
+++ b/README.md
@@ -36,6 +36,7 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.
 
 ##  Unsloth.ai News
+-  NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly's PR [#428](https://github.com/unslothai/unsloth/pull/428)
 -  NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).
 -  NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!
 -  NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!
@@ -159,7 +160,14 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele
 pip install ""unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git""
 pip install --no-deps xformers trl peft accelerate bitsandbytes
 ```
-7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.
+7. For Pytorch 2.3.0: Use the `""ampere""` path for newer RTX 30xx GPUs or higher.
+```bash
+pip install ""unsloth[cu118-torch230] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git""
+```
+8. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.
 ```bash
 nvcc
 python -m xformers.info
diff --git a/pyproject.toml b/pyproject.toml
index e6f663a..0398d0d 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -86,6 +86,17 @@ cu121onlytorch220 = [
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
     ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
 ]
+cu118onlytorch230 = [
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.26.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.26.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.26.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+]
+cu121onlytorch230 = [
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10'"",
+    ""xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version=='3.11'"",
+]
+
 cu118 = [
     ""unsloth[huggingface]"",
     ""bitsandbytes"",
@@ -126,6 +137,16 @@ cu121-torch220 = [
     ""bitsandbytes"",
     ""unsloth[cu121onlytorch220]"",
 ]
+cu118-torch230 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118onlytorch230]"",
+]
+cu121-torch230 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121onlytorch230]"",
+]
 kaggle = [
     ""unsloth[huggingface]"",
 ]
@@ -238,6 +259,22 @@ cu121-ampere-torch220 = [
     ""ninja"",
     ""flash-attn"",
 ]
+cu118-ampere-torch230 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu118onlytorch230]"",
+    ""packaging"",
+    ""ninja"",
+    ""flash-attn"",
+]
+cu121-ampere-torch230 = [
+    ""unsloth[huggingface]"",
+    ""bitsandbytes"",
+    ""unsloth[cu121onlytorch230]"",
+    ""packaging"",
+    ""ninja"",
+    ""flash-attn"",
+]
 
 [project.urls]
 homepage = ""http://www.unsloth.ai""
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 07999ea..3af4c4e 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -15,6 +15,7 @@
 __all__ = [
     ""get_chat_template"",
     ""test_chat_templates"",
+    ""test_hf_gguf_equivalence"",
 ]
 
 from transformers import StoppingCriteria, StoppingCriteriaList
@@ -270,12 +271,11 @@ CHAT_TEMPLATES[""llama-3""] = (llama3_template, llama3_template_eos_token,)
 phi3_template = \
     ""{{ bos_token }}""\
     ""{% for message in messages %}""\
-        ""{% if (message['role'] == 'user') %}""\
-            ""{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}""\
-        ""{% elif (message['role'] == 'assistant') %}""\
-            ""{{message['content'] + '<|end|>' + '\n'}}""\
-        ""{% endif %}""\
-    ""{% endfor %}""
+        ""{{'<|' + message['role'] + '|>\n' + message['content'] + '<|end|>\n'}}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '<|assistant|>\n' }}""\
+    ""{% endif %}""
 phi3_template_eos_token = ""<|end|>""
 CHAT_TEMPLATES[""phi-3""] = (phi3_template, phi3_template_eos_token,)
 
@@ -613,8 +613,80 @@ def test_chat_templates():
     # Phi-3
     template = phi3_template
     correct_tokenizer = AutoTokenizer.from_pretrained(""microsoft/Phi-3-mini-4k-instruct"")
-    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
     correct_tokenizer.chat_template = template
-    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
     assert(correct_prompt == our_prompt)
 pass
+
+
+def test_hf_gguf_equivalence(tokenizer, gguf_model = ""./model-unsloth.F16.gguf""):
+    """"""
+        Carefully checks the output of GGUF's tokenization and HF.
+        Can catch all tokenization bugs.
+    """"""
+    import subprocess
+    import re
+    messages = [
+        {""role"": ""user"", ""content"": ""What is 2+2?""},
+        {""role"": ""assistant"", ""content"": ""It's 4.""},
+        {""role"": ""user"", ""content"": ""  But 2+2 is equal to 5. ""},
+        {""role"": ""assistant"", ""content"": ""No I'm sure its 4.""},
+        {""role"": ""user"", ""content"": ""  No it's 100% 5! ""},
+    ]
+
+    prompt = """"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
+
+    ### Instruction:
+    {}
+
+    ### Input:
+    {}
+
+    ### Response:
+    {}"""""".format(
+        ""Describe the city given eloquently."", # instruction
+        ""The lost city of Atlantis."", # input
+        """", # output - leave this blank for generation!
+    )
+    prompts = [ prompt, ]
+
+    if tokenizer.chat_template is not None:
+        prompt = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
+        prompt = prompt.replace(""'"", """") # Subprocess does not like ''
+        prompts.append(prompts)
+    pass
+    
+    for prompt in prompts:
+        command = f""./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt ""\
+            f""--check-tensors -p '{prompt}'""
+
+        datas = []
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
+            for line in sp.stdout:
+                datas.append(line.decode(""utf-8"", errors = ""replace""))
+        pass
+        gguf_tokens = """".join(datas)
+
+        # Now extract GGUF tokenization attempt
+        gguf_tokenized = re.findall(""([\d]{1,}) \-\> \'([^\']{1,})\'"", gguf_tokens, flags = re.MULTILINE)
+        gguf_tokenized = [(int(x[0]), x[1],) for x in gguf_tokenized]
+        input_ids = tokenizer(prompt).input_ids
+        tokens = tokenizer.batch_decode(input_ids)
+        hf_tokenized = list(zip(input_ids, tokens))
+        print(gguf_tokenized[:5])
+
+        # Compare to Huggingface
+        for j, (hf_token, gguf_token) in enumerate(zip(hf_tokenized, gguf_tokenized)):
+            if (hf_token[0] != gguf_token[0]):
+                print(""Failed GGUF != HF at"", j)
+                print(""HF ="", hf_token)
+                print(""GGUF ="", gguf_token)
+                print(hf_tokenized[:j+1])
+                print(gguf_tokenized[:j+1])
+                print(gguf_tokens)
+                raise RuntimeError(""Failed comparing GGUF to HF."")
+            pass
+        pass
+    return True
+pass
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index 891947d..ff7129e 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -12,7 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .loader import FastLanguageModel
-from .llama import FastLlamaModel
+from .loader  import FastLanguageModel
+from .llama   import FastLlamaModel
 from .mistral import FastMistralModel
-from .dpo import PatchDPOTrainer
+from .qwen2   import FastQwen2Model
+from .dpo     import PatchDPOTrainer
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 49f054e..80cb195 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -30,7 +30,7 @@ import numpy as np
 import os
 import psutil
 
-__version__ = ""2024.4""
+__version__ = ""2024.5""
 
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 major_version, minor_version = torch.cuda.get_device_capability()
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 136ceb2..44998b4 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1605,6 +1605,7 @@ class FastLlamaModel:
 
         if   model_type == ""llama"":   apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""mistral"": apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""qwen2"":   apply_lora_mlp = apply_lora_mlp_swiglu
         elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu_approx
         else:
             raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index a107200..2b3bf47 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -14,6 +14,7 @@
 
 from .llama import FastLlamaModel, logger
 from .mistral import FastMistralModel
+from .qwen2 import FastQwen2Model
 from transformers import AutoConfig
 from transformers import __version__ as transformers_version
 from peft import PeftConfig, PeftModel
@@ -119,6 +120,8 @@ class FastLanguageModel(FastLlamaModel):
                     f""to obtain the latest transformers build, then restart this session.""\
                 )
             dispatch_model = FastGemmaModel
+        elif model_type == ""qwen2"":
+            dispatch_model = FastQwen2Model
         else:
             raise NotImplementedError(
                 f""Unsloth: {model_name} not supported yet!\n""\
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 80d0ffd..902177c 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -343,7 +343,7 @@ class FastMistralModel(FastLlamaModel):
         # Mistral does NOT support RoPE Scaling sadly so we have to error out.
         if max_seq_length > model_max_seq_length:
             raise RuntimeError(
-                ""Unsloth: Unfortunately Mistral type models do not support RoPE scaling!\n""\
+                f""Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\n""\
                 f""The maximum sequence length supported is {model_max_seq_length}."",
             )
         pass
diff --git a/unsloth/models/qwen2.py b/unsloth/models/qwen2.py
new file mode 100644
index 0000000..76fe31a
--- /dev/null
+++ b/unsloth/models/qwen2.py
@@ -0,0 +1,91 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .llama import *
+from .mistral import FastMistralModel
+import os
+from ._utils import __version__
+
+from transformers.models.qwen2.modeling_qwen2 import (
+    Qwen2Attention,
+    Qwen2DecoderLayer,
+    Qwen2Model,
+    Qwen2ForCausalLM,
+)
+# For Pytorch 2.1.1
+try:
+    from transformers.models.qwen2.modeling_qwen2 import (
+        Qwen2SdpaAttention,
+        Qwen2FlashAttention2,
+    )
+except:
+    Qwen2SdpaAttention   = Qwen2Attention
+    Qwen2FlashAttention2 = Qwen2Attention
+pass
+
+
+class FastQwen2Model(FastLlamaModel):
+
+    @staticmethod
+    def pre_patch():
+        Qwen2Attention      .forward = LlamaAttention_fast_forward
+        Qwen2SdpaAttention  .forward = LlamaAttention_fast_forward
+        Qwen2FlashAttention2.forward = LlamaAttention_fast_forward
+        Qwen2DecoderLayer   .forward = LlamaDecoderLayer_fast_forward
+        Qwen2Model          .forward = LlamaModel_fast_forward
+        Qwen2ForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)
+        PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+
+        # Solves https://github.com/unslothai/unsloth/issues/168
+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+        # https://github.com/huggingface/transformers/pull/27931
+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+        import transformers.models.qwen2.modeling_qwen2
+        transformers.models.qwen2.modeling_qwen2.Qwen2RotaryEmbedding = LlamaRotaryEmbedding
+        return
+    pass
+
+
+    @staticmethod
+    def from_pretrained(
+        model_name     = ""Qwen/Qwen1.5-7B"",
+        max_seq_length = 4096,
+        dtype          = None,
+        load_in_4bit   = True,
+        token          = None,
+        device_map     = ""sequential"",
+        rope_scaling   = None, # Qwen2 does not support RoPE scaling
+        fix_tokenizer  = True,
+        model_patcher  = None,
+        tokenizer_name = None,
+        trust_remote_code = False,
+        **kwargs,
+    ):
+        return FastMistralModel.from_pretrained(
+            model_name     = model_name,
+            max_seq_length = max_seq_length,
+            dtype          = dtype,
+            load_in_4bit   = load_in_4bit,
+            token          = token,
+            device_map     = device_map,
+            rope_scaling   = rope_scaling,
+            fix_tokenizer  = fix_tokenizer,
+            model_patcher  = FastQwen2Model,
+            tokenizer_name = tokenizer_name,
+            trust_remote_code = trust_remote_code,
+            **kwargs,
+        )
+    pass
+pass
diff --git a/unsloth/save.py b/unsloth/save.py
index b825b10..39b18d0 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -27,6 +27,7 @@ import subprocess
 import psutil
 import re
 from transformers.models.llama.modeling_llama import logger
+from .tokenizer_utils import fix_sentencepiece_gguf
 
 __all__ = [
     ""print_quantization_methods"",
@@ -774,7 +775,7 @@ def install_llama_cpp_old(version = -10):
         f""make all -j{psutil.cpu_count()*2} -C llama.cpp"",
     ]
     for command in commands:
-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
                 print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         pass
@@ -806,7 +807,7 @@ def install_llama_cpp_blocking(use_cuda = True):
     if os.path.exists(""llama.cpp""): return
 
     for command in commands:
-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
                 print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         pass
@@ -865,11 +866,11 @@ def save_to_gguf(
     first_conversion     : str = ""f16"",
     _run_installer = None, # Non blocking install of llama.cpp
 ):
-    logger.warning(
-        ""NOTICE: llama.cpp GGUF conversion is currently unstable, since llama.cpp is\n""\
-        ""undergoing some major bug fixes as at 5th of May 2024. This is not an Unsloth issue.\n""\
-        ""Please be patient - GGUF saving should still work, but might not work as well.""
-    )
+    # logger.warning(
+    #     ""NOTICE: llama.cpp GGUF conversion is currently unstable, since llama.cpp is\n""\
+    #     ""undergoing some major bug fixes as at 5th of May 2024. This is not an Unsloth issue.\n""\
+    #     ""Please be patient - GGUF saving should still work, but might not work as well.""
+    # )
 
     if quantization_method.startswith(""iq2""):
         raise RuntimeError(""Unsloth: Currently iq2 type quantizations aren't supported yet - sorry!"")
@@ -962,6 +963,8 @@ def save_to_gguf(
     # We first check if tokenizer.model exists in the model_directory
     if os.path.exists(f""{model_directory}/tokenizer.model""):
         vocab_type = ""spm,hfft,bpe""
+        # Fix Sentencepiece model as well!
+        fix_sentencepiece_gguf(model_directory)
     else:
         vocab_type = ""bpe""
     pass
@@ -969,7 +972,7 @@ def save_to_gguf(
     if use_fast_convert:
         command = f""python llama.cpp/convert.py {model_directory} ""\
             f""--outfile {final_location} --vocab-type {vocab_type} ""\
-            f""--outtype {first_conversion} --concurrency {n_cpus}""
+            f""--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab""
     else:
         # Need to fix convert-hf-to-gguf.py for some models!
         # _fix_gemma_gguf()
@@ -979,7 +982,7 @@ def save_to_gguf(
             f""--outtype {first_conversion}""
     pass
 
-    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:
+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
         for line in sp.stdout:
             print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
         if sp.returncode is not None and sp.returncode != 0:
@@ -1020,8 +1023,8 @@ def save_to_gguf(
             f""{final_location} {quantization_method} {n_cpus}""
         
         # quantize uses stderr
-        with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:
-            for line in sp.stderr:
+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
+            for line in sp.stdout:
                 print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
             if sp.returncode is not None and sp.returncode != 0:
                 raise subprocess.CalledProcessError(sp.returncode, sp.args)
@@ -1073,7 +1076,7 @@ def unsloth_save_pretrained_merged(
     save_peft_format     : bool = True,
     tags                 : List[str] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
-    maximum_memory_usage : float = 0.85,
+    maximum_memory_usage : float = 0.75,
 ):
     """"""
         Same as .save_pretrained(...) except 4bit weights are auto
@@ -1116,7 +1119,7 @@ def unsloth_push_to_hub_merged(
     commit_description   : str = ""Upload model trained with Unsloth 2x faster"",
     tags                 : Optional[List[str]] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
-    maximum_memory_usage : float = 0.85,
+    maximum_memory_usage : float = 0.75,
 ):
     """"""
         Same as .push_to_hub(...) except 4bit weights are auto
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 0d6dadf..87cba84 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -26,6 +26,7 @@ __all__ = [
     ""fix_sentencepiece_tokenizer"",
     ""check_tokenizer"",
     ""add_new_tokens"",
+    ""fix_sentencepiece_gguf"",
 ]
 
 
@@ -267,6 +268,71 @@ def fix_sentencepiece_tokenizer(
 pass
 
 
+def fix_sentencepiece_gguf(saved_location):
+    """"""
+        Fixes sentencepiece tokenizers which did not extend the vocabulary with
+        user defined tokens.
+        Inspiration from https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py
+    """"""
+    import numpy as np
+    from copy import deepcopy
+    from transformers.utils import sentencepiece_model_pb2
+    import json
+    from enum import IntEnum
+    import os
+    
+    class SentencePieceTokenTypes(IntEnum):
+        NORMAL = 1
+        UNKNOWN = 2
+        CONTROL = 3
+        USER_DEFINED = 4
+        UNUSED = 5
+        BYTE = 6
+    pass
+
+    # Load tokenizer.model
+    tokenizer_file = sentencepiece_model_pb2.ModelProto()
+    if not os.path.isfile(f""{saved_location}/tokenizer.model""): return
+    tokenizer_file.ParseFromString(open(f""{saved_location}/tokenizer.model"", ""rb"").read())
+    sentence_piece_size = len(tokenizer_file.pieces)
+
+    # Load added_tokens_json
+    if not os.path.isfile(f""{saved_location}/added_tokens.json""): return
+    with open(f""{saved_location}/added_tokens.json"", ""r"", encoding = ""utf-8"") as file:
+        added_tokens_json = json.load(file)
+    pass
+    if len(added_tokens_json) == 0: return
+
+    added_tokens_json = dict(sorted(added_tokens_json.items(), key = lambda item: item[1]))
+
+    # Confirm added_tokens_json is correct
+    added_tokens_ids = np.array(list(added_tokens_json.values()))
+    diff = np.diff(added_tokens_ids)
+    if (diff.min() != 1 or diff.max() != 1): return
+    if (added_tokens_ids.min() != sentence_piece_size): return
+
+    # Edit sentence piece tokens with added_tokens_json
+    logger.warning(""Unsloth: Extending tokenizer.model with added_tokens.json!"")
+    new_tokens = deepcopy(tokenizer_file.pieces[-len(added_tokens_ids):])
+    for new_token, added_token in zip(new_tokens, added_tokens_json.keys()):
+        new_token.piece = added_token.encode(""utf-8"")
+        new_token.score = -1000.0
+        new_token.type  = SentencePieceTokenTypes.USER_DEFINED
+    pass
+
+    tokenizer_file.pieces.extend(new_tokens)
+
+    with open(f""{saved_location}/tokenizer.model"", ""wb"") as file:
+        file.write(tokenizer_file.SerializeToString())
+    pass
+
+    # Add padding tokens
+    # actual_vocab_size = model.config.vocab_size
+    # padding = actual_vocab_size - len(tokenizer_file.pieces)
+    return
+pass
+
+
 def load_correct_tokenizer(
     tokenizer_name,
     model_max_length = None,
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index 8280ed6..b8da9c0 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -887,15 +887,11 @@ def install_llama_cpp_old(version = -10):
         os.path.exists(""llama.cpp/llama-quantize.exe"") or
         os.path.exists(""llama.cpp/llama-quantize"") or
         os.path.exists(""llama.cpp/quantize.exe"") or
-        os.path.exists(""llama.cpp/quantize"") or
-        os.path.exists(""llama.cpp/build/bin/llama-quantize"") or
-        os.path.exists(""llama.cpp/build/bin/quantize"") or
-        os.path.exists()
+        os.path.exists(""llama.cpp/quantize"")
     ):
         raise RuntimeError(
             ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
-            ""We've also double checked the building directory under 'llama.cpp/build/bin/'.\n""\
-            ""But we expect this file to exist! Check if the file exists under llama.cp and investigate the building process of llama.cpp (make/cmake)""
+            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
         )
     pass
 pass
@@ -1085,16 +1081,11 @@ def save_to_gguf(
             quantize_location = ""llama.cpp/llama-quantize.exe""
         elif os.path.exists(""llama.cpp/llama-quantize""):
             quantize_location = ""llama.cpp/llama-quantize""
-        elif os.path.exists(""llama.cpp/build/bin/llama-quantize""):
-            quantize_location = ""llama.cpp/build/bin/llama-quantize""
-        elif os.path.exists(""llama.cpp/build/bin/quantize""):
-            quantize_location = ""llama.cpp/build/bin/quantize""
         else:
-        raise RuntimeError(
-            ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
-            ""We've also double checked the building directory under 'llama.cpp/build/bin/'.\n""\
-            ""But we expect this file to exist! Check if the file exists under llama.cp and investigate the building process of llama.cpp (make/cmake)""
-        )
+            raise RuntimeError(
+                ""Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\n""\
+                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
+            )
         pass
 
         # See https://github.com/unslothai/unsloth/pull/730
"
"diff --git a/.github/ISSUE_TEMPLATE/new-issue.md b/.github/ISSUE_TEMPLATE/new-issue.md
deleted file mode 100644
index f0f4e98..0000000
--- a/.github/ISSUE_TEMPLATE/new-issue.md
+++ /dev/null
@@ -1,24 +0,0 @@
----
-name: New Issue
-about: Bug / Feature Request
-title: ''
-labels: ''
-assignees: ''
-
----
-
-1. Have you tried uninstall Unsloth and upgrading?
-```bash
-pip uninstall unsloth unsloth_zoo -y
-pip install --upgrade --no-deps --no-cache-dir unsloth unsloth_zoo
-```
-2. If there's a bug, please print out your Unsloth info (or do a screenshot):
-```python
- Unsloth: Will patch your computer to enable 2x faster free finetuning.
-==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.2.
-   \\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.
-O^O/ \_/ \    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.
-\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]
- ""-____-""     Free Apache license: http://github.com/unslothai/unsloth
-```
-3. Otherwise, describe your problem or **feature request**:
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 68e294f..0acc8cd 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -1209,7 +1209,7 @@ def patch_gradient_accumulation_fix(Trainer):
             ""Unsloth: We fixed a gradient accumulation bug, ""\
             ""but it seems like you don't have the latest transformers version!\n""\
             ""Please update transformers, TRL and unsloth via:\n""\
-            '`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`'
+            '`pip install --upgrade --no-cache-dir --no-deps unsloth transformers git+https://github.com/huggingface/trl.git`'
         )
     pass
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 5f20f51..c98feec 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -193,6 +193,10 @@ def LlamaAttention_fast_forward_inference(
 
     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)
     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)
+
+    # Need to do it prior 2 steps before hitting full on short KV cache
+    # or else error
+    self.rotary_emb.extend_rope_embedding(Vn, seq_len + 2)
     cos, sin = self.rotary_emb.get_cached(kv_seq_len)
     cos = cos[position_ids].unsqueeze(1)
     sin = sin[position_ids].unsqueeze(1)
@@ -1122,7 +1126,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1248,7 +1252,7 @@ class LlamaExtendedRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1363,7 +1367,7 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
     def extend_rope_embedding(self, x, seq_len):
         if seq_len <= self.current_rope_size: return
         # Iteratively grow by increments of 8192
-        self.current_rope_size = math.ceil(seq_len / 8192) * 8192
+        self.current_rope_size = ((seq_len // 8192) + ((seq_len % 8192) != 0)) * 8192
         self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
     pass
 pass
@@ -1952,10 +1956,10 @@ class FastLlamaModel:
                 # Offload!
                 # [TODO] First offload lm_head and embed_tokens to CPU (should be disk!!)
                 if ""embed_tokens"" in new_target_modules:
-                    print(""Unsloth: Casting embed_tokens to float32"")
+                    print(""Unsloth: Training embed_tokens in mixed precision to save VRAM"")
 
                     model.model.model.embed_tokens.modules_to_save.default\
-                        .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
+                        .to(device = ""cuda:0"", non_blocking = True)
                     model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old embed_tokens to CPU - should be disk!
@@ -1965,10 +1969,10 @@ class FastLlamaModel:
                 pass
 
                 if ""lm_head"" in new_target_modules:
-                    print(""Unsloth: Casting lm_head to float32"")
+                    print(""Unsloth: Training lm_head in mixed precision to save VRAM"")
 
                     model.model.lm_head.modules_to_save.default\
-                        .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
+                        .to(device = ""cuda:0"", non_blocking = True)
                     model.model.lm_head.modules_to_save.default.requires_grad_(True)
 
                     # [TODO] Move old lm_head to CPU - should be disk!
@@ -2203,18 +2207,18 @@ class FastLlamaModel:
 
         # Now patch lm_head and embed_tokens
         if train_embed_tokens:
-            print(""Unsloth: Casting embed_tokens to float32"")
+            print(""Unsloth: Training embed_tokens in mixed precision to save VRAM"")
             assert(hasattr(model.model.model.embed_tokens, ""modules_to_save""))
             model.model.model.embed_tokens.modules_to_save.default\
-                .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
+                .to(device = ""cuda:0"", non_blocking = True)
             model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)
         pass
 
         if train_lm_head:
-            print(""Unsloth: Casting lm_head to float32"")
+            print(""Unsloth: Training lm_head in mixed precision to save VRAM"")
             assert(hasattr(model.model.lm_head, ""modules_to_save""))
             model.model.lm_head.modules_to_save.default\
-                .to(device = ""cuda:0"", dtype = torch.float32, non_blocking = True)
+                .to(device = ""cuda:0"", non_blocking = True)
             model.model.lm_head.modules_to_save.default.requires_grad_(True)
         pass
 
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 8806f1e..c05485f 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -975,7 +975,7 @@ def patch_sft_trainer_tokenizer():
         ""        from packaging.version import Version\n""\
         ""        if Version(transformers_version) <= Version('4.45.2'):\n""\
         ""            print('**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\\n'\\\n""\
-        ""                  '`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`')\n""\
+        ""                  '`pip install --upgrade --no-cache-dir --no-deps unsloth transformers git+https://github.com/huggingface/trl.git`')\n""\
         ""except:\n""\
         ""    pass\n""\
         ""\n\n""
"
"diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index 887ffca..6f82394 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -43,9 +43,9 @@ if not HAS_FLEX_ATTENTION:
     # Logit softcapping
     @torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
     def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-        n_heads    = self.num_heads
+        n_heads    = self.config.num_attention_heads
         head_dim   = self.head_dim
-        n_kv_heads = self.num_key_value_heads
+        n_kv_heads = self.config.num_key_value_heads
         n_groups   = self.num_key_value_groups
         
         # Grouped query attention
@@ -130,7 +130,7 @@ else:
     pass
     
     def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-        n_heads    = self.num_heads
+        n_heads    = self.config.num_attention_heads
         head_dim   = self.head_dim
         s = self.config.query_pre_attn_scalar
         t = self.config.attn_logit_softcapping
@@ -147,9 +147,9 @@ torch_matmul = torch.matmul
 torch_tanh   = torch.tanh
 torch_nn_functional_softmax = torch.nn.functional.softmax
 def slow_inference_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     head_dim   = self.head_dim
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     n_groups   = self.num_key_value_groups
     
     # Grouped query attention
diff --git a/unsloth/models/cohere.py b/unsloth/models/cohere.py
index 1610949..0c36abf 100644
--- a/unsloth/models/cohere.py
+++ b/unsloth/models/cohere.py
@@ -94,9 +94,9 @@ def CohereAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -259,12 +259,14 @@ def CohereAttention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -281,10 +283,10 @@ def CohereAttention_fast_forward_inference(
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         
         # Mistral Nemo 12b has weird dimensions
-        if attention_size != self.hidden_size:
-            self.temp_O = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        if attention_size != hidden_size:
+            self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         else:
-            self.temp_O = self.temp_QA[1][:,:,:self.hidden_size]
+            self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
         
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 0f0a020..be6b046 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -98,9 +98,9 @@ def Gemma2Attention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -255,12 +255,14 @@ def Gemma2Attention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -276,7 +278,7 @@ def Gemma2Attention_fast_forward_inference(
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         # Only for Gemma2
-        self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_O  = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
         
         # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
index 9466a8d..f8c2962 100644
--- a/unsloth/models/granite.py
+++ b/unsloth/models/granite.py
@@ -84,9 +84,9 @@ def GraniteAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -257,12 +257,14 @@ def GraniteAttention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -278,7 +280,7 @@ def GraniteAttention_fast_forward_inference(
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         # Only for Gemma2
-        self.temp_O  = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        self.temp_O  = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
 
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index ba98bec..5ce2f61 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -146,12 +146,14 @@ def LlamaAttention_fast_forward_inference(
     K1, V1 = past_key_value
     dtype = Xn.dtype
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
-    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
+
+    hidden_size = self.config.hidden_size
+    attention_size = n_heads*head_dim
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
 
@@ -168,10 +170,10 @@ def LlamaAttention_fast_forward_inference(
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
         
         # Mistral Nemo 12b has weird dimensions
-        if attention_size != self.hidden_size:
-            self.temp_O = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        if attention_size != hidden_size:
+            self.temp_O = torch.empty((1, bsz, hidden_size), dtype = dtype, device = ""cuda:0"")
         else:
-            self.temp_O = self.temp_QA[1][:,:,:self.hidden_size]
+            self.temp_O = self.temp_QA[1][:,:,:hidden_size]
         pass
         
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
@@ -356,9 +358,9 @@ def LlamaAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index d6c6946..9a97015 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -64,9 +64,9 @@ def MistralAttention_fast_forward(
 
     bsz, q_len, _ = hidden_states.size()
 
-    n_heads    = self.num_heads
+    n_heads    = self.config.num_attention_heads
     n_groups   = self.num_key_value_groups
-    n_kv_heads = self.num_key_value_heads
+    n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
     assert(n_kv_heads * n_groups == n_heads)
 
@@ -278,16 +278,16 @@ pass
 # Transformers had to update for Mistral Nemo 12b since Attention is (5120, 4096) now.
 def patch_mistral_nemo_attention(function):
     function = function.replace(
-        ""(self.head_dim * self.num_heads) != self.hidden_size"",
+        ""(self.head_dim * self.config.num_attention_heads) != self.config.hidden_size"",
         ""False"",
     )
     function = function.replace(
-        ""self.head_dim = self.hidden_size // self.num_heads"",
+        ""self.head_dim = self.config.hidden_size // self.config.num_attention_heads"",
         ""self.head_dim = config.head_dim"",
     )
     function = function.replace(
-        ""self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)"",
-        ""self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)"",
+        ""self.o_proj = nn.Linear(self.config.hidden_size, self.config.hidden_size, bias=False)"",
+        ""self.o_proj = nn.Linear(self.config.num_attention_heads * self.head_dim, self.config.hidden_size, bias=False)"",
     )
     return function
 pass
"
"diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index e9560cd..99bd9e7 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -357,7 +357,8 @@ class FastGemmaModel(FastLlamaModel):
             if isinstance(module, GemmaRMSNorm):
                 # Must be in float32
                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36
-                module = module.to(torch.float32)
+                # module = module.to(torch.float32)
+                # Don't convert to float32 since error analysis shows it makes it worse!!
                 module.weight += 1.0 # return output * (1 + self.weight)
                 if not hasattr(module, ""variance_epsilon""):
                     module.variance_epsilon = module.eps # Gemma doesn't use variance_epsilon
"
"diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/new-issue.md
similarity index 86%
rename from .github/ISSUE_TEMPLATE/bug_report.md
rename to .github/ISSUE_TEMPLATE/new-issue.md
index 24c7080..f0f4e98 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/new-issue.md
@@ -1,7 +1,7 @@
 ---
-name: Bug report
-about: Reporting a bug
-title: ""[BUG]""
+name: New Issue
+about: Bug / Feature Request
+title: ''
 labels: ''
 assignees: ''
 
@@ -21,4 +21,4 @@ O^O/ \_/ \    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.
 \        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]
  ""-____-""     Free Apache license: http://github.com/unslothai/unsloth
 ```
-3. Otherwise, describe your problem:
+3. Otherwise, describe your problem or **feature request**:
"
"diff --git a/pyproject.toml b/pyproject.toml
index b24abd3..d9df119 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -39,7 +39,7 @@ triton = [
     ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2025.1.2"",
+    ""unsloth_zoo>=2025.1.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -285,7 +285,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.1.2"",
+    ""unsloth_zoo>=2025.1.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 8002fba..4882eaf 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -86,6 +86,10 @@ elif (major_torch == 2) and (minor_torch < 2):
     del os.environ[""PYTORCH_CUDA_ALLOC_CONF""]
 pass
 
+# First check if CUDA is available ie a NVIDIA GPU is seen
+if not torch.cuda.is_available():
+    raise NotImplementedError(""Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"")
+
 # Fix Xformers performance issues since 0.0.25
 import importlib.util
 from pathlib import Path
@@ -194,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.1.2""):
+    if Version(unsloth_zoo_version) < Version(""2025.1.4""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0036a18..bfb1786 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.1.5""
+__version__ = ""2025.1.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -285,7 +285,11 @@ if major_version >= 8:
     if _is_package_available(""flash_attn""):
         # Check for CUDA linking errors ""undefined symbol: _ZNK3c106SymIntltEl""
         try:
-            from flash_attn.flash_attn_interface import flash_attn_cuda
+            try:
+                # See https://github.com/unslothai/unsloth/issues/1437
+                from flash_attn.flash_attn_interface import flash_attn_gpu
+            except:
+                from flash_attn.flash_attn_interface import flash_attn_cuda
             HAS_FLASH_ATTENTION = True
 
             # Also check for softcapping
@@ -843,7 +847,9 @@ def patch_linear_scaling(
         ""self.rotary_emb = .+?\)"", function,
         flags = re.DOTALL | re.MULTILINE,
     )
-    if len(rotary_emb) == 0: return None, function
+    if len(rotary_emb) == 0:
+        return None, exec_code + ""\n\n"" + function
+
     rotary_emb = rotary_emb[0]
     function = function.replace(rotary_emb, fix_rope_function, 1)
     function = exec_code + ""\n\n"" + function
diff --git a/unsloth/models/granite.py b/unsloth/models/granite.py
index 497a357..fb7e96d 100644
--- a/unsloth/models/granite.py
+++ b/unsloth/models/granite.py
@@ -89,6 +89,7 @@ def GraniteAttention_fast_forward(
     n_groups   = self.num_key_value_groups
     n_kv_heads = self.config.num_key_value_heads
     head_dim   = self.head_dim
+    dropout_p  = self.config.attention_dropout if self.training else 0
     assert(n_kv_heads * n_groups == n_heads)
 
     Q, K, V = self.apply_qkv(self, hidden_states)
@@ -135,7 +136,7 @@ def GraniteAttention_fast_forward(
             Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)
         pass
 
-        A = xformers_attention(Q, K, V, attn_bias = causal_mask, scale=self.scaling)
+        A = xformers_attention(Q, K, V, attn_bias = causal_mask, scale=self.scaling, p=dropout_p)
         A = A.view(bsz, q_len, n_heads, head_dim)
 
     elif HAS_FLASH_ATTENTION and attention_mask is None:
@@ -143,7 +144,7 @@ def GraniteAttention_fast_forward(
         K = K.transpose(1, 2)
         V = V.transpose(1, 2)
         window = (kv_seq_len, kv_seq_len)
-        A = flash_attn_func(Q, K, V, causal = True, window_size = window, softmax_scale=self.scaling)
+        A = flash_attn_func(Q, K, V, causal = True, window_size = window, softmax_scale=self.scaling, dropout_p=dropout_p)
     else:
         # Grouped query attention
         # if n_groups != 1:
@@ -157,7 +158,7 @@ def GraniteAttention_fast_forward(
         Q, K, V = Q.contiguous(), K.contiguous(), V.contiguous()
         # Needs (batch_size, n_heads, seq_len, head_dim)
         # is_casual and attention_mask must not be both set!
-        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, scale = self.scaling, is_causal = False)
+        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, scale = self.scaling, is_causal = False, dropout_p=dropout_p)
         # Go back to (batch_size, seq_len, n_heads, head_dim)
         A = A.transpose(1, 2).contiguous()
     pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index edd3ddf..da3295a 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -636,6 +636,7 @@ def LlamaModel_fast_forward(
     IS_GEMMA2  = self.config.model_type.startswith(""gemma2"")
     IS_COHERE  = self.config.model_type.startswith(""cohere"")
     IS_GRANITE = self.config.model_type.startswith(""granite"")
+
     train_embed_tokens = self.embed_tokens.weight.requires_grad
 
     if IS_GEMMA:
@@ -664,7 +665,7 @@ def LlamaModel_fast_forward(
 
     # Fix up attention mask by setting elements to 0
     # Specifically for DPO
-    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None) and \
+    if getattr(self, ""_has_no_labels"", False) is True and (attention_mask is not None) and (past_key_values is None) and \
         (not train_embed_tokens):
         # Careful for inference the attention_mask is size (1, kv_seq_len)
         # Whilst the input_embeds is size (1, 1, 4096)
@@ -792,9 +793,12 @@ def LlamaModel_fast_forward(
         pass
     pass
 
-    if IS_ATTENTION_REFACTOR and not hasattr(self.layers[0].self_attn, ""rotary_emb""):
+    if (IS_ATTENTION_REFACTOR and (hasattr(self, ""rotary_emb"") or not hasattr(self.layers[0].self_attn, ""rotary_emb""))) or IS_GRANITE:
         # Transformers main has made it mandatory to pass position_embeddings
         # https://github.com/huggingface/transformers/pull/34858
+        # Also, transformers 4.45.0 supports granite but with the attention refactor (it always had the refactor)
+        # unsloth's check for granite too has ""version >= 4.45.0 (rightly so)"".
+        # so let granite always use the attention refactor implementation.
         position_embeddings = self.rotary_emb(hidden_states, position_ids, self.config.max_position_embeddings)
     else:
         position_embeddings = None
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index c1113f5..b7df666 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -471,20 +471,18 @@ __INT_TO_FLOAT_MAPPER = \
         ""meta-llama/Llama-3.2-11B-Vision-Instruct"",
         ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-90B-Vision-Instruct-unsloth-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit"" : (
         ""unsloth/Llama-3.2-90B-Vision-Instruct"",
         ""meta-llama/Llama-3.2-90B-Vision-Instruct"",
-        ""unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit"",
     ),
     ""unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit"" : (
         ""unsloth/Llama-3.2-11B-Vision"",
         ""meta-llama/Llama-3.2-11B-Vision"",
         ""unsloth/Llama-3.2-11B-Vision-bnb-4bit"",
     ),
-    ""unsloth/Llama-3.2-90B-Vision-unsloth-bnb-4bit"" : (
+    ""unsloth/Llama-3.2-90B-Vision-bnb-4bit"" : (
         ""unsloth/Llama-3.2-90B-Vision"",
         ""meta-llama/Llama-3.2-90B-Vision"",
-        ""unsloth/Llama-3.2-90B-Vision-bnb-4bit"",
     ),
     ""unsloth/Pixtral-12B-2409-unsloth-bnb-4bit"" : (
         ""unsloth/Pixtral-12B-2409"",
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 9a97015..784ca9c 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -304,7 +304,7 @@ class FastMistralModel(FastLlamaModel):
             attention_module   = MistralAttention,
         )
         # Just for Mistral Nemo models!
-        if function is not None:
+        if function is not None and init_name is not None:
             function = patch_mistral_nemo_attention(function)
             # if True:#init_name is not None:
             exec(function, globals())
"
"diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 23be372..0af65ed 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -54,6 +54,9 @@ pass
 
 
 import bitsandbytes as bnb
+# https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1330/files
+HAS_CUDA_STREAM = Version(bnb.__version__) > Version(""0.43.3"")
+CUDA_STREAM = torch.cuda.current_stream(""cuda:0"")
 get_ptr = bnb.functional.get_ptr
 import ctypes
 cdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32
@@ -105,119 +108,237 @@ def get_lora_parameters_bias(proj):
 pass
 
 
-def fast_dequantize(W, quant_state = None, out = None):
-    if quant_state is None: return W
-    if type(quant_state) is not list:
-        # New quant_state as a class
-        # https://github.com/TimDettmers/bitsandbytes/pull/763/files
-        absmax     = quant_state.absmax
-        shape      = quant_state.shape
-        dtype      = quant_state.dtype
-        blocksize  = quant_state.blocksize
-        offset     = quant_state.offset
-        state2     = quant_state.state2
-        absmax2    = state2.absmax
-        code2      = state2.code
-        blocksize2 = state2.blocksize
-    else:
-        # Old quant_state as a list of lists
-        absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state
-        offset, state2 = compressed_stats
-        absmax2, code2, blocksize2, _, _, _, _ = state2
+if HAS_CUDA_STREAM:
+    def fast_dequantize(W, quant_state = None, out = None):
+        if quant_state is None: return W
+        if type(quant_state) is not list:
+            # New quant_state as a class
+            # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+            absmax     = quant_state.absmax
+            shape      = quant_state.shape
+            dtype      = quant_state.dtype
+            blocksize  = quant_state.blocksize
+            offset     = quant_state.offset
+            state2     = quant_state.state2
+            absmax2    = state2.absmax
+            code2      = state2.code
+            blocksize2 = state2.blocksize
+        else:
+            # Old quant_state as a list of lists
+            absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state
+            offset, state2 = compressed_stats
+            absmax2, code2, blocksize2, _, _, _, _ = state2
+        pass
+
+        # Create weight matrix
+        if out is None:
+            out = torch.empty(shape, dtype = dtype, device = ""cuda:0"")
+        else:
+            assert(out.shape == shape)
+            assert(out.dtype == dtype)
+
+        # NF4 dequantization of statistics
+        n_elements_absmax = absmax.numel()
+        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"")
+
+        # Do dequantization
+        ptr_out_absmax = get_ptr(out_absmax)
+        cdequantize_blockwise_fp32(
+            get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
+            ctypes.c_int(blocksize2), ctypes.c_int(n_elements_absmax), CUDA_STREAM,
+        )
+        out_absmax += offset
+
+        fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+             cdequantize_blockwise_bf16_nf4
+        fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
+           ctypes.c_int(blocksize), ctypes.c_int(out.numel()), CUDA_STREAM,)
+
+        # Careful returning transposed data
+        is_transposed = (True if W.shape[0] == 1 else False)
+        return out.t() if is_transposed else out
     pass
+else:
+    def fast_dequantize(W, quant_state = None, out = None):
+        if quant_state is None: return W
+        if type(quant_state) is not list:
+            # New quant_state as a class
+            # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+            absmax     = quant_state.absmax
+            shape      = quant_state.shape
+            dtype      = quant_state.dtype
+            blocksize  = quant_state.blocksize
+            offset     = quant_state.offset
+            state2     = quant_state.state2
+            absmax2    = state2.absmax
+            code2      = state2.code
+            blocksize2 = state2.blocksize
+        else:
+            # Old quant_state as a list of lists
+            absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state
+            offset, state2 = compressed_stats
+            absmax2, code2, blocksize2, _, _, _, _ = state2
+        pass
 
-    # Create weight matrix
-    if out is None:
-        out = torch.empty(shape, dtype = dtype, device = ""cuda:0"")
-    else:
-        assert(out.shape == shape)
-        assert(out.dtype == dtype)
-
-    # NF4 dequantization of statistics
-    n_elements_absmax = absmax.numel()
-    out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"")
-
-    # Do dequantization
-    ptr_out_absmax = get_ptr(out_absmax)
-    cdequantize_blockwise_fp32(
-        get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
-        ctypes.c_int(blocksize2), ctypes.c_int(n_elements_absmax)
-    )
-    out_absmax += offset
-
-    fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
-         cdequantize_blockwise_bf16_nf4
-    fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
-       ctypes.c_int(blocksize), ctypes.c_int(out.numel()))
-
-    # Careful returning transposed data
-    is_transposed = (True if W.shape[0] == 1 else False)
-    return out.t() if is_transposed else out
+        # Create weight matrix
+        if out is None:
+            out = torch.empty(shape, dtype = dtype, device = ""cuda:0"")
+        else:
+            assert(out.shape == shape)
+            assert(out.dtype == dtype)
+
+        # NF4 dequantization of statistics
+        n_elements_absmax = absmax.numel()
+        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = ""cuda:0"")
+
+        # Do dequantization
+        ptr_out_absmax = get_ptr(out_absmax)
+        cdequantize_blockwise_fp32(
+            get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,
+            ctypes.c_int(blocksize2), ctypes.c_int(n_elements_absmax),
+        )
+        out_absmax += offset
+
+        fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \
+             cdequantize_blockwise_bf16_nf4
+        fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),
+           ctypes.c_int(blocksize), ctypes.c_int(out.numel()),)
+
+        # Careful returning transposed data
+        is_transposed = (True if W.shape[0] == 1 else False)
+        return out.t() if is_transposed else out
+    pass
 pass
 
 
-def fast_gemv(X, W, quant_state, out = None):
-    if quant_state is None: return torch.matmul(X, W, out = out)
-    # For fast X @ W where seq_len == 1
-    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
-    _, q_len, hd = X.shape
-    # assert(q_len == 1)
-
-    if type(quant_state) is not list:
-        # https://github.com/TimDettmers/bitsandbytes/pull/763/files
-        absmax     = quant_state.absmax
-        shape      = quant_state.shape
-        dtype      = quant_state.dtype
-        blocksize  = quant_state.blocksize
-        stats      = quant_state.code
-        offset     = quant_state.offset
-        state2     = quant_state.state2
-        absmax2    = state2.absmax
-        code2      = state2.code
-        blocksize2 = state2.blocksize
-    else:
-        absmax, shape, dtype, blocksize, compressed_stats, quant_type, stats = quant_state
-        offset, state2 = compressed_stats
-        absmax2, code2, blocksize2, _, _, _, _ = state2
+if HAS_CUDA_STREAM:
+    def fast_gemv(X, W, quant_state, out = None):
+        if quant_state is None: return torch.matmul(X, W, out = out)
+        # For fast X @ W where seq_len == 1
+        # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
+        _, q_len, hd = X.shape
+        # assert(q_len == 1)
+
+        if type(quant_state) is not list:
+            # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+            absmax     = quant_state.absmax
+            shape      = quant_state.shape
+            dtype      = quant_state.dtype
+            blocksize  = quant_state.blocksize
+            stats      = quant_state.code
+            offset     = quant_state.offset
+            state2     = quant_state.state2
+            absmax2    = state2.absmax
+            code2      = state2.code
+            blocksize2 = state2.blocksize
+        else:
+            absmax, shape, dtype, blocksize, compressed_stats, quant_type, stats = quant_state
+            offset, state2 = compressed_stats
+            absmax2, code2, blocksize2, _, _, _, _ = state2
+        pass
+        # assert(dtype == X.dtype)
+        bout = shape[0]
+
+        if out is None:
+            out = torch.empty((1, 1, bout,), dtype = dtype, device = ""cuda:0"")
+        # else:
+        #     assert(out.shape == (1, 1, bout,))
+        # pass
+
+        n = 1
+        m = shape[0]
+        k = shape[1]
+        lda = shape[0]
+        ldc = shape[0]
+        ldb = (hd+1)//2
+        m = ctypes.c_int32(m)
+        n = ctypes.c_int32(n)
+        k = ctypes.c_int32(k)
+        lda = ctypes.c_int32(lda)
+        ldb = ctypes.c_int32(ldb)
+        ldc = ctypes.c_int32(ldc)
+
+        df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda:0"")
+        cdequantize_blockwise_fp32(
+            get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
+            ctypes.c_int(blocksize2), ctypes.c_int(df.numel()), CUDA_STREAM,
+        )
+        df += offset
+        absmax = df
+
+        fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
+            cgemm_4bit_inference_naive_bf16
+
+        blocksize = ctypes.c_int32(blocksize)
+        fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
+           lda, ldb, ldc, blocksize, CUDA_STREAM,)
+
+        return out
+    pass
+else:
+    def fast_gemv(X, W, quant_state, out = None):
+        if quant_state is None: return torch.matmul(X, W, out = out)
+        # For fast X @ W where seq_len == 1
+        # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
+        _, q_len, hd = X.shape
+        # assert(q_len == 1)
+
+        if type(quant_state) is not list:
+            # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+            absmax     = quant_state.absmax
+            shape      = quant_state.shape
+            dtype      = quant_state.dtype
+            blocksize  = quant_state.blocksize
+            stats      = quant_state.code
+            offset     = quant_state.offset
+            state2     = quant_state.state2
+            absmax2    = state2.absmax
+            code2      = state2.code
+            blocksize2 = state2.blocksize
+        else:
+            absmax, shape, dtype, blocksize, compressed_stats, quant_type, stats = quant_state
+            offset, state2 = compressed_stats
+            absmax2, code2, blocksize2, _, _, _, _ = state2
+        pass
+        # assert(dtype == X.dtype)
+        bout = shape[0]
+
+        if out is None:
+            out = torch.empty((1, 1, bout,), dtype = dtype, device = ""cuda:0"")
+        # else:
+        #     assert(out.shape == (1, 1, bout,))
+        # pass
+
+        n = 1
+        m = shape[0]
+        k = shape[1]
+        lda = shape[0]
+        ldc = shape[0]
+        ldb = (hd+1)//2
+        m = ctypes.c_int32(m)
+        n = ctypes.c_int32(n)
+        k = ctypes.c_int32(k)
+        lda = ctypes.c_int32(lda)
+        ldb = ctypes.c_int32(ldb)
+        ldc = ctypes.c_int32(ldc)
+
+        df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda:0"")
+        cdequantize_blockwise_fp32(
+            get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
+            ctypes.c_int(blocksize2), ctypes.c_int(df.numel()),
+        )
+        df += offset
+        absmax = df
+
+        fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
+            cgemm_4bit_inference_naive_bf16
+
+        blocksize = ctypes.c_int32(blocksize)
+        fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
+           lda, ldb, ldc, blocksize,)
+
+        return out
     pass
-    # assert(dtype == X.dtype)
-    bout = shape[0]
-
-    if out is None:
-        out = torch.empty((1, 1, bout,), dtype = dtype, device = ""cuda:0"")
-    # else:
-    #     assert(out.shape == (1, 1, bout,))
-    # pass
-
-    n = 1
-    m = shape[0]
-    k = shape[1]
-    lda = shape[0]
-    ldc = shape[0]
-    ldb = (hd+1)//2
-    m = ctypes.c_int32(m)
-    n = ctypes.c_int32(n)
-    k = ctypes.c_int32(k)
-    lda = ctypes.c_int32(lda)
-    ldb = ctypes.c_int32(ldb)
-    ldc = ctypes.c_int32(ldc)
-
-    df = torch.empty(absmax.shape, dtype = torch.float32, device = ""cuda:0"")
-    cdequantize_blockwise_fp32(
-        get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
-        ctypes.c_int(blocksize2), ctypes.c_int(df.numel()),
-    )
-    df += offset
-    absmax = df
-
-    fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \
-        cgemm_4bit_inference_naive_bf16
-
-    blocksize = ctypes.c_int32(blocksize)
-    fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),
-       lda, ldb, ldc, blocksize)
-
-    return out
 pass
 
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index ed6207b..15e9efc 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -181,6 +181,7 @@ def MistralForCausalLM_fast_forward(
     output_attentions: Optional[bool] = None,
     output_hidden_states: Optional[bool] = None,
     return_dict: Optional[bool] = None,
+    num_logits_to_keep: Optional[int] = 0,
     *args, **kwargs,
 ) -> Union[Tuple, CausalLMOutputWithPast]:
 
@@ -236,6 +237,8 @@ def MistralForCausalLM_fast_forward(
     if bsz == 1 and q_len == 1:
         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))
         logits = logits.unsqueeze(0).unsqueeze(0)
+    elif num_logits_to_keep != 0:
+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :].to(lm_head.dtype))
     else:
         logits = self.lm_head(hidden_states.to(lm_head.dtype))
     pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 88a9e96..77509cd 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -95,11 +95,22 @@ def prepare_model_for_kbit_training(
     """"""
 
     # Freeze all parameters except LoRA
-    for name, param in model.named_parameters():
-        if "".lora_A."" in name or "".lora_B."" in name or "".lora_magnitude_vector"" in name:
-            param.requires_grad_(True)
-        else:
-            param.requires_grad_(False)
+    import re
+    with torch.inference_mode():
+        for name, param in model.named_parameters():
+            if "".lora_A."" in name or "".lora_B."" in name or "".lora_magnitude_vector"" in name:
+                param.requires_grad_(True)
+                # Also must be in float32!
+                if param.dtype != torch.float32:
+                    name = name.replace(""base_model"", ""model"", 1)
+                    layer_number = re.search(r""\.[\d]{1,}\."", name).group(0)
+                    name = name.replace(layer_number, f""[{layer_number[1:-1]}]."")
+                    name = name.replace("".weight"", """", 1)
+                    exec(f""{name}.to(torch.float32)"")
+                pass
+            else:
+                param.requires_grad_(False)
+        pass
     pass
 
     # Gradient checkpointing!
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 0b8092e..202c692 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1030,7 +1030,7 @@ class FastLlamaModel:
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
         model_patcher.pre_patch()
-        get_statistics()
+        # get_statistics()
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index f24108b..b1d2fae 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -122,10 +122,6 @@ __INT_TO_FLOAT_MAPPER = \
         ""unsloth/codegemma-7b"",
         ""google/codegemma-7b"",
     ),
-    ""unsloth/codegemma-2b-it-bnb-4bit"" : (
-        ""unsloth/codegemma-2b-it"",
-        ""google/codegemma-2b-it"",
-    ),
     ""unsloth/codegemma-7b-it-bnb-4bit"" : (
         ""unsloth/codegemma-7b-it"",
         ""google/codegemma-7b-it"",
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 87f5c85..3034b83 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -319,7 +319,7 @@ class FastMistralModel(FastLlamaModel):
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
         model_patcher.pre_patch()
-        get_statistics()
+        # get_statistics()
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 1176f58..c974a46 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -562,7 +562,9 @@ class FastModel(FastBaseModel):
         # Sesame
         elif ""csm-1b"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1"" # Sesame fails
-            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""all;torch.float32;torch.float16;if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16)""
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
+                ""all;torch.float32;torch.float16;""\
+                ""if name.endswith(('_proj', 'fc1', 'fc2', 'codebook', 'head')): module.to(torch.float16);""
         # Granite 4
         elif 'granite-4' in lowered_model_name:
             # granite-4 rms norms are stored as 16 bit, but we upcast
@@ -574,7 +576,11 @@ class FastModel(FastBaseModel):
         # Gemma 3N
         elif ""gemma-3n"" in lowered_model_name:
             os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
-            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = ""float16;torch.float16;torch.float16;if name.endswith(('.conv')): module.to(torch.float32)""
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
+                ""float16;torch.float16;torch.float16;""\
+                ""if name.endswith(('.conv')): module.to(torch.float32);""\
+                ""from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()""
+            
             if transformers_version < Version(""4.53.0""):
                 raise RuntimeError(""Unsloth: Gemma 3N only works on transformers >= 4.53.0"" + LATEST)
         else:
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index a8acdce..bff27a6 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -350,8 +350,8 @@ class FastBaseModel:
         correct_dtype = None
         if os.environ.get(""UNSLOTH_FORCE_CUSTOM_DTYPE"", """") != """":
             custom_datatype = os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""]
-            assert custom_datatype.count("";"") == 3
-            checker, _dtype, _bnb_compute_dtype, _custom_datatype = custom_datatype.split("";"", 3)
+            assert custom_datatype.count("";"") >= 4
+            checker, _dtype, _bnb_compute_dtype, _custom_datatype, execute_code = custom_datatype.split("";"", 4)
 
             # Allow custom dtypes on all runs
             allow_all_runs = (checker == ""all"")
@@ -363,6 +363,9 @@ class FastBaseModel:
                 bnb_compute_dtype = eval(_bnb_compute_dtype)
                 correct_dtype = bnb_compute_dtype
                 custom_datatype = _custom_datatype
+                # Execute code as well
+                print(execute_code)
+                exec(execute_code)
             pass
         pass
 
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 359761c..54e016d 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1216,6 +1216,8 @@ class FastLlamaModel:
             )
         pass
 
+        if loftq_config is None: loftq_config = {}
+
         import inspect
         signature = str(inspect.signature(LoraConfig))
         SUPPORTS_LOFTQ  = ""loftq_config"" in signature
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index d5651d5..d94eeb8 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -176,3 +176,6 @@ from .save import *
 from .chat_templates import *
 from .tokenizer_utils import *
 from .trainer import *
+
+# patch sft trainer
+_patch_trl_trainer()
\ No newline at end of file
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index 25bb434..b03cc34 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -12,9 +12,13 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import warnings
 from dataclasses import dataclass, field
 from typing import Optional
+from functools import wraps
 
+import trl
+import inspect
 from trl import SFTTrainer
 try:
     from trl import SFTConfig as TrainingArguments
@@ -24,30 +28,38 @@ pass
 from . import is_bfloat16_supported
 from unsloth_zoo.training_utils import unsloth_train as _unsloth_train
 from packaging.version import Version
+import dataclasses
+
+__all__ = [
+    ""UnslothTrainingArguments"",
+    ""UnslothTrainer"",
+    ""unsloth_train"",
+    ""_patch_trl_trainer"",
+]
 
 # Unsloth gradient accumulation fix:
 from transformers import __version__ as transformers_version
 if Version(transformers_version) > Version(""4.45.2""):
-    def unsloth_train(trainer):
-        return trainer.train()
+    def unsloth_train(trainer, *args, **kwargs):
+        return trainer.train(*args, **kwargs)
     pass
 else:
-    def unsloth_train(trainer):
+    def unsloth_train(trainer, *args, **kwargs):
+        if len(args) != 0 or len(kwargs) != 0:
+            raise RuntimeError(
+                ""Unsloth: Our custom gradient accumulation fixed trainer does not support other arguments.\n""\
+                ""If you want to use our fix inside of HF, please update `transformers` to the latest version via:\n""\
+                '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir transformers`'
+            )
         print(
             ""Unsloth: Using our custom gradient accumulation fixed trainer, which is not feature complete.\n""\
             ""If you want to use our fix inside of HF, please update `transformers` to the latest version via:\n""\
-            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir ""git+https://github.com/huggingface/transformers.git""`'
+            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir transformers`'
         )
         return _unsloth_train(trainer)
     pass
 pass
 
-__all__ = [
-    ""UnslothTrainingArguments"",
-    ""UnslothTrainer"",
-    ""unsloth_train"",
-]
-
 
 @dataclass
 class UnslothTrainingArguments(TrainingArguments):
@@ -119,3 +131,83 @@ class UnslothTrainer(SFTTrainer):
         return self.optimizer
     pass
 pass
+
+# From `trl>=0.13.0`, they changed how to pass several params to the trainer
+# We need to patch to make the transition smooth
+def create_backwards_compatible_trainer(trainer_class, config_class):
+    original_init = trainer_class.__init__
+    
+    @wraps(original_init)
+    def new_init(self, *args, **kwargs):
+        # All Trainer tokenizer is now called processing_class
+        if ""tokenizer"" in kwargs:
+            kwargs[""processing_class""] = kwargs.pop(""tokenizer"")
+
+        if ""args"" in kwargs:
+            training_args = kwargs.pop(""args"", None)
+
+            # Get parameters that Trainer.__init__ actually expects
+            trainer_params = set(inspect.signature(original_init).parameters.keys())
+            trainer_params.remove('self')
+            trainer_params.remove('args')
+
+            # Get fields that should be passed to Config init
+            config_fields = {
+                field.name: field for field in dataclasses.fields(config_class) 
+                if field.init
+            }
+            
+            # Create config dict with valid fields from training_args
+            config_dict = {
+                name: getattr(training_args, name)
+                for name in config_fields
+                if hasattr(training_args, name)
+            }
+
+            # Get parameters that exist in Config but not in TrainingArguments
+            moved_params = \
+                set(inspect.signature(config_class)     .parameters.keys()) - \
+                set(inspect.signature(TrainingArguments).parameters.keys())
+            
+            # Separate kwargs into trainer kwargs and config kwargs
+            trainer_kwargs = {}
+            additional_config_kwargs = {}
+
+            for key, value in kwargs.items():
+                if key in trainer_params: trainer_kwargs[key] = value
+                elif key in moved_params or key in config_fields:
+                    additional_config_kwargs[key] = value
+                else:
+                    additional_config_kwargs[key] = value
+            pass
+
+            # Update config_dict with additional kwargs
+            config_dict.update(additional_config_kwargs)
+
+            # Create Config with all the collected parameters
+            config = config_class(**config_dict)
+            
+            # Reconstruct kwargs for Trainer
+            kwargs = trainer_kwargs
+            kwargs[""args""] = config
+        pass
+        original_init(self, *args, **kwargs)
+    pass
+    return new_init
+
+if Version(trl.__version__) >= Version(""0.13.0.dev0""):
+    # print(""Patching TRL Trainer to maintain backward compatibility with the old syntax."")
+    def _patch_trl_trainer():
+        import trl.trainer
+        trl_classes = dir(trl.trainer)
+
+        non_convertable_trainer = set([""PPOv2"", ""AlignProp""])
+        trl_trainers = set(x[:-len(""Trainer"")] for x in trl_classes if x.endswith(""Trainer"")) - non_convertable_trainer
+        trl_configs  = set(x[:-len(""Config"")]  for x in trl_classes if x.endswith(""Config"")) - non_convertable_trainer
+        trl_classes = list(trl_trainers & trl_configs)
+        for x in trl_classes:
+            exec(f""trl.{x}Trainer.__init__ = create_backwards_compatible_trainer(trl.{x}Trainer, trl.{x}Config)"", globals())
+    pass
+else:
+    def _patch_trl_trainer(): return
+pass
"
"diff --git a/pyproject.toml b/pyproject.toml
index e563ba6..6f6f225 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.8.4"",
+    ""unsloth_zoo>=2025.8.5"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
@@ -384,7 +384,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.8.4"",
+    ""unsloth_zoo>=2025.8.5"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0"",
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index ab2694f..c84fd11 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.8.5""
+__version__ = ""2025.8.6""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index ab7f4bf..ae03a68 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -701,8 +701,9 @@ def LlamaModel_fast_forward(
     # Fix out of bounds tokenization
     if hasattr(self, ""max_seq_length""):
         if seq_length > self.max_seq_length:
+            shape = input_ids.shape if input_ids is not None else inputs_embeds.shape
             logger.warning_once(
-                f""Unsloth: Input IDs of length {seq_length} > the model's max sequence length of {self.max_seq_length}.\n""\
+                f""Unsloth: Input IDs of shape {shape} with length {seq_length} > the model's max sequence length of {self.max_seq_length}.\n""\
                 ""We shall truncate it ourselves. It's imperative if you correct this issue first.""
             )
         if input_ids is not None:
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 829fe29..e8fc55c 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -941,6 +941,16 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen3-4B-Thinking-2507"",
         ""unsloth/Qwen3-4B-Thinking-2507-bnb-4bit"",
     ),
+    ""unsloth/gemma-3-270m-it-unsloth-bnb-4bit"" : (
+        ""unsloth/gemma-3-270m-it"",
+        ""google/gemma-3-270m-it"",
+        ""unsloth/gemma-3-270m-it-bnb-4bit"",
+    ),
+    ""unsloth/gemma-3-270m-unsloth-bnb-4bit"" : (
+        ""unsloth/gemma-3-270m"",
+        ""google/gemma-3-270m"",
+        ""unsloth/gemma-3-270m-bnb-4bit"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER  = {}
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 520c998..5487e9d 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -253,13 +253,15 @@ def get_chat_template(
     mapping = {""role"" : ""role"", ""content"" : ""content"", ""user"" : ""user"", ""assistant"" : ""assistant""},
     map_eos_token = True,
 ):
+    old_tokenizer = tokenizer
+
     if map_eos_token is False:
         assert(""Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported."")
     pass
 
-    # if tokenizer.__class__.__name__.startswith(""Gemma"") and chat_template == ""chatml"":
-    #     chat_template = ""gemma_chatml""
-    # pass
+    if tokenizer.__class__.__name__.startswith(""Gemma"") and chat_template == ""chatml"":
+        chat_template = ""gemma_chatml""
+    pass
 
     old_padding_side = tokenizer.padding_side
 
@@ -340,6 +342,17 @@ def get_chat_template(
     tokenizer.padding_side  = old_padding_side
     tokenizer.chat_template = chat_template
 
+    # Also fix up other tokens
+    old_pad_token = getattr(old_tokenizer, ""pad_token"", None)
+    old_bos_token = getattr(old_tokenizer, ""bos_token"", None)
+    old_unk_token = getattr(old_tokenizer, ""unk_token"", None)
+    new_pad_token = getattr(tokenizer,     ""pad_token"", None)
+    new_bos_token = getattr(tokenizer,     ""bos_token"", None)
+    new_unk_token = getattr(tokenizer,     ""unk_token"", None)
+    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token
+    if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token
+    if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token
+
     #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)
 
     return tokenizer#, stopping_criteria
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 3e3b8ff..95a032b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -95,7 +95,7 @@ def prepare_model_for_kbit_training(
 
     # Freeze all parameters except LoRA
     for name, param in model.named_parameters():
-        if "".lora_A."" in name or "".lora_B."" in name:
+        if "".lora_A."" in name or "".lora_B."" in name or "".lora_magnitude_vector"" in name:
             param.requires_grad_(True)
         else:
             param.requires_grad_(False)
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 30348b6..e949337 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -511,26 +511,36 @@ def LlamaModel_fast_forward(
 
     # Mormalized from Gemma
     IS_GEMMA = self.config.model_type == ""gemma""
+    train_embed_tokens = self.embed_tokens.weight.requires_grad
+
     if IS_GEMMA:
-        inputs_requires_grad = inputs_embeds.requires_grad
-        if not inputs_embeds.is_leaf:
-            inputs_embeds = inputs_embeds.detach()
-            inputs_requires_grad = True
-        elif inputs_requires_grad:
-            inputs_embeds.requires_grad_(False)
-        pass
         # Match Gemma exactly by casting to bfloat16 / float16
         # inputs_embeds *= math_sqrt(self.config.hidden_size)
         # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32
         # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32
-        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)
-        # inputs_embeds *= math_sqrt(self.config.hidden_size)
-        if inputs_requires_grad: inputs_embeds.requires_grad_(True)
+        normalizer = torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)
+
+        if train_embed_tokens:
+            # Careful we must not do an inplace op!
+            inputs_embeds = inputs_embeds * normalizer
+        else:
+            inputs_requires_grad = inputs_embeds.requires_grad
+            if not inputs_embeds.is_leaf:
+                inputs_embeds = inputs_embeds.detach()
+                inputs_requires_grad = True
+            elif inputs_requires_grad:
+                inputs_embeds.requires_grad_(False)
+            pass
+            inputs_embeds *= normalizer
+            # inputs_embeds *= math_sqrt(self.config.hidden_size)
+            if inputs_requires_grad: inputs_embeds.requires_grad_(True)
+        pass
     pass
 
     # Fix up attention mask by setting elements to 0
     # Specifically for DPO
-    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):
+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None) and \
+        (not train_embed_tokens):
         # Careful for inference the attention_mask is size (1, kv_seq_len)
         # Whilst the input_embeds is size (1, 1, 4096)
         inputs_requires_grad = inputs_embeds.requires_grad
@@ -1226,6 +1236,7 @@ class FastLlamaModel:
         random_state        = 3407,
         max_seq_length      = 2048, # not used anymore
         use_rslora          = False,
+        modules_to_save     = None,
         init_lora_weights   = True,
         loftq_config        = {},
         **kwargs,
@@ -1312,15 +1323,45 @@ class FastLlamaModel:
         accepted_modules = frozenset((""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
                                       ""gate_proj"", ""up_proj"", ""down_proj"",),)
         model.config.update({""unsloth_version"" : __version__})
+
+        train_lm_head = False
+        train_embed_tokens = False
+        final_modules = []
         for module in target_modules:
-            assert(module in accepted_modules)
+            if module == ""lm_head"":
+                logger.warning_once(
+                    ""Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.""\
+                    ""We shall do it for you!""
+                )
+                train_lm_head = True
+
+            elif module == ""embed_tokens"":
+                logger.warning_once(
+                    ""Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.""\
+                    ""We shall do it for you!""
+                )
+                train_embed_tokens = True
+
+            else:
+                assert(module in accepted_modules)
+                final_modules.append(module)
+        pass
+
+        # Check modules_to_save
+        if modules_to_save is not None:
+            for module in modules_to_save:
+                if module == ""lm_head"":
+                    train_lm_head = True
+                elif module == ""embed_tokens"":
+                    train_embed_tokens = True
+            pass
         pass
 
         # Get LoRA
         arguments = dict(
             r                   = r,
             lora_alpha          = lora_alpha,
-            target_modules      = target_modules,
+            target_modules      = final_modules,
             lora_dropout        = lora_dropout,
             bias                = bias,
             task_type           = TaskType.CAUSAL_LM,
@@ -1328,6 +1369,7 @@ class FastLlamaModel:
             init_lora_weights   = init_lora_weights,
             loftq_config        = loftq_config,
             use_rslora          = use_rslora,
+            modules_to_save     = modules_to_save,
             **kwargs,
         )
         if not SUPPORTS_LOFTQ:  del arguments[""loftq_config""]
@@ -1337,6 +1379,14 @@ class FastLlamaModel:
         model = _get_peft_model(model, lora_config)
 
         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)
+
+        # Now patch lm_head and embed_tokens
+        if train_embed_tokens:
+            model.model.model.embed_tokens.requires_grad_(True)
+        if train_lm_head:
+            model.model.lm_head.requires_grad_(True)
+        pass
+
         return model
     pass
 
@@ -1427,9 +1477,12 @@ class FastLlamaModel:
                 if  hasattr(gate_proj, ""lora_A"") and \
                     hasattr(  up_proj, ""lora_A"") and \
                     hasattr(down_proj, ""lora_A"") and \
-                    (gate_proj.base_layer if hasattr(gate_proj, ""base_layer"") else gate_proj).bias is None and \
-                    (  up_proj.base_layer if hasattr(  up_proj, ""base_layer"") else   up_proj).bias is None and \
-                    (down_proj.base_layer if hasattr(down_proj, ""base_layer"") else down_proj).bias is None:
+                    ((gate_proj.base_layer if hasattr(gate_proj, ""base_layer"") else gate_proj).bias is None) and \
+                    ((  up_proj.base_layer if hasattr(  up_proj, ""base_layer"") else   up_proj).bias is None) and \
+                    ((down_proj.base_layer if hasattr(down_proj, ""base_layer"") else down_proj).bias is None) and \
+                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, ""lora_magnitude_vector"") else None) is None) and \
+                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, ""lora_magnitude_vector"") else None) is None) and \
+                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, ""lora_magnitude_vector"") else None) is None):
 
                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)
@@ -1448,9 +1501,12 @@ class FastLlamaModel:
                 if  hasattr(q_proj, ""lora_A"") and \
                     hasattr(k_proj, ""lora_A"") and \
                     hasattr(v_proj, ""lora_A"") and \
-                    (q_proj.base_layer if hasattr(q_proj, ""base_layer"") else q_proj).bias is None and \
-                    (k_proj.base_layer if hasattr(k_proj, ""base_layer"") else k_proj).bias is None and \
-                    (v_proj.base_layer if hasattr(v_proj, ""base_layer"") else v_proj).bias is None:
+                    ((q_proj.base_layer if hasattr(q_proj, ""base_layer"") else q_proj).bias is None) and \
+                    ((k_proj.base_layer if hasattr(k_proj, ""base_layer"") else k_proj).bias is None) and \
+                    ((v_proj.base_layer if hasattr(v_proj, ""base_layer"") else v_proj).bias is None) and \
+                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, ""lora_magnitude_vector"") else None) is None) and \
+                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, ""lora_magnitude_vector"") else None) is None) and \
+                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, ""lora_magnitude_vector"") else None) is None):
 
                     layer.self_attn.apply_qkv = apply_lora_qkv
                     n_qkv += 1
@@ -1464,7 +1520,8 @@ class FastLlamaModel:
                 # O attention patching
                 o_proj = layer.self_attn.o_proj
                 if hasattr(o_proj, ""lora_A"") and \
-                    (o_proj.base_layer if hasattr(o_proj, ""base_layer"") else o_proj).bias is None:
+                    ((o_proj.base_layer if hasattr(o_proj, ""base_layer"") else o_proj).bias is None) and \
+                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, ""lora_magnitude_vector"") else None) is None):
 
                     layer.self_attn.apply_o = apply_lora_o
                     n_o += 1
diff --git a/unsloth/save.py b/unsloth/save.py
index 42d326e..7543eef 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -203,7 +203,11 @@ def unsloth_save_model(
 
         print(""Unsloth: Merging 4bit and LoRA weights to 4bit..."")
         print(""This might take 5 minutes..."")
-        model = model.merge_and_unload()
+
+        # Counteract no LoRA adapters!
+        if hasattr(model, ""merge_and_unload""):
+            model = model.merge_and_unload()
+        pass
         print(""Done."")
     pass
 
@@ -573,6 +577,21 @@ def install_llama_cpp_old(version = -10):
     latest = releases[-1]
     version = releases[version].split("" "")[0]
 
+    # Check if the llama.cpp exists
+    if os.path.exists(""llama.cpp""):
+        print(
+            ""**[WARNING]** You have a llama.cpp old directory which is broken.\n""\
+            ""Unsloth will DELETE the broken directory and install a new one.\n""\
+            ""Press CTRL + C / cancel this if this is wrong. We shall wait 10 seconds.\n""
+        )
+        import time
+        for i in range(10):
+            print(f""**[WARNING]** Deleting llama.cpp directory... {10-i} seconds left."")
+            time.sleep(1)
+        import shutil
+        shutil.rmtree(""llama.cpp"")
+    pass
+
     # Clone a specific commit
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
"
"diff --git a/README.md b/README.md
index 872bf7f..4e2ebe3 100644
--- a/README.md
+++ b/README.md
@@ -33,7 +33,7 @@ If you trained a model with Unsloth, we made a cool sticker!!
 
 # Installation Instructions - Conda
 Unsloth currently only supports Linux distros and Pytorch == 2.1.
-```
+```bash
 conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \
   -c pytorch -c nvidia -c xformers -c conda-forge -y
 pip install ""unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git""
@@ -41,16 +41,16 @@ pip install ""unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git""
 
 # Installation Instructions - Pip
 1. Find your CUDA version via
-```
+```python
 import torch; torch.version.cuda
 ```
 2. We only support Pytorch 2.1 (2.1.1 bugs out for now): You can update Pytorch via Pip (interchange cu121 / cu118)
-```
+```bash
 pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \
   --index-url https://download.pytorch.org/whl/cu121
 ```
 2. Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the ""ampere"" path.
-```
+```bash
 pip install ""unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git""
@@ -59,13 +59,13 @@ pip install ""unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.gi
 Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.
 
 4. If you get errors, try the below first, then go back to step 1:
-```
+```bash
 pip install --upgrade pip
 ```
 
 # Documentation
 We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
-```
+```python
 from unsloth import FastLlamaModel, FastMistralModel
 import torch
 max_seq_length = 2048 # Can change to any number <= 4096
@@ -305,7 +305,7 @@ $$
 
 # Troubleshooting
 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:
-```
+```bash
 !ldconfig /usr/lib64-nvidia
 ```
 2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.
@@ -315,5 +315,5 @@ $$
 # Credits
 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support
 2. [152334H](https://github.com/152334H) for experimental DPO support
-
+3. [atgctg](https://github.com/atgctg) for syntax highlighting
 <img src=""./images/unsloth loading page render.png"" width=""300"" />
diff --git a/images/unsloth made with love.png b/images/unsloth made with love.png
index 20dac04..9bf7ec9 100644
Binary files a/images/unsloth made with love.png and b/images/unsloth made with love.png differ
diff --git a/images/unsloth new logo.png b/images/unsloth new logo.png
index fa05a8e..20dac04 100644
Binary files a/images/unsloth new logo.png and b/images/unsloth new logo.png differ
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index de124c9..769669a 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -20,13 +20,36 @@ import gc
 warnings.filterwarnings(action = ""ignore"", category = UserWarning, module = ""torch"")
 import bitsandbytes as bnb
 from transformers.models.llama.modeling_llama import logger
-import platform
+from platform import system as platform_system
+platform_system = platform_system()
 
 __version__ = ""2023.12""
+
+# Get Flash Attention v2 if Ampere (RTX 30xx, A100)
+major_version, minor_version = torch.cuda.get_device_capability()
+if major_version >= 8:
+    try:
+        from flash_attn import flash_attn_func
+        HAS_FLASH_ATTENTION = True
+    except:
+        HAS_FLASH_ATTENTION = False
+else:
+    # Tri Dao's benchmark shows xformers is faster for now.
+    HAS_FLASH_ATTENTION = False
+pass
+import xformers.ops.fmha as xformers
+xformers_attention = xformers.memory_efficient_attention
+from xformers import __version__ as xformers_version
+
 __all__ = [
     ""prepare_model_for_kbit_training"",
     ""patch_tokenizer"",
-    ""print_unsloth_message"",
+    ""xformers"",
+    ""xformers_attention"",
+    ""xformers_version"",
+    ""__version__"",
+    ""HAS_FLASH_ATTENTION"",
+    ""platform_system"",
 ]
 
 
@@ -71,6 +94,7 @@ pass
 
 
 def patch_tokenizer(model, tokenizer):
+    model.config.update({""unsloth_version"" : __version__})
     if not hasattr(tokenizer, ""pad_token"") or tokenizer.pad_token is None:
         # Fixes https://github.com/unslothai/unsloth/issues/5
         if hasattr(tokenizer, ""unk_token""):
@@ -88,18 +112,3 @@ def patch_tokenizer(model, tokenizer):
     pass
     return model, tokenizer
 pass
-
-
-def print_unsloth_message(name):
-    SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
-    gpu_stats = torch.cuda.get_device_properties(0)
-    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
-
-    statistics = \
-       f""==((====))==  Unsloth: Fast {name} patching release {__version__}\n""\
-       f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
-       f""O^O/ \_/ \\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\n""\
-       f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
-       f' ""-____-""     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform.system()}\n'
-    print(statistics)
-pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 1f0046e..be39b90 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -23,21 +23,9 @@ from transformers.models.llama.modeling_llama import (
 )
 from ..kernels import *
 from ._utils import *
-
-# Get Flash Attention v2 if Ampere (RTX 30xx, A100)
-major_version, minor_version = torch.cuda.get_device_capability()
-if major_version >= 8:
-    try:
-        from flash_attn import flash_attn_func
-        HAS_FLASH_ATTENTION = True
-    except:
-        HAS_FLASH_ATTENTION = False
-else:
-    # Tri Dao's benchmark shows xformers is faster for now.
-    HAS_FLASH_ATTENTION = False
-pass
-import xformers.ops.fmha as xformers
-xformers_attention = xformers.memory_efficient_attention
+from ._utils import __version__
+if HAS_FLASH_ATTENTION:
+    from flash_attn import flash_attn_func
 
 # Final patching code
 from transformers.models.llama.modeling_llama import (
@@ -139,19 +127,20 @@ def LlamaAttention_fast_forward_inference(
     # V = repeat_kv(V, n_groups)
     if n_groups != 1:
         _, _, cached_len, _ = Kn.shape
-        Kn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
-        Vn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
-        Kn = Kn.reshape(bsz, n_heads, cached_len, head_dim)
-        Vn = Vn.reshape(bsz, n_heads, cached_len, head_dim)
-    pass
+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)
+        Knn = Knn.view(bsz, n_heads, cached_len, head_dim)
+        Vnn = Vnn.view(bsz, n_heads, cached_len, head_dim)
+    else:
+        Knn, Vnn = Kn, Vn
 
     # Attention
-    A = torch.matmul(Qn, Kn.transpose(2, 3))
+    A = torch.matmul(Qn, Knn.transpose(2, 3))
     A *= 1.0 / (self.head_dim**0.5)
     A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)
-    A = torch.matmul(A, Vn)
+    A = torch.matmul(A, Vnn)
     A = A.transpose(1, 2)
-    A = A.reshape(bsz, 1, self.hidden_size)
+    A = A.view(bsz, 1, self.hidden_size)
     A = original_apply_o(self, A)
     return A, (Kn, Vn)
 pass
@@ -359,13 +348,13 @@ def LlamaModel_fast_forward(
 
     # retrieve input_ids and inputs_embeds
     if input_ids is not None and inputs_embeds is not None:
-        raise ValueError(""You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"")
+        raise ValueError(""Unsloth: You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"")
     elif input_ids is not None:
         batch_size, seq_length = input_ids.shape
     elif inputs_embeds is not None:
         batch_size, seq_length, _ = inputs_embeds.shape
     else:
-        raise ValueError(""You have to specify either decoder_input_ids or decoder_inputs_embeds"")
+        raise ValueError(""Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds"")
 
     seq_length_with_past = seq_length
     past_key_values_length = 0
@@ -419,7 +408,7 @@ def LlamaModel_fast_forward(
     if self.gradient_checkpointing and self.training:
         if use_cache:
             logger.warning_once(
-                ""`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...""
+                ""Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`""
             )
             use_cache = False
     pass
@@ -614,7 +603,16 @@ class FastLlamaModel:
         rope_scaling = None,
     ):
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
-        print_unsloth_message(""Llama"")
+        gpu_stats = torch.cuda.get_device_properties(0)
+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
+
+        statistics = \
+           f""==((====))==  Unsloth: Fast Llama patching release {__version__}\n""\
+           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
+           f""O^O/ \_/ \\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
+           f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
+           f' ""-____-""     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\n'
+        logger.warning_once(statistics)
         FastLlamaModel.pre_patch()
 
         if dtype is None:
@@ -632,7 +630,7 @@ class FastLlamaModel:
         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):
             rope_scaling = max_seq_length / model_max_seq_length
             logger.warning_once(
-                f""Unsloth: {model_name} can only handle sequence lengths of of most ""\
+                f""Unsloth: {model_name} can only handle sequence lengths of at most ""\
                 f""{model_max_seq_length}.\nBut with kaiokendev's RoPE scaling of ""\
                 f""{round(rope_scaling, 3)}, it can be magically be extended to ""\
                 f""{max_seq_length}!""
@@ -686,6 +684,7 @@ class FastLlamaModel:
         # Torch.compile fails on embedding matrix??
         # Workaround randomnly fixes it for torch versions < 2.2
         model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)
+        model.config.update({""unsloth_version"" : __version__})
 
         # We also do this for the lm_head
         lm_head = torch.nn.Linear(1, 1, bias = None)
@@ -747,6 +746,7 @@ class FastLlamaModel:
 
         accepted_modules = frozenset((""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
                                       ""gate_proj"", ""up_proj"", ""down_proj"",),)
+        model.config.update({""unsloth_version"" : __version__})
         for module in target_modules:
             assert(module in accepted_modules)
         pass
@@ -771,6 +771,9 @@ class FastLlamaModel:
         model = _get_peft_model(model, lora_config)
 
         # Do patching
+        n_mlp = 0
+        n_qkv = 0
+        n_o   = 0
         for idx, layer in enumerate(model.model.model.layers):
 
             # MLP patching
@@ -780,6 +783,7 @@ class FastLlamaModel:
 
                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module
                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)
+                n_mlp += 1
             pass
 
             # QKV attention patching
@@ -788,15 +792,22 @@ class FastLlamaModel:
                 hasattr(layer.self_attn.v_proj, ""lora_A""):
 
                 layer.self_attn.apply_qkv = apply_lora_qkv
+                n_qkv += 1
             pass
 
             # O attention patching
             if hasattr(layer.self_attn.o_proj, ""lora_A""):
 
                 layer.self_attn.apply_o = apply_lora_o
+                n_o += 1
             pass
         pass
 
+        logger.warning_once(
+            f""Unsloth {__version__} patched {len(model.model.model.layers)} layers with ""\
+            f""{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers."",
+        )
+
         # Patch cross entropy loss labels
         # Fixes https://github.com/unslothai/unsloth/issues/10
         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = ""cuda"")
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index f75ebbc..0ace3f4 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -45,7 +45,7 @@ class FastLanguageModel:
             )
         elif model_type == ""mistral"":
             if rope_scaling is not None:
-                logger.warning_once(""Mistral models do not support RoPE scaling."")
+                logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
             return FastMistralModel.from_pretrained(
                 model_name = model_name,
                 max_seq_length = max_seq_length,
@@ -57,7 +57,8 @@ class FastLanguageModel:
             )
         else:
             raise NotImplementedError(
-                f""{model_name} not supported yet! Make an issue to https://github.com/unslothai/unsloth!"",
+                f""Unsloth: {model_name} not supported yet!\n""\
+                ""Make an issue to https://github.com/unslothai/unsloth!"",
             )
     pass
 pass
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 9a91a8f..323ec39 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 from .llama import *
+from ._utils import __version__
 
 from transformers.models.mistral.modeling_mistral import (
     MistralAttention,
@@ -245,7 +246,16 @@ class FastMistralModel(FastLlamaModel):
         # rope_scaling = None, Mistral does not support RoPE scaling
     ):
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
-        print_unsloth_message(""Mistral"")
+        gpu_stats = torch.cuda.get_device_properties(0)
+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
+
+        statistics = \
+           f""==((====))==  Unsloth: Fast Mistral patching release {__version__}\n""\
+           f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\n""\
+           f""O^O/ \_/ \\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
+           f""\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\n""\
+           f' ""-____-""     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\n'
+        logger.warning_once(statistics)
         FastMistralModel.pre_patch()
 
         if dtype is None:
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index e3fab29..262d403 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -479,11 +479,6 @@ class FastModel(FastBaseModel):
                 ""Also, we by default set `load_in_4bit = True`.\n""\
                 ""If you want 8bit finetuning, set both `load_in_4bit = False` and `load_in_8bit = True`""
             )
-        if load_in_4bit: pass
-        elif load_in_8bit: pass
-        elif not load_in_4bit and not load_in_8bit and not full_finetuning:
-            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
-            load_in_4bit = True
         pass
 
         old_model_name = model_name
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 8799e01..bf11bc6 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -263,15 +263,7 @@ class FastBaseModel:
                 llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
             )
         elif not load_in_4bit and not load_in_8bit and not full_finetuning:
-            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
-            load_in_4bit = True
-            bnb_config = BitsAndBytesConfig(
-                load_in_4bit              = True,
-                bnb_4bit_use_double_quant = True,
-                bnb_4bit_quant_type       = ""nf4"",
-                bnb_4bit_compute_dtype    = bnb_compute_dtype,
-                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
-            )
+            print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to 16bit LoRA."")
         pass
 
         if full_finetuning:
"
"diff --git a/README.md b/README.md
index 6bff98c..f658e6c 100644
--- a/README.md
+++ b/README.md
@@ -212,6 +212,9 @@ For **advanced installation instructions** or if you see weird errors during ins
 - Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
 - We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!
 - We're in Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!
+- If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.
+
+> unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.
 
 ```python
 from unsloth import FastLanguageModel 
diff --git a/unsloth-cli.py b/unsloth-cli.py
index ddb0ac8..b7613f9 100644
--- a/unsloth-cli.py
+++ b/unsloth-cli.py
@@ -30,11 +30,14 @@ Happy fine-tuning!
 """"""
 
 import argparse
+import os
+
 
 def run(args):
     import torch
     from unsloth import FastLanguageModel
     from datasets import load_dataset
+    from transformers.utils import strtobool
     from trl import SFTTrainer
     from transformers import TrainingArguments
     from unsloth import is_bfloat16_supported
@@ -86,8 +89,13 @@ def run(args):
             texts.append(text)
         return {""text"": texts}
 
-    # Load and format dataset
-    dataset = load_dataset(args.dataset, split=""train"")
+    use_modelscope = strtobool(os.environ.get('UNSLOTH_USE_MODELSCOPE', 'False'))
+    if use_modelscope:
+        from modelscope import MsDataset
+        dataset = MsDataset.load(args.dataset, split=""train"")
+    else:
+        # Load and format dataset
+        dataset = load_dataset(args.dataset, split=""train"")
     dataset = dataset.map(formatting_prompts_func, batched=True)
     print(""Data is formatted and ready!"")
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 657072a..e9caad0 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -31,6 +31,15 @@ except:
 pass
 from huggingface_hub import HfFileSystem
 
+# [TODO] Move USE_MODELSCOPE to utils
+USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
+if USE_MODELSCOPE:
+    import importlib
+    if importlib.util.find_spec(""modelscope"") is None:
+        raise ImportError(f'You are using the modelscope hub, please install modelscope by `pip install modelscope -U`')
+    pass
+pass
+
 # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!
 from unsloth_zoo.utils import Version, _get_dtype
 transformers_version = Version(transformers_version)
@@ -72,6 +81,11 @@ class FastLanguageModel(FastLlamaModel):
         if not use_exact_model_name:
             model_name = get_model_name(model_name, load_in_4bit)
 
+        if USE_MODELSCOPE and not os.path.exists(model_name):
+            from modelscope import snapshot_download
+            model_name = snapshot_download(model_name)
+        pass
+
         # First check if it's a normal model via AutoConfig
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         was_disabled = are_progress_bars_disabled()
@@ -355,6 +369,11 @@ class FastVisionModel(FastBaseVisionModel):
         if not use_exact_model_name:
             model_name = get_model_name(model_name, load_in_4bit)
 
+        if USE_MODELSCOPE and not os.path.exists(model_name):
+            from modelscope import snapshot_download
+            model_name = snapshot_download(model_name)
+        pass
+
         # First check if it's a normal model via AutoConfig
         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
         was_disabled = are_progress_bars_disabled()
"
"diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 7cb5b2e..900767a 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -261,11 +261,12 @@ def grpo_trainer__get_per_token_logps(function_name, function):
         os.environ[""UNSLOTH_RETURN_HIDDEN_STATES""] = ""1""
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
             # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
-            print(""input_ids Unsloth 264"", input_ids.shape)
-            print(""logits_to_keep Unsloth 264"", logits_to_keep)
-            hidden_states = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
-            print(""hidden_states Unsloth 264"", hidden_states.shape)
-            #logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
+            hidden_states = model(
+                input_ids = input_ids,
+                attention_mask = attention_mask,
+                logits_to_keep = logits_to_keep + 1,
+            ).logits
+            # logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
             return hidden_states
             # input_ids = input_ids[:, -logits_to_keep:]
             # For transformers<=4.48, logits_to_keep argument isn't supported, so here we drop logits ourselves.
@@ -318,14 +319,8 @@ def grpo_trainer_compute_loss(function_name, function):
         logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens
         _input_ids = input_ids
         _logits_to_keep = logits_to_keep
-        print(""prompt_mask Unsloth 320"", prompt_mask.shape)
-        print(""completion_mask Unsloth 320"", completion_mask.shape)
-        print(""input_ids Unsloth 320"", input_ids.shape)
-        print(""logits_to_keep Unsloth 320"", logits_to_keep)
 
         per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
-        if per_token_logps is not None:
-            print(""per_token_logps Unsloth 320"", per_token_logps.shape)
 
         # Compute the KL divergence between the model and the reference model
         # _prepare_inputs doesn't return reference log probs anymore. We need to calculate it ourselves.
@@ -333,25 +328,16 @@ def grpo_trainer_compute_loss(function_name, function):
         if self.beta != 0.0:
             with torch.inference_mode(), model.disable_adapter():
                 ref_per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
-                if ref_per_token_logps is not None:
-                    print(""ref_per_token_logps Unsloth 320"", ref_per_token_logps.shape)
         else:
             ref_per_token_logps = None
         # per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
         # x - x.detach() allows for preserving gradients from x
         advantages = inputs[""advantages""]
-        print(""advantages Unsloth 320"", advantages.shape)
         # per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
         # per_token_loss = -(per_token_loss - self.beta * per_token_kl)
         # loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         old_hidden_states = inputs.get(""old_per_token_logps"", None)
-        if old_hidden_states is not None:
-            print(""old_hidden_states Unsloth 320"", old_hidden_states.shape)
-
-        print(""input_ids Unsloth 320"", input_ids.shape)
-        print(""logits_to_keep Unsloth 320"", logits_to_keep)
         input_ids = input_ids[:, -logits_to_keep:]
-        print(""input_ids Unsloth 320"", input_ids.shape)
 
         # Get logit softcapping and logit scale
         logit_softcapping = getattr(model.config, ""final_logit_softcapping"", 0) # Gemma
@@ -365,14 +351,9 @@ def grpo_trainer_compute_loss(function_name, function):
         if per_token_logps is not None:
 
             if ref_per_token_logps is not None:
-                print(""ref_per_token_logps Unsloth 320"", ref_per_token_logps.shape)
                 ref_per_token_logps = ref_per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-                print(""ref_per_token_logps Unsloth 320"", ref_per_token_logps.shape)
-            
-            print(""per_token_logps Unsloth 320"", per_token_logps.shape)
             per_token_logps = per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
-            print(""per_token_logps Unsloth 320"", per_token_logps.shape)
-            
+
             loss, completion_length, mean_kl = grpo_compute_loss_slow(
                 ref_per_token_logps,
                 per_token_logps,
@@ -428,13 +409,12 @@ def grpo_trainer_compute_loss(function_name, function):
                     logit_scale_divide = logit_scale_divide,
                     attention_mask = attention_mask,
                 )
-
+            pass
+        pass
         # Log the metrics
         # completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()
-
         # mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         # self._metrics[""kl""].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())
-
         if ""train"" in self._metrics:
             mode = ""eval"" if self.control.should_evaluate else ""train""
             self._metrics[mode][""completion_length""].append(completion_length.item())
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 5abed6a..1c8f8c8 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -411,7 +411,7 @@ replacement = """"""    loss = None
             labels = shift_labels,
             logit_softcapping = logit_softcapping,
             logit_scaling     = logit_scaling,
-            n_items           = kwargs.get(""n_items"", None),
+            n_items           = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None),
         )
     else:
         if logit_scaling != 0:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index aa7a69c..7a13f0b 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.10.0""
+__version__ = ""2024.10.1""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -43,6 +43,7 @@ __all__ = [
     ""accelerate_new_send_to_device"",
     ""patch_gradient_checkpointing"",
     ""unpatch_gradient_checkpointing"",
+    ""patch_gradient_accumulation_fix"",
 ]
 
 import torch
@@ -1138,3 +1139,63 @@ def test_mask_creation():
         assert(torch.all(correct_mask == our_mask))
     pass
 pass
+
+
+def _unsloth_get_batch_samples(self, epoch_iterator, num_batches):
+    batch_samples = []
+    num_items_in_batch = None
+    for _ in range(num_batches):
+        try:
+            batch_samples += [next(epoch_iterator)]
+        except StopIteration:
+            break
+    if len(batch_samples) > 0 and ""labels"" in batch_samples[0]:
+        try:
+            num_items_in_batch = sum(
+                [torch.count_nonzero(x[""labels""][..., 1:] != -100) for x in batch_samples]
+            )
+        except TypeError:
+            pass
+    return batch_samples, num_items_in_batch
+pass
+
+
+def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
+    if ""num_items_in_batch"" in kwargs:
+        if ""num_items_in_batch"" not in inputs:
+            inputs[""num_items_in_batch""] = kwargs[""num_items_in_batch""]
+        pass
+    pass
+    return self._old_compute_loss(model, inputs, args, kwargs)
+pass
+
+
+def patch_gradient_accumulation_fix(Trainer):
+    # Fixes gradient accumulation 
+    if hasattr(Trainer, ""get_batch_samples""):
+        from inspect import getsource
+        if \
+            not getsource(Trainer.get_batch_samples).strip()\
+            .endswith(""return batch_samples, num_items_in_batch""):
+
+            raise NotImplementedError(""Unsloth: Please make a Github issue immediately!!"")
+        else:
+            if Trainer.get_batch_samples.__name__ != ""_unsloth_get_batch_samples"":
+                Trainer.get_batch_samples = _unsloth_get_batch_samples
+            pass
+
+            # Also fix passing in num_items_in_batch
+            if not hasattr(Trainer, ""_old_compute_loss""):
+                Trainer._old_compute_loss = Trainer.compute_loss
+                Trainer.compute_loss = _unsloth_pre_compute_loss
+            pass
+        pass
+    else:
+        logger.warning_once(
+            ""Unsloth: We fixed a gradient accumulation bug, ""\
+            ""but it seems like you don't have the latest transformers version!\n""\
+            ""Please update transformers via:\n""\
+            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir ""git+https://github.com/huggingface/transformers.git""`'
+        )
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 4cd512a..f043720 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -982,7 +982,7 @@ def CausalLM_fast_forward(fast_forward_inference):
                 labels = shift_labels,
                 logit_softcapping = logit_softcapping,
                 logit_scaling     = logit_scaling,
-                n_items           = kwargs.get(""n_items"", None),
+                n_items           = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None),
             )
         else:
             if logit_scaling != 0:
@@ -1777,6 +1777,9 @@ class FastLlamaModel:
         patch_saving_functions(model)
         Trainer._inner_training_loop = _fast_inner_training_loop
 
+        # Fix gradient accumulation
+        patch_gradient_accumulation_fix(Trainer)
+
         # Save tokenizer for inference purposes
         tokenizer.padding_side = ""left"" # Force inference
         internal_model = model
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 63d07c9..ffe9933 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -928,8 +928,23 @@ def patch_sft_trainer_tokenizer():
         ""    torch.cuda.empty_cache()\n""\
         ""pass\n""\
         ""\n""\
-        ""fix_untrained_tokens(self.model, self.tokenizer, self.train_dataset, IGNORED_TOKENIZER_NAMES, eps = 1e-16)\n\n""\
-        ""fix_zero_training_loss(self.model, self.tokenizer, self.train_dataset)\n\n""
+        ""tokenizer = self.processing_class if hasattr(self, 'processing_class') else self.tokenizer\n""\
+        ""fix_untrained_tokens(self.model, tokenizer, self.train_dataset, IGNORED_TOKENIZER_NAMES, eps = 1e-16)\n\n""\
+        ""fix_zero_training_loss(self.model, tokenizer, self.train_dataset)\n\n""
+
+        # Warn on gradient accumulation steps if it's used
+        check_text += \
+        ""\n""\
+        ""try:\n""\
+        ""    gradient_accumulation_steps = self.args.gradient_accumulation_steps\n""\
+        ""    if type(gradient_accumulation_steps) is int and gradient_accumulation_steps > 1:\n""\
+        ""        from transformers import __version__ as transformers_version\n""\
+        ""        from packaging.version import Version\n""\
+        ""        if Version(transformers_version) <= Version('4.45.2'):\n""\
+        ""            print('**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers and Unsloth!')\n""\
+        ""except:\n""\
+        ""    pass\n""\
+        ""\n\n""
 
         # Add NEFTune since it doesn't seem to work?? We need to manually inject it
         check_text += \
diff --git a/unsloth/trainer.py b/unsloth/trainer.py
index c9c0ca2..25bb434 100644
--- a/unsloth/trainer.py
+++ b/unsloth/trainer.py
@@ -22,7 +22,25 @@ except:
     from transformers import TrainingArguments
 pass
 from . import is_bfloat16_supported
-from unsloth_zoo.training_utils import unsloth_train
+from unsloth_zoo.training_utils import unsloth_train as _unsloth_train
+from packaging.version import Version
+
+# Unsloth gradient accumulation fix:
+from transformers import __version__ as transformers_version
+if Version(transformers_version) > Version(""4.45.2""):
+    def unsloth_train(trainer):
+        return trainer.train()
+    pass
+else:
+    def unsloth_train(trainer):
+        print(
+            ""Unsloth: Using our custom gradient accumulation fixed trainer, which is not feature complete.\n""\
+            ""If you want to use our fix inside of HF, please update `transformers` to the latest version via:\n""\
+            '`pip uninstall transformers -y && pip install --upgrade --no-cache-dir ""git+https://github.com/huggingface/transformers.git""`'
+        )
+        return _unsloth_train(trainer)
+    pass
+pass
 
 __all__ = [
     ""UnslothTrainingArguments"",
"
"diff --git a/pyproject.toml b/pyproject.toml
index c504530..8e73577 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -37,7 +37,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.6.3"",
+    ""unsloth_zoo>=2025.6.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
@@ -381,7 +381,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.6.3"",
+    ""unsloth_zoo>=2025.6.4"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.51.3,!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 2de324e..889bbd4 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -486,6 +486,21 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         arguments = re.sub(x, y, arguments)
     pass
 
+    # Fix GRPO beta default as 0.001 TRL used to be 0.04, now 0.00!
+    # https://github.com/huggingface/trl/pull/3516
+    # https://verl.readthedocs.io/en/latest/examples/config.html
+    if trainer_file == ""grpo_trainer"":
+        replacements = {
+            ""beta"" : 0.001,
+        }
+        for k, v in replacements.items():
+            x = f""{k}( = [^,\n]{{1,}})?,\n""
+            y = f""'{v}'"" if type(v) is str else f""{v}""
+            y = f""{k} = {y},\n""
+            arguments = re.sub(x, y, arguments)
+        pass
+    pass
+
     # Warn on too large or too small learning rate
     if "" learning_rate"" in call_args:
         learning_rate_check = \
@@ -553,6 +568,17 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         extra_args += check_num_generations
     pass
 
+    # Check temperature must not be <= 0. Also stop if >= 10
+    if ""temperature"" in call_args: 
+        check_temperature = \
+        ""if temperature <= 0:\n""\
+        ""    raise MathError('Unsloth: Please set a positive non-zero temperature since your results will be wrong.')\n""\
+        ""elif temperature >= 10:\n""\
+        ""    raise MathError('Unsloth: Please set a positive non-zero temperature less than 10, since sampling will be quite erratic.')\n""\
+        ""\n""
+        extra_args += check_temperature
+    pass
+
     # Edit config with anything extra
     if trainer_file in RL_CONFIG_CHANGES:
         process_extra_args = RL_CONFIG_CHANGES[trainer_file]
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index df95f73..18f7720 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -269,6 +269,8 @@ def grpo_trainer__get_per_token_logps(function_name, function):
             # See https://github.com/huggingface/trl/issues/2770
             # logits = logits[:, -logits_to_keep:]
             # return logits
+            # See https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo#policy-training-implementation-details
+            # logits = logits / self.temperature
             # logps = selective_log_softmax(logits, input_ids)
 
             # row_indices, col_indices = torch.where(logps < -20)
@@ -325,7 +327,6 @@ def grpo_trainer_compute_loss(function_name, function):
         else:
             ref_per_token_logps = None
         # per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
-
         # x - x.detach() allows for preserving gradients from x
         advantages = inputs[""advantages""]
         # per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
@@ -335,10 +336,13 @@ def grpo_trainer_compute_loss(function_name, function):
             old_hidden_states = inputs[""old_per_token_logps""]
         else:
             old_hidden_states = None
+
         input_ids = input_ids[:, -logits_to_keep:]
         if per_token_logps is not None:
 
-            ref_per_token_logps = ref_per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
+            if ref_per_token_logps is not None:
+                ref_per_token_logps = ref_per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
+
             per_token_logps = per_token_logps[:, :-1, :] # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
             
             loss, completion_length, mean_kl = grpo_compute_loss_slow(
@@ -354,6 +358,7 @@ def grpo_trainer_compute_loss(function_name, function):
                 epsilon_high = self.epsilon_high,
                 max_completion_length = self.args.max_completion_length,
                 delta = self.args.delta,
+                temperature = self.args.temperature,
             )
         else:
             if hasattr(self.args, ""loss_type""):
@@ -370,6 +375,7 @@ def grpo_trainer_compute_loss(function_name, function):
                     epsilon_high = self.epsilon_high,
                     max_completion_length = self.args.max_completion_length,
                     delta = self.args.delta,
+                    temperature = self.args.temperature,
                 )
             else:
                 # to ensure backwards compatibility with trl 0.15.2 and maybe even 0.17
@@ -381,6 +387,7 @@ def grpo_trainer_compute_loss(function_name, function):
                     advantages,
                     old_hidden_states,
                     n_chunks = self.args.unsloth_num_chunks,
+                    temperature = self.args.temperature,
                 )
 
         # Log the metrics
"
"diff --git a/pyproject.toml b/pyproject.toml
index fc9c825..e0c5d93 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,10 +33,10 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
-    ""unsloth_zoo"",
+    ""unsloth_zoo>=2024.11.1"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.44.2"",
+    ""transformers>=4.46.1"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
@@ -244,10 +244,10 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo"",
+    ""unsloth_zoo>=2024.11.1"",
     ""packaging"",
     ""tyro"",
-    ""transformers>=4.44.2"",
+    ""transformers>=4.46.1"",
     ""datasets>=2.16.0"",
     ""sentencepiece>=0.2.0"",
     ""tqdm"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 458c269..5102d8f 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -27,13 +27,6 @@ import numpy as np
 #     pass
 # pass
 
-# Check for unsloth_zoo
-try:
-    import unsloth_zoo
-except:
-    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth-zoo`"")
-pass
-
 # Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so
 # enabling it will require much more work, so we have to prioritize. Please understand!
 # We do have a beta version, which you can contact us about!
@@ -60,6 +53,14 @@ pass
 # Reduce VRAM usage by reducing fragmentation
 os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""expandable_segments:True""
 
+# Hugging Face Hub faster downloads
+if ""HF_HUB_ENABLE_HF_TRANSFER"" not in os.environ:
+    os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
+pass
+
+# Log Unsloth is being used
+os.environ[""UNSLOTH_IS_PRESENT""] = ""1""
+
 try:
     import torch
 except ModuleNotFoundError:
@@ -71,12 +72,6 @@ except Exception as exception:
     raise exception
 pass
 
-# Hugging Face Hub faster downloads (only enable during Colab and Kaggle sessions)
-keynames = ""\n"" + ""\n"".join(os.environ.keys())
-if ""\nCOLAB_""  in keynames or ""\nKAGGLE_"" in keynames:
-    os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
-pass
-
 # We support Pytorch 2
 # Fixes https://github.com/unslothai/unsloth/issues/38
 torch_version = torch.__version__.split(""."")
@@ -165,6 +160,13 @@ if ""SPACE_AUTHOR_NAME"" not in os.environ and ""SPACE_REPO_NAME"" not in os.environ
     pass
 pass
 
+# Check for unsloth_zoo
+try:
+    import unsloth_zoo
+except:
+    raise ImportError(""Unsloth: Please install unsloth_zoo via `pip install unsloth-zoo`"")
+pass
+
 from .models import *
 from .save import *
 from .chat_templates import *
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 3e55332..82e7641 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -14,8 +14,8 @@
 
 from .cross_entropy_loss import (
     fast_cross_entropy_loss,
-    patch_llama_for_causal_lm,
-    unpatch_llama_for_causal_lm,
+    post_patch_loss_function,
+    patch_loss_functions,
 )
 from .rms_layernorm import (
     fast_rms_layernorm,
@@ -25,7 +25,6 @@ from .rms_layernorm import (
 from .layernorm import (
     fast_layernorm,
     patch_layernorm,
-    unpatch_layernorm,
 )
 from .rope_embedding import fast_rope_embedding, inplace_rope_embedding
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
@@ -54,8 +53,12 @@ from .flex_attention import (
     create_flex_attention_sliding_window_mask,
 )
 
-try:
-    print("" Unsloth: Will patch your computer to enable 2x faster free finetuning."")
-except:
-    print(""Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+import os
+if ""UNSLOTH_ZOO_IS_PRESENT"" not in os.environ:
+    try:
+        print("" Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+    except:
+        print(""Unsloth: Will patch your computer to enable 2x faster free finetuning."")
+    pass
 pass
+del os
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index f2377d5..f0193c7 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -17,24 +17,31 @@ import triton.language as tl
 import torch
 from .utils import calculate_settings, MAX_FUSED_SIZE, triton_tanh
 from transformers.models.llama.modeling_llama import logger
+from packaging.version import Version
+
+from unsloth_zoo.loss_utils import (
+    patch_loss_functions as _patch_loss_functions,
+    post_patch_loss_function,
+)
 
 
 @triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
-    ""DO_LOGIT_SCALING"": lambda args: args[""DO_LOGIT_SCALING""],
+    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
 })
 @triton.jit
 def _cross_entropy_forward(
-    logits_ptr, logits_row_stride,
-    loss_ptr,
-    logsumexp_ptr,
-    labels_ptr,
-    VOCAB_SIZE      : tl.constexpr,
-    BLOCK_SIZE      : tl.constexpr,
-    DO_SOFTCAPPING  : tl.constexpr,
-    SOFTCAP         : tl.constexpr,
-    DO_LOGIT_SCALING: tl.constexpr,
-    LOGIT_SCALE     : tl.constexpr,
+    logits_ptr        ,
+    logits_row_stride ,
+    loss_ptr          ,
+    logsumexp_ptr     ,
+    labels_ptr        ,
+    VOCAB_SIZE        ,
+    BLOCK_SIZE        : tl.constexpr,
+    DO_SOFTCAPPING    ,
+    SOFTCAP           ,
+    DO_LOGIT_SCALING  ,
+    LOGIT_SCALE       ,
 ):
     """"""
         Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]
@@ -57,7 +64,7 @@ def _cross_entropy_forward(
         This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.
     """"""
     row_idx = tl.program_id(0)
-    logits_ptr    += row_idx * logits_row_stride.to(tl.int64)
+    logits_ptr    += row_idx * tl.cast(logits_row_stride, tl.int64)
     loss_ptr      += row_idx
     logsumexp_ptr += row_idx
     labels_ptr    += row_idx
@@ -71,7 +78,7 @@ def _cross_entropy_forward(
     # Go logit scaling for Cohere: t * x
     if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
+    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits.to(tl.float32) / SOFTCAP).to(logits.dtype)
 
     logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
@@ -92,22 +99,23 @@ pass
 
 
 @triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
-    ""DO_LOGIT_SCALING"": lambda args: args[""DO_LOGIT_SCALING""],
+    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
 })
 @triton.jit
 def _chunked_cross_entropy_forward(
-    logits_ptr, logits_row_stride,
-    loss_ptr,
-    logsumexp_ptr,
-    labels_ptr,
-    VOCAB_SIZE      : tl.constexpr,
-    N_CHUNKS        : tl.constexpr,
-    BLOCK_SIZE      : tl.constexpr,
-    DO_SOFTCAPPING  : tl.constexpr,
-    SOFTCAP         : tl.constexpr,
-    DO_LOGIT_SCALING: tl.constexpr,
-    LOGIT_SCALE     : tl.constexpr,
+    logits_ptr        ,
+    logits_row_stride ,
+    loss_ptr          ,
+    logsumexp_ptr     ,
+    labels_ptr        ,
+    VOCAB_SIZE        ,
+    N_CHUNKS          ,
+    BLOCK_SIZE        : tl.constexpr,
+    DO_SOFTCAPPING    ,
+    SOFTCAP           ,
+    DO_LOGIT_SCALING  ,
+    LOGIT_SCALE       ,
 ):
     """"""
         256K vocab divided in 4 chunks
@@ -135,7 +143,7 @@ def _chunked_cross_entropy_forward(
     """"""
     row_idx   = tl.program_id(0)
     chunk_idx = tl.program_id(1)
-    logits_ptr    += row_idx * logits_row_stride.to(tl.int64)
+    logits_ptr    += row_idx * tl.cast(logits_row_stride, tl.int64)
     loss_ptr      += row_idx
     logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx
     labels_ptr    += row_idx
@@ -149,7 +157,7 @@ def _chunked_cross_entropy_forward(
     # Go logit scaling for Cohere: t * x
     if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
-    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits / SOFTCAP)
+    if DO_SOFTCAPPING:   logits = SOFTCAP * triton_tanh(logits.to(tl.float32) / SOFTCAP).to(logits.dtype)
 
     logits = logits.to(tl.float32)
     c = tl.max(logits, 0)
@@ -174,21 +182,23 @@ pass
 
 
 @triton.heuristics({
-    ""DO_SOFTCAPPING"":   lambda args: args[""DO_SOFTCAPPING""  ],
-    ""DO_LOGIT_SCALING"": lambda args: args[""DO_LOGIT_SCALING""],
+    ""DO_SOFTCAPPING"":   lambda args: bool(args[""DO_SOFTCAPPING""  ]),
+    ""DO_LOGIT_SCALING"": lambda args: bool(args[""DO_LOGIT_SCALING""]),
 })
 @triton.jit
 def _cross_entropy_backward(
-    logits_ptr, logits_row_stride,
-    dloss_ptr,   dloss_row_stride,
-    logsumexp_ptr,
-    labels_ptr,
-    VOCAB_SIZE      : tl.constexpr,
-    BLOCK_SIZE      : tl.constexpr,
-    DO_SOFTCAPPING  : tl.constexpr,
-    SOFTCAP         : tl.constexpr,
-    DO_LOGIT_SCALING: tl.constexpr,
-    LOGIT_SCALE     : tl.constexpr,
+    logits_ptr        ,
+    logits_row_stride ,
+    dloss_ptr         ,
+    dloss_row_stride  ,
+    logsumexp_ptr     ,
+    labels_ptr        ,
+    VOCAB_SIZE        ,
+    BLOCK_SIZE        : tl.constexpr,
+    DO_SOFTCAPPING    ,
+    SOFTCAP           ,
+    DO_LOGIT_SCALING  ,
+    LOGIT_SCALE       ,
 ):
     """"""
         CE_i = -y log(P) = y * (log[sum(exp(x))] - x)
@@ -208,7 +218,7 @@ def _cross_entropy_backward(
     row_idx   = tl.program_id(0)
     block_idx = tl.program_id(1)
 
-    logits_ptr += row_idx * logits_row_stride.to(tl.int64)
+    logits_ptr += row_idx * tl.cast(logits_row_stride, tl.int64)
     dloss_ptr  += row_idx *  dloss_row_stride
     col_offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
     mask = col_offsets < VOCAB_SIZE
@@ -228,9 +238,10 @@ def _cross_entropy_backward(
     pass
 
     # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
+    partial = x
     if DO_SOFTCAPPING:
         # d/dx [t * tanh(1/t * x)] = 1 - tanh^2(1/t * x)
-        partial = triton_tanh(x / SOFTCAP)
+        partial = triton_tanh(x.to(tl.float32) / SOFTCAP).to(x.dtype)
         x = SOFTCAP * partial
     pass
 
@@ -261,16 +272,20 @@ MAX_FUSED_SIZE = 65536 # 2**16
 
 class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
-    def forward(ctx, logits, labels, logit_softcapping = 0, logit_scaling = 0):
+    def forward(ctx, logits, labels, logit_softcapping : float = 0, logit_scaling : float = 0):
+        n_rows : int
+        vocab_size : int
         n_rows, vocab_size = logits.shape
 
         div, mod = divmod(vocab_size, MAX_FUSED_SIZE)
-        n_chunks = div + (mod != 0)
+        n_chunks : int = div + (mod != 0)
         losses = torch.empty(n_rows, dtype = torch.float32, device = ""cuda:0"")
 
-        DO_SOFTCAPPING   = (logit_softcapping != 0)
-        DO_LOGIT_SCALING = (logit_scaling != 0)
+        DO_SOFTCAPPING   : bool = bool(logit_softcapping != 0)
+        DO_LOGIT_SCALING : bool = bool(logit_scaling != 0)
 
+        BLOCK_SIZE : int
+        num_warps  : int
         if n_chunks == 1:
             # For small vocabs <= 65336 like Llama, Mistral
             BLOCK_SIZE, num_warps = calculate_settings(vocab_size)
@@ -325,11 +340,13 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
     def backward(ctx, dlosses):
         logits, logsumexp, labels = ctx.saved_tensors
+        n_rows : int
+        vocab_size : int
         n_rows, vocab_size = logits.shape
 
-        BLOCK_SIZE = 4096
+        BLOCK_SIZE : int = 4096
         div, mod = divmod(vocab_size, BLOCK_SIZE)
-        n_blocks = div + (mod != 0)
+        n_blocks : int = div + (mod != 0)
 
         _cross_entropy_backward[(n_rows, n_blocks,)](
             logits,   logits.stride(0),
@@ -342,14 +359,13 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
             SOFTCAP          = ctx.logit_softcapping,
             DO_LOGIT_SCALING = ctx.DO_LOGIT_SCALING,
             LOGIT_SCALE      = ctx.logit_scaling,
-            num_warps      = 8,
+            num_warps        = 8,
         )
         return logits, None, None, None,
     pass
 pass
 
 
-@torch._disable_dynamo
 def fast_cross_entropy_loss(
     logits,
     labels,
@@ -377,96 +393,12 @@ def fast_cross_entropy_loss(
         n_items = torch.count_nonzero(labels != -100)
     return loss.sum() / n_items
 pass
-
-
-from transformers.models.llama.modeling_llama import (
-    LlamaForCausalLM,
-    CausalLMOutputWithPast,
-    Optional,
-    Union,
-    Cache,
-    List,
-    Tuple,
-)
-
-# Transformers 4.47 need Unpack, KwargsForCausalLM
-try:
-    from transformers.models.llama.modeling_llama import Unpack, KwargsForCausalLM
-except:
-    pass
-pass
-
-import inspect, re
-function = inspect.getsource(LlamaForCausalLM.forward)
-function = function.split(""\n"")
-i = re.match(r""[ ]{1,}"", function[0]).span(0)[1]
-function = [x[i:] for x in function]
-function = ""\n"".join(function)
-function = function[function.find(""def forward""):]
-replacement = """"""    loss = None
-    logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
-    logit_scaling     = getattr(self.config, ""logit_scale"", 0)
-    if labels is not None:
-        shift_logits = logits
-        if not hasattr(self, ""extra_ignored_labels""):
-            # Fixes https://github.com/unslothai/unsloth/issues/10
-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = ""cuda:0"")
-        pass
-        
-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))
-        loss = fast_cross_entropy_loss(
-            logits = shift_logits,
-            labels = shift_labels,
-            logit_softcapping = logit_softcapping,
-            logit_scaling     = logit_scaling,
-            n_items           = kwargs.get(""num_items_in_batch"", None) or kwargs.get(""n_items"", None),
-        )
-    else:
-        if logit_scaling != 0:
-            if logits.requires_grad:
-                logits = logit_scaling * logits
-            else:
-                logits *= logit_scaling
-            pass
-        pass
-        if logit_softcapping != 0:
-            if logits.requires_grad:
-                logits = (1.0 / logit_softcapping) * logits
-                logits = torch.tanh(logits)
-                logits = logit_softcapping * logits
-            else:
-                logits *= (1.0 / logit_softcapping)
-                torch.tanh(logits, out = logits)
-                logits *= logit_softcapping
-            pass
-        pass
-    pass
-""""""
-function = \
-    function[:function.find(""    loss = None"")] + \
-    replacement + \
-    function[ function.find(""    if not return_dict""):]
-function = function.replace(""logits = logits.float()"", ""\n"")
-# Missed spaces
-function = function.split(""\n"")
-# Not the first one though!
-function = [function[0]] + ["" ""*4 + x for x in function[1:]]
-function = ""\n"".join(function)
-function = f""class Unsloth_LlamaForCausalLM(LlamaForCausalLM):\n""\
-f""    {function}\n""
-exec(function, globals())
-del function, replacement, inspect, re
-
-
-def patch_llama_for_causal_lm():
-    import transformers.models.llama.modeling_llama
-    transformers.models.llama.modeling_llama.LlamaForCausalLM = Unsloth_LlamaForCausalLM
-    return
+if (Version(torch.__version__) < Version(""2.4.0"")) and \
+    not hasattr(fast_cross_entropy_loss, ""__wrapped__""):
+    fast_cross_entropy_loss = torch._disable_dynamo(fast_cross_entropy_loss)
 pass
 
-
-def unpatch_llama_for_causal_lm():
-    import transformers.models.llama.modeling_llama
-    transformers.models.llama.modeling_llama.LlamaForCausalLM = LlamaForCausalLM
-    return
+# Patch CE Losses in transformers
+def patch_loss_functions():
+    _patch_loss_functions(fast_cross_entropy_loss)
 pass
diff --git a/unsloth/kernels/layernorm.py b/unsloth/kernels/layernorm.py
index 48ade6d..a5f7926 100644
--- a/unsloth/kernels/layernorm.py
+++ b/unsloth/kernels/layernorm.py
@@ -17,6 +17,9 @@ import triton
 import triton.language as tl
 import torch
 from .utils import calculate_settings
+from unsloth_zoo.patching_utils import (
+    patch_layernorm,
+)
 
 
 @triton.jit
@@ -162,27 +165,6 @@ def fast_layernorm(layernorm, X):
 pass
 
 
-from torch.nn import LayerNorm
-class Unsloth_LayerNorm(LayerNorm):
-    def forward(self, X):
-        return fast_layernorm(self, X)
-    pass
-pass
-
-
-def patch_layernorm():
-    import torch.nn
-    torch.nn.LayerNorm = Unsloth_LayerNorm
-    return
-pass
-
-
-def unpatch_layernorm():
-    import torch.nn
-    torch.nn.LayerNorm = LayerNorm
-    return
-pass
-
 
 def test_layernorm(
     dim = 1024, eps = 1e-5, dtype = torch.float16,
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index 13faf08..4b22f8c 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -53,14 +53,14 @@ def _rms_layernorm_forward(
 pass
 
 
-@triton.heuristics({""GEMMA"": lambda args: args[""GEMMA""],})
+@triton.heuristics({""GEMMA"": lambda args: bool(args[""GEMMA""]),})
 @triton.jit
 def _rms_layernorm_backward(
     dY, dY_row_stride,
     X,   X_row_stride,
     W,   W_row_stride,
     r,   r_row_stride,
-    dW, dW_row_stride,
+    # dW, dW_row_stride,
     n_cols, eps,
     GEMMA      : tl.constexpr,
     BLOCK_SIZE : tl.constexpr,
@@ -130,11 +130,15 @@ pass
 
 class Fast_RMS_Layernorm(torch.autograd.Function):
     @staticmethod
-    def forward(ctx, X, W, eps, gemma = False):
+    def forward(ctx, X : torch.Tensor, W : torch.Tensor, eps : float, gemma : bool = False):
         shape = X.shape
-        dim = shape[-1]
+        dim : int = shape[-1]
         X = X.view(-1, dim)
+        n_rows : int
+        n_cols : int
         n_rows, n_cols = X.shape
+        BLOCK_SIZE : int
+        num_warps  : int
         BLOCK_SIZE, num_warps = calculate_settings(n_cols)
 
         Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = ""cuda:0"")
@@ -159,20 +163,22 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
     pass
 
     @staticmethod
-    def backward(ctx, dY):
+    def backward(ctx, dY : torch.Tensor):
         shape = dY.shape
-        dim = shape[-1]
+        dim : int = shape[-1]
         dY = dY.view(-1, dim)
         X, W, r = ctx.saved_tensors
+        n_rows : int
+        n_cols : int
         n_rows, n_cols = dY.shape
-        dW = X
+        # dW = X
 
         _rms_layernorm_backward[(n_rows,)](
             dY, dY.stride(0),
             X,  X .stride(0),
             W,  W .stride(0),
             r,  r .stride(0),
-            dW, dW.stride(0),
+            # dW, dW.stride(0),
             n_cols, ctx.eps,
             GEMMA      = ctx.GEMMA,
             BLOCK_SIZE = ctx.BLOCK_SIZE,
@@ -184,9 +190,11 @@ class Fast_RMS_Layernorm(torch.autograd.Function):
 pass
 
 
-def fast_rms_layernorm(layernorm, X, gemma = False):
-    W   = layernorm.weight
-    eps = layernorm.variance_epsilon if \
+# [TODO] Unsure why RMS Layernorm is not torch.compiling properly
+@torch.compiler.disable
+def fast_rms_layernorm(layernorm, X : torch.Tensor, gemma : bool = False):
+    W : torch.Tensor = layernorm.weight
+    eps : float = layernorm.variance_epsilon if \
         hasattr(layernorm, ""variance_epsilon"") \
         else layernorm.eps
     out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)
diff --git a/unsloth/kernels/rope_embedding.py b/unsloth/kernels/rope_embedding.py
index 2934ac4..7fe15d0 100644
--- a/unsloth/kernels/rope_embedding.py
+++ b/unsloth/kernels/rope_embedding.py
@@ -16,9 +16,9 @@ import triton
 import triton.language as tl
 import torch
 from .utils import calculate_settings
-ROPE_GROUP_SIZE = 4
+ROPE_GROUP_SIZE : int = 4
 
-@triton.heuristics({""BACKWARD_PASS"": lambda args: args[""BACKWARD_PASS""],})
+@triton.heuristics({""BACKWARD_PASS"": lambda args: bool(args[""BACKWARD_PASS""]),})
 @triton.jit
 def _rope_embedding(
     Q,     Q_row_stride,
@@ -75,8 +75,14 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
     @staticmethod
     def forward(ctx, Q, cos, sin):
         cos, sin = cos.squeeze(), sin.squeeze()
+        batch    : int
+        seq_len  : int
+        n_heads  : int
+        head_dim : int
         batch, seq_len, n_heads, head_dim = Q.shape
         Q = Q.view(batch*seq_len, n_heads*head_dim)
+        n_rows : int
+        n_cols : int
         n_rows, n_cols = Q.shape
         assert(seq_len <= cos.shape[0])
 
@@ -85,8 +91,10 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)
         
         # group_size = 4 # 4 or 8, too large group_size can hurt performance.
+        div : int
+        mod : int
         div, mod = divmod(n_heads, ROPE_GROUP_SIZE)
-        n_groups = div + (mod != 0)
+        n_groups : int = div + (mod != 0)
 
         _rope_embedding[(n_rows, n_groups, )](
               Q,   Q.stride(0),
@@ -108,9 +116,15 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
 
     @staticmethod
     def backward(ctx, dY):
+        batch    : int
+        seq_len  : int
+        n_heads  : int
+        head_dim : int
         batch, seq_len, n_heads, head_dim = dY.shape
         dY = dY.reshape(batch*seq_len, n_heads*head_dim)
         # Must be reshape not view
+        n_rows : int
+        n_cols : int
         n_rows, n_cols = dY.shape
 
         cos = ctx.cos
@@ -130,7 +144,8 @@ class Fast_RoPE_Embedding(torch.autograd.Function):
     pass
 pass
 
-
+# [TODO] Unsure why RoPE Embedding is not torch.compiling properly
+@torch.compiler.disable
 def fast_rope_embedding(Q, K, cos, sin):
     Q = Fast_RoPE_Embedding.apply(Q.transpose(1, 2), cos, sin).transpose(1, 2)
     K = Fast_RoPE_Embedding.apply(K.transpose(1, 2), cos, sin).transpose(1, 2)
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index a8c20c7..b394d12 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -13,7 +13,7 @@
 # limitations under the License.
 
 import triton
-MAX_FUSED_SIZE = 65536
+MAX_FUSED_SIZE : int = 65536
 next_power_of_2 = triton.next_power_of_2
 
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
@@ -40,12 +40,12 @@ else:
 pass
 
 
-def calculate_settings(n):
-    BLOCK_SIZE = next_power_of_2(n)
+def calculate_settings(n : int) -> (int, int,):
+    BLOCK_SIZE : int = next_power_of_2(n)
     if BLOCK_SIZE > MAX_FUSED_SIZE:
         raise RuntimeError(f""Cannot launch Triton kernel since n = {n} exceeds ""\
                            f""the maximum CUDA blocksize = {MAX_FUSED_SIZE}."")
-    num_warps = 4
+    num_warps : int = 4
     if   BLOCK_SIZE >= 32768: num_warps = 32
     elif BLOCK_SIZE >=  8192: num_warps = 16
     elif BLOCK_SIZE >=  2048: num_warps = 8
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 51c63fc..94cf1b7 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2024.10.7""
+__version__ = ""2024.11.1""
 
 __all__ = [
     ""prepare_model_for_kbit_training"",
@@ -41,9 +41,17 @@ __all__ = [
     ""torch_amp_custom_bwd"",
     ""accelerate_old_send_to_device"",
     ""accelerate_new_send_to_device"",
+    ""patch_gradient_accumulation_fix"",
+    ""patch_compiling_bitsandbytes"",
+    ""patch_regional_compilation"",
+    ""patch_layernorm"",
+    ""patch_torch_compile"",
+    ""patch_model_and_tokenizer"",
+
+    ""patch_unsloth_gradient_checkpointing"",
+    ""unpatch_unsloth_gradient_checkpointing"",
     ""patch_gradient_checkpointing"",
     ""unpatch_gradient_checkpointing"",
-    ""patch_gradient_accumulation_fix"",
 ]
 
 import torch
@@ -54,6 +62,28 @@ import numpy as np
 import warnings, subprocess, re, inspect, psutil, os, math
 from packaging.version import Version
 
+from unsloth_zoo.tokenizer_utils import (
+    patch_tokenizer as _patch_tokenizer,
+)
+from unsloth_zoo.patching_utils import (
+    patch_compiling_bitsandbytes,
+    patch_layernorm,
+    patch_torch_compile,
+    patch_regional_compilation,
+    patch_model_and_tokenizer,
+)
+from unsloth_zoo.gradient_checkpointing import (
+    Unsloth_Offloaded_Gradient_Checkpointer,
+    unsloth_offloaded_gradient_checkpoint,
+    patch_unsloth_gradient_checkpointing,
+    unpatch_unsloth_gradient_checkpointing,
+
+    Unsloth_Gradient_Checkpointer,
+    unsloth_gradient_checkpoint,
+    patch_gradient_checkpointing,
+    unpatch_gradient_checkpointing,
+)
+
 # =============================================
 # Disable some warnings which can get annoying
 warnings.filterwarnings(action = ""ignore"", category = UserWarning,    module = ""torch"")
@@ -70,6 +100,18 @@ warnings.filterwarnings(action = ""ignore"", category = RuntimeWarning, module = ""
 # Stop ""Special tokens have been added in the vocabulary, ...""
 import logging
 logging.getLogger(""transformers.tokenization_utils_base"").setLevel(logging.CRITICAL+1)
+
+# Ignore logging messages
+class HideLoggingMessage(logging.Filter):
+    def __init__(self, text): self.text = text
+    def filter(self, x): return not x.getMessage().startswith(self.text)
+pass
+
+# The speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.
+from transformers.training_args import logger as transformers_training_args_logger
+transformers_training_args_logger.addFilter(HideLoggingMessage(""The speedups""))
+del transformers_training_args_logger
+
 # =============================================
 
 # =============================================
@@ -129,7 +171,6 @@ pass
 
 # =============================================
 # torch.cuda.amp.custom_fwd is deprecated >= 2.4
-import torch
 torch_version = torch.__version__
 if Version(torch_version) < Version(""2.4.0""):
     torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
@@ -333,7 +374,8 @@ pass
 
 # =============================================
 # Torch compile settings
-
+UNSLOTH_COMPILE_DEBUG   = ""UNSLOTH_COMPILE_DEBUG""   in os.environ
+UNSLOTH_COMPILE_MAXIMUM = ""UNSLOTH_COMPILE_MAXIMUM"" in os.environ
 # Just remove max_autotune_gemm warning
 import functools
 @functools.lru_cache(None)
@@ -345,47 +387,27 @@ def is_big_gpu(index):
     return True
 import torch._inductor.utils
 torch._inductor.utils.is_big_gpu = is_big_gpu
+patch_torch_compile(debug = UNSLOTH_COMPILE_DEBUG, O3 = UNSLOTH_COMPILE_MAXIMUM)
 
-
-# Torch compile arguments
-torch_compile_arguments = [
-    ""config.dce = True"",
-    ""config.memory_planning = True"",
-    ""config.memory_pool = 'combined'"",
-    ""config.coordinate_descent_tuning = True"",
-    ""config.max_autotune_gemm = False"", # GEMM is unnecessary
-    ""config.autotune_multi_device = False"",
-    ""config.max_autotune_gemm_backends = 'TRITON,ATEN,CPP'"", # Not much faster
-    ""config.aggressive_fusion = False"", # Careful changes results!
-    ""config.cuda.enable_cuda_lto = True"",
-    ""config.cuda.use_fast_math = True"",
-    ""config.cuda.compile_opt_level = '-O2'"",
-]
-# Torch dynamo arguments
-torch_dynamo_arguments = [
-    ""config.accumulated_cache_size_limit = 1024"", # Bump up a bit from 256
-    ""config.suppress_errors = True"", # Supress errors for now
-    ""config.do_not_emit_runtime_asserts = True"",
-    ""config.cache_size_limit = 1024"", # Flex Attention
-    ""config.inline_inbuilt_nn_modules = True"", # Torch 2.5 Regional recompilation
-]
-import torch._inductor.config as config
-for _try_compile_argument in torch_compile_arguments:
-    try:    exec(_try_compile_argument)
-    except: pass
-pass
-import torch._dynamo.config as config
-for _try_dynamo_argument in torch_dynamo_arguments:
-    try:    exec(_try_dynamo_argument)
-    except: pass
-pass
 torch_compile_options = {
     ""epilogue_fusion""   : True,
     ""max_autotune""      : True,
     ""shape_padding""     : True,
-    ""trace.enabled""     : False, # Output Triton kernel outputs!
+    ""trace.enabled""     : UNSLOTH_COMPILE_DEBUG,
     ""triton.cudagraphs"" : False,
 }
+
+import accelerate
+def torch_compile_kwargs(*args, **kwargs):
+    print(""Unsloth: Enabled auto compiling"")
+    return {""dynamic"" : True, ""fullgraph"" : False, ""options"" : torch_compile_options,}
+pass
+
+accelerate.utils.dataclasses.TorchDynamoPlugin.to_kwargs = torch_compile_kwargs
+accelerate.utils.TorchDynamoPlugin.to_kwargs             = torch_compile_kwargs
+accelerate.accelerator.TorchDynamoPlugin.to_kwargs       = torch_compile_kwargs
+del accelerate
+
 # =============================================
 
 def prepare_model_for_kbit_training(
@@ -455,137 +477,6 @@ def prepare_model_for_kbit_training(
     return model
 pass
 
-
-def patch_tokenizer(model, tokenizer):
-    """"""
-        Phi3's pad_token isn't set. We set it to <|placeholder...
-        Llama-3 is <|reserved...
-        Llama-2 is <unk>
-        Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!
-        Fixes https://github.com/unslothai/unsloth/issues/5
-    """"""
-    possible_reserved_tokens = (
-        ""<|finetune_right_pad_id|>"", # Llama-3.1
-        ""<pad>"",                     # Mistral Nemo
-        ""<|reserved"",                # Llama-3
-        ""<|placeholder"",             # Phi-3
-        ""[control"",                  # Mistral type models
-    )
-    joiner = ""\1\0=+=\0\1""
-    number_repetitions = 3 - 1 # Number of reserved tokens needed
-
-    if model is not None:
-        model.config.update({""unsloth_version"" : __version__})
-
-    bad_pad_token = False
-    if hasattr(tokenizer, ""pad_token"") and tokenizer.pad_token is not None:
-        # Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!
-        bad_pad_token = tokenizer.eos_token == tokenizer.pad_token
-    elif hasattr(tokenizer, ""pad_token"") and tokenizer.pad_token is None:
-        bad_pad_token = True
-    else:
-        bad_pad_token = False
-    pass
-
-    if bad_pad_token:
-        # Find a better pad token
-        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]
-        all_added_tokens = joiner.join(added_tokens[::-1])
-        all_added_tokens += joiner
-
-        final_pad_token  = None
-        final_good_match = False
-
-        for possible_reserved_token in possible_reserved_tokens:
-            possible_reserved_token = re.escape(possible_reserved_token)
-            found = re.finditer(f""{possible_reserved_token}"", all_added_tokens)
-            first_match = None
-            good_match  = False
-            for j, x in enumerate(found):
-                if j == 0: first_match = x
-                if j >= number_repetitions:
-                    good_match = True
-                    break
-                pass
-            pass
-
-            if first_match is None: continue
-
-            # If it ends with |> or > etc, then set it as a good pad token!
-            start = first_match.span(0)[0]
-            possible_pad_token = first_match.group(0)
-            end = all_added_tokens.find(joiner, start)
-            first_match = all_added_tokens[start:end]
-
-            if first_match is not None:
-                good_match = possible_pad_token.endswith(("">"", ""|>"", ""]"", "")""))
-            pass
-            possible_pad_token = first_match
-
-            # Replace current pad token if another exact match is found
-            if not final_good_match and good_match:
-                final_good_match = True
-                final_pad_token = possible_pad_token
-                break
-            else:
-                final_good_match = False
-                final_pad_token = possible_pad_token
-            pass
-        pass
-        possible_pad_token = final_pad_token
-
-        # Try unk_token
-        if possible_pad_token is None and hasattr(tokenizer, ""unk_token""):
-            possible_pad_token = tokenizer.unk_token
-        pass
-
-        # Check pad token's id must be less than vocab size
-        if possible_pad_token is not None:
-            check_pad_token = tokenizer(possible_pad_token, add_special_tokens = False).input_ids
-            if len(check_pad_token) != 1:
-                possible_pad_token = None
-            if model is not None and check_pad_token[0] >= model.config.vocab_size:
-                possible_pad_token = None
-        pass
-
-        if possible_pad_token is None:
-            # Failure to find a good replacement!! We shall manually add one!
-            new_pad_token = ""<|PAD_TOKEN|>""
-            while new_pad_token in tokenizer.get_vocab():
-                new_pad_token = f""<{new_pad_token}>""
-            pass
-            possible_pad_token = new_pad_token
-        pass
-
-        name = model.config._name_or_path if model is not None else ""Model""
-        logger.warning_once(
-            f""{name} does not have a padding token! Will use pad_token = {possible_pad_token}.""
-        )
-        
-        # Edit pad_token
-        tokenizer.add_special_tokens({""pad_token"" : possible_pad_token})
-        tokenizer.pad_token = possible_pad_token
-        if model is not None:
-            model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
-            if getattr(model, ""generation_config"") is not None:
-                model.generation_config.update(pad_token_id = tokenizer.pad_token_id)
-    else:
-        if model is not None:
-            if model.config.pad_token_id is None:
-                model.config.update({""pad_token_id"" : tokenizer.pad_token_id})
-                if getattr(model, ""generation_config"") is not None:
-                    model.generation_config.update(pad_token_id = tokenizer.pad_token_id)
-        pass
-    pass
-
-    if model is not None:
-        if getattr(model, ""generation_config"") is not None:
-            model.generation_config.update(max_length = model.config.max_position_embeddings)
-
-    return model, tokenizer
-pass
-
-
 # =============================================
 # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??
 # For mixed precision, we need it to be in float32 not float16.
@@ -618,6 +509,7 @@ if Version(peft_version) < Version(""0.12.0""):
         )
     pass
 pass
+
 # =============================================
 
 import psutil
@@ -678,7 +570,9 @@ def get_statistics():
     # We log some basic stats about which environment is being used.
     # We simply download a README.md file from HF - all data is made public.
     # This is simply so we can check if some envs are broken or not.
-    # You can disable this by commenting the below out
+    # You can disable this by setting UNSLOTH_DISABLE_STATISTICS
+    import os
+    if ""UNSLOTH_DISABLE_STATISTICS"" in os.environ: return
     from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
     disabled = False
     if not are_progress_bars_disabled():
@@ -710,139 +604,6 @@ def get_statistics():
 pass
 
 
-def _calculate_n_gradient_checkpoints(
-    n_layers : int,
-    method   : Optional[Union[str, int]] = ""sqrt"",
-) -> List[int]:
-    assert(type(n_layers) is int and n_layers > 0)
-
-    if method is None: method = ""sqrt""
-
-    if method == ""sqrt"":
-        n_checkpoints = int(n_layers**0.5)
-    elif type(method) is int and method > 0:
-        n_checkpoints = int(np.ceil(n_layers / method))
-    else:
-        raise ValueError(""method must be 'sqrt' or an int >0 and <= n_layers."")
-
-    size = n_layers // n_checkpoints
-    sizes = np.full(n_checkpoints, size, dtype = int)
-    leftovers = n_layers % n_checkpoints
-    # We append leftovers from the right
-    for k in range(leftovers):
-        sizes[n_checkpoints-1-k] += 1
-    boundaries = np.hstack((0, np.cumsum(sizes)))
-    boundaries = boundaries.tolist()
-    return boundaries
-pass
-
-
-def calculate_n_gradient_checkpoints(
-    n_layers              : int,
-    layers_per_checkpoint : Optional[Union[str, int]] = ""sqrt"",
-) -> List[int]:
-    assert(type(n_layers) is int and n_layers > 0)
-
-    if layers_per_checkpoint is None or layers_per_checkpoint == 1:
-        return None
-
-    boundaries = _calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)
-
-    assert(boundaries[0] == 0 and boundaries[-1] == n_layers)
-    assert(min(boundaries) == 0 and max(boundaries) == n_layers)
-    assert(np.diff(boundaries).min() >= 0)
-    return boundaries
-pass
-
-
-def prepare_n_gradient_checkpoints(
-    model                 : Any,
-    layers_per_checkpoint : Optional[Union[str, int]] = ""sqrt"",
-    use_reentrant         : Optional[bool] = True,
-) -> None:
-    """"""
-    Calculates where to place the gradient checkpoints given n_layers.
-
-    Args:
-        model: Any LlamaModel with layers.
-        layers_per_checkpoint (`Union[str, int]`, *optional*):
-            Can either be `sqrt` or an integer for how many layers per checkpoint you want.
-            The more, the less memory usage, but can be slower. Default is `sqrt`.
-            Choose 1 for Pytorch gradient checkpointing. 2 to wrap 2 layers in 1 module etc.
-        use_reentrant (`bool`, *optional*):
-            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354
-            Optimal gradient checkpointing algorithm `use_reentrant=False` which will
-            be the default in future Pytorch versions doesn't seem to work??
-    """"""
-    _model = None
-    if hasattr(model, ""layers""):
-        _model = model
-    elif hasattr(model, ""model""):
-        if hasattr(model.model, ""layers""):
-            _model = model.model
-    if _model is None:
-        raise TypeError(""`model` or `model.model` does not have attribute `layers`. Are you sure this is a model?"")
-    pass
-
-    if use_reentrant is False:
-        use_reentrant = True
-    pass
-
-    n_layers = len(_model.layers)
-    boundaries = calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)
-    _model._gradient_checkpointing_boundaries    = boundaries
-    _model._gradient_checkpointing_use_reentrant = use_reentrant
-pass
-
-
-class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
-    """"""
-    Saves VRAM by smartly offloading to RAM.
-    Tiny hit to performance, since we mask the movement via non blocking calls.
-    """"""
-    @staticmethod
-    @torch_amp_custom_fwd
-    def forward(ctx, forward_function, hidden_states, *args):
-        saved_hidden_states = hidden_states.to(""cpu"", non_blocking = True)
-        with torch.no_grad():
-            output = forward_function(hidden_states, *args)
-        ctx.save_for_backward(saved_hidden_states)
-        ctx.forward_function = forward_function
-        ctx.args = args
-        return output
-    pass
-
-    @staticmethod
-    @torch_amp_custom_bwd
-    def backward(ctx, dY):
-        (hidden_states,) = ctx.saved_tensors
-        hidden_states = hidden_states.to(""cuda:0"", non_blocking = True).detach()
-        hidden_states.requires_grad_(True)
-        with torch.enable_grad():
-            (output,) = ctx.forward_function(hidden_states, *ctx.args)
-        torch.autograd.backward(output, dY)
-        return (None, hidden_states.grad,) + (None,)*len(ctx.args)
-    pass
-pass
-
-
-@torch._disable_dynamo
-def unsloth_offloaded_gradient_checkpoint(function, *args, use_reentrant = None, **kwargs):
-    return Unsloth_Offloaded_Gradient_Checkpointer.apply(function, *args)
-pass
-
-
-import torch.utils
-old_checkpoint = torch.utils.checkpoint
-def patch_gradient_checkpointing():
-    torch.utils.checkpoint = unsloth_offloaded_gradient_checkpoint
-pass
-
-def unpatch_gradient_checkpointing():
-    torch.utils.checkpoint = old_checkpoint
-pass
-
-
 # =============================================
 # Fixes Bitsandbytes to remove missing warnings
 from transformers.utils.quantization_config import BitsAndBytesConfig, QuantizationMethod
@@ -1189,6 +950,7 @@ def patch_gradient_accumulation_fix(Trainer):
     # Fixes gradient accumulation 
     import inspect
     if hasattr(Trainer, ""get_batch_samples""):
+        if Trainer.get_batch_samples.__name__ == ""_unsloth_get_batch_samples"": return
         if \
             not inspect.getsource(Trainer.get_batch_samples).strip()\
             .endswith(""return batch_samples, num_items_in_batch""):
@@ -1215,6 +977,7 @@ def patch_gradient_accumulation_fix(Trainer):
     pass
 
     # Also fix up loss scaling ie negate loss *= self.args.gradient_accumulation_steps
+    if Trainer.training_step.__name__ == ""_unsloth_training_step"": return
     if ""num_items_in_batch"" not in inspect.signature(Trainer.training_step).parameters: return
 
     function = inspect.getsource(Trainer.training_step)
@@ -1243,3 +1006,11 @@ def patch_gradient_accumulation_fix(Trainer):
     exec(function, globals())
     Trainer.training_step = _unsloth_training_step
 pass
+
+
+def patch_tokenizer(model, tokenizer):
+    model, tokenizer = _patch_tokenizer(model, tokenizer)
+    if model is not None:
+        model.config.update({""unsloth_version"" : __version__})
+    return model, tokenizer
+pass
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
index 45f14c1..1d9a0c1 100644
--- a/unsloth/models/gemma.py
+++ b/unsloth/models/gemma.py
@@ -339,60 +339,9 @@ class FastGemmaModel(FastLlamaModel):
 
 
     @staticmethod
-    def post_patch(model):
-        # Patch model for Gemma
-        layers = model.model.layers
-
-        # Torch.compile fails on embedding matrix??
-        # Workaround randomnly fixes it for torch versions < 2.2
-        model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)
-        model.config.update({""unsloth_version"" : __version__})
-
-        # We also do this for the lm_head
-        lm_head = torch.nn.Linear(1, 1, bias = None)
-        del lm_head.weight
-        lm_head.weight = model.lm_head.weight
-        lm_head.in_features  = lm_head.weight.shape[1]
-        lm_head.out_features = lm_head.weight.shape[0]
-        model.lm_head = lm_head
-
-        # Gemma has tied weights! This means lm_head == embed_tokens
-        if model.model.embed_tokens.weight.data_ptr() != model.lm_head.weight.data_ptr():
-            lm_head = torch.nn.Linear(1, 1, bias = None)
-            del lm_head.weight
-            lm_head.weight = model.model.embed_tokens.weight
-            lm_head.in_features  = lm_head.weight.shape[1]
-            lm_head.out_features = lm_head.weight.shape[0]
-            model.lm_head = lm_head
-        pass
-
-        # Also patch all dtypes - BnB seems to not allocate the correct type?
-        # BnB default dtype seems to be float16!
-        correct_dtype = lm_head.weight.dtype
-
-        for name, module in model.named_modules():
-            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
-                weight = module.weight
-                quant_state = weight.quant_state
-
-                if type(quant_state) is list:
-                    # BnB seems to have float16 as default!
-                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
-                else:
-                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
-                    quant_state.dtype = correct_dtype
-                pass
-            pass
-            # Downcast RoPE embedding to correct data type
-            # RoPE must be done in float32 for Gemma
-            # if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
-            #     and (module.cos_cached.dtype != correct_dtype):
-
-            #     module.cos_cached = module.cos_cached.to(correct_dtype)
-            #     module.sin_cached = module.sin_cached.to(correct_dtype)
-            #     pass
-            # pass
-        pass
+    def post_patch(model, tokenizer):
+        # Gemma does not downcast RoPE
+        model, tokenizer = patch_model_and_tokenizer(model, tokenizer, downcast_rope = False)
 
         # Add 1 to weight
         # return output * (1 + self.weight)
@@ -425,6 +374,6 @@ class FastGemmaModel(FastLlamaModel):
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()
-        return model
+        return model, tokenizer
     pass
 pass
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index bf40ea8..4eb9d64 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -490,60 +490,9 @@ class FastGemma2Model(FastLlamaModel):
 
 
     @staticmethod
-    def post_patch(model):
-        # Patch model for Gemma
-        layers = model.model.layers
-
-        # Torch.compile fails on embedding matrix??
-        # Workaround randomnly fixes it for torch versions < 2.2
-        model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)
-        model.config.update({""unsloth_version"" : __version__})
-
-        # We also do this for the lm_head
-        lm_head = torch.nn.Linear(1, 1, bias = None)
-        del lm_head.weight
-        lm_head.weight = model.lm_head.weight
-        lm_head.in_features  = lm_head.weight.shape[1]
-        lm_head.out_features = lm_head.weight.shape[0]
-        model.lm_head = lm_head
-
-        # Gemma has tied weights! This means lm_head == embed_tokens
-        if model.model.embed_tokens.weight.data_ptr() != model.lm_head.weight.data_ptr():
-            lm_head = torch.nn.Linear(1, 1, bias = None)
-            del lm_head.weight
-            lm_head.weight = model.model.embed_tokens.weight
-            lm_head.in_features  = lm_head.weight.shape[1]
-            lm_head.out_features = lm_head.weight.shape[0]
-            model.lm_head = lm_head
-        pass
-
-        # Also patch all dtypes - BnB seems to not allocate the correct type?
-        # BnB default dtype seems to be float16!
-        correct_dtype = lm_head.weight.dtype
-
-        for name, module in model.named_modules():
-            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
-                weight = module.weight
-                quant_state = weight.quant_state
-
-                if type(quant_state) is list:
-                    # BnB seems to have float16 as default!
-                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
-                else:
-                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
-                    quant_state.dtype = correct_dtype
-                pass
-            pass
-            # Downcast RoPE embedding to correct data type
-            # RoPE must be done in float32 for Gemma
-            # if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
-            #     and (module.cos_cached.dtype != correct_dtype):
-
-            #     module.cos_cached = module.cos_cached.to(correct_dtype)
-            #     module.sin_cached = module.sin_cached.to(correct_dtype)
-            #     pass
-            # pass
-        pass
+    def post_patch(model, tokenizer):
+        # Gemma does not downcast RoPE
+        model, tokenizer = patch_model_and_tokenizer(model, tokenizer, downcast_rope = False)
 
         # Add 1 to weight
         # return output * (1 + self.weight)
@@ -576,6 +525,6 @@ class FastGemma2Model(FastLlamaModel):
         for _ in range(3):
             gc.collect()
             torch.cuda.empty_cache()
-        return model
+        return model, tokenizer
     pass
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index cf05d43..3c4d8f3 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -57,8 +57,6 @@ from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING
 from transformers import set_seed as transformers_set_seed
 from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
 from peft import PeftModelForCausalLM
-from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit
-from peft.tuners.lora import Linear4bit as Peft_Linear4bit
 from ..save import patch_saving_functions
 import re, os, inspect, math, sys
 try:
@@ -1518,6 +1516,7 @@ class FastLlamaModel:
         pass
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
+        os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = ""1""
 
         model_patcher.pre_patch()
         get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
@@ -1621,7 +1620,7 @@ class FastLlamaModel:
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
-        model = model_patcher.post_patch(model)
+        model, tokenizer = model_patcher.post_patch(model, tokenizer)
 
         # Patch up QKV / O and MLP
         for idx, layer in enumerate(model.model.layers):
@@ -1797,93 +1796,15 @@ class FastLlamaModel:
             internal_model = internal_model.model
         pass
         internal_model._saved_temp_tokenizer = tokenizer
-
-        # Also fix torch_dtype
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""config""):
-                if   internal_model.config.torch_dtype ==  ""float32"":
-                    internal_model.config.torch_dtype = torch.float32
-                elif internal_model.config.torch_dtype == ""bfloat16"":
-                    internal_model.config.torch_dtype = torch.bfloat16
-                elif internal_model.config.torch_dtype ==  ""float16"":
-                    internal_model.config.torch_dtype = torch.float16
-                pass
-            pass
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""config""):
-            if   internal_model.config.torch_dtype ==  ""float32"":
-                internal_model.config.torch_dtype = torch.float32
-            elif internal_model.config.torch_dtype == ""bfloat16"":
-                internal_model.config.torch_dtype = torch.bfloat16
-            elif internal_model.config.torch_dtype ==  ""float16"":
-                internal_model.config.torch_dtype = torch.float16
-            pass
-        pass
         
         return model, tokenizer
     pass
 
 
     @staticmethod
-    def post_patch(model):
-        # Patch model
-        layers = model.model.layers
-
-        # Torch.compile fails on embedding matrix??
-        # Workaround randomnly fixes it for torch versions < 2.
-        model.set_input_embeddings(torch.nn.Embedding.from_pretrained(model.get_input_embeddings().weight))
-        model.config.update({""unsloth_version"" : __version__})
-
-        # We also do this for the lm_head
-        lm_head = torch.nn.Linear(1, 1, bias = None)
-        del lm_head.weight
-        lm_head.weight = model.get_output_embeddings().weight
-        lm_head.in_features  = lm_head.weight.shape[1]
-        lm_head.out_features = lm_head.weight.shape[0]
-        model.lm_head = lm_head
-        
-        # Also patch all dtypes - BnB seems to not allocate the correct type?
-        # BnB default dtype seems to be float16!
-        correct_dtype = lm_head.weight.dtype
-
-        for name, module in model.named_modules():
-            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
-                weight = module.weight
-                quant_state = weight.quant_state
-
-                if type(quant_state) is list:
-                    # BnB seems to have float16 as default!
-                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
-                else:
-                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
-                    quant_state.dtype = correct_dtype
-                pass
-            pass
-            # Downcast RoPE embedding to correct data type
-            if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")):
-
-                if hasattr(module, ""cos_cached"") and \
-                    (module.cos_cached.dtype != correct_dtype):
-
-                    module.cos_cached = module.cos_cached.to(correct_dtype)
-                    module.sin_cached = module.sin_cached.to(correct_dtype)
-
-                elif hasattr(module, ""short_cos_cached"") and \
-                    (module.short_cos_cached.dtype != correct_dtype):
-                    
-                    module.short_cos_cached = module.short_cos_cached.to(correct_dtype)
-                    module.short_sin_cached = module.short_sin_cached.to(correct_dtype)
-                pass
-            pass
-        pass
-
-        # Clear deleted GPU items
-        for _ in range(3):
-            gc.collect()
-            torch.cuda.empty_cache()
-        return model
+    def post_patch(model, tokenizer):
+        model, tokenizer = patch_model_and_tokenizer(model, tokenizer, downcast_rope = True)
+        return model, tokenizer
     pass
 
 
@@ -1910,6 +1831,11 @@ class FastLlamaModel:
     ):
         transformers_set_seed(random_state)
 
+        if type(r) is not int:
+            raise TypeError(f""Unsloth: Rank of {str(r)} must be an integer."")
+        if r <= 0:
+            raise TypeError(f""Unsloth: Rank of {str(r)} must be larger than 0."")
+
         if isinstance(model, PeftModelForCausalLM):
             # Check if exactly the same and then pass through!
             assert(hasattr(model, ""peft_config""))
diff --git a/unsloth/save.py b/unsloth/save.py
index ccda79a..b4c6b49 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -49,6 +49,7 @@ __all__ = [
 keynames = ""\n"" + ""\n"".join(os.environ.keys())
 IS_COLAB_ENVIRONMENT  = ""\nCOLAB_""  in keynames
 IS_KAGGLE_ENVIRONMENT = ""\nKAGGLE_"" in keynames
+KAGGLE_TMP = ""/tmp""
 del keynames
 
 # Weights
@@ -447,13 +448,20 @@ def unsloth_save_model(
     if push_to_hub and ""/"" in save_directory:
 
         # +1 solves absolute path issues
-        username = save_directory[:save_directory.find(""/"")]
-        new_save_directory = save_directory[save_directory.find(""/"")+1:]
-
-        logger.warning_once(
-            f""Unsloth: You are pushing to hub, but you passed your HF username = {username}.\n""\
-            f""We shall truncate {save_directory} to {new_save_directory}""
-        )
+        new_save_directory = save_directory
+        username = new_save_directory[:new_save_directory.find(""/"")]
+        new_save_directory = new_save_directory[new_save_directory.find(""/"")+1:]
+        if IS_KAGGLE_ENVIRONMENT:
+            new_save_directory = os.path.join(KAGGLE_TMP, new_save_directory[new_save_directory.find(""/"")+1:])
+            logger.warning_once(
+                ""Unsloth: You are pushing to hub in Kaggle environment.\n""\
+                f""To save memory, we shall move {save_directory} to {new_save_directory}""
+            )
+        else:
+            logger.warning_once(
+                f""Unsloth: You are pushing to hub, but you passed your HF username = {username}.\n""\
+                f""We shall truncate {save_directory} to {new_save_directory}""
+            )
 
         save_pretrained_settings[""save_directory""] = new_save_directory
         tokenizer_save_settings [""save_directory""] = new_save_directory
@@ -507,6 +515,10 @@ def unsloth_save_model(
           f""{round(max_ram/1024/1024/1024, 2)} out of ""\
           f""{round(psutil.virtual_memory().total/1024/1024/1024, 2)} RAM for saving."")
 
+    # Move temporary_location to /tmp in Kaggle
+    if IS_KAGGLE_ENVIRONMENT:
+        temporary_location = os.path.join(KAGGLE_TMP, temporary_location)
+
     # Max directory for disk saving
     if not os.path.exists(temporary_location):
         os.makedirs(temporary_location)
@@ -708,7 +720,7 @@ def unsloth_save_model(
     print(""Done."")
 
     if push_to_hub and hasattr(model, ""config""):
-        print(f""Saved merged model to https://huggingface.co/{username}/{save_directory.lstrip('/')}"")
+        print(f""Saved merged model to https://huggingface.co/{username}/{save_directory.lstrip('/').split('/')[-1]}"")
     pass
 
     save_pretrained_settings[""state_dict""] = None
@@ -1108,14 +1120,17 @@ def save_to_gguf(
     # Check if quantization succeeded!
     if not os.path.isfile(final_location):
         if IS_KAGGLE_ENVIRONMENT:
-            raise RuntimeError(
-                f""Unsloth: Quantization failed for {final_location}\n""\
-                ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
-                ""Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\n""\
-                ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
-                ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
-                ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
-            )
+            if not Path(final_location).resolve().is_relative_to(Path('/tmp').resolve()):
+                raise RuntimeError(
+                    f""Unsloth: Quantization failed for {final_location}\n""\
+                    ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
+                    ""Kaggle only provides 20GB of disk space in the working directory.\n""\
+                    ""Merging to 16bit for 7b models use 16GB of space.\n""\
+                    ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
+                    ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
+                    ""You can try saving it to the `/tmp` directory for larger disk space.\n""\
+                    ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
+                )
         else:
             raise RuntimeError(
                 f""Unsloth: Quantization failed for {final_location}\n""\
@@ -1156,14 +1171,17 @@ def save_to_gguf(
             # Check if quantization succeeded!
             if not os.path.isfile(final_location):
                 if IS_KAGGLE_ENVIRONMENT:
-                    raise RuntimeError(
-                        f""Unsloth: Quantization failed for {final_location}\n""\
-                        ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
-                        ""Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\n""\
-                        ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
-                        ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
-                        ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
-                    )
+                    if not Path(final_location).resolve().is_relative_to(Path('/tmp').resolve()):
+                        raise RuntimeError(
+                            f""Unsloth: Quantization failed for {final_location}\n""\
+                            ""You are in a Kaggle environment, which might be the reason this is failing.\n""\
+                            ""Kaggle only provides 20GB of disk space in the working directory.\n""\
+                            ""Merging to 16bit for 7b models use 16GB of space.\n""\
+                            ""This means using `model.{save_pretrained/push_to_hub}_merged` works, but\n""\
+                            ""`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\n""\
+                            ""You can try saving it to the `/tmp` directory for larger disk space.\n""\
+                            ""I suggest you to save the 16bit model first, then use manual llama.cpp conversion.""
+                        )
                 else:
                     raise RuntimeError(
                         ""Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\n""\
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index c05485f..c639dbf 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -64,6 +64,7 @@ IGNORED_TOKENIZER_NAMES = frozenset(
 keynames = ""\n"" + ""\n"".join(os.environ.keys())
 IS_COLAB_ENVIRONMENT  = ""\nCOLAB_""  in keynames
 IS_KAGGLE_ENVIRONMENT = ""\nKAGGLE_"" in keynames
+KAGGLE_TMP = ""/tmp""
 del keynames
 
 
@@ -470,8 +471,12 @@ def _load_correct_tokenizer(
     cache_dir = ""huggingface_tokenizers_cache"",
     fix_tokenizer = True,
 ):
-    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:
+    if IS_COLAB_ENVIRONMENT:
         cache_dir = cache_dir
+    elif IS_KAGGLE_ENVIRONMENT:
+        # /tmp of Kaggle seems has a 80GB limit!
+        # Let's utilize them
+        cache_dir = os.path.join(KAGGLE_TMP, cache_dir)
     else:
         cache_dir = None
     pass
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 596548d..d31f4c7 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -21,6 +21,7 @@ __all__ = [
     ""to_sharegpt"",
     ""standardize_sharegpt"",
     ""apply_chat_template"",
+    ""train_on_responses_only"",
 
     ""test_construct_chat_template"",
 ]
@@ -1063,7 +1064,6 @@ default_system_message = \
     ""Below are some instructions that describe some tasks. Write responses that appropriately complete each request."",
 
 extra_eos_tokens = None,
-  
 ):
     """"""
     Creates a Ollama modelfile and a HF Jinja template from a custom
@@ -1072,6 +1072,9 @@ extra_eos_tokens = None,
 
     You must use {INPUT}, {OUTPUT} twice, and {SYSTEM} is optional.
     """"""
+    # Strip only the left
+    chat_template = chat_template.lstrip()
+
     assert(tokenizer is not None)
 
     if extra_eos_tokens is None: extra_eos_tokens = []
@@ -1128,19 +1131,47 @@ extra_eos_tokens = None,
         chat_template = re.sub(r""{OUTPUT}"", r""{OUTPUT}"" + eos, chat_template)
     pass
 
-    # O(N^2) search finding 2 repeatted pieces of text
-    j = len(chat_template)-1
-    at_least_one = False
-    while j > 0:
-        found = chat_template.rfind(chat_template[j:], 0, j)
-        if found == -1: break
-        j -= 1
-        at_least_one = True
-    pass
-    if j > 0: j += 1
-    else: raise RuntimeError(error_msg)
+    # This forces you to provide 2 input and outputs
+    final_combined_check = False
+
+    try:
+        # O(N^2) search finding 2 repeatted pieces of text
+        j = len(chat_template)-1
+        at_least_one = False
+        while j > 0:
+            found = chat_template.rfind(chat_template[j:], 0, j)
+            if found == -1: break
+            j -= 1
+            at_least_one = True
+        pass
+        if j > 0: j += 1
+        else: raise RuntimeError(error_msg)
+
+        if not at_least_one: raise RuntimeError(error_msg)
 
-    if not at_least_one: raise RuntimeError(error_msg)
+        # Must be equivalent to left
+        final_combined_check = True
+    except:
+        # Simple 1 singular input and output
+        system_count = chat_template.count(""{SYSTEM}"")
+        input_count  = chat_template.count(""{INPUT}"")
+        output_count = chat_template.count(""{OUTPUT}"")
+        if system_count > 1:
+            raise RuntimeError(""You must only provide 1 {SYSTEM} in the chat template"")
+        if input_count > 1:
+            raise RuntimeError(""You must only provide 1 {INPUT} in the chat template"")
+        if output_count > 1:
+            raise RuntimeError(""You must only provide 1 {OUTPUT} in the chat template"")
+
+        if system_count != 0:
+            j = next(re.finditer(r""\{SYSTEM\}[\s]{0,}"", chat_template)).span(0)[1]
+        else:
+            j = 0
+        pass
+
+        # Must be equivalent to the original text
+        final_combined_check = False
+    pass
 
     # Repeatted text
     instruction_response = chat_template[j:]
@@ -1153,6 +1184,8 @@ extra_eos_tokens = None,
     # 2nd Instruction, Output pair
     right = chat_template[j:]
 
+    final_combined_check = left if final_combined_check else chat_template
+
     # Isolate input
     extra_eos_tokens_regex = ""|"".join(f""(?:{re.escape(x)})"" for x in extra_eos_tokens)
     if len(extra_eos_tokens_regex) != 0:
@@ -1170,13 +1203,14 @@ extra_eos_tokens = None,
     output_part = right[input_end:]
 
     # Isolate system
-    system_part = left[:left.find(input_part)]
+    where_system = left.find(input_part)
+    system_part = left[:where_system if where_system != -1 else len(left)]
 
     # Check if the user provided a correct prompt
     combined = system_part + input_part + output_part
-    if combined != left:
-        combined_changed = combined.replace('\n', '\\n')
-        left_changed     = left    .replace('\n', '\\n')
+    if combined != final_combined_check:
+        combined_changed = combined            .replace('\n', '\\n')
+        left_changed     = final_combined_check.replace('\n', '\\n')
         raise RuntimeError(
             ""Unsloth: The prompt template you provided isn't correct. You gave:\n""\
             f""{combined_changed}\n\n""\
@@ -1285,6 +1319,15 @@ extra_eos_tokens = None,
             jinja_template = ""{{ bos_token }}"" + jinja_template
     pass
 
+    # Fix missing loop_messages
+    if ""{% set loop_messages = messages %}"" not in jinja_template:
+        jinja_template = jinja_template.replace(
+            ""{% for message in loop_messages %}"",
+            ""{% for message in messages %}"",
+            1, # Only replace the first one
+        )
+    pass
+
     # Check if system part is the same!
     jinja_template = re.sub(
         r""\{\% if messages\[0\]\['role'\] \=\= 'system' \%\}\{\{ '(.+?)' \}\}""\
@@ -1300,8 +1343,11 @@ extra_eos_tokens = None,
         if not jinja_template.startswith(""{{ bos_token }}""):
             jinja_template = ""{{ bos_token }}"" + jinja_template
     pass
-    
-    return modelfile, jinja_template
+
+    # Get instruction and output parts for train_on_inputs = False
+    input_part  = input_part [:input_part .find(""{INPUT}"")]
+    output_part = output_part[:output_part.find(""{OUTPUT}"")]
+    return modelfile, jinja_template, input_part, output_part
 pass
 
 
@@ -1327,7 +1373,7 @@ def test_construct_chat_template():
       
     extra_eos_tokens = None
 
-    modelfile, jinja_template = construct_chat_template(
+    modelfile, jinja_template, _, _ = construct_chat_template(
         tokenizer = tokenizer,
         chat_template = chat_template,
         extra_eos_tokens = extra_eos_tokens,
@@ -1380,7 +1426,7 @@ extra_eos_tokens = None,
 
     You must use {INPUT}, {OUTPUT} twice, and {SYSTEM} is optional.
     """"""
-    modelfile, jinja_template = construct_chat_template(
+    modelfile, jinja_template, input_part, output_part = construct_chat_template(
         tokenizer = tokenizer,
         chat_template = chat_template,
         default_system_message = default_system_message,
@@ -1391,12 +1437,90 @@ extra_eos_tokens = None,
         texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
         return { ""text"" : texts, }
     pass
+
     tokenizer.chat_template = jinja_template
     tokenizer._ollama_modelfile = modelfile
+    tokenizer._unsloth_input_part  = input_part
+    tokenizer._unsloth_output_part = output_part
+
     return dataset.map(formatting_prompts_func, batched = True,)
 pass
 
 
+def train_on_responses_only(
+    trainer,
+    instruction_part = None,
+    response_part    = None,
+):
+    """"""
+    Trains only on responses and not on the instruction by masking out
+    the labels with -100 for the instruction part.
+    """"""
+    tokenizer = trainer.tokenizer
+    
+    if  not hasattr(tokenizer, ""_unsloth_input_part"") or \
+        not hasattr(tokenizer, ""_unsloth_output_part""):
+        
+        if instruction_part is None or response_part is None:
+            raise ValueError(""Unsloth: instruction_part and response_part must be given!"")
+        pass
+    elif (instruction_part is not None or response_part is not None) and \
+        (hasattr(tokenizer, ""_unsloth_input_part"") or hasattr(tokenizer, ""_unsloth_output_part"")):
+
+        raise ValueError(""Unsloth: Your tokenizer already has instruction and response parts set - do not give custom ones!"")
+    else:
+        instruction_part = tokenizer._unsloth_input_part
+        response_part    = tokenizer._unsloth_output_part
+    pass
+
+    instruction_ids = tokenizer(instruction_part,  add_special_tokens = False).input_ids
+    response_ids    = tokenizer(response_part, add_special_tokens = False).input_ids
+
+    instruction_length = len(instruction_ids)
+    response_length    = len(response_ids)
+    max_length = max(instruction_length, response_length)
+
+    def _train_on_responses_only(examples):
+        input_ids_ = examples[""input_ids""]
+        all_labels = []
+
+        for input_ids in input_ids_:
+
+            labels = [-100] * len(input_ids)
+            m = len(input_ids) - max_length
+            first_response    = response_ids[0]
+            first_instruction = instruction_ids[0]
+            j = 0
+            while j < m:
+                if input_ids[j] == first_response:
+                    if input_ids[j : j+response_length] == response_ids:
+                        j = j + response_length
+                        start = j
+                        while j < m:
+                            if input_ids[j] == first_instruction and input_ids[j : j+instruction_length] == instruction_ids:
+                                j = j + instruction_length
+                                labels[start : j] = input_ids[start : j]
+                                break
+                            elif j == (m-1):
+                                j = m
+                                labels[start:] = input_ids[start:]
+                                break
+                            pass
+                            j += 1
+                        pass
+                    pass
+                pass
+                j += 1
+            pass
+            all_labels.append(labels)
+        pass
+        return { ""labels"" : all_labels }
+    pass
+    trainer.train_dataset = trainer.train_dataset.map(_train_on_responses_only, batched = True)
+    return trainer
+pass
+
+
 def create_stopping_criteria(tokenizer, stop_word = ""eos_token""):
     class StoppingCriteriaSub(StoppingCriteria):
         __slots__ = ""stop_token"", ""single_match"", ""length"",
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index 8f7aea5..8f41017 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -19,6 +19,8 @@ from .utils import (
     get_lora_parameters,
     get_lora_parameters_bias,
     matmul_lora,
+    torch_amp_custom_fwd,
+    torch_amp_custom_bwd,
 )
 
 
@@ -61,7 +63,7 @@ class LoRA_MLP(torch.autograd.Function):
     Don't forget to see our blog post for more details!
     """"""
     @staticmethod
-    @torch.cuda.amp.custom_fwd
+    @torch_amp_custom_fwd
     def forward(ctx, X : torch.Tensor,
                 gateW, gateW_quant, gateA, gateB, gateS,
                   upW,   upW_quant, upA,   upB,   upS,
@@ -87,7 +89,7 @@ class LoRA_MLP(torch.autograd.Function):
 
 
     @staticmethod
-    @torch.cuda.amp.custom_bwd
+    @torch_amp_custom_bwd
     def backward(ctx, dY : torch.Tensor):
         gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, \
             _backward_function = ctx.custom_saved_tensors
@@ -223,7 +225,7 @@ class LoRA_QKV(torch.autograd.Function):
     dC/dBv = A.T @ X.T @ D(Wv)
     """"""
     @staticmethod
-    @torch.cuda.amp.custom_fwd
+    @torch_amp_custom_fwd
     def forward(ctx, X : torch.Tensor,
                 QW, QW_quant, QA, QB, QS,
                 KW, KW_quant, KA, KB, KS,
@@ -244,7 +246,7 @@ class LoRA_QKV(torch.autograd.Function):
     pass
 
     @staticmethod
-    @torch.cuda.amp.custom_bwd
+    @torch_amp_custom_bwd
     def backward(ctx, dQ, dK, dV):
         QW, QW_quant, QS, KW, KW_quant, KS, VW, VW_quant, VS = \
             ctx.custom_saved_tensors
@@ -352,7 +354,7 @@ class LoRA_W(torch.autograd.Function):
     dC/dBv = A.T @ X.T @ D(Wv)
     """"""
     @staticmethod
-    @torch.cuda.amp.custom_fwd
+    @torch_amp_custom_fwd
     def forward(ctx, X : torch.Tensor,
                 W, W_quant, A, B, S):
         dtype = X.dtype
@@ -363,7 +365,7 @@ class LoRA_W(torch.autograd.Function):
     pass
 
     @staticmethod
-    @torch.cuda.amp.custom_bwd
+    @torch_amp_custom_bwd
     def backward(ctx, dY : torch.Tensor):
         W, W_quant, S = ctx.custom_saved_tensors
         A, B, X = ctx.saved_tensors
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 935f1d4..4b78900 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -16,6 +16,18 @@ import triton
 MAX_FUSED_SIZE = 65536
 next_power_of_2 = triton.next_power_of_2
 
+# torch.cuda.amp.custom_fwd is deprecated >= 2.4
+import torch
+from packaging.version import Version
+if Version(torch.__version__) < Version(""2.4.0""):
+    torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
+    torch_amp_custom_bwd = torch.cuda.amp.custom_bwd
+else:
+    torch_amp_custom_fwd = torch.amp.custom_fwd(device_type = ""cuda"")
+    torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""cuda"")
+pass
+
+
 def calculate_settings(n):
     BLOCK_SIZE = next_power_of_2(n)
     if BLOCK_SIZE > MAX_FUSED_SIZE:
@@ -32,7 +44,6 @@ pass
 import bitsandbytes as bnb
 get_ptr = bnb.functional.get_ptr
 import ctypes
-import torch
 cdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32
 cdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_nf4
 cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index e543287..b224e85 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -35,6 +35,8 @@ __all__ = [
     ""patch_linear_scaling"",
     ""check_nvidia"",
     ""create_boolean_mask"",
+    ""torch_amp_custom_fwd"",
+    ""torch_amp_custom_bwd"",
 ]
 
 import torch
@@ -92,6 +94,19 @@ for model_name in model_architectures:
 pass
 # =============================================
 
+# =============================================
+# torch.cuda.amp.custom_fwd is deprecated >= 2.4
+import torch
+from packaging.version import Version
+if Version(torch.__version__) < Version(""2.4.0""):
+    torch_amp_custom_fwd = torch.cuda.amp.custom_fwd
+    torch_amp_custom_bwd = torch.cuda.amp.custom_bwd
+else:
+    torch_amp_custom_fwd = torch.amp.custom_fwd(device_type = ""cuda"")
+    torch_amp_custom_bwd = torch.amp.custom_bwd(device_type = ""cuda"")
+pass
+# =============================================
+
 # =============================================
 # Get Flash Attention v2 if Ampere (RTX 30xx, A100)
 import bitsandbytes as bnb
@@ -176,11 +191,22 @@ torch_compile_arguments = [
     ""config.cuda.use_fast_math = True"",
     ""config.cuda.compile_opt_level = '-O2'"",
 ]
+# Torch dynamo arguments
+torch_dynamo_arguments = [
+    ""config.accumulated_cache_size_limit = 512"", # Bump up a bit from 256
+    ""config.suppress_errors = True"", # Supress errors for now
+    ""config.do_not_emit_runtime_asserts = True"",
+]
 import torch._inductor.config as config
 for _try_compile_argument in torch_compile_arguments:
     try:    exec(_try_compile_argument)
     except: pass
 pass
+import torch._dynamo.config as config
+for _try_dynamo_argument in torch_dynamo_arguments:
+    try:    exec(_try_dynamo_argument)
+    except: pass
+pass
 torch_compile_options = {
     ""epilogue_fusion""   : True,
     ""max_autotune""      : True,
@@ -358,15 +384,13 @@ except:
 pass
 # =============================================
 
-
-def _get_statistics(statistics = None):
+import psutil
+def _get_statistics(statistics = None, force_download = True):
     # We log some basic stats about which environment is being used.
     # We simply download a README.md file from HF - all data is made public.
     # This is simply so we can check if some envs are broken or not.
     # You can disable this by commenting the below out
     try:
-        from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
-        import psutil
         n_cpus = psutil.cpu_count(logical = False)
 
         keynames = ""\n"" + ""\n"".join(os.environ.keys())
@@ -382,21 +406,12 @@ def _get_statistics(statistics = None):
         else: statistics = ""other""
 
         if statistics is not None:
-            disabled = False
-            if not are_progress_bars_disabled():
-                disable_progress_bars()
-                disabled = True
-            pass
-
             from transformers import AutoModelForCausalLM
             stats_model = AutoModelForCausalLM.from_pretrained(
                 f""unslothai/{statistics}"",
-                force_download = True,
+                force_download = force_download,
             )
             del stats_model
-            if disabled:
-                enable_progress_bars()
-            pass
         pass
     except:
         pass
@@ -408,7 +423,14 @@ def get_statistics():
     # We simply download a README.md file from HF - all data is made public.
     # This is simply so we can check if some envs are broken or not.
     # You can disable this by commenting the below out
+    from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled
+    disabled = False
+    if not are_progress_bars_disabled():
+        disable_progress_bars()
+        disabled = True
+    pass
     _get_statistics(None)
+    _get_statistics(""repeat"", force_download = False)
     try:
         vram = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
         if   vram <= 8 : vram = 8
@@ -423,6 +445,12 @@ def get_statistics():
     except:
         pass
     pass
+    try:
+        devices = torch.cuda.device_count()
+        _get_statistics(f""{devices if devices <= 8 else 9}"")
+    except:
+        pass
+    if disabled: enable_progress_bars()
 pass
 
 
@@ -517,7 +545,7 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     Tiny hit to performance, since we mask the movement via non blocking calls.
     """"""
     @staticmethod
-    @torch.cuda.amp.custom_fwd
+    @torch_amp_custom_fwd
     def forward(ctx, forward_function, hidden_states, *args):
         saved_hidden_states = hidden_states.to(""cpu"", non_blocking = True)
         with torch.no_grad():
@@ -529,7 +557,7 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
     pass
 
     @staticmethod
-    @torch.cuda.amp.custom_bwd
+    @torch_amp_custom_bwd
     def backward(ctx, dY):
         (hidden_states,) = ctx.saved_tensors
         hidden_states = hidden_states.to(""cuda:0"", non_blocking = True).detach()
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index cf1936d..9c055ff 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -70,6 +70,17 @@ def fast_rms_layernorm_gemma2_compiled(layernorm, X, gemma = True):
 pass
 
 
+# Flex Attention in torch 2.5 and higher
+# try:
+#     from torch.nn.attention._flex_attention import _flex_attention
+#     from functools import lru_cache
+#     @lru_cache
+#     def create_block_mask_from_score_mod(score_mod, B, H, M, N):
+#         SPARSE_BLOCK = 128
+#         block_mask = _create_block_mask(score_mod, B, H, M, N, device = ""cuda:0"")
+#         return block_mask
+
+
 # Logit softcapping
 @torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
 def gemma2_attention(Q, K, V, causal_mask, self, bsz, q_len):
diff --git a/unsloth/save.py b/unsloth/save.py
index 293e430..7075fec 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -840,6 +840,21 @@ def install_llama_cpp_blocking(use_cuda = False):
 pass
 
 
+def get_executable(executables):
+    # Get system locations (System Path).split(system separator)
+    system_directories = os.environ.get(""PATH"").split(os.pathsep)
+
+    for directory in system_directories:
+        for executable in executables:
+            path = os.path.join(directory, executable)
+            # Check if the executable exists and is executable
+            if os.path.exists(path) and os.access(path, os.X_OK): return path
+        pass
+    pass
+    return None
+pass
+
+
 def save_to_gguf(
     model_type           : str,
     model_dtype          : str,
@@ -932,48 +947,56 @@ def save_to_gguf(
         )
     pass
 
-    print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
-    if _run_installer is not None:
-        error = _run_installer.wait()
+    # Determine whether the system already has llama.cpp installed and the scripts are executable
+    quantize_location = get_executable([""llama-quantize"", ""quantize""])
+    convert_location  = get_executable([""convert-hf-to-gguf.py"", ""convert_hf_to_gguf.py""])
+    
+    if quantize_location is not None and convert_location is not None:
+        print(""Unsloth: llama.cpp found in the system. We shall skip installation."")
     else:
-        error = 0
-        install_llama_cpp_blocking()
-    pass
+        print(""Unsloth: [0] Installing llama.cpp. This will take 3 minutes..."")
+        if _run_installer is not None:
+            error = _run_installer.wait()
+        else:
+            error = 0
+            install_llama_cpp_blocking()
+        pass
 
-    # Check if successful. If not install 10th latest release
+        # Check if successful. If not install 10th latest release
 
-    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize
-    # and llama.cpp/main changed to llama.cpp/llama-cli
-    # See https://github.com/ggerganov/llama.cpp/pull/7809
-    quantize_location = None
-    if os.path.exists(""llama.cpp/quantize""):
-        quantize_location = ""llama.cpp/quantize""
-    elif os.path.exists(""llama.cpp/llama-quantize""):
-        quantize_location = ""llama.cpp/llama-quantize""
-    else:
-        raise RuntimeError(
-            ""Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\n""\
-            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
-        )
-    pass
+        # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize
+        # and llama.cpp/main changed to llama.cpp/llama-cli
+        # See https://github.com/ggerganov/llama.cpp/pull/7809
+        quantize_location = None
+        if os.path.exists(""llama.cpp/quantize""):
+            quantize_location = ""llama.cpp/quantize""
+        elif os.path.exists(""llama.cpp/llama-quantize""):
+            quantize_location = ""llama.cpp/llama-quantize""
+        else:
+            raise RuntimeError(
+                ""Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\n""\
+                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+            )
+        pass
 
-    # See https://github.com/unslothai/unsloth/pull/730
-    # Filenames changed again!
-    convert_location = None
-    if os.path.exists(""llama.cpp/convert-hf-to-gguf.py""):
-        convert_location = ""llama.cpp/convert-hf-to-gguf.py""
-    elif os.path.exists(""llama.cpp/convert_hf_to_gguf.py""):
-        convert_location = ""llama.cpp/convert_hf_to_gguf.py""
-    else:
-        raise RuntimeError(
-            ""Unsloth: The file 'llama.cpp/convert-hf-to-gguf.py' or 'llama.cpp/convert_hf_to_gguf.py' does not exist.\n""\
-            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
-        )
-    pass
+        # See https://github.com/unslothai/unsloth/pull/730
+        # Filenames changed again!
+        convert_location = None
+        if os.path.exists(""llama.cpp/convert-hf-to-gguf.py""):
+            convert_location = ""llama.cpp/convert-hf-to-gguf.py""
+        elif os.path.exists(""llama.cpp/convert_hf_to_gguf.py""):
+            convert_location = ""llama.cpp/convert_hf_to_gguf.py""
+        else:
+            raise RuntimeError(
+                ""Unsloth: The file 'llama.cpp/convert-hf-to-gguf.py' or 'llama.cpp/convert_hf_to_gguf.py' does not exist.\n""\
+                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+            )
+        pass
 
-    if error != 0 or quantize_location is None or convert_location is None:
-        print(f""Unsloth: llama.cpp error code = {error}."")
-        install_llama_cpp_old(-10)
+        if error != 0 or quantize_location is None or convert_location is None:
+            print(f""Unsloth: llama.cpp error code = {error}."")
+            install_llama_cpp_old(-10)
+        pass
     pass
 
     # Determine maximum first_conversion state
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6ed52a7..30348b6 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -705,26 +705,24 @@ def CausalLM_fast_forward(fast_forward_inference):
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
 
-        if causal_mask is None and past_key_values is None:
-            causal_mask = xformers.attn_bias.LowerTriangularMask()
-
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        self.model._has_no_labels = labels is None
-
-        if past_key_values is not None and \
-            hasattr(self.model.layers[0].self_attn, ""paged_attention""):
+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, ""paged_attention""):
             outputs = fast_forward_inference(
                 self.model,
                 input_ids,
                 past_key_values,
             )
         else:
+            causal_mask = xformers.attn_bias.LowerTriangularMask()
+    
+            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+            output_hidden_states = (
+                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+            )
+            return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
+            self.model._has_no_labels = labels is None
+
             outputs = self.model(
                 input_ids=input_ids,
                 causal_mask=causal_mask,
@@ -988,7 +986,7 @@ class FastLlamaModel:
             padding_side     = ""right"",
             token            = token,
         )
-        
+
         model, tokenizer = patch_tokenizer(model, tokenizer)
         model = model_patcher.post_patch(model)
 
diff --git a/unsloth/save.py b/unsloth/save.py
index 5971d76..42d326e 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -90,14 +90,14 @@ def _merge_lora(layer, name):
             W = fast_dequantize(W, quant_state)
         else:
             dtype = W.dtype
-        W = W.to(torch.float32).t()
-        # W = W.t()
+        # W = W.to(torch.float32).t()
+        W = W.t()
 
         if A is not None:
             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))
             # W += sAB
-            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)
-            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)
+            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)
+            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)
             # if not torch.isfinite(W).all():
             maximum_element = torch.max(W.min().abs(), W.max())
             if not torch.isfinite(maximum_element).item():
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 6a2d999..4640681 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -89,7 +89,7 @@ if (major_torch == 2) and (minor_torch >= 5):
         return old_is_bf16_supported(including_emulation)
     torch.cuda.is_bf16_supported = is_bf16_supported
 else:
-    def is_bf16_supported(): SUPPORTS_BFLOAT16
+    def is_bf16_supported(): return SUPPORTS_BFLOAT16
     torch.cuda.is_bf16_supported = is_bf16_supported
 pass
 
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index 1eb2486..a992a02 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -25,18 +25,23 @@ torch_compile_options = {
 }
 
 # Flex Attention supported from torch 2.5 onwards only
-import torch.nn.attention
-if hasattr(torch.nn.attention, ""flex_attention""):
-    import torch.nn.attention.flex_attention
-    from torch.nn.attention.flex_attention import flex_attention
-    from torch.nn.attention.flex_attention import create_block_mask
-    FLEX_ATTENTION_PADDING = getattr(
-        torch.nn.attention.flex_attention,
-        ""_DEFAULT_SPARSE_BLOCK_SIZE"",
-        1,
-    )
-    flex_attention = torch.compile(flex_attention, dynamic = False)
-    HAS_FLEX_ATTENTION = True
+import torch.nn
+if hasattr(torch.nn, ""attention""):
+    import torch.nn.attention
+    if hasattr(torch.nn.attention, ""flex_attention""):
+        import torch.nn.attention.flex_attention
+        from torch.nn.attention.flex_attention import flex_attention
+        from torch.nn.attention.flex_attention import create_block_mask
+        FLEX_ATTENTION_PADDING = getattr(
+            torch.nn.attention.flex_attention,
+            ""_DEFAULT_SPARSE_BLOCK_SIZE"",
+            1,
+        )
+        flex_attention = torch.compile(flex_attention, dynamic = False)
+        HAS_FLEX_ATTENTION = True
+    else:
+        HAS_FLEX_ATTENTION = False
+    pass
 else:
     HAS_FLEX_ATTENTION = False
 pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9bea364..ba45bbb 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -158,6 +158,14 @@ def LlamaAttention_fast_forward_inference(
         self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda:0"")
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda:0"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
+        
+        # Mistral Nemo 12b has weird dimensions
+        if attention_size != self.hidden_size:
+            self.temp_O = torch.empty((1, bsz, self.hidden_size), dtype = dtype, device = ""cuda:0"")
+        else:
+            self.temp_O = self.temp_QA[1][:,:,:self.hidden_size]
+        pass
+        
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda:0"")
         self.scalar = 1.0 / math_sqrt(self.head_dim)
         self.half_head_dim = head_dim // 2
@@ -239,7 +247,7 @@ def LlamaAttention_fast_forward_inference(
     pass
     A = A.transpose(1, 2)
     A = A.reshape(bsz, 1, attention_size)
-    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_O)
     return A, (Kn, Vn)
 pass
 
@@ -335,6 +343,9 @@ def LlamaAttention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
+    # Extend RoPE dynamically to fit in VRAM
+    self.rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
+
     if position_ids is None:
         cos = self.rotary_emb.cos_cached
         sin = self.rotary_emb.sin_cached
@@ -971,19 +982,21 @@ class LlamaRotaryEmbedding(torch.nn.Module):
         self.dim = dim
         self.max_position_embeddings = max_position_embeddings
         self.base = base
+        # Dynamic RoPE we first set it to a max of 4 * 8192 tokens then we iteratively grow this
+        self.current_rope_size = min(4 * 8192, self.max_position_embeddings)
 
         # Build here to make `torch.jit.trace` work.
-        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())
+        self._set_cos_sin_cache(seq_len=self.current_rope_size, device=device, dtype=torch.get_default_dtype())
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
         # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and
         # in FP32. They are applied (multiplied) in FP32 as well.
-        self.max_seq_len_cached = seq_len
+        self.current_rope_size = seq_len
         inv_freq = 1.0 / (
             self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=""cpu"").float() / self.dim)
         )
-        t = torch.arange(self.max_seq_len_cached, device=""cpu"", dtype=torch.int64).float()
+        t = torch.arange(self.current_rope_size, device=""cpu"", dtype=torch.int64).float()
 
         freqs = torch.outer(t, inv_freq)
         # Different from paper, but it uses a different permutation in order to obtain the same calculation
@@ -994,14 +1007,21 @@ class LlamaRotaryEmbedding(torch.nn.Module):
 
     def forward(self, x, position_ids=None, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
-        if seq_len > self.max_seq_len_cached:
+        if seq_len > self.current_rope_size:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
         return (
-            self.cos_cached[:seq_len].to(dtype=x.dtype),
-            self.sin_cached[:seq_len].to(dtype=x.dtype),
+            self.cos_cached[:seq_len].to(dtype = x.dtype),
+            self.sin_cached[:seq_len].to(dtype = x.dtype),
         )
     pass
+
+    def extend_rope_embedding(self, x, seq_len):
+        if seq_len <= self.current_rope_size: return
+        # Iteratively grow by increments of 8192
+        self.current_rope_size = int(round(seq_len / 8192)) * 8192
+        self._set_cos_sin_cache(self.current_rope_size, device = ""cuda:0"", dtype = x.dtype)
+    pass
 pass
 
 
@@ -1016,11 +1036,11 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):
     pass
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.current_rope_size = seq_len
         inv_freq = 1.0 / (
             self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=""cpu"").float() / self.dim)
         )
-        t = torch.arange(self.max_seq_len_cached, device=""cpu"", dtype=torch.int64).float()
+        t = torch.arange(self.current_rope_size, device=""cpu"", dtype=torch.int64).float()
         t = t / self.scaling_factor
 
         freqs = torch.outer(t, inv_freq)
@@ -1140,6 +1160,12 @@ class FastLlamaModel:
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
+
+        # Warn about fast transfers
+        if os.environ.get(""HF_HUB_ENABLE_HF_TRANSFER"", ""0"") == ""1"":
+            logger.warning_once(""Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!"")
+        pass
+
         model_patcher.pre_patch()
         get_statistics() # For debugging - we use a download counter to see if environments are not breaking 
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 6eb3fcc..b253105 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -78,6 +78,9 @@ def MistralAttention_fast_forward(
     if past_key_value is not None:
         kv_seq_len += past_key_value[0].shape[-2]
 
+    # Extend RoPE dynamically to fit in VRAM
+    self.rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
+
     if position_ids is None:
         cos = self.rotary_emb.cos_cached
         sin = self.rotary_emb.sin_cached
@@ -158,7 +161,7 @@ def MistralAttention_fast_forward(
         A = A.transpose(1, 2).contiguous()
     pass
     
-    attn_output = A.reshape(bsz, q_len, self.hidden_size)
+    attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
     attn_output = self.apply_o(self, attn_output)
     attn_weights = None
     return attn_output, attn_weights, past_key_value
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index dc0c7da..060c1cc 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -38,6 +38,17 @@ __all__ = [
 IGNORED_TOKENIZER_CHECKING = frozenset((
     ""CodeLlamaTokenizerFast"",
     ""CodeLlamaTokenizer"",
+    """"
+))
+
+
+IGNORED_TOKENIZER_NAMES = frozenset((
+    ""unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"",
+    ""unsloth/Mistral-Nemo-Instruct-2407"",
+    ""mistralai/Mistral-Nemo-Instruct-2407"",
+    ""unsloth/Mistral-Nemo-Base-2407-bnb-4bit"",
+    ""unsloth/Mistral-Nemo-Base-2407"",
+    ""mistralai/Mistral-Nemo-Base-2407"",
 ))
 
 # Check environments
@@ -488,7 +499,7 @@ def load_correct_tokenizer(
         cache_dir         = cache_dir,
     )
 
-    if slow_tokenizer is not None:
+    if tokenizer_name not in IGNORED_TOKENIZER_NAMES and slow_tokenizer is not None:
         if hasattr(fast_tokenizer, ""add_bos_token"") and hasattr(slow_tokenizer, ""add_bos_token""):
             fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token
         if hasattr(fast_tokenizer, ""add_eos_token"") and hasattr(slow_tokenizer, ""add_eos_token""):
"
"diff --git a/unsloth/models/falcon_h1.py b/unsloth/models/falcon_h1.py
index 0db9c1c..5643e3d 100644
--- a/unsloth/models/falcon_h1.py
+++ b/unsloth/models/falcon_h1.py
@@ -51,7 +51,7 @@ try:
     from transformers.models.falcon_h1.modeling_falcon_h1 import (
         FalconH1Attention,
     )
-except:
+except ModuleNotFoundError:
     # if we are on a old version of transformers technically it should fail in the try except above
     # but if somehow we make it here, we need to raise an error since FalconH1Attention is not available
     # or renamed
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 9b920fd..3c0d501 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -780,7 +780,7 @@ def LlamaModel_fast_forward(
     # Fix up attention mask by setting elements to 0
     # Specifically for DPO
     if getattr(self, ""_has_no_labels"", False) is True and (attention_mask is not None) and (past_key_values is None) and \
-        (not train_embed_tokens):
+        (not train_embed_tokens) and self.training:
         # Careful for inference the attention_mask is size (1, kv_seq_len)
         # Whilst the input_embeds is size (1, 1, 4096)
         inputs_requires_grad = inputs_embeds.requires_grad
@@ -865,21 +865,21 @@ def LlamaModel_fast_forward(
 
             # https://github.com/pytorch/pytorch/issues/103749
             # Need to convert to float and not using bool
-            attention_mask = (1.0 - attention_mask.float()) * torch.finfo(inputs_embeds.dtype).min
+            # attention_mask = (1.0 - attention_mask.float()) * torch.finfo(inputs_embeds.dtype).min
             dynamic_SWA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                 attention_mask,
                 (batch_size, seq_length),
                 inputs_embeds,
                 past_key_values_length,
                 sliding_window = self.config.sliding_window,
-            )[0][0]
+            )
             dynamic_GA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                 attention_mask,
                 (batch_size, seq_length),
                 inputs_embeds,
                 past_key_values_length,
                 sliding_window = None,
-            )[0][0]
+            )
             use_static_mask = False
 
         elif not hasattr(self, ""SWA_mask""):
@@ -953,7 +953,7 @@ def LlamaModel_fast_forward(
         else:
             layer_outputs = decoder_layer(
                 hidden_states,
-                causal_mask=mask,
+                causal_mask         = mask,
                 attention_mask      = attention_mask,
                 position_ids        = position_ids,
                 past_key_value      = past_key_value,
"
"diff --git a/unsloth/save.py b/unsloth/save.py
index caa5aaa..033f6eb 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -777,9 +777,10 @@ def install_llama_cpp_old(version = -10):
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
+                line = line.decode(""utf-8"", errors = ""replace"")
                 if ""undefined reference"" in line:
                     raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
+                print(line, flush = True, end = """")
         pass
     pass
     # Check if successful
@@ -811,9 +812,10 @@ def install_llama_cpp_blocking(use_cuda = True):
     for command in commands:
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
+                line = line.decode(""utf-8"", errors = ""replace"")
                 if ""undefined reference"" in line:
                     raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
+                print(line, flush = True, end = """")
         pass
     pass
 pass
@@ -988,9 +990,10 @@ def save_to_gguf(
 
     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
         for line in sp.stdout:
+            line = line.decode(""utf-8"", errors = ""replace"")
             if ""undefined reference"" in line:
                 raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-            print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
+            print(line, flush = True, end = """")
         if sp.returncode is not None and sp.returncode != 0:
             raise subprocess.CalledProcessError(sp.returncode, sp.args)
     pass
@@ -1031,9 +1034,10 @@ def save_to_gguf(
         # quantize uses stderr
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:
             for line in sp.stdout:
+                line = line.decode(""utf-8"", errors = ""replace"")
                 if ""undefined reference"" in line:
                     raise RuntimeError(""Failed compiling llama.cpp. Please report this ASAP!"")
-                print(line.decode(""utf-8"", errors = ""replace""), flush = True, end = """")
+                print(line, flush = True, end = """")
             if sp.returncode is not None and sp.returncode != 0:
                 raise subprocess.CalledProcessError(sp.returncode, sp.args)
         pass
"
"diff --git a/.github/ISSUE_TEMPLATE/bug---issue.md b/.github/ISSUE_TEMPLATE/bug---issue.md
index 2849538..8868313 100644
--- a/.github/ISSUE_TEMPLATE/bug---issue.md
+++ b/.github/ISSUE_TEMPLATE/bug---issue.md
@@ -10,9 +10,11 @@ assignees: ''
 1. Did you update? `pip install --upgrade unsloth unsloth_zoo`
 2. `Colab` or `Kaggle` or local / cloud
 3. Number GPUs used, use `nvidia-smi`
-4. Which notebook?
-5. Paste `Unsloth` printout with :sloth: sloth emoji
+4. Which notebook? Please link!
+5. Which Unsloth version, TRL version, transformers version, PyTorch version?
 6. Which trainer? `SFTTrainer`, `GRPOTrainer` etc
-7. **Minimal code to reproduce error Remove Hugging Face token!**
 
  You can also ask via our Reddit page: https://www.reddit.com/r/unsloth/
+```python
+Put Minimal code to reproduce error here ###Remove Hugging Face token###
+```
"
"diff --git a/.github/ISSUE_TEMPLATE/question.md b/.github/ISSUE_TEMPLATE/question.md
index 5aa2a67..0df891d 100644
--- a/.github/ISSUE_TEMPLATE/question.md
+++ b/.github/ISSUE_TEMPLATE/question.md
@@ -1,8 +1,8 @@
 ---
 name: Submit question
-about: Ask a general question about CUTLASS
+about: Ask a general question about unsloth
 title: ""[QST]""
-labels: ""? - Needs Triage, question""
+labels: ""question""
 assignees: ''
 
 ---
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 56749d6..d31b6cf 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -23,10 +23,7 @@ from transformers.models.llama.modeling_llama import logger
 from .save import patch_saving_functions
 import os
 import shutil
-from .tokenizer_utils import (
-    load_correct_tokenizer,
-    fix_sentencepiece_tokenizer,
-)
+from .tokenizer_utils import *
 from .models._utils import patch_tokenizer
 
 CHAT_TEMPLATES = {}
@@ -266,7 +263,7 @@ llama3_template = \
         ""{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}""\
     ""{% endif %}""
 llama3_template_eos_token = ""eos_token""
-CHAT_TEMPLATES[""llama-3""] = (llama3_template, gemma_chatml_eos_token,)
+CHAT_TEMPLATES[""llama-3""] = (llama3_template, llama3_template_eos_token,)
 
 
 def get_chat_template(
@@ -288,6 +285,8 @@ def get_chat_template(
     is_fast_tokenizer = getattr(tokenizer, ""is_fast"", False)
     old_padding_side = tokenizer.padding_side
 
+    same_padding_token = False
+
     if type(chat_template) in (list, tuple,):
         chat_template, stop_word = chat_template
         assert(type(chat_template) is str)
@@ -342,10 +341,24 @@ def get_chat_template(
             if skipped != len(token_mapping):
                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
 
+                # Careful on pad_token
+                old_pad_token = tokenizer.pad_token
+                if old_pad_token == tokenizer.eos_token:
+                    old_pad_token = stop_word
+                    same_padding_token = True
+                pass
+
                 if map_eos_token:
-                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+                    new_tokenizer = tokenizer.__class__(
+                        tokenizer_object = new_tokenizer,
+                        eos_token = stop_word,
+                        pad_token = old_pad_token,
+                    )
                 else:
-                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)
+                    new_tokenizer = tokenizer.__class__(
+                        tokenizer_object = new_tokenizer,
+                        pad_token = old_pad_token,
+                    )
                 pass
 
                 # Must fix the sentence piece tokenizer since there's no tokenizer.model file!
@@ -380,6 +393,13 @@ def get_chat_template(
                 string_vocab = string_vocab.replace(old_eos_token, stop_word)
             pass
             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
+
+            # Careful on pad_token
+            if old_pad_token == old_eos_token:
+                old_pad_token = stop_word
+                same_padding_token = True
+            pass
+
             new_tokenizer = tokenizer.__class__(
                 tokenizer_object = new_tokenizer,
                 bos_token = old_bos_token,
@@ -424,9 +444,11 @@ def get_chat_template(
     new_pad_token = getattr(tokenizer,     ""pad_token"", None)
     new_bos_token = getattr(tokenizer,     ""bos_token"", None)
     new_unk_token = getattr(tokenizer,     ""unk_token"", None)
-    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token
     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token
     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token
+    if not same_padding_token:
+        if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token
+    pass
 
     # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 32da0a7..9c4ae8f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -349,3 +349,4 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):
         return (None, hidden_states.grad,) + (None,)*len(ctx.args)
     pass
 pass
+
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 6f70bc5..45c7501 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1445,6 +1445,10 @@ class FastLlamaModel:
                                       ""gate_proj"", ""up_proj"", ""down_proj"",),)
         model.config.update({""unsloth_version"" : __version__})
 
+        if type(modules_to_save) is tuple:
+            modules_to_save = list(modules_to_save)
+        pass
+
         train_lm_head = False
         train_embed_tokens = False
         final_modules = []
@@ -1472,6 +1476,29 @@ class FastLlamaModel:
                 final_modules.append(module)
         pass
 
+        # Check if we added new tokens!
+        if hasattr(model, ""_need_to_train_embeddings""):
+            if not train_lm_head or not train_embed_tokens:
+                print(
+                    ""Unsloth: You added new tokens but did not specify if you wanted to ""\
+                    ""train the lm_head and embed_tokens.\nWe must turn it on for you.""
+                )
+                train_lm_head = True
+                train_embed_tokens = True
+
+                if modules_to_save is None: modules_to_save = [""embed_tokens""]
+                else: modules_to_save.append(""embed_tokens"")
+
+                if modules_to_save is None: modules_to_save = [""lm_head""]
+                else: modules_to_save.append(""lm_head"")
+            pass
+        pass
+
+        # First fix untrained tokens
+        if train_embed_tokens or train_lm_head:
+            fix_untrained_tokens(model, eps = 1e-16)
+        pass
+
         # Check modules_to_save
         if modules_to_save is not None:
             for module in modules_to_save:
@@ -1479,8 +1506,15 @@ class FastLlamaModel:
                     train_lm_head = True
                 elif module == ""embed_tokens"":
                     train_embed_tokens = True
+                else:
+                    raise TypeError(
+                        f""Unsloth: Module = {module} is not allowed. Only 'lm_head' and 'embed_tokens' is allowed.""
+                    )
             pass
         pass
+        if isinstance(modules_to_save, (tuple, list)):
+            modules_to_save = list(set(modules_to_save))
+        pass
 
         # Get LoRA
         arguments = dict(
diff --git a/unsloth/save.py b/unsloth/save.py
index 655d1c5..6e9d82c 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -922,9 +922,16 @@ def save_to_gguf(
           f""The output location will be {final_location}\n""\
           ""This will take 3 minutes..."")
 
+    # We first check if tokenizer.model exists in the model_directory
+    if os.path.exists(f""{model_directory}/tokenizer.model""):
+        vocab_type = ""hfft""
+    else:
+        vocab_type = ""bpe""
+    pass
+
     if use_fast_convert:
         command = f""python llama.cpp/convert.py {model_directory} ""\
-            f""--outfile {final_location} --vocab-type hfft ""\
+            f""--outfile {final_location} --vocab-type {vocab_type} ""\
             f""--outtype {first_conversion} --concurrency {n_cpus}""
     else:
         # Need to fix convert-hf-to-gguf.py for some models!
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 76d9372..f1a9daa 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -18,11 +18,15 @@ from transformers import PreTrainedTokenizerFast
 import re
 import os
 from transformers.models.llama.modeling_llama import logger
+from peft import PeftModelForCausalLM
+import torch
 
 __all__ = [
     ""load_correct_tokenizer"",
     ""fix_sentencepiece_tokenizer"",
     ""check_tokenizer"",
+    ""fix_untrained_tokens"",
+    ""add_new_tokens"",
 ]
 
 
@@ -255,7 +259,11 @@ def fix_sentencepiece_tokenizer(
 
     # And load it!
     from transformers import AutoTokenizer
-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)
+    tokenizer = AutoTokenizer.from_pretrained(
+        temporary_location,
+        eos_token = new_tokenizer.eos_token,
+        pad_token = new_tokenizer.pad_token,
+    )
     return tokenizer
 pass
 
@@ -466,3 +474,124 @@ def check_tokenizer(
     pass
     return convert_to_fast_tokenizer(tokenizer)
 pass
+
+
+@torch.inference_mode
+def fix_untrained_tokens(model, eps = 1e-16):
+    """"""
+    Llama-3 for eg has untrained vectors in the base model.
+    These include <|eot_id|>, <|start_header_id|>, <|end_header_id|>
+    We reset them to the mean of the rest of the tokens
+    """"""
+    embedding_matrix = model.get_input_embeddings ().weight.data
+    lm_head_matrix   = model.get_output_embeddings().weight.data
+
+    # Get untrained tokens
+    indicator_untrained = torch.amax(embedding_matrix, axis = 1) <= eps
+    where_untrained = torch.where(indicator_untrained)[0]
+    n_untrained = where_untrained.shape[0]
+    n_trained = embedding_matrix.shape[0] - n_untrained
+    if n_untrained != 0:
+        print(
+            f""Unsloth: Not an error, but your model has {n_untrained} untrained tokens.\n""\
+            ""We shall set them to the mean of the other trained tokens.""
+        )
+    pass
+
+    # First set untrained to all 0s - sometimes it's not! 1e-23 for bfloat16
+    embedding_matrix[where_untrained] = 0
+    lm_head_matrix  [where_untrained] = 0
+
+    # Find sum
+    sum_embedding  = torch.sum(embedding_matrix, dtype = torch.float32, axis = 0)
+    sum_lm_head    = torch.sum(lm_head_matrix,   dtype = torch.float32, axis = 0)
+
+    # Find correct average by dividing by sum of trained tokens
+    mean_embedding = (sum_embedding / n_trained).to(embedding_matrix.dtype)
+    mean_lm_head   = (sum_lm_head   / n_trained).to(lm_head_matrix  .dtype)
+
+    # Set them to the mean
+    embedding_matrix[where_untrained] = mean_embedding
+    lm_head_matrix  [where_untrained] = mean_lm_head
+
+    return mean_embedding, mean_lm_head
+pass
+
+
+@torch.inference_mode
+def add_new_tokens(
+    model,
+    tokenizer,
+    new_tokens = [],
+    method = ""mean"",
+    interpolation = 0.05,
+):
+    """"""
+    Smartly resizes the tokenizer and adds new tokens to the model.
+    We also disregard untrained tokens by removing them from the mean calculation.
+    """"""
+    assert(isinstance(new_tokens, (list, tuple)))
+    assert(len(new_tokens) > 0)
+    assert(method == ""mean"" or method == ""interpolation"")
+    assert(interpolation >= 0 and interpolation <= 1)
+
+    # Check if tokens already exist
+    overlapping_tokens = set(new_tokens) & set(tokenizer.vocab.keys())
+    if len(overlapping_tokens) != 0:
+        print(
+            f""Unsloth: You're adding new_tokens = {new_tokens}\n""\
+            f""There are tokens which are overlapping = {list(overlapping_tokens)}\n""\
+            f""We shall safely ignore these overlapping tokens.""
+        )
+        new_tokens = [x for x in new_tokens if x not in overlapping_tokens]
+    pass
+
+    # Get mean of trained tokens
+    mean_embedding, mean_lm_head = fix_untrained_tokens(model)
+    mean_embedding = mean_embedding.to(torch.float32)
+    mean_lm_head   = mean_lm_head  .to(torch.float32)
+
+    # Add tokens!
+    old_length = len(tokenizer)
+    tokenizer.add_tokens(new_tokens)
+    model.resize_token_embeddings(len(tokenizer))
+
+    # If we use interpolation, we interpolate between the mean embeddings and
+    # the Word2Vec sum of the other vectors
+    embedding_matrix = model.get_input_embeddings ().weight.data
+    lm_head_matrix   = model.get_output_embeddings().weight.data
+
+    if method == ""interpolation"":
+        print(
+            ""Unsloth: You are using interpolation to add new tokens.\n""\
+            f""We shall set new tokens = mean(embeddings)*{1-interpolation} + mean(new_tokens)*{interpolation}""
+        )
+        for j, token in enumerate(new_tokens):
+            input_ids = tokenizer(token, add_special_tokens = False).input_ids
+            mean_embedding_token = embedding_matrix[input_ids].mean(axis = 0, dtype = torch.float32)
+            mean_lm_head_token   = lm_head_matrix  [input_ids].mean(axis = 0, dtype = torch.float32)
+
+            # Interpolate
+            mean_embedding_token = mean_embedding*(1-interpolation) + mean_embedding_token*interpolation
+            mean_lm_head_token   = mean_lm_head  *(1-interpolation) + mean_lm_head_token  *interpolation
+
+            # Set the new vector
+            embedding_matrix[old_length+j] = mean_embedding_token
+            lm_head_matrix  [old_length+j] = mean_lm_head_token
+        pass
+    else:
+        # Now set the new tokens to the mean!
+        embedding_matrix[old_length:] = mean_embedding
+        lm_head_matrix  [old_length:] = mean_lm_head
+    pass
+
+    # We set a flag to say we need to train embeddings
+    internal_model = model
+    while hasattr(internal_model, ""model""):
+        internal_model._need_to_train_embeddings = True
+        internal_model = internal_model.model
+    pass
+    internal_model._need_to_train_embeddings = True
+    
+    return
+pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7cbecfa..2af59f9 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -1093,6 +1093,22 @@ def patch_gradient_accumulation_fix(Trainer):
         ""if self.model_accepts_loss_kwargs:"",
         ""if False:"",
     )
+
+    # Fix when num_items_in_batch is nothing
+    # https://github.com/huggingface/transformers/pull/35207
+    function = re.sub(
+        r""else:\n""\
+        r""([\s]{4,})self\.accelerator\.backward\(loss, \*\*kwargs\)\n""\
+        r""(.+?)if num_items_in_batch is None\:\n""\
+        r""(.+?)return loss\.detach\(\) \/ self\.args\.gradient_accumulation_steps"",
+
+        ""else:\n""\
+        ""\2if num_items_in_batch is None:\n""\
+        ""\3loss /= self.args.gradient_accumulation_steps\n""\
+        ""\1self.accelerator.backward(loss, **kwargs)"",
+        
+        function,
+    )
     
     exec(function, globals())
     Trainer.training_step = _unsloth_training_step
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 52b3710..3820245 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.2.13""
+__version__ = ""2025.2.14""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index f6b3fdb..8f34607 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -137,7 +137,6 @@ class Unsloth{RLConfig_name}({RLConfig_name}):
     ):
 {RLConfig_extra_args}
         super().__init__({RLConfig_call_args}{RLConfig_kwargs})
-        assert(hasattr(vllm_sampling_params, '_set_kwargs'))
         self.vllm_sampling_params = vllm_sampling_params
         self.unsloth_num_chunks = unsloth_num_chunks
 pass
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index cd8dd6d..cef886e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.4.6""
+__version__ = ""2025.4.7""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/loader_utils.py b/unsloth/models/loader_utils.py
index 49baae5..db81b5c 100644
--- a/unsloth/models/loader_utils.py
+++ b/unsloth/models/loader_utils.py
@@ -110,7 +110,8 @@ def get_model_name(model_name, load_in_4bit = True):
     )
     # In the rare case, we convert bad model names to other names
     # For eg too large dynamic quants or MoEs
-    if new_model_name.lower() in BAD_MAPPINGS:
+    if new_model_name is not None and type(new_model_name) is str and \
+        new_model_name.lower() in BAD_MAPPINGS:
         new_model_name = BAD_MAPPINGS[new_model_name.lower()]
 
     if new_model_name is None and model_name.count(""/"") == 1 and model_name[0].isalnum():
"
"diff --git a/pyproject.toml b/pyproject.toml
index 01636e7..7dfca63 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -40,7 +40,7 @@ triton = [
 ]
 
 huggingface = [
-    ""unsloth_zoo>=2025.3.5"",
+    ""unsloth_zoo>=2025.3.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -354,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.3.5"",
+    ""unsloth_zoo>=2025.3.7"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 9ed356d..38453f3 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.5""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.7""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 7ac35d7..37c69ef 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.7""
+__version__ = ""2025.3.8""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 6a96e8d..0044c7e 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -1243,6 +1243,7 @@ for j, function in enumerate(functions):
         except: continue
 pass
 
+import importlib
 USE_MODELSCOPE = os.environ.get(""UNSLOTH_USE_MODELSCOPE"", ""0"") == ""1""
 if USE_MODELSCOPE:
     if importlib.util.find_spec(""modelscope"") is None:
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 2e88e21..a036970 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -76,6 +76,7 @@ platform_system = platform_system()
 import numpy as np
 import contextlib
 import re
+import functools
 import warnings, subprocess, re, inspect, psutil, os, math
 from unsloth_zoo.utils import Version
 from unsloth import DEVICE_TYPE, DEVICE_COUNT
@@ -422,6 +423,7 @@ HAS_FLASH_ATTENTION_SOFTCAPPING = False
 
 if DEVICE_TYPE == ""cuda"":
     major_version, minor_version = torch.cuda.get_device_capability()
+    torch.cuda.get_device_capability = functools.cache(torch.cuda.get_device_capability)
 
     if major_version >= 8:
         SUPPORTS_BFLOAT16 = True
@@ -586,7 +588,6 @@ UNSLOTH_COMPILE_DEBUG         = os.environ.get(""UNSLOTH_COMPILE_DEBUG"",
 UNSLOTH_COMPILE_MAXIMUM       = os.environ.get(""UNSLOTH_COMPILE_MAXIMUM"",       ""0"") == ""1""
 UNSLOTH_COMPILE_IGNORE_ERRORS = os.environ.get(""UNSLOTH_COMPILE_IGNORE_ERRORS"", ""1"") == ""1""
 # Just remove max_autotune_gemm warning
-import functools
 from torch._inductor.runtime.hints import DeviceProperties
 
 @functools.lru_cache(None)
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 77f953a..15a5061 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -591,20 +591,12 @@ class FastModel(FastBaseModel):
                 ""if name.endswith(('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'head')): module.to(torch.float16); ""\
                 ""os.environ['TRITON_F32_DEFAULT'] = 'ieee';""
         elif ""gpt-oss"" in lowered_model_name:
-            os.environ[""UNSLOTH_DISABLE_STATIC_GENERATION""] = ""1""
-	     # the temporary patches for init need UNSLOTH_MODEL_NAME to be set
-            # which doesn't happen at import so manually call here
-            # before creating the compiled cache
-            try:
-                from unsloth_zoo.temporary_patches.gpt_oss import (
-                    patch_GptOssExperts_MXFP4,
-                    patch_GptOssExperts_bitsandbytes,
-                )
-
-                patch_GptOssExperts_MXFP4()
-                patch_GptOssExperts_bitsandbytes()
-            except:
-                pass
+            os.environ[""UNSLOTH_FORCE_CUSTOM_DTYPE""] = \
+                ""all;None;None;""\
+                ""x = 'gate_up_proj_bias'\n""\
+                ""if hasattr(module, x): setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n""\
+                ""x = 'down_proj_bias'\n""\
+                ""if hasattr(module, x): setattr(module, x, torch.nn.Parameter(getattr(module, x).to(torch.float32)) if isinstance(getattr(module, x), torch.nn.Parameter) else getattr(module, x).to(torch.float32))\n;""
         else:
             for check_model_name in DISABLE_COMPILE_MODEL_NAMES:
                 if check_model_name in lowered_model_name:
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 623f263..b9cc01e 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -365,8 +365,10 @@ class FastBaseModel:
             allow_float16_runs = (checker == ""float16"" and dtype == torch.float16)
 
             if allow_all_runs or allow_float16_runs:
-                dtype = eval(_dtype)
-                bnb_compute_dtype = eval(_bnb_compute_dtype)
+                if eval(_dtype) is not None:
+                    dtype = eval(_dtype)
+                if eval(_bnb_compute_dtype) is not None:
+                    bnb_compute_dtype = eval(_bnb_compute_dtype)
                 correct_dtype = bnb_compute_dtype
                 custom_datatype = _custom_datatype
                 # Execute code as well
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 0c07035..cc3dbb1 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -84,12 +84,12 @@ def _cross_entropy_forward(
     logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
     if label_idx != -100:
-        x = tl.load(logits_ptr + label_idx)
+        x = tl.load(logits_ptr + label_idx).to(tl.float32)
         # Go logit scaling for Cohere: t * x
         if DO_LOGIT_SCALING: x = LOGIT_SCALE * x
         # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
         if DO_SOFTCAPPING:   x = SOFTCAP * triton_tanh(x / SOFTCAP)
-        loss = logsumexp - x.to(tl.float32)
+        loss = logsumexp - x
     else:
         loss = 0.0
     tl.store(logsumexp_ptr, logsumexp)
@@ -170,7 +170,7 @@ def _chunked_cross_entropy_forward(
             if DO_LOGIT_SCALING: x = LOGIT_SCALE * x
             # Do logit softcapping for Gemma 2: t * tanh(1/t * x)
             if DO_SOFTCAPPING:   x = SOFTCAP * triton_tanh(x / SOFTCAP)
-            loss = -1.0 * x.to(tl.float32)
+            loss = -1.0 * x
         else:
             loss = 0.0
         tl.store(loss_ptr, loss)
"
"diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index 9068df9..74a0adc 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -149,12 +149,24 @@ def fix_triton_ops():
         import triton.ops
     except:
         # Triton 3.2 removed triton.ops
-        from .kernels.matmul_perf_model import (
-            early_config_prune,
-            estimate_matmul_time,
+        from .matmul_perf_model import (
+            early_config_prune   as _early_config_prune,
+            estimate_matmul_time as _estimate_matmul_time,
         )
-        triton.ops.matmul_perf_model.early_config_prune   = early_config_prune
-        triton.ops.matmul_perf_model.estimate_matmul_time = estimate_matmul_time
+        class PerfOps:
+            def __init__(self): return
+            @staticmethod
+            def early_config_prune(*args, **kwargs):
+                return _early_config_prune(*args, **kwargs)
+            @staticmethod
+            def estimate_matmul_time(*args, **kwargs):
+                return _estimate_matmul_time(*args, **kwargs)
+        pass
+        class TritonOps:
+            __slots__ = ""matmul_perf_model"",
+            def __init__(self): self.matmul_perf_model = PerfOps()
+        pass
+        triton.ops = TritonOps()
     pass
 pass
 fix_triton_ops()
diff --git a/unsloth/kernels/matmul_perf_model.py b/unsloth/matmul_perf_model.py
similarity index 100%
rename from unsloth/kernels/matmul_perf_model.py
rename to unsloth/matmul_perf_model.py
"
"diff --git a/README.md b/README.md
index 45312a4..4bdd7e2 100644
--- a/README.md
+++ b/README.md
@@ -193,7 +193,7 @@ print(f'pip install --upgrade pip && pip install ""unsloth[{x}] @ git+https://git
 ### Windows Installation
 
 To run Unsloth directly on Windows:
-- Install Triton from this Windows fork and follow the instructions: https://github.com/woct0rdho/triton-windows
+- Install Triton from this Windows fork and follow the instructions: https://github.com/woct0rdho/triton-windows (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)
 - In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:
 ```python
 trainer = SFTTrainer(
@@ -202,12 +202,15 @@ trainer = SFTTrainer(
 )
 ```
 
+### Advanced/Troubleshooting
+
 For **advanced installation instructions** or if you see weird errors during installations:
 
 1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`
 2. Confirm if CUDA is installated correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.
 3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.
-4.  Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`
+4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. 
+5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`
 
 ##  [Documentation](https://docs.unsloth.ai)
 - Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!
diff --git a/pyproject.toml b/pyproject.toml
index 59a7c44..96aa069 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -39,7 +39,7 @@ triton = [
     ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post5/triton-3.1.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 huggingface = [
-    ""unsloth_zoo>=2025.2.5"",
+    ""unsloth_zoo>=2025.2.6"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -196,6 +196,10 @@ cu126onlytorch260 = [
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl ; python_version=='3.11' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl ; python_version=='3.12' and platform_system == 'Linux'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp39-cp39-win_amd64.whl ; python_version=='3.9' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp310-cp310-win_amd64.whl ; python_version=='3.10' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp311-cp311-win_amd64.whl ; python_version=='3.11' and platform_system == 'Windows'"",
+    ""xformers @ https://download.pytorch.org/whl/cu126/xformers-0.0.29.post3-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'"",
 ]
 cu118 = [
     ""unsloth[huggingface]"",
@@ -344,7 +348,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.2.5"",
+    ""unsloth_zoo>=2025.2.6"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index f0600f3..a3b3e68 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -196,7 +196,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.2.4""):
+    if Version(unsloth_zoo_version) < Version(""2025.2.6""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/models/__init__.py b/unsloth/models/__init__.py
index b15e04a..29ad78d 100644
--- a/unsloth/models/__init__.py
+++ b/unsloth/models/__init__.py
@@ -20,4 +20,4 @@ from .mistral import FastMistralModel
 from .qwen2   import FastQwen2Model
 from .dpo     import PatchDPOTrainer, PatchKTOTrainer
 from ._utils  import is_bfloat16_supported
-from .rl      import PatchFastRL
+from .rl      import PatchFastRL, vLLMSamplingParams
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0c51c17..52b3710 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.2.12""
+__version__ = ""2025.2.13""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 1eae97f..909dfc3 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -700,6 +700,7 @@ def LlamaModel_fast_forward(
         elif inputs_requires_grad:
             inputs_embeds.requires_grad_(False)
         pass
+        attention_mask = attention_mask[:,:self.max_seq_length] # Must resize!
         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)
         if inputs_requires_grad: inputs_embeds.requires_grad_(True)
     pass
@@ -774,9 +775,12 @@ def LlamaModel_fast_forward(
             self.SWA_mask = True
             self.GA_mask  = False
         elif attention_mask is not None:
-
             # Fixes https://github.com/unslothai/unsloth/issues/853
             # Unsloth needs a 2D mask, not a [2, 1, n, n] mask!
+
+            # https://github.com/pytorch/pytorch/issues/103749
+            # Need to convert to float and not using bool
+            attention_mask = (1.0 - attention_mask.float()) * torch.finfo(inputs_embeds.dtype).min
             dynamic_SWA_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                 attention_mask,
                 (batch_size, seq_length),
@@ -1030,6 +1034,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         num_logits_to_keep: Optional[int] = 0,
+        logits_to_keep: Optional[int] = 0,
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
         
@@ -1053,16 +1058,16 @@ def CausalLM_fast_forward(fast_forward_inference):
             # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
             self.model._has_no_labels = labels is None
             outputs = self.model(
-                input_ids=input_ids,
-                causal_mask=causal_mask,
-                attention_mask=attention_mask,
-                position_ids=position_ids,
-                past_key_values=past_key_values,
-                inputs_embeds=inputs_embeds,
-                use_cache=use_cache,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-                return_dict=return_dict,
+                input_ids = input_ids,
+                causal_mask = causal_mask,
+                attention_mask = attention_mask,
+                position_ids = position_ids,
+                past_key_values = past_key_values,
+                inputs_embeds = inputs_embeds,
+                use_cache = use_cache,
+                output_attentions = output_attentions,
+                output_hidden_states = output_hidden_states,
+                return_dict = return_dict,
             )
         pass
         hidden_states = outputs[0]
@@ -1072,6 +1077,20 @@ def CausalLM_fast_forward(fast_forward_inference):
         logit_softcapping = getattr(self.config, ""final_logit_softcapping"", 0)
         logit_scaling     = getattr(self.config, ""logit_scale"", 0)
         dtype = lm_head.dtype
+        num_logits_to_keep = max(num_logits_to_keep, logits_to_keep)
+
+        # Output last hidden states without logits if asked
+        if os.environ.get(""UNSLOTH_RETURN_HIDDEN_STATES"", ""0"") == ""1"":
+            if num_logits_to_keep != 0:
+                hidden_states = hidden_states[:, -num_logits_to_keep:, :]
+            return CausalLMOutputWithPast(
+                loss = None,
+                logits = hidden_states,
+                past_key_values = outputs.past_key_values,
+                hidden_states = outputs.hidden_states,
+                attentions=  outputs.attentions,
+            )
+        pass
 
         if bsz == 1 and q_len == 1:
             logits = torch.mv(lm_head, hidden_states.ravel().to(dtype))
@@ -1166,11 +1185,11 @@ def CausalLM_fast_forward(fast_forward_inference):
             return (loss,) + output if loss is not None else output
 
         return CausalLMOutputWithPast(
-            loss=loss,
-            logits=logits,
-            past_key_values=outputs.past_key_values,
-            hidden_states=outputs.hidden_states,
-            attentions=outputs.attentions,
+            loss = loss,
+            logits = logits,
+            past_key_values = outputs.past_key_values,
+            hidden_states = outputs.hidden_states,
+            attentions=  outputs.attentions,
         )
     pass
     return _CausalLM_fast_forward
@@ -1180,28 +1199,30 @@ pass
 @torch._disable_dynamo
 def PeftModelForCausalLM_fast_forward(
     self,
-    input_ids=None,
-    causal_mask=None,
-    attention_mask=None,
-    inputs_embeds=None,
-    labels=None,
-    output_attentions=None,
-    output_hidden_states=None,
-    return_dict=None,
-    task_ids=None,
-    num_logits_to_keep=0,
+    input_ids = None,
+    causal_mask = None,
+    attention_mask = None,
+    inputs_embeds = None,
+    labels = None,
+    output_attentions = None,
+    output_hidden_states = None,
+    return_dict = None,
+    task_ids = None,
+    num_logits_to_keep = 0,
+    logits_to_keep = 0,
     **kwargs,
 ):
     return self.base_model(
-        input_ids=input_ids,
-        causal_mask=causal_mask,
-        attention_mask=attention_mask,
-        inputs_embeds=inputs_embeds,
-        labels=labels,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-        num_logits_to_keep=num_logits_to_keep,
+        input_ids = input_ids,
+        causal_mask = causal_mask,
+        attention_mask = attention_mask,
+        inputs_embeds = inputs_embeds,
+        labels = labels,
+        output_attentions = output_attentions,
+        output_hidden_states = output_hidden_states,
+        return_dict = return_dict,
+        num_logits_to_keep = num_logits_to_keep,
+        logits_to_keep = logits_to_keep,
         **kwargs,
     )
 pass
@@ -1694,9 +1715,9 @@ class FastLlamaModel:
         elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:
             logger.warning_once(""Device does not support bfloat16. Will change to float16."")
             dtype = torch.float16
-        elif dtype == torch.float16 and SUPPORTS_BFLOAT16:
-            logger.warning_once(""Device supports bfloat16 but you selected float16. Will change to bfloat16."")
-            dtype = torch.bfloat16
+        # elif dtype == torch.float16 and SUPPORTS_BFLOAT16:
+        #     logger.warning_once(""Device supports bfloat16 but you selected float16. Will change to bfloat16."")
+        #     dtype = torch.bfloat16
 
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 39b367e..186545c 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -24,10 +24,14 @@ from peft import PeftConfig, PeftModel
 from .loader_utils import get_model_name
 import os, contextlib, sys
 try:
-    from huggingface_hub.utils import get_token
+    from huggingface_hub import get_token
 except:
-    # Old HF Hub versions <= 0.0.25
-    from huggingface_hub.utils._token import get_token
+    try:
+        from huggingface_hub.utils import get_token
+    except:
+        # For older versions of huggingface_hub
+        from huggingface_hub.utils._token import get_token
+    pass
 pass
 from huggingface_hub import HfFileSystem
 import importlib.util
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 2e85d30..da7f449 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -601,11 +601,6 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen2.5-VL-72B-Instruct"",
         ""unsloth/Qwen2.5-VL-72B-Instruct-bnb-4bit"",
     ),
-    ""unsloth/DeepHermes-3-Llama-3-8B-Preview-unsloth-bnb-4bit"" : (
-        ""unsloth/DeepHermes-3-Llama-3-8B-Preview"",
-        ""NousResearch/DeepHermes-3-Llama-3-8B-Preview"",
-        ""unsloth/DeepHermes-3-Llama-3-8B-Preview-bnb-4bit"",
-    ),
     ""unsloth/DeepScaleR-1.5B-Preview-unsloth-bnb-4bit"" : (
         ""unsloth/DeepHermes-3-Llama-3-8B-Preview"",
         ""agentica-org/DeepScaleR-1.5B-Preview"",
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 7b363d8..f6b3fdb 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -14,6 +14,7 @@
 
 __all__ = [
     ""PatchFastRL"",
+    ""vLLMSamplingParams"",
 ]
 
 import torch
@@ -36,12 +37,20 @@ selective_log_softmax = RL_REPLACEMENTS[""selective_log_softmax""]
 
 torch_compile_options = {
     ""epilogue_fusion""   : True,
-    ""max_autotune""      : True,
+    ""max_autotune""      : False, # Disable Triton mm kernels
     ""shape_padding""     : True,
     ""trace.enabled""     : False,
     ""triton.cudagraphs"" : False,
 }
 
+
+def vLLMSamplingParams(**kwargs):
+    from vllm import SamplingParams
+    sampling_params = SamplingParams(**kwargs)
+    sampling_params._set_kwargs = kwargs
+    return sampling_params
+pass
+
 def PatchRL(FastLanguageModel):
 
     from trl.models.utils import unwrap_model_for_generation
@@ -94,11 +103,12 @@ from typing import *
 from dataclasses import dataclass, field
 from packaging.version import Version
 import torch
+import numpy as np
 from contextlib import nullcontext
 from torch.nn import functional as F
 torch_compile_options = {{
     ""epilogue_fusion""   : True,
-    ""max_autotune""      : True,
+    ""max_autotune""      : False,
     ""shape_padding""     : True,
     ""trace.enabled""     : False,
     ""triton.cudagraphs"" : False,
@@ -112,16 +122,24 @@ class Unsloth{RLConfig_name}({RLConfig_name}):
     """"""
     {__RLConfig_doc__}
     """"""
-    sampling_params: Optional[Any] = field(
+    vllm_sampling_params: Optional[Any] = field(
         default = None,
         metadata = {{'help': 'vLLM SamplingParams'}},
     )
+    unsloth_num_chunks : Optional[int] = field(
+        default = -1,
+        metadata = {{'help': 'Chunk size to reduce memory usage. -1 is most efficient.'}},
+    )
     def __init__({RLConfig_arguments},
-        sampling_params = None,
+        vllm_sampling_params = None,
+        unsloth_num_chunks = -1,
         **kwargs,
     ):
 {RLConfig_extra_args}
         super().__init__({RLConfig_call_args}{RLConfig_kwargs})
+        assert(hasattr(vllm_sampling_params, '_set_kwargs'))
+        self.vllm_sampling_params = vllm_sampling_params
+        self.unsloth_num_chunks = unsloth_num_chunks
 pass
 
 {RLTrainer_extras}
@@ -422,7 +440,9 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     # Create full module
     exec(f""from trl.trainer import ({RLTrainer_name}, {RLConfig_name},)"")
     __RLTrainer_doc__ = eval(f""trl.trainer.{RLTrainer_name}"").__doc__
+    if __RLTrainer_doc__ is None: __RLTrainer_doc__ = """"
     __RLConfig_doc__  = eval(f""trl.trainer.{RLConfig_name}"") .__doc__
+    if __RLConfig_doc__ is None: __RLConfig_doc__ = """"
 
     # Get all pre-modules
     if trainer_file in RL_PRE_ITEMS:
@@ -431,6 +451,11 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         RL_pre = """"
     pass
 
+    # Check if SamplingParams is in there
+    if ""SamplingParams"" in old_RLTrainer_source:
+        RL_pre = RL_pre + ""\n"" + inspect.getsource(vLLMSamplingParams)
+    pass
+    
     # Selective log softmax
     selective_log_softmax_code = inspect.getsource(selective_log_softmax)
 
@@ -457,6 +482,14 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         selective_log_softmax_code = selective_log_softmax_code,
     )
 
+    # Remove multiple doc strings
+    if __RLConfig_doc__ != """" and RLTrainer_source.count(__RLTrainer_doc__) == 2:
+        RLTrainer_source = RLTrainer_source.replace(__RLTrainer_doc__, """", 1)
+    pass
+
+    # Remove multiple newlines
+    RLTrainer_source = re.sub(r""[\n]{3,}"", ""\n"", RLTrainer_source)
+
     # Create new function
     created_module = create_new_function(
         f""Unsloth{RLTrainer_name}"",
@@ -501,7 +534,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
             replacer = replacer[0]
             vllm_setter = ""\n"" + "" ""*8 + \
             ""if hasattr(model, 'vllm_engine') and ""\
-            ""getattr(args, 'use_vllm') and getattr(args, 'use_vllm', False): ""\
+            ""hasattr(args, 'use_vllm') and (getattr(args, 'use_vllm', False) == False): ""\
             ""args.use_vllm = True\n""
             init = init.replace(replacer, replacer + vllm_setter)
         pass
@@ -529,14 +562,31 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
         )
         if len(sampling_params) == 1:
             sampling_params = sampling_params[0]
+
+            # Fix guided_decoding
+            sampling_params = sampling_params.replace(
+                ""guided_decoding=guided_decoding,"",
+                'guided_decoding='\
+                'GuidedDecodingParams(backend=""outlines"", regex=args.vllm_guided_decoding_regex) '\
+                'if getattr(args, ""vllm_guided_decoding_regex"", None) is not None else None,',
+            )
             # Replace with our vLLM engine
             sampling_params = \
                 "" ""*12 + ""self.llm = model.vllm_engine; self._last_loaded_step = 0; "" + \
                 sampling_params # Add spaces
+
+            # Add extra arguments to SamplingParams
+            extra = ""**getattr(getattr(args, 'vllm_sampling_params', vLLMSamplingParams()), '_set_kwargs', {})""
+            # Backwards replace
+            to_replace = "","" + extra + "","" + "")""
+            sampling_params = to_replace.join(sampling_params.rsplit("")"", 1))
+            # Strip multiple commas
+            sampling_params = re.sub(r""[\,][\s]{0,}\,"", "","", sampling_params)
+
             new_vllm_part = \
-                f""\n{' '*8}if {args}.use_vllm:\n{sampling_params} ""\
-                f""if getattr(args, 'sampling_params', None) is None else ""\
-                f""getattr(args, 'sampling_params', None)\n{' '*8}else:\n""
+                f""\n{' '*8}if {args}.use_vllm:\n{sampling_params}""\
+                f""\n{' '*8}else:\n""
+
             init = init.replace(vllm_part, new_vllm_part)
         pass
     pass
@@ -607,6 +657,7 @@ def patch_functions(RLTrainer, trainer_file, RLTrainer_name, all_imports, import
         old, new = changed[function]
         RLTrainer_source = RLTrainer_source.replace(old, new)
     pass
+
     RLTrainer_source = RLTrainer_source.replace(
         f""class {RLTrainer_name}"", f""class _Unsloth{RLTrainer_name}"", 1
     )
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index b2501c9..23b3117 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -40,7 +40,7 @@ torch_compile_options = {
 }
 
 # Check untrained tokens
-def sft_trainer_fix_untraiend_tokens(call_args, extra_args):
+def sft_trainer_fix_untrained_tokens(call_args, extra_args):
     if ""model"" in call_args and ""train_dataset"" in call_args:
         fix_tokenizer = \
         ""IGNORED_TOKENIZER_NAMES = os.environ.get('UNSLOTH_IGNORED_TOKENIZER_NAMES', '').split('\\n')\n""\
@@ -52,7 +52,7 @@ def sft_trainer_fix_untraiend_tokens(call_args, extra_args):
         return fix_tokenizer
     return """"
 pass
-RL_EXTRA_ARGS[""sft_trainer""].append(sft_trainer_fix_untraiend_tokens)
+RL_EXTRA_ARGS[""sft_trainer""].append(sft_trainer_fix_untrained_tokens)
 
 
 # Remove DPO columns which might randomnly be tokenized
@@ -177,6 +177,7 @@ def grpo_trainer__get_per_token_logps(function_name, function):
     if  function_name != ""_get_per_token_logps"": return function
 
     def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
+        return None # Unsloth efficient GRPO
         if not hasattr(self, '_autocast_dtype'):
             self._autocast_dtype = torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION', 'fp16') == 'fp16' else torch.bfloat16
         with torch.amp.autocast(device_type = 'cuda', dtype = self._autocast_dtype):
@@ -198,8 +199,12 @@ def grpo_trainer__get_per_token_logps(function_name, function):
 pass
 RL_FUNCTIONS[""grpo_trainer""].append(grpo_trainer__get_per_token_logps)
 
-grpo_compute_loss = RL_REPLACEMENTS[""grpo_compute_loss""]
+grpo_compute_loss     = RL_REPLACEMENTS[""grpo_compute_loss""]
+UnslothEfficientGRPO  = RL_REPLACEMENTS[""UnslothEfficientGRPO""]
+grpo_accumulated_loss = RL_REPLACEMENTS[""grpo_accumulated_loss""]
 RL_PRE_ITEMS[""grpo_trainer""].append(inspect.getsource(grpo_compute_loss))
+RL_PRE_ITEMS[""grpo_trainer""].append(inspect.getsource(UnslothEfficientGRPO))
+RL_PRE_ITEMS[""grpo_trainer""].append(inspect.getsource(grpo_accumulated_loss))
 
 # Edit _get_per_token_logps to handle mixed precision
 def grpo_trainer_compute_loss(function_name, function):
@@ -213,10 +218,12 @@ def grpo_trainer_compute_loss(function_name, function):
         prompt_ids, prompt_mask = inputs[""prompt_ids""], inputs[""prompt_mask""]
         completion_ids, completion_mask = inputs[""completion_ids""], inputs[""completion_mask""]
         input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
+        bsz, qlen = input_ids.shape
         # attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
         attention_mask = None
         logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens
-
+        _input_ids = input_ids
+        _logits_to_keep = logits_to_keep
         per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)
 
         # Compute the KL divergence between the model and the reference model
@@ -229,9 +236,16 @@ def grpo_trainer_compute_loss(function_name, function):
         # per_token_loss = -(per_token_loss - self.beta * per_token_kl)
         # loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
         input_ids = input_ids[:, -logits_to_keep:]
-        loss, completion_length, mean_kl = grpo_compute_loss(
-            ref_per_token_logps, per_token_logps, input_ids, completion_mask, self.beta, advantages,
-        )
+        if False:#per_token_logps is not None:
+            loss, completion_length, mean_kl = grpo_compute_loss(
+                ref_per_token_logps, per_token_logps, input_ids, completion_mask, self.beta, advantages,
+            )
+        else:
+            loss, completion_length, mean_kl = grpo_accumulated_loss(
+                self, _input_ids, logits_to_keep, completion_mask, advantages,
+                n_chunks = self.args.unsloth_num_chunks,
+            )
+        
         # Log the metrics
         # completion_length = self.accelerator.gather_for_metrics(completion_mask.sum(1)).float().mean().item()
         self._metrics[""completion_length""].append(completion_length.item())
@@ -256,7 +270,7 @@ def grpo_trainer_fix_batch_size(RLTrainer_source, RLConfig_source):
     check_batch_size = \
     ""div = per_device_train_batch_size // num_generations\n""\
     ""if div * num_generations != per_device_train_batch_size:\n""\
-    ""    print('Unsloth: We know expect `per_device_train_batch_size` to be a multiple of `num_generations`.\\n""\
+    ""    print('Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\\n""\
                ""We will change the batch size of ' + str(per_device_train_batch_size) + ' to the `num_generations` of ' + str(num_generations))\n""\
     ""    per_device_train_batch_size = num_generations\n""
     return check_batch_size
diff --git a/unsloth/save.py b/unsloth/save.py
index d3ba192..eaddfa0 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -31,10 +31,14 @@ from transformers.models.llama.modeling_llama import logger
 from .tokenizer_utils import fix_sentencepiece_gguf
 from huggingface_hub import HfApi
 try:
-    from huggingface_hub.utils import get_token
+    from huggingface_hub import get_token
 except:
-    # Old HF Hub versions <= 0.0.25
-    from huggingface_hub.utils._token import get_token
+    try:
+        from huggingface_hub.utils import get_token
+    except:
+        # For older versions of huggingface_hub
+        from huggingface_hub.utils._token import get_token
+    pass
 pass
 from pathlib import Path
 
@@ -254,7 +258,7 @@ def unsloth_save_model(
     # First check for a token!
     if push_to_hub:
         from huggingface_hub import whoami
-        try: 
+        try:
             username = whoami(token = token)[""name""]
         except:
             raise RuntimeError(
@@ -385,7 +389,7 @@ def unsloth_save_model(
     else:
         internal_model = model
     pass
-        
+
     # Cannot be converted properly!
     if (save_method == ""merged_4bit"") or (save_method == ""lora"") or (
         not hasattr(model, ""model"") or \
@@ -481,7 +485,7 @@ def unsloth_save_model(
         gb_found = re.match(""([0-9]{1,})[\s]{0,}GB"", max_shard_size, flags = re.IGNORECASE)
         mb_found = re.match(""([0-9]{1,})[\s]{0,}MB"", max_shard_size, flags = re.IGNORECASE)
         if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024
-        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 
+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024
     elif type(max_shard_size) is int:
         sharded_ram_usage = sharded_ram_usage
     pass
@@ -612,7 +616,7 @@ def unsloth_save_model(
     # Edit save_pretrained_settings
     # [TODO] _create_repo has errors due to **kwargs getting accepted
     save_pretrained_settings[""state_dict""] = state_dict
-    
+
     # commit_description does not seem to work?
     what_to_delete = (""use_temp_dir"", ""commit_message"", ""create_pr"", ""revision"", ""commit_description"", ""tags"",) \
         if not push_to_hub else \
@@ -665,7 +669,7 @@ def unsloth_save_model(
 
         # Revert back padding side
         tokenizer.padding_side = old_padding_side
-            
+
         print("" Done."")
     else:
         print()
@@ -877,10 +881,15 @@ def install_llama_cpp_old(version = -10):
     pass
 
     # Check if successful
-    if not os.path.exists(""llama.cpp/quantize"") and not os.path.exists(""llama.cpp/llama-quantize""):
+    if not (
+        os.path.exists(""llama.cpp/llama-quantize.exe"") or
+        os.path.exists(""llama.cpp/llama-quantize"") or
+        os.path.exists(""llama.cpp/quantize.exe"") or
+        os.path.exists(""llama.cpp/quantize"")
+    ):
         raise RuntimeError(
             ""Unsloth: The file 'llama.cpp/llama-quantize' or `llama.cpp/quantize` does not exist.\n""\
-            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+            ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
         )
     pass
 pass
@@ -957,7 +966,7 @@ def save_to_gguf(
     else:
         raise TypeError(""Unsloth: quantization_method can only be a string or a list of strings"")
     pass
-    
+
     # Check if bfloat16 is supported
     if model_dtype == ""bf16"" and not torch.cuda.is_bf16_supported():
         logger.warning(
@@ -973,7 +982,7 @@ def save_to_gguf(
     pass
 
     # Check I quants
-    for quant_method in quantization_method: 
+    for quant_method in quantization_method:
         if quant_method.startswith(""iq2""):
             raise RuntimeError(""Unsloth: Currently iq2 type quantizations aren't supported yet - sorry!"")
     pass
@@ -1026,9 +1035,9 @@ def save_to_gguf(
     pass
 
     # Determine whether the system already has llama.cpp installed and the scripts are executable
-    quantize_location = get_executable([""llama-quantize"", ""quantize""])
+    quantize_location = get_executable([""llama-quantize"", ""quantize"", ""llama-quantize.exe"", ""quantize.exe""])
     convert_location  = get_executable([""convert-hf-to-gguf.py"", ""convert_hf_to_gguf.py""])
-    
+
     error = 0
     if quantize_location is not None and convert_location is not None:
         print(""Unsloth: llama.cpp found in the system. We shall skip installation."")
@@ -1062,14 +1071,18 @@ def save_to_gguf(
         # and llama.cpp/main changed to llama.cpp/llama-cli
         # See https://github.com/ggerganov/llama.cpp/pull/7809
         quantize_location = None
-        if os.path.exists(""llama.cpp/quantize""):
+        if os.path.exists(""llama.cpp/quantize.exe""):
+            quantize_location = ""llama.cpp/quantize.exe""
+        elif os.path.exists(""llama.cpp/quantize""):
             quantize_location = ""llama.cpp/quantize""
+        elif os.path.exists(""llama.cpp/llama-quantize.exe""):
+            quantize_location = ""llama.cpp/llama-quantize.exe""
         elif os.path.exists(""llama.cpp/llama-quantize""):
             quantize_location = ""llama.cpp/llama-quantize""
         else:
             raise RuntimeError(
-                ""Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\n""\
-                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name?""
+                ""Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\n""\
+                ""But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.""
             )
         pass
 
@@ -1150,7 +1163,7 @@ def save_to_gguf(
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
 
     final_location = str((Path(model_directory) / f""unsloth.{first_conversion.upper()}.gguf"").absolute())
-    
+
     print(f""Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\n""\
           f""The output location will be {final_location}\n""\
           ""This might take 3 minutes..."")
@@ -1217,7 +1230,7 @@ def save_to_gguf(
 
             command = f""./{quantize_location} {full_precision_location} ""\
                 f""{final_location} {quant_method} {n_cpus}""
-            
+
             try_execute([command,], force_complete = True)
 
             # Check if quantization succeeded!
@@ -1378,7 +1391,7 @@ def _determine_username(save_directory, old_username, token):
     save_directory = save_directory.lstrip(""./"")
     if ""/"" not in save_directory:
         from huggingface_hub import whoami
-        try: 
+        try:
             username = whoami(token = token)[""name""]
             if type(old_username) is str and username != old_username:
                 username = old_username
@@ -1412,7 +1425,7 @@ def create_huggingface_repo(
             repo_type = ""model"",
             exist_ok  = False,
             private   = private,
-        ) 
+        )
 
         # Create model card
         from huggingface_hub import ModelCard
@@ -1453,7 +1466,7 @@ def upload_to_huggingface(
             repo_type = ""model"",
             exist_ok  = False,
             private   = private,
-        ) 
+        )
 
         # Create model card
         from huggingface_hub import ModelCard
@@ -1527,7 +1540,7 @@ def fix_tokenizer_bos_token(tokenizer):
     # Check if BOS added already, then warn
     fix_bos_token = False
     chat_template = getattr(tokenizer, ""chat_template"", None)
-    
+
     if (tokenizer(""A"").input_ids[0] == getattr(tokenizer, ""bos_token_id"", None)):
         if chat_template is not None and \
             (
@@ -1546,7 +1559,7 @@ def fix_tokenizer_bos_token(tokenizer):
             new_chat_template = re.sub(r""\{[\s]{0,}\{[\s]{0,}bos\_token[\s]{0,}\}[\s]{0,}\}"", """", chat_template)
             # Remove {{bos_token +
             new_chat_template = re.sub(r""\{[\s]{0,}\{[\s]{0,}bos\_token[\s]{0,}\+[\s]{0,}"", """", new_chat_template)
-            
+
             tokenizer.chat_template = new_chat_template
 
         pass
@@ -1580,7 +1593,7 @@ def create_ollama_modelfile(tokenizer, gguf_location):
     modelfile = modelfile\
         .replace(FILE_LOCATION_REPLACER, ""{__FILE_LOCATION__}"")\
         .replace(EOS_TOKEN_REPLACER,     ""{__EOS_TOKEN__}"")
-    
+
     if ""__EOS_TOKEN__"" in modelfile:
         modelfile = modelfile.format(
             __FILE_LOCATION__  = gguf_location,
@@ -1591,7 +1604,7 @@ def create_ollama_modelfile(tokenizer, gguf_location):
             __FILE_LOCATION__  = gguf_location,
         )
     pass
-    
+
     modelfile = modelfile\
         .replace(""@#"", ""{"")\
         .replace(""@#"", ""}"")\
@@ -1733,7 +1746,7 @@ def unsloth_save_pretrained_gguf(
 
     # Save to GGUF
     all_file_locations, want_full_precision = save_to_gguf(
-        model_type, model_dtype, is_sentencepiece_model, 
+        model_type, model_dtype, is_sentencepiece_model,
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
@@ -1911,7 +1924,7 @@ def unsloth_push_to_hub_gguf(
 
     # Save to GGUF
     all_file_locations, want_full_precision = save_to_gguf(
-        model_type, model_dtype, is_sentencepiece_model, 
+        model_type, model_dtype, is_sentencepiece_model,
         new_save_directory, quantization_method, first_conversion, makefile,
     )
 
@@ -1928,7 +1941,7 @@ def unsloth_push_to_hub_gguf(
 
     # If not needing full precision, skip the first
     if not want_full_precision: all_file_locations = all_file_locations[1:]
-    
+
     for file_location in all_file_locations:
         print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
         username = upload_to_huggingface(
@@ -2044,8 +2057,8 @@ def unsloth_convert_lora_to_ggml_and_push_to_hub(
 
 def unsloth_convert_lora_to_ggml_and_save_locally(
     self,
-    save_directory: str, # Added parameter for the folder name 
-    tokenizer, 
+    save_directory: str, # Added parameter for the folder name
+    tokenizer,
     temporary_location: str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage: float = 0.85,
 ):
@@ -2162,7 +2175,7 @@ def unsloth_generic_save_pretrained_merged(
     tags                 : List[str] = None,
     temporary_location   : str = ""_unsloth_temporary_saved_buffers"",
     maximum_memory_usage : float = 0.75,
-):   
+):
     """"""
         Same as .push_to_hub(...) except 4bit weights are auto
         converted to float16 with as few overhead as possible.
"
"diff --git a/README.md b/README.md
index 422abcf..a348e48 100644
--- a/README.md
+++ b/README.md
@@ -7,7 +7,7 @@
 ## 2-5x faster 60% less memory local QLoRA finetuning
 * Supports Llama 7b, 13b, 70b, CodeLlama 34b, Mistral 7b, TinyLlama and all Llama archs!
 * Llama 7b [Colab T4 example](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) on 1 T4 2x faster, uses 43% less VRAM (8.4GB) LAION dataset. [Alpaca T4 example](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) 2x faster on 1 T4, using 6.4GB VRAM.
-* Mistral 7b [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) on 1 A100 2.2x faster, uses 62% less VRAM (12.4GB).
+* Mistral 7b [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) on 1 A100 2.2x faster, uses 62% less VRAM (12.4GB). [Colab T4 example](https://colab.research.google.com/drive/15pyLgRN97B_jA56HS0esx56knA9I5tuv?usp=sharing)
 * CodeLlama 34b [Colab example](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) does not OOM is 1.9x faster, uses 32% less VRAM (27GB).
 * Kaggle 2 Tesla T4s 5.28x faster on Alpaca. [Kaggle example](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp)
 * All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language.
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 8b1fd0f..7eb651c 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -37,6 +37,17 @@ def MistralAttention_fast_forward(
     bsz, q_len, _ = hidden_states.size()
     Q, K, V = self.apply_qkv(self, hidden_states)
 
+    # Check for inference
+    if use_cache and past_key_value is not None and q_len == 1:
+        A, past_key_value = LlamaAttention_fast_forward_inference(
+            self,
+            hidden_states,
+            past_key_value,
+            position_ids,
+        )
+        return A, None, past_key_value
+    pass
+
     n_heads    = self.num_heads
     n_groups   = self.num_key_value_groups
     n_kv_heads = self.num_key_value_heads
@@ -152,8 +163,10 @@ def MistralForCausalLM_fast_forward(
         elif q_len <= sliding_window:
             causal_mask = xformers.attn_bias.LowerTriangularMask()
         else:
-            causal_mask = xformers.attn_bias.BlockDiagonalCausalLocalAttentionMask.\
-                make_local_attention(window_size = sliding_window)
+            # Fix from https://github.com/Rypo
+            causal_mask = xformers.attn_bias.BlockDiagonalCausalMask\
+                .from_seqlens([qlen]*bsz)\
+                .make_local_attention(window_size = sliding_window)
     pass
 
     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
"
"diff --git a/pyproject.toml b/pyproject.toml
index 73e69dc..5a9d922 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -7,7 +7,7 @@ name = ""unsloth""
 dynamic = [""version""]
 description = ""2-5X faster LLM finetuning""
 readme = ""README.md""
-requires-python = "">=3.9""
+requires-python = "">=3.9,<=3.12""
 license = {file = ""LICENSE""}
 keywords = [""ai"", ""llm"",]
 authors = [
@@ -39,8 +39,8 @@ triton = [
     ""triton @ https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post10/triton-3.2.0-cp312-cp312-win_amd64.whl ; python_version=='3.12' and platform_system == 'Windows'""
 ]
 
-windows=[
-    ""unsloth_zoo>=2025.3.1"",
+huggingface = [
+    ""unsloth_zoo>=2025.3.2"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -51,34 +51,18 @@ windows=[
     ""wheel>=0.42.0"",
     ""numpy"",
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0,<=0.15.2"",
     ""peft>=0.7.1,!=0.11.0"",
     ""protobuf<4.0.0"",
     ""huggingface_hub"",
     ""hf_transfer"",
     ""unsloth[triton]"",
+]
+windows=[
+    ""unsloth[huggingface]"",
     ""bitsandbytes>=0.41.1 ; platform_system == 'Windows'"",
     ""xformers>=0.0.22.post7 ; platform_system == 'Windows'"",
 ]
-huggingface = [
-    ""unsloth_zoo>=2025.3.1"",
-    ""packaging"",
-    ""tyro"",
-    ""transformers>=4.46.1,!=4.47.0"",
-    ""datasets>=2.16.0"",
-    ""sentencepiece>=0.2.0"",
-    ""tqdm"",
-    ""psutil"",
-    ""wheel>=0.42.0"",
-    ""numpy"",
-    ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
-    ""peft>=0.7.1,!=0.11.0"",
-    ""protobuf<4.0.0"",
-    ""huggingface_hub"",
-    ""hf_transfer"",
-    ""unsloth[triton]"",
-]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9' and platform_system == 'Linux'"",
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version=='3.10' and platform_system == 'Linux'"",
@@ -370,7 +354,7 @@ colab-ampere-torch220 = [
     ""flash-attn>=2.6.3"",
 ]
 colab-new = [
-    ""unsloth_zoo>=2025.2.7"",
+    ""unsloth_zoo>=2025.3.1"",
     ""packaging"",
     ""tyro"",
     ""transformers>=4.46.1,!=4.47.0"",
@@ -388,7 +372,7 @@ colab-new = [
 ]
 colab-no-deps = [
     ""accelerate>=0.34.1"",
-    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0"",
+    ""trl>=0.7.9,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,!=0.15.0,<=0.15.2"",
     ""peft>=0.7.1"",
     ""xformers"",
     ""bitsandbytes>=0.46.1"",
diff --git a/unsloth/__init__.py b/unsloth/__init__.py
index c8f2926..8439ab8 100644
--- a/unsloth/__init__.py
+++ b/unsloth/__init__.py
@@ -198,7 +198,7 @@ pass
 # Check for unsloth_zoo
 try:
     unsloth_zoo_version = importlib_version(""unsloth_zoo"")
-    if Version(unsloth_zoo_version) < Version(""2025.3.1""):
+    if Version(unsloth_zoo_version) < Version(""2025.3.2""):
         try:
             os.system(""pip install --upgrade --no-cache-dir --no-deps unsloth_zoo"")
         except:
diff --git a/unsloth/kernels/utils.py b/unsloth/kernels/utils.py
index 8da152b..db1d73c 100644
--- a/unsloth/kernels/utils.py
+++ b/unsloth/kernels/utils.py
@@ -104,6 +104,11 @@ cdequantize_blockwise_fp16_nf4  = bnb.functional.lib.cdequantize_blockwise_fp16_
 cdequantize_blockwise_bf16_nf4  = bnb.functional.lib.cdequantize_blockwise_bf16_nf4
 cgemm_4bit_inference_naive_fp16 = bnb.functional.lib.cgemm_4bit_inference_naive_fp16
 cgemm_4bit_inference_naive_bf16 = bnb.functional.lib.cgemm_4bit_inference_naive_bf16
+torch_mm = torch.mm
+torch_mv = torch.mv
+torch_matmul = torch.matmul
+torch_addmm  = torch.addmm
+torch_empty  = torch.empty
 
 def QUANT_STATE(W): return getattr(W, ""quant_state"", None)
 
@@ -194,8 +199,8 @@ if HAS_CUDA_STREAM:
             WEIGHT_BUFFER = WEIGHT_BUFFERS[device_index]
             ABSMAX_BUFFER = ABSMAX_BUFFERS[device_index]
             if WEIGHT_BUFFER is None:
-                WEIGHT_BUFFERS[device_index] = WEIGHT_BUFFER = torch.empty(size, dtype = dtype, device = device, requires_grad = False)
-                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch.empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+                WEIGHT_BUFFERS[device_index] = WEIGHT_BUFFER = torch_empty(size, dtype = dtype, device = device, requires_grad = False)
+                ABSMAX_BUFFERS[device_index] = ABSMAX_BUFFER = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
 
             if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)
             if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)
@@ -204,11 +209,11 @@ if HAS_CUDA_STREAM:
             out_absmax = ABSMAX_BUFFER[:n_elements_absmax]
         else:
             if out is None:
-                out = torch.empty(shape, dtype = dtype, device = device, requires_grad = False)
+                out = torch_empty(shape, dtype = dtype, device = device, requires_grad = False)
             else:
                 assert(out.shape == shape)
                 assert(out.dtype == dtype)
-            out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+            out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
         pass
 
         # NF4 dequantization of statistics
@@ -258,11 +263,11 @@ else:
 
         # Create weight matrix
         if out is None:
-            out = torch.empty(shape, dtype = dtype, device = device, requires_grad = False)
+            out = torch_empty(shape, dtype = dtype, device = device, requires_grad = False)
         else:
             assert(out.shape == shape)
             assert(out.dtype == dtype)
-        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
+        out_absmax = torch_empty(n_elements_absmax, dtype = torch.float32, device = device, requires_grad = False)
 
         # Do dequantization
         ptr_out_absmax = get_ptr(out_absmax)
@@ -286,7 +291,7 @@ pass
 
 if HAS_CUDA_STREAM:
     def fast_gemv(X, W, quant_state, out = None):
-        if quant_state is None: return torch.matmul(X, W, out = out)
+        if quant_state is None: return torch_matmul(X, W, out = out)
         # For fast X @ W where seq_len == 1
         # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469
         _, q_len, hd = X.shape
@@ -318,7 +323,7 @@ if HAS_CUDA_STREAM:
         bout = shape[0]
 
         if out is None:
-            out = torch.empty((1, 1, bout,), dtype = dtype, device = device)
+            out = torch_empty((1, 1, bout,), dtype = dtype, device = device)
         # else:
         #     assert(out.shape == (1, 1, bout,))
         # pass
@@ -336,7 +341,7 @@ if HAS_CUDA_STREAM:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch.empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
         with torch_cuda_device(device):
             cdequantize_blockwise_fp32(
                 get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
@@ -385,7 +390,7 @@ else:
         device = W.device
 
         if out is None:
-            out = torch.empty((1, 1, bout,), dtype = dtype, device = device)
+            out = torch_empty((1, 1, bout,), dtype = dtype, device = device)
         # else:
         #     assert(out.shape == (1, 1, bout,))
         # pass
@@ -403,7 +408,7 @@ else:
         ldb = ctypes_c_int32(ldb)
         ldc = ctypes_c_int32(ldc)
 
-        df = torch.empty(absmax.shape, dtype = torch.float32, device = device)
+        df = torch_empty(absmax.shape, dtype = torch.float32, device = device)
         cdequantize_blockwise_fp32(
             get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), get_ptr(df),
             ctypes_c_int(blocksize2), ctypes_c_int(df.numel()),
@@ -423,10 +428,6 @@ else:
 pass
 
 
-torch_mm = torch.mm
-torch_mv = torch.mv
-torch_matmul = torch.matmul
-torch_addmm  = torch.addmm
 def fast_linear_forward(proj, X, temp_lora = None, out = None):
 
     W, W_quant, lora_A, lora_B, lora_S, bias = get_lora_parameters_bias(proj)
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 43828a3..66926bc 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.3""
+__version__ = ""2025.3.4""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
@@ -39,8 +39,8 @@ __all__ = [
     ""create_boolean_mask"",
     ""torch_amp_custom_fwd"",
     ""torch_amp_custom_bwd"",
-    ""accelerate_old_send_to_device"",
-    ""accelerate_new_send_to_device"",
+    # ""accelerate_old_send_to_device"",
+    # ""accelerate_new_send_to_device"",
     ""patch_gradient_accumulation_fix"",
     ""patch_compiling_bitsandbytes"",
     ""patch_regional_compilation"",
@@ -241,24 +241,24 @@ pass
 
 # =============================================
 # Fix KeyError: 'Cache only has 0 layers, attempted to access layer with index 0'
-import transformers.cache_utils
-if hasattr(transformers.cache_utils, ""DynamicCache"") and \
-    transformers.cache_utils.DynamicCache.__getitem__.__name__ != ""__cache_utils_getitem__"":
-
-    source = inspect.getsource(transformers.cache_utils.DynamicCache.__getitem__)
-    start = source.find(""def"")
-    spaces = start*"" ""
-    source = source.split(""\n"")
-    source = ""\n"".join(x[start:] for x in source)
-    where = source.find(""raise KeyError"")
-    source = source[:where] + \
-        f""if len(self) == 0:\n{spaces}{spaces}""\
-        ""    raise RuntimeError('Unsloth: You must call `FastLanguageModel.for_inference(model)` before doing inference for Unsloth models.')\n"" + \
-        f""{spaces}{spaces}else:\n{spaces}{spaces}{spaces}"" + source[where:]
-    source = source.replace(""__getitem__"", ""__cache_utils_getitem__"", 1)
-    exec(source)
-    transformers.cache_utils.DynamicCache.__getitem__ = __cache_utils_getitem__
-pass
+# import transformers.cache_utils
+# if hasattr(transformers.cache_utils, ""DynamicCache"") and \
+#     transformers.cache_utils.DynamicCache.__getitem__.__name__ != ""__cache_utils_getitem__"":
+
+#     source = inspect.getsource(transformers.cache_utils.DynamicCache.__getitem__)
+#     start = source.find(""def"")
+#     spaces = start*"" ""
+#     source = source.split(""\n"")
+#     source = ""\n"".join(x[start:] for x in source)
+#     where = source.find(""raise KeyError"")
+#     source = source[:where] + \
+#         f""if len(self) == 0:\n{spaces}{spaces}""\
+#         ""    raise RuntimeError('Unsloth: You must call `FastLanguageModel.for_inference(model)` before doing inference for Unsloth models.')\n"" + \
+#         f""{spaces}{spaces}else:\n{spaces}{spaces}{spaces}"" + source[where:]
+#     source = source.replace(""__getitem__"", ""__cache_utils_getitem__"", 1)
+#     exec(source)
+#     transformers.cache_utils.DynamicCache.__getitem__ = __cache_utils_getitem__
+# pass
 # =============================================
 
 # =============================================
@@ -411,25 +411,25 @@ pass
 
 # =============================================
 # Fix new Xformers versions TypeError: Multiple dispatch failed for 'torch._ops.aten.to.dtype_layout'
-accelerate_old_send_to_device = None
-accelerate_new_send_to_device = None
-if xformers_version is not None and Version(xformers_version) >= Version(""0.0.27""):
-    import accelerate.utils.operations
-    if hasattr(accelerate.utils.operations, ""send_to_device"") and \
-        accelerate.utils.operations.send_to_device.__name__ != ""_fixed_send_to_device"":
-        accelerate_old_send_to_device = accelerate.utils.operations.send_to_device
-        from accelerate.utils.operations import *
-        send_to_device = inspect.getsource(accelerate.utils.operations.send_to_device)
-        send_to_device = re.sub(
-            r""([ ]{4,})return tensor\.to\(device\)"",
-            r""\1try: return tensor.to(device)\n\1except: return tensor"",
-            send_to_device,
-        ).replace(""def send_to_device"", ""def _fixed_send_to_device"")
-        exec(send_to_device)
-        # accelerate.utils.operations.send_to_device = _fixed_send_to_device
-        accelerate_new_send_to_device = _fixed_send_to_device
-    pass
-pass
+# accelerate_old_send_to_device = None
+# accelerate_new_send_to_device = None
+# if xformers_version is not None and Version(xformers_version) >= Version(""0.0.27""):
+#     import accelerate.utils.operations
+#     if hasattr(accelerate.utils.operations, ""send_to_device"") and \
+#         accelerate.utils.operations.send_to_device.__name__ != ""_fixed_send_to_device"":
+#         accelerate_old_send_to_device = accelerate.utils.operations.send_to_device
+#         from accelerate.utils.operations import *
+#         send_to_device = inspect.getsource(accelerate.utils.operations.send_to_device)
+#         send_to_device = re.sub(
+#             r""([ ]{4,})return tensor\.to\(device\)"",
+#             r""\1try: return tensor.to(device)\n\1except: return tensor"",
+#             send_to_device,
+#         ).replace(""def send_to_device"", ""def _fixed_send_to_device"")
+#         exec(send_to_device)
+#         # accelerate.utils.operations.send_to_device = _fixed_send_to_device
+#         accelerate_new_send_to_device = _fixed_send_to_device
+#     pass
+# pass
 
 # Transformers 4.46 breaks dynamic caching. This is a hack
 import transformers.generation.configuration_utils
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index bcabbd5..3dacf5c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -15,7 +15,7 @@
 import torch
 import gc
 import math
-from functools import partial
+import functools
 from typing import Optional, Tuple, List, Union
 from ._utils import *
 from ._utils import patch_unsloth_smart_gradient_checkpointing
@@ -65,6 +65,7 @@ from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
 from peft import PeftModelForCausalLM
 from ..save import patch_saving_functions
 import re, os, inspect, math, sys
+import types
 try:
     from huggingface_hub.utils import get_token
 except:
@@ -217,14 +218,14 @@ def LlamaAttention_fast_forward_inference(
     RH_Q = self.RH_Q
     RH_Q[:,:,:,:h] = Qn[:,:,:,h:]
     RH_Q[:,:,:,h:] = Qn[:,:,:,:h]
-    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
+    RH_Q[:,:,:,:h].neg_() # torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])
     Qn *= cos
     Qn.addcmul_(RH_Q, sin)
 
     RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = ""cuda:0"")
     RH_K[:,:,:,:h] = Kn[:,:,:,h:]
     RH_K[:,:,:,h:] = Kn[:,:,:,:h]
-    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
+    RH_K[:,:,:,:h].neg_() #torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])
     Kn *= cos
     Kn.addcmul_(RH_K, sin)
     
@@ -400,19 +401,20 @@ def LlamaAttention_fast_forward(
     else:
         # Extend RoPE dynamically to fit in VRA
         rotary_emb = self.rotary_emb
-        rotary_emb.extend_rope_embedding(V, seq_len=kv_seq_len)
+        rotary_emb.extend_rope_embedding(V, seq_len = kv_seq_len)
 
         if position_ids is None:
             # Useful for LongRoPE
             cos, sin = rotary_emb.get_cached(kv_seq_len)
         else:
-            cos, sin = rotary_emb(V, seq_len=kv_seq_len)
+            cos, sin = rotary_emb(V, seq_len = kv_seq_len)
 
-    Q, K = (
-        fast_rope_embedding(Q, K, cos, sin) 
-        if position_ids is None 
-        else inplace_rope_embedding(Q, K, cos, sin, position_ids)
-    )
+    # Q, K = (
+    #     fast_rope_embedding(Q, K, cos, sin)
+    #     if position_ids is None
+    #     else inplace_rope_embedding(Q, K, cos, sin, position_ids)
+    # )
+    Q, K = fast_rope_embedding(Q, K, cos, sin)
 
     if past_key_value is not None:
         K = torch.cat([past_key_value[0], K], dim = 2)
@@ -924,7 +926,6 @@ def LlamaModel_fast_forward_inference(
     X = X.to(self.config.torch_dtype)
     bsz, q_len, hd = X.shape
     assert(q_len == 1)
-    
     # Get saved buffers to reduce memory movement
     residual = torch.empty((bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
     _XX = torch.empty((2, bsz, q_len, hd), dtype = torch.float32, device = ""cuda:0"")
@@ -1020,7 +1021,6 @@ def CausalLM_fast_forward(fast_forward_inference):
         logits_to_keep: Optional[int] = 0,
         *args, **kwargs,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
-        
         if past_key_values is not None:
             outputs = fast_forward_inference(
                 self,
@@ -1069,7 +1069,7 @@ def CausalLM_fast_forward(fast_forward_inference):
         if labels is not None: labels = labels.to(lm_head_device)
 
         # Output last hidden states without logits if asked
-        if os.environ.get(""UNSLOTH_RETURN_HIDDEN_STATES"", ""0"") == ""1"":
+        if self.training and os.environ.get(""UNSLOTH_RETURN_HIDDEN_STATES"", ""0"") == ""1"":
             if num_logits_to_keep != 0:
                 hidden_states = hidden_states[:, -num_logits_to_keep:, :]
             return CausalLMOutputWithPast(
@@ -1534,78 +1534,58 @@ class LongRopeRotaryEmbedding(torch.nn.Module):
 pass
 
 
-def _wrap_fast_inference(generate, device_type, dtype, model):
-    # Wraps inference with bfloat16 / float16
-    @torch.inference_mode
-    def _fast_generate(*args, **kwargs):
-
-        if hasattr(model, ""config"") and hasattr(model.config, ""max_position_embeddings""):
-            if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
-                if kwargs[""input_ids""].shape[-1] + kwargs[""max_new_tokens""] > model.config.max_position_embeddings:
-                    raise ValueError(
-                        f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {model.config.max_position_embeddings}!\n'\
-                        'You will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`.'
-                    )
-        pass
-
-        # Set a flag for generation!
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            internal_model._flag_for_generation = True
-            internal_model = internal_model.model
-        pass
-        internal_model._flag_for_generation = True
+def unsloth_fast_generate(
+    self,
+    *args,
+    **kwargs,
+):
+    FastLlamaModel.for_inference(self)
 
-        # Must patch accelerate for Xformers
-        if accelerate_new_send_to_device is not None:
-            import accelerate.utils.operations
-            accelerate.utils.operations.send_to_device = accelerate_new_send_to_device
-        pass
+    dtype = _get_dtype(self.config.torch_dtype)
 
-        # For newer HF
-        kwargs[""cache_implementation""] = ""dynamic""
-        # For num_logits_to_keep
-        kwargs[""num_logits_to_keep""] = 1
+    if hasattr(self, ""config"") and hasattr(self.config, ""max_position_embeddings""):
+        if ""input_ids"" in kwargs and kwargs[""input_ids""] is not None and ""max_new_tokens"" in kwargs:
+            if kwargs[""input_ids""].shape[-1] + kwargs[""max_new_tokens""] > self.config.max_position_embeddings:
+                raise ValueError(
+                    f'Unsloth: input length {kwargs[""input_ids""].shape[-1]} + max_new_tokens {kwargs[""max_new_tokens""]} exceeds the maximum sequence length of {model.config.max_position_embeddings}!\n'\
+                    'You will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`.'
+                )
+    pass
 
-        # Remove token_type_ids
-        kwargs.pop(""token_type_ids"", None)
+    # Must patch accelerate for Xformers
+    # if accelerate_new_send_to_device is not None:
+    #     import accelerate.utils.operations
+    #     accelerate.utils.operations.send_to_device = accelerate_new_send_to_device
+    # pass
 
-        # Check pad_token
-        model_eos_token_id = getattr(model.config, ""eos_token_id"", None)
-        if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
-            model_eos_token_id = model_eos_token_id[0]
+    # For newer HF
+    kwargs[""cache_implementation""] = ""dynamic""
+    # For num_logits_to_keep
+    kwargs[""num_logits_to_keep""] = 1
 
-        kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
+    # Remove token_type_ids
+    kwargs.pop(""token_type_ids"", None)
 
-        # Set pad token
-        # old_pad_token_id = getattr(model.config, ""pad_token_id"", None)
-        # old_eos_token_id = getattr(model.config, ""eos_token_id"", None)
-        # model.config.pad_token_id = old_eos_token_id
+    # Check pad_token
+    model_eos_token_id = getattr(self.config, ""eos_token_id"", None)
+    if model_eos_token_id is not None and hasattr(model_eos_token_id, ""__iter__""):
+        model_eos_token_id = model_eos_token_id[0]
 
-        # Autocasted
-        with torch.autocast(device_type = device_type, dtype = dtype):
-            output = generate(*args, **kwargs)
-        pass
+    kwargs[""pad_token_id""] = kwargs.pop(""pad_token_id"", model_eos_token_id)
 
-        # Revert
-        # model.config.pad_token_id = old_pad_token_id
+    # Mixed precision autocast
+    with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
+        output = self._old_generate(*args, **kwargs)
+    pass
 
-        # Unset a flag for generation!
-        internal_model = model
-        while hasattr(internal_model, ""model""):
-            if hasattr(internal_model, ""_flag_for_generation""): del internal_model._flag_for_generation
-            internal_model = internal_model.model
-        pass
-        if hasattr(internal_model, ""_flag_for_generation""): del internal_model._flag_for_generation
+    # Return accelerate back
+    # if accelerate_new_send_to_device is not None:
+    #     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device
+    # pass
 
-        # Return accelerate back
-        if accelerate_new_send_to_device is not None:
-            accelerate.utils.operations.send_to_device = accelerate_old_send_to_device
-        pass
+    FastLlamaModel.for_training(self)
 
-        return output
-    pass
-    return _fast_generate
+    return output
 pass
 
 
@@ -1682,8 +1662,12 @@ class FastLlamaModel:
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
+        from importlib.metadata import version as importlib_version
+        try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
+        except: vllm_version = """"
+
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_patcher.__name__[4:-5]} patching. Transformers: {transformers_version}.{vllm_version}\n""\
            f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
            f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
@@ -1825,7 +1809,7 @@ class FastLlamaModel:
             model = convert_vllm_to_huggingface(quant_state_dict, model_config, dtype)
             model.vllm_engine = llm
             model.fast_generate = model.vllm_engine.generate
-            model.fast_generate_batches = partial(generate_batches, model.vllm_engine)
+            model.fast_generate_batches = functools.partial(generate_batches, model.vllm_engine)
         pass
         # Return old flag
         os.environ[""HF_HUB_ENABLE_HF_TRANSFER""] = old_hf_transfer
@@ -1986,6 +1970,11 @@ class FastLlamaModel:
                 layer.self_attn.rotary_emb = rotary_emb
         pass
         
+        # Patch generate
+        if model.generate.__name__ != ""unsloth_fast_generate"":
+            model._old_generate = model.generate
+            unsloth_fast_generate.__doc__ = model._old_generate.__doc__
+            model.generate = types.MethodType(unsloth_fast_generate, model)
         return model, tokenizer
     pass
 
@@ -2410,12 +2399,20 @@ class FastLlamaModel:
             model.fast_generate_batches = vllm_fast_generate_batches
 
             # Also saving and loading LoRA
-            from functools import partial
             from unsloth_zoo.vllm_utils import save_lora, load_lora
-            model.save_lora = partial(save_lora, model)
-            model.load_lora = partial(load_lora, model)
+            model.save_lora = functools.partial(save_lora, model)
+            model.load_lora = functools.partial(load_lora, model)
         pass
 
+        # Add for_inference and for_training
+        model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
+        model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
+        
+        # Patch generate
+        if model.generate.__name__ != ""unsloth_fast_generate"":
+            model._old_generate = model.generate
+            unsloth_fast_generate.__doc__ = model._old_generate.__doc__
+            model.generate = types.MethodType(unsloth_fast_generate, model)
         return model
     pass
 
@@ -2486,7 +2483,6 @@ class FastLlamaModel:
         n_mlp = 0
         n_qkv = 0
         n_o   = 0
-        import types
 
         active_adapter = model.active_adapters[0] if \
             hasattr(model, ""active_adapters"") else model.active_adapter
@@ -2496,9 +2492,8 @@ class FastLlamaModel:
         bias         = model.peft_config[active_adapter].bias
 
         # We also do not inplace edit QKV for Cohere!
-        from functools import partial
         _apply_lora_mlp = \
-            partial(apply_lora_mlp, inplace = False) \
+            functools.partial(apply_lora_mlp, inplace = False) \
             if model_type == ""cohere"" else \
             apply_lora_mlp
         pass
@@ -2611,52 +2606,30 @@ class FastLlamaModel:
         pass
 
         # Add for_inference and for_training
-        model.for_training  = partial(FastLlamaModel.for_training,  model)
-        model.for_inference = partial(FastLlamaModel.for_inference, model)
+        model.for_training  = functools.partial(FastLlamaModel.for_training,  model)
+        model.for_inference = functools.partial(FastLlamaModel.for_inference, model)
         return model
     pass
 
 
     @staticmethod
     def for_inference(model):
-        # if model.config.model_type == ""qwen2"":
-        #     FastLlamaModel.for_training(model)
-        #     return
-        # pass
+        if not hasattr(model, ""parameters""):
+            raise TypeError(""Unsloth: I think you're passing a tokenizer, not the model to for_inference!"")
 
+        def _for_inference(m):
+            if hasattr(m, ""gradient_checkpointing""): m.gradient_checkpointing = False
+            if hasattr(m, ""training""): m.training = False
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""): m._saved_temp_tokenizer.padding_side = ""left""
+            # Set a flag for generation!
+            m._flag_for_generation = True
+        pass
         m = model
         while hasattr(m, ""model""):
-            if hasattr(m, ""gradient_checkpointing""):
-                m.gradient_checkpointing = False
-            if hasattr(m, ""training""):
-                m.training = False
-            # Pad tokenizer to the left
-            if hasattr(m, ""_saved_temp_tokenizer""):
-                m._saved_temp_tokenizer.padding_side = ""left""
+            _for_inference(m)
             m = m.model
-        pass
-        if hasattr(m, ""gradient_checkpointing""):
-            m.gradient_checkpointing = False
-        if hasattr(m, ""training""):
-            m.training = False
-        # Pad tokenizer to the left
-        if hasattr(m, ""_saved_temp_tokenizer""):
-            m._saved_temp_tokenizer.padding_side = ""left""
-
-        # Also check if lm_head / embeddings are trained
-        internal_model = model
-        while not hasattr(internal_model, ""lm_head""):
-            internal_model = internal_model.model
-        pass
-        lm_head = internal_model.lm_head.weight
-        device_type = lm_head.device.type
-        dtype = _get_dtype(model.config.torch_dtype)
-
-        # Wrap model.generate
-        if model.generate.__name__ != ""_fast_generate"":
-            model._unwrapped_old_generate = model.generate
-            model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)
-        pass
+        _for_inference(m)
 
         # Also disable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -2667,13 +2640,14 @@ class FastLlamaModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = False
         pass
-
         return model
     pass
 
 
     @staticmethod
     def for_training(model, use_gradient_checkpointing = True):
+        if not hasattr(model, ""parameters""):
+            raise TypeError(""Unsloth: I think you're passing a tokenizer, not the model to for_training!"")
 
         # Delete all fast inference loras
         for param in model.parameters():
@@ -2681,30 +2655,19 @@ class FastLlamaModel:
                 del param._fast_lora
         pass
 
+        def _for_training(m):
+            if hasattr(m, ""gradient_checkpointing""): m.gradient_checkpointing = use_gradient_checkpointing
+            if hasattr(m, ""training""): m.training = True
+            # Pad tokenizer to the left
+            if hasattr(m, ""_saved_temp_tokenizer""): m._saved_temp_tokenizer.padding_side = ""right""
+            # Set a flag for generation!
+            if hasattr(m, ""_flag_for_generation""): del m._flag_for_generation
+        pass
         m = model
         while hasattr(m, ""model""):
-            if hasattr(m, ""gradient_checkpointing""):
-                m.gradient_checkpointing = use_gradient_checkpointing
-            if hasattr(m, ""training""):
-                m.training = True
-            # Pad tokenizer to the right
-            if hasattr(m, ""_saved_temp_tokenizer""):
-                m._saved_temp_tokenizer.padding_side = ""right""
+            _for_training(m)
             m = m.model
-        pass
-        if hasattr(m, ""gradient_checkpointing""):
-            m.gradient_checkpointing = use_gradient_checkpointing
-        if hasattr(m, ""training""):
-            m.training = True
-        # Pad tokenizer to the right
-        if hasattr(m, ""_saved_temp_tokenizer""):
-            m._saved_temp_tokenizer.padding_side = ""right""
-
-        # Also revert model.generate
-        if hasattr(model, ""_unwrapped_old_generate""):
-            model.generate = model._unwrapped_old_generate
-            del model._unwrapped_old_generate
-        pass
+        _for_training(m)
 
         # Also re-enable training for embeddings for NEFTune
         if hasattr(model, ""get_input_embeddings""):
@@ -2715,7 +2678,6 @@ class FastLlamaModel:
             embeddings = model.get_output_embeddings()
             if hasattr(embeddings, ""training""): embeddings.training = True
         pass
-
         return model
     pass
 pass
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 3a9d651..c9ea922 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -60,7 +60,7 @@ def PatchRL(FastLanguageModel):
     def unsloth_unwrap_model_for_generation(model, *args, **kwargs):
         with unwrap_model_for_generation(model, *args, **kwargs) as unwrapped_model:
             # Put the model in inference mode.
-            FastLanguageModel.for_inference(unwrapped_model)
+            FastLanguageModel.for_inference(model)
 
             # We must use .clone for Unsloth since we force inference_mode
             # Rather we should have used no_grad
diff --git a/unsloth/models/rl_replacements.py b/unsloth/models/rl_replacements.py
index 5ea61cb..7462d55 100644
--- a/unsloth/models/rl_replacements.py
+++ b/unsloth/models/rl_replacements.py
@@ -78,6 +78,25 @@ def sft_trainer_prepare_dataset(function_name, function):
     if  function_name != ""_prepare_non_packed_dataloader"" and \
         function_name != ""_prepare_dataset"": return function
 
+    fast_sft_prepare_dataset = RL_REPLACEMENTS.get(""sft_prepare_dataset"", None)
+    if fast_sft_prepare_dataset is not None and ""pack_examples"" in function:
+        params = inspect.signature(fast_sft_prepare_dataset).parameters.keys()
+        params = "".*?"".join(params)
+        matched = re.match(
+            r""[\s]{0,}def _prepare_dataset\(.*?"" + params + r"".*?\)"",
+            function,
+            flags = re.MULTILINE | re.DOTALL,
+        )
+        if matched:
+            # Use fast version!
+            function = inspect.getsource(fast_sft_prepare_dataset)
+            function = function.split(""\n"")
+            function = ""\n"".join("" ""*4 + x for x in function)
+            function = function.replace(""def sft_prepare_dataset"", ""def _prepare_dataset"")
+            return function
+        pass
+    pass
+
     check_text = \
     ""if 'tokenizer'          not in locals(): tokenizer = processing_class\n""\
     ""if 'formatting_func'    not in locals(): raise RuntimeError('Unsloth: Please file a bug report - `formatting_func` does not exist!')\n""\
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index d81413a..e19bea0 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -1033,7 +1033,7 @@ def to_sharegpt(
     merged_prompt = """",
     merged_column_name = ""instruction"",
     output_column_name = ""output"",
-    remove_unsued_columns = True,
+    remove_unused_columns = True,
     conversation_extension = 1,
     random_state = 3407,
 ):
@@ -1047,7 +1047,7 @@ def to_sharegpt(
     merged_prompt = """",                 Prompt to merge columns into 1 input
     merged_column_name = ""instruction"", Final column name for the input  field
     output_column_name = ""output"",      Final column name for the output field
-    remove_unsued_columns = True,
+    remove_unused_columns = True,
     conversation_extension = 1,         Automatically combines `conversation_extension` convos into 1
     random_state = 3407,
     """"""
@@ -1068,8 +1068,8 @@ def to_sharegpt(
         assistants = examples[output_column_name]
         texts = [
             [
-                {""from"" : ""user"",      ""content"" : str(user)     },
-                {""from"" : ""assistant"", ""content"" : str(assistant)},
+                {""from"" : ""human"", ""value"" : str(user)     },
+                {""from"" : ""gpt"",   ""value"" : str(assistant)},
             ] \
             for user, assistant in zip(users, assistants)
         ]
@@ -1080,8 +1080,8 @@ def to_sharegpt(
         __convert_to_sharegpt__,
         batched = True,
         desc = ""Converting to ShareGPT"",
-        # Remove unsued columns!
-        remove_columns = dataset.column_names if remove_unsued_columns else None,
+        # Remove unused columns!
+        remove_columns = dataset.column_names if remove_unused_columns else None,
     )
 
     # Randomnly concat conversations to create a long stream!
@@ -1115,8 +1115,8 @@ def to_sharegpt(
         __combine_conversations__,
         batched = True,
         desc = ""Extending conversations"",
-        # Remove unsued columns!
-        remove_columns = dataset.column_names if remove_unsued_columns else None,
+        # Remove unused columns!
+        remove_columns = dataset.column_names if remove_unused_columns else None,
     )
     return dataset
 pass
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index 26f632e..cd1d90f 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -37,14 +37,10 @@ from .flex_attention import (
     HAS_FLEX_ATTENTION,
     slow_attention_softcapping,
     slow_inference_attention_softcapping,
+    create_flex_attention_causal_mask,
+    create_flex_attention_sliding_window_mask,
 )
 
-if HAS_FLEX_ATTENTION:
-    from .flex_attention import (
-        FLEX_ATTENTION_PADDING,
-    )
-pass
-
 try:
     print("" Unsloth: Will patch your computer to enable 2x faster free finetuning."")
 except:
diff --git a/unsloth/kernels/flex_attention.py b/unsloth/kernels/flex_attention.py
index 9cf999e..2fba359 100644
--- a/unsloth/kernels/flex_attention.py
+++ b/unsloth/kernels/flex_attention.py
@@ -25,59 +25,120 @@ torch_compile_options = {
 }
 
 # Flex Attention supported from torch 2.5 onwards only
-import torch.nn
-if hasattr(torch.nn, ""attention""):
-    import torch.nn.attention
-    if hasattr(torch.nn.attention, ""flex_attention""):
-        import torch.nn.attention.flex_attention
-        from torch.nn.attention.flex_attention import flex_attention
-        from torch.nn.attention.flex_attention import create_block_mask
-        FLEX_ATTENTION_PADDING = getattr(
-            torch.nn.attention.flex_attention,
-            ""_DEFAULT_SPARSE_BLOCK_SIZE"",
-            1,
-        )
-        flex_attention = torch.compile(flex_attention, dynamic = False)
-        HAS_FLEX_ATTENTION = True
-    else:
-        HAS_FLEX_ATTENTION = False
-    pass
-else:
+try:
+    from torch.nn.attention.flex_attention import (
+        flex_attention as _flex_attention,
+        create_block_mask as _create_block_mask,
+    )
+    _flex_attention = torch.compile(_flex_attention, dynamic = True, options = torch_compile_options)
+    HAS_FLEX_ATTENTION = True
+except:
     HAS_FLEX_ATTENTION = False
 pass
 
-# Logit softcapping
-@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
-def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
-    n_heads    = self.num_heads
-    head_dim   = self.head_dim
-    n_kv_heads = self.num_key_value_heads
-    n_groups   = self.num_key_value_groups
-    
-    # Grouped query attention
-    K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-    V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
-    K = K.reshape(bsz, n_heads, q_len, head_dim)
-    V = V.reshape(bsz, n_heads, q_len, head_dim)
 
-    # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
-    # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
-    # We default to using the config file itself
-    # s = self.config.hidden_size // self.config.num_attention_heads
-    s = self.config.query_pre_attn_scalar
-    t = self.config.attn_logit_softcapping
+if not HAS_FLEX_ATTENTION:
 
-    Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
-    A = torch.matmul(Q, K.transpose(2, 3))
-    A = t * torch.tanh(A / t) # Logit softcapping
-    A += causal_mask[:q_len, :q_len]
-    # Much slower in torch compile!
-    # A.masked_fill_(causal_mask[:q_len, :q_len], -float(""inf""))
-    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
-    A = torch.matmul(A, V)
-    A = A.transpose(1, 2).contiguous()
-    A = A.reshape(bsz, q_len, n_heads*head_dim)
-    return A
+    # Logit softcapping
+    @torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)
+    def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
+        n_heads    = self.num_heads
+        head_dim   = self.head_dim
+        n_kv_heads = self.num_key_value_heads
+        n_groups   = self.num_key_value_groups
+        
+        # Grouped query attention
+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)
+        K = K.reshape(bsz, n_heads, q_len, head_dim)
+        V = V.reshape(bsz, n_heads, q_len, head_dim)
+
+        # See https://github.com/google/gemma_pytorch/commit/03e657582d17cb5a8617ebf333c1c16f3694670e
+        # Gemma 9b should use 256 and not 224 (hs / nah). 27b uses the below
+        # We default to using the config file itself
+        # s = self.config.hidden_size // self.config.num_attention_heads
+        s = self.config.query_pre_attn_scalar
+        t = self.config.attn_logit_softcapping
+
+        Q = Q * torch.tensor(s**-0.5, dtype = Q.dtype) # Follow Keras exactly
+        A = torch.matmul(Q, K.transpose(2, 3))
+        A = t * torch.tanh(A / t) # Logit softcapping
+        A += causal_mask[:q_len, :q_len]
+        # Much slower in torch compile!
+        # A.masked_fill_(causal_mask[:q_len, :q_len], -float(""inf""))
+        A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(Q.dtype)
+        A = torch.matmul(A, V)
+        A = A.transpose(1, 2).contiguous()
+        A = A.reshape(bsz, q_len, n_heads*head_dim)
+        return A
+    pass
+
+    create_flex_attention_causal_mask = None
+    create_flex_attention_sliding_window_mask = None
+else:
+    # See https://github.com/pytorch-labs/attention-gym/blob/main/examples/flex_attn.ipynb
+    # for more examples
+    # BSD 3-Clause License Copyright (c) 2023, Driss Guessous, Horace He et al
+    import functools, math
+
+    def generate_tanh_softcap(t):
+        def tanh_softcap(x, b, h, q_idx, kv_idx):
+            return t * torch.tanh(x / t)
+        return tanh_softcap
+    pass
+    def causal_masker(b, h, q_idx, kv_idx):
+        return q_idx >= kv_idx
+    pass
+
+    @functools.lru_cache
+    def sliding_window_masker(size = 4096):
+        def sliding_window(b, h, q_idx, kv_idx):
+            causal_mask = q_idx >= kv_idx
+            window_mask = q_idx - kv_idx <= size 
+            return causal_mask & window_mask
+        return sliding_window
+    pass
+
+    @functools.lru_cache
+    def create_block_mask(mask, n = 128):
+        return _create_block_mask(
+            mask, 1, 1, n, n,
+            BLOCK_SIZE = 128,
+            _compile = True,
+        )
+    pass
+
+    def create_flex_attention_causal_mask(max_seq_length = 8192):
+        causal_mask = create_block_mask(causal_masker, max_seq_length)
+        return causal_mask
+    pass
+
+    def create_flex_attention_sliding_window_mask(max_seq_length = 8192, sliding_window = 4096):
+        sliding_masker = sliding_window_masker(sliding_window)
+        causal_mask = create_block_mask(sliding_masker, max_seq_length)
+        return causal_mask
+    pass
+
+    @functools.lru_cache
+    def flex_attention(s, t):
+        scale = 1.0 / math.sqrt(s)
+        score_mod = generate_tanh_softcap(t)
+        return functools.partial(
+            _flex_attention, score_mod = score_mod, scale = scale, enable_gqa = True,
+        )
+    pass
+    
+    def slow_attention_softcapping(Q, K, V, causal_mask, self, bsz, q_len):
+        n_heads    = self.num_heads
+        head_dim   = self.head_dim
+        s = self.config.query_pre_attn_scalar
+        t = self.config.attn_logit_softcapping
+        fx = flex_attention(s, t)
+        A = fx(query = Q, key = K, value = V, block_mask = causal_mask)
+        A = A.transpose(1, 2).contiguous()
+        A = A.reshape(bsz, q_len, n_heads*head_dim)
+        return A
+    pass
 pass
 
 
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 6dd17e7..b5a57a7 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -330,7 +330,7 @@ torch_compile_arguments = [
     ""config.coordinate_descent_tuning = True"",
     ""config.max_autotune_gemm = False"", # GEMM is unnecessary
     ""config.autotune_multi_device = False"",
-    ""config.max_autotune_gemm_backends = 'ATEN'"", # Not much faster
+    ""config.max_autotune_gemm_backends = 'TRITON,ATEN,CPP'"", # Not much faster
     ""config.aggressive_fusion = False"", # Careful changes results!
     ""config.cuda.enable_cuda_lto = True"",
     ""config.cuda.use_fast_math = True"",
@@ -338,9 +338,10 @@ torch_compile_arguments = [
 ]
 # Torch dynamo arguments
 torch_dynamo_arguments = [
-    ""config.accumulated_cache_size_limit = 512"", # Bump up a bit from 256
+    ""config.accumulated_cache_size_limit = 1024"", # Bump up a bit from 256
     ""config.suppress_errors = True"", # Supress errors for now
     ""config.do_not_emit_runtime_asserts = True"",
+    ""config.cache_size_limit = 1024"", # Flex Attention
 ]
 import torch._inductor.config as config
 for _try_compile_argument in torch_compile_arguments:
diff --git a/unsloth/models/gemma2.py b/unsloth/models/gemma2.py
index 218849e..bf40ea8 100644
--- a/unsloth/models/gemma2.py
+++ b/unsloth/models/gemma2.py
@@ -156,7 +156,6 @@ def Gemma2Attention_fast_forward(
         )
         A = A.reshape(bsz, q_len, n_heads*head_dim)
     else:
-        mask = causal_mask if attention_mask is None else attention_mask
         fx = slow_inference_attention_softcapping \
             if ""_flag_for_generation"" in kwargs else \
             slow_attention_softcapping
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3999812..22dcb25 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -711,12 +711,6 @@ def LlamaModel_fast_forward(
             offloaded_gradient_checkpointing = True
     pass
 
-    # Check for Flex Attention
-    # if IS_GEMMA2 and HAS_FLEX_ATTENTION:
-    #     if not (seq_length % FLEX_ATTENTION_PADDING == 0):
-    #     USE_FLEX_ATTENTION = True
-
-
     # Gemma2 has alternating SWA and global attn
     if IS_GEMMA2:
         if HAS_FLASH_ATTENTION_SOFTCAPPING and attention_mask is None:
@@ -738,23 +732,29 @@ def LlamaModel_fast_forward(
                 sliding_window = None,
             )
         elif not hasattr(self, ""SWA_mask""):
-            n = self.max_seq_length # self.config.max_position_embeddings
-            # masked_fill is making stuff slower!
-            # self. GA_mask = create_boolean_mask(n = n, sliding_window = 0)
-            # self.SWA_mask = create_boolean_mask(n = n, sliding_window = self.config.sliding_window)
-            from transformers.modeling_attn_mask_utils import AttentionMaskConverter
-            self.SWA_mask = AttentionMaskConverter(
-                is_causal = True,
-                sliding_window = self.config.sliding_window,
-            )\
-                .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
-                .squeeze(0).squeeze(0)
-
-            self.GA_mask = AttentionMaskConverter(
-                is_causal = True,
-            )\
-                .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
-                .squeeze(0).squeeze(0)
+            if HAS_FLEX_ATTENTION:
+                # Use Flex Attention instead!
+                self.SWA_mask = create_flex_attention_sliding_window_mask(self.max_seq_length, self.config.sliding_window)
+                self.GA_mask  = create_flex_attention_causal_mask(self.max_seq_length)
+            else:
+                n = self.max_seq_length # self.config.max_position_embeddings
+                # masked_fill is making stuff slower!
+                # self. GA_mask = create_boolean_mask(n = n, sliding_window = 0)
+                # self.SWA_mask = create_boolean_mask(n = n, sliding_window = self.config.sliding_window)
+                from transformers.modeling_attn_mask_utils import AttentionMaskConverter
+                self.SWA_mask = AttentionMaskConverter(
+                    is_causal = True,
+                    sliding_window = self.config.sliding_window,
+                )\
+                    .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
+                    .squeeze(0).squeeze(0)
+
+                self.GA_mask = AttentionMaskConverter(
+                    is_causal = True,
+                )\
+                    .to_causal_4d(1, n, n, dtype = inputs_embeds.dtype, device = ""cuda:0"",)\
+                    .squeeze(0).squeeze(0)
+            pass
         pass
     pass
 
@@ -821,7 +821,7 @@ def LlamaModel_fast_forward(
             (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\
             (self.norm, hidden_states)
     elif IS_COHERE:
-        hidden_states = fast_layernorm_compiled(self.norm, hidden_states)
+        hidden_states = self.norm(hidden_states)
     else:
         hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)
     pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 044629e..b8f710b 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -1144,7 +1144,7 @@ def patch_sft_trainer_tokenizer():
 
     # Patch train with fix_untrained_tokens
     for path_to_trainer in \
-        (""sft_trainer.SFTTrainer"", ""dpo_trainer.DPOTrainer"",):
+        (""sft_trainer.SFTTrainer"", ""dpo_trainer.DPOTrainer"", ""kto_trainer.KTOTrainer""):
 
         function_name, replacer = ""train"", ""if resume_from_checkpoint is False:""
         function = getsource(eval(f""trl.trainer.{path_to_trainer}.{function_name}""))
"
"diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 1cad00d..4b9fd5e 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -915,7 +915,7 @@ def patch_sft_trainer_tokenizer():
         check_text = \
         ""\n""\
         ""if 'tokenizer' not in locals(): tokenizer = processing_class\n""\
-        ""test_text = dataset[0][dataset_text_field] if (formatting_func is not None and dataset_text_field is None) else formatting_func(dataset[0])[0]\n""\
+        ""test_text = dataset[0][dataset_text_field] if (formatting_func is None and dataset_text_field is not None) else formatting_func(dataset[0])[0]\n""\
         ""chat_template = getattr(tokenizer, 'chat_template', None)\n""\
         ""chat_template = '' if chat_template is None else chat_template\n""\
         ""has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) ""\
"
"diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index cac5acd..3cd8508 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -469,10 +469,14 @@ class FastModel(FastBaseModel):
         return_logits              = False, # Return logits
         fullgraph                  = True, # No graph breaks
         use_exact_model_name       = False,
+        auto_model                 = None,
+        whisper_language           = None,
+        whisper_task               = None,
         *args, **kwargs,
     ):
         if token is None: token = get_token()
-
+        if whisper_language is not None: assert(type(whisper_language) is str)
+        if whisper_task is not None: assert(type(whisper_task) is str)
         SUPPORTS_BFLOAT16 = is_bfloat16_supported()
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
@@ -709,7 +713,8 @@ class FastModel(FastBaseModel):
         # Check if VLM
         is_vlm = any(x.endswith(""ForConditionalGeneration"") for x in model_config.architectures)
         is_vlm = is_vlm or hasattr(model_config, ""vision_config"")
-        auto_model = AutoModelForVision2Seq if is_vlm else AutoModelForCausalLM
+        if auto_model is None:
+            auto_model = AutoModelForVision2Seq if is_vlm else AutoModelForCausalLM
 
         model, tokenizer = FastBaseModel.from_pretrained(
             model_name        = model_name,
@@ -727,6 +732,8 @@ class FastModel(FastBaseModel):
             auto_model        = auto_model,
             use_gradient_checkpointing = use_gradient_checkpointing,
             supports_sdpa     = supports_sdpa,
+            whisper_language  = whisper_language,
+            whisper_task      = whisper_task,            
             *args, **kwargs,
         )
 
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index f05cc95..d212c22 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -236,6 +236,8 @@ class FastBaseModel:
         auto_model        = AutoModelForVision2Seq,
         use_gradient_checkpointing = ""unsloth"",
         supports_sdpa     = True,
+        whisper_language  = None,
+        whisper_task      = None,
         **kwargs,
     ):
         if model_types is None:
@@ -304,7 +306,8 @@ class FastBaseModel:
             do_forced_float32 = True
         pass
         # Stop SDPA for some archs like Pixtral / Mistral3
-        kwargs[""attn_implementation""] = ""sdpa""
+        if not (""attn_implementation"" in kwargs):
+            kwargs[""attn_implementation""] = ""sdpa""
         if not supports_sdpa:
             print(f""Unsloth: {model_type_arch.title()} does not support SDPA - switching to eager!"")
             del kwargs[""attn_implementation""]
@@ -352,6 +355,7 @@ class FastBaseModel:
         # Check if using forced float32 - we load it in bfloat16, then cast to float16!
         torch_dtype = dtype
         if do_forced_float32: torch_dtype = torch.bfloat16
+
         model = auto_model.from_pretrained(
             model_name,
             device_map              = device_map,
@@ -367,12 +371,23 @@ class FastBaseModel:
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
-        auto_processor = AutoProcessor if auto_model is AutoModelForVision2Seq else AutoTokenizer
-        tokenizer = auto_processor.from_pretrained(
-            tokenizer_name,
-            padding_side = ""right"",
-            token        = token,
-        )
+        is_vlm = (auto_model is AutoModelForVision2Seq)
+        is_whisper = (whisper_language is not None and whisper_task is not None)
+        auto_processor = AutoProcessor if (is_vlm or is_whisper) else AutoTokenizer
+        if whisper_language and whisper_task:
+           tokenizer = auto_processor.from_pretrained(
+                tokenizer_name,
+                padding_side = ""right"",
+                token        = token,
+                language     = whisper_language,
+                task         = whisper_task,
+            )
+        else:
+            tokenizer = auto_processor.from_pretrained(
+                tokenizer_name,
+                padding_side = ""right"",
+                token        = token,
+            )
         if hasattr(tokenizer, ""tokenizer""):
             __tokenizer = tokenizer.tokenizer
             # Add padding side as well
@@ -469,6 +484,7 @@ class FastBaseModel:
         modules_to_save            = None,
         init_lora_weights          = True,
         loftq_config               = {},
+        task_type                  = TaskType.CAUSAL_LM,
         temporary_location         = ""_unsloth_temporary_saved_buffers"",
         **kwargs,
     ):
@@ -492,7 +508,7 @@ class FastBaseModel:
             finetune_attention_modules = True
             finetune_mlp_modules       = True
         pass
-        if target_modules is None:
+        if target_modules is None or target_modules == ""all-linear"":
             target_modules = get_peft_regex(
                 model,
                 finetune_vision_layers     = finetune_vision_layers,
@@ -503,7 +519,7 @@ class FastBaseModel:
         else:
             assert(type(target_modules) in (list, tuple,))
         pass
-
+        
         # Clear deleted GPU items
         for _ in range(3):
             gc.collect()
@@ -516,7 +532,7 @@ class FastBaseModel:
             target_modules  = target_modules,
             lora_dropout    = lora_dropout,
             bias            = bias,
-            task_type       = TaskType.CAUSAL_LM,
+            task_type       = task_type,
         )
         model = prepare_model_for_kbit_training(
             model,
"
"diff --git a/pyproject.toml b/pyproject.toml
index 9c862a2..e6f663a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -44,6 +44,7 @@ huggingface = [
     ""accelerate>=0.26.1"",
     ""trl>=0.7.9"",
     ""peft>=0.7.1"",
+    ""protobuf<4.0.0"",
 ]
 cu118only = [
     ""xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version=='3.9'"",
@@ -170,6 +171,7 @@ colab-new = [
     ""psutil"",
     ""wheel>=0.42.0"",
     ""numpy"",
+    ""protobuf<4.0.0"",
 ]
 colab-no-deps = [
     ""accelerate>=0.26.1"",
@@ -177,6 +179,7 @@ colab-no-deps = [
     ""peft>=0.7.1"",
     ""xformers"",
     ""bitsandbytes"",
+    ""protobuf<4.0.0"",
 ]
 colab = [
     ""unsloth[cu121]"",
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index a7ade9f..f39d34f 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -1068,17 +1068,39 @@ class FastLlamaModel:
         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12
         # RoPE Scaling's max_position_embeddings must be updated
         max_position_embeddings = max(max_seq_length, model_max_seq_length)
-        model = AutoModelForCausalLM.from_pretrained(
-            model_name,
-            device_map              = device_map,
-            torch_dtype             = dtype,
-            quantization_config     = bnb_config,
-            token                   = token,
-            rope_scaling            = rope_scaling,
-            max_position_embeddings = max_position_embeddings,
-            trust_remote_code       = trust_remote_code,
-            **kwargs,
-        )
+        try:
+            model = AutoModelForCausalLM.from_pretrained(
+                model_name,
+                device_map              = device_map,
+                torch_dtype             = dtype,
+                quantization_config     = bnb_config,
+                token                   = token,
+                rope_scaling            = rope_scaling,
+                max_position_embeddings = max_position_embeddings,
+                trust_remote_code       = trust_remote_code,
+                **kwargs,
+            )
+        except Exception as error:
+            if ""rope_scaling"" in str(error):
+                if rope_scaling is not None:
+                    raise TypeError(""Unsloth: {model_name} does not support rope_scaling."")
+                pass
+
+                # Counteract missing rope_scaling
+                model = AutoModelForCausalLM.from_pretrained(
+                    model_name,
+                    device_map              = device_map,
+                    torch_dtype             = dtype,
+                    quantization_config     = bnb_config,
+                    token                   = token,
+                    max_position_embeddings = max_position_embeddings,
+                    trust_remote_code       = trust_remote_code,
+                    **kwargs,
+                )
+            else:
+                raise error
+            pass
+        pass
 
         # Counteract saved tokenizers
         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
diff --git a/unsloth/save.py b/unsloth/save.py
index 49d88bf..636a3d8 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -684,7 +684,7 @@ pass
 
 
 def install_llama_cpp_make_non_blocking():
-    env = { **os.environ, ""LLAMA_CUBLAS"": ""1"", }
+    env = { **os.environ, ""LLAMA_CUDA"": ""1"", }
     n_jobs = max(int(psutil.cpu_count()*1.5), 1)
     # Force make clean
     os.system(""make clean -C llama.cpp"")
@@ -752,7 +752,7 @@ pass
 
 
 def install_llama_cpp_blocking(use_cuda = True):
-    use_cuda = ""LLAMA_CUBLAS=1"" if use_cuda else """"
+    use_cuda = ""LLAMA_CUDA=1"" if use_cuda else """"
 
     commands = [
         ""git clone https://github.com/ggerganov/llama.cpp"",
@@ -937,7 +937,7 @@ def save_to_gguf(
             ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
             ""You must run this in the same folder as you're saving your model.\n""\
             ""git clone https://github.com/ggerganov/llama.cpp\n""\
-            ""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\n""\
+            ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
             ""Once that's done, redo the quantization.""
         )
     pass
@@ -966,7 +966,7 @@ def save_to_gguf(
                 ""You do not need to close this Python program. Run the following commands in a new terminal:\n""\
                 ""You must run this in the same folder as you're saving your model.\n""\
                 ""git clone https://github.com/ggerganov/llama.cpp\n""\
-                ""cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\n""\
+                ""cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\n""\
                 ""Once that's done, redo the quantization.""
             )
         pass
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 46de1c9..2640f32 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -31,6 +31,12 @@ IGNORED_TOKENIZER_CHECKING = frozenset((
     ""CodeLlamaTokenizer"",
 ))
 
+# Check environments
+keynames = ""\n"" + ""\n"".join(os.environ.keys())
+IS_COLAB_ENVIRONMENT  = ""\nCOLAB_""  in keynames
+IS_KAGGLE_ENVIRONMENT = ""\nKAGGLE_"" in keynames
+del keynames
+
 
 def try_fix_tokenizer(tokenizer, prepend = True):
 
@@ -179,10 +185,19 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):
         if x.endswith(""_token"") and x.count(""_"") == 1
     )))
     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))
-    string = ""\n"".join(all_special_tokens) + \
-        ""A quick brown fox jumps over the lazy dog!!\n\n"" + \
-        """".join(all_special_tokens)
-    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids
+    try:
+        string = ""\n"".join(all_special_tokens) + \
+            ""A quick brown fox jumps over the lazy dog!!\n\n"" + \
+            """".join(all_special_tokens)
+        return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids
+    except:
+        # For eg see https://github.com/unslothai/unsloth/issues/292
+        # Sometimes tokenizer has weird tokens, causing a combined tokenization to fail.
+        # [TODO] We temporarily disable this for CodeLlama tokenizers
+        if slow_tokenizer.__repr__().split(""("", 1)[0] in IGNORED_TOKENIZER_CHECKING:
+            return True
+        else:
+            return False
 pass
 
 
@@ -203,7 +218,6 @@ def fix_sentencepiece_tokenizer(
     # First save the old tokenizer
     old_tokenizer.save_pretrained(temporary_location)
 
-    from sentencepiece import SentencePieceProcessor
     tokenizer_file = sentencepiece_model_pb2.ModelProto()
     tokenizer_file.ParseFromString(open(f""{temporary_location}/tokenizer.model"", ""rb"").read())
 
@@ -220,7 +234,11 @@ def fix_sentencepiece_tokenizer(
             continue
         pass
         ids = ids[0]
-        tokenizer_piece = tokenizer_file.pieces[ids]
+        # [TODO] Hack for Starling - try except
+        try:
+            tokenizer_piece = tokenizer_file.pieces[ids]
+        except:
+            continue
         assert(tokenizer_piece.piece == old_token)
         tokenizer_piece.piece = new_token
     pass
@@ -243,7 +261,14 @@ def load_correct_tokenizer(
     padding_side = ""right"",
     token = None,
     trust_remote_code = False,
+    cache_dir = ""huggingface_tokenizers_cache"",
 ):
+    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:
+        cache_dir = cache_dir
+    else:
+        cache_dir = None
+    pass
+
     slow_tokenizer = AutoTokenizer.from_pretrained(
         tokenizer_name,
         model_max_length  = model_max_length,
@@ -251,6 +276,7 @@ def load_correct_tokenizer(
         token             = token,
         trust_remote_code = trust_remote_code,
         use_fast          = False,
+        cache_dir         = cache_dir,
     )
     fast_tokenizer = AutoTokenizer.from_pretrained(
         tokenizer_name,
@@ -258,6 +284,7 @@ def load_correct_tokenizer(
         padding_side      = padding_side,
         token             = token,
         trust_remote_code = trust_remote_code,
+        cache_dir         = cache_dir,
     )
     fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token
     fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token
@@ -375,6 +402,12 @@ def check_tokenizer(
                 )
             pass
             
+            if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:
+                cache_dir = ""huggingface_tokenizers_cache""
+            else:
+                cache_dir = None
+            pass
+
             # Try slow tokenizer which can fix things!
             tokenizer = AutoTokenizer.from_pretrained(
                 model_name,
@@ -382,6 +415,7 @@ def check_tokenizer(
                 padding_side = padding_side,
                 token = token,
                 use_fast = False,
+                cache_dir = cache_dir,
             )
             return check_tokenizer(
                 model = model,
"
"diff --git a/README.md b/README.md
index 98f83e0..759057f 100644
--- a/README.md
+++ b/README.md
@@ -10,7 +10,7 @@
 <a href=""https://discord.gg/u54VK8m8tk""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
 <a href=""https://ko-fi.com/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png"" height=""48""></a>
 
-### Finetune Mistral, Llama 2-5x faster with 70% less memory!
+### Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory!
 
 ![](https://i.ibb.co/sJ7RhGG/image-41.png)
 
@@ -22,28 +22,30 @@ All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and
 
 | Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
 |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
+| **Gemma 7b**      | [ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 58% less |
 | **Mistral 7b**    | [ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
 | **Llama-2 7b**      | [ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |
-| **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |
 | **TinyLlama**  | [ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |
 | **CodeLlama 34b** A100   | [ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |
 | **Mistral 7b** 1xT4  | [ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\* | 62% less |
+| **DPO - Zephyr**     | [ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |
 
 - This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
 - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
-- Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.
-- \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.
+- \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.
 
 ##  Unsloth.ai News
--  [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.
--  [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.
--  We did a [blog](https://huggingface.co/blog/unsloth-trl) with Hugging Face, and we're in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).
--  Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!
--  **Download models 4x faster** from Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!
+-  [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) on 6T tokens now works. And [Gemma 2b notebook](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)
+-  Added [conversational notebooks](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) and [raw text notebooks](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing)
+-  [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models
+-  [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO
+-  We did a [blog](https://huggingface.co/blog/unsloth-trl) with Hugging Face and are in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)
+-  [Download models 4x faster](https://huggingface.co/collections/unsloth/)  from Hugging Face. Eg: `unsloth/mistral-7b-bnb-4bit`
 
 ##  Links and Resources
 | Type                            | Links                               |
 | ------------------------------- | --------------------------------------- |
+|  **Wiki & FAQ**              | [Read Our Wiki](https://github.com/unslothai/unsloth/wiki) |
 |  **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |
 |  **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|
 | <img height=""14"" src=""https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg"" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
@@ -113,8 +115,8 @@ pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \
 ```bash
 pip install ""unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git""
 pip install ""unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118-ampere] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-ampere] @ git+https://github.com/unslothai/unsloth.git""
 ```
 3. For Pytorch 2.1.1: Use the `""ampere""` path for newer RTX 30xx GPUs or higher.
 ```bash
@@ -122,10 +124,10 @@ pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton \
   --index-url https://download.pytorch.org/whl/cu121
 ```
 ```bash
-pip install ""unsloth[cu118_torch211] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118-torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118-ampere-torch211] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-ampere-torch211] @ git+https://github.com/unslothai/unsloth.git""
 ```
 4. For Pytorch 2.2.0: Use the `""ampere""` path for newer RTX 30xx GPUs or higher.
 ```bash
@@ -133,10 +135,10 @@ pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \
   --index-url https://download.pytorch.org/whl/cu121
 ```
 ```bash
-pip install ""unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git""
-pip install ""unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118-torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu118-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git""
+pip install ""unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git""
 ```
 5. If you get errors, try the below first, then go back to step 1:
 ```bash
diff --git a/pyproject.toml b/pyproject.toml
index 0497112..7e8956c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -33,7 +33,7 @@ exclude = [""images*""]
 
 [project.optional-dependencies]
 huggingface = [
-    ""transformers>=4.37.0"",
+    ""transformers>=4.38.0"",
     ""datasets"",
     ""sentencepiece"",
     ""accelerate>=0.26.1"",
diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index eb61056..6f0237c 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -217,6 +217,35 @@ alpaca_eos_token = ""eos_token""
 CHAT_TEMPLATES[""alpaca""] = (alpaca_template, alpaca_eos_token,)
 
 
+# https://huggingface.co/google/gemma-7b-it
+# Notice we must use |trim for lstrip and rstrip. <start_of_turn> maps to 106.
+# <end_of_turn> maps to 107. user and model are normal 1 word tokens.
+gemma_template = \
+    ""{% for message in messages %}""\
+        ""{% if message['role'] == 'user' %}""\
+            ""{{'<start_of_turn>user\n' + message['content'] | trim + '<end_of_turn>\n'}}""\
+        ""{% elif message['role'] == 'assistant' %}""\
+            ""{{'<start_of_turn>model\n' + message['content'] | trim + '<end_of_turn>\n' }}""\
+        ""{% else %}""\
+            ""{{ '<start_of_turn>system\n' + message['content'] | trim + '<end_of_turn>\n' }}""\
+        ""{% endif %}""\
+    ""{% endfor %}""\
+    ""{% if add_generation_prompt %}""\
+        ""{{ '<start_of_turn>model\n' }}""\
+    ""{% endif %}""
+gemma_eos_token = ""<end_of_turn>""
+CHAT_TEMPLATES[""gemma""] = (gemma_template, gemma_eos_token,)
+
+
+# Gemma with ChatML instead
+gemma_chatml_template = chatml_template
+gemma_chatml_eos_token = (
+    {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""},
+    ""<|im_end|>"",
+)
+CHAT_TEMPLATES[""gemma_chatml""] = (gemma_chatml_template, gemma_chatml_eos_token,)
+
+
 def get_chat_template(
     tokenizer,
     chat_template = ""chatml"",
@@ -229,7 +258,7 @@ def get_chat_template(
 
     old_padding_side = tokenizer.padding_side
 
-    if type(chat_template) in (list, tuple):
+    if type(chat_template) in (list, tuple,):
         chat_template, stop_word = chat_template
         assert(type(chat_template) is str)
         assert(type(stop_word) is str)
@@ -238,7 +267,38 @@ def get_chat_template(
 
         chat_template, stop_word = CHAT_TEMPLATES[chat_template]
 
-        if stop_word != ""eos_token"":
+        if type(stop_word) in (list, tuple,):
+            token_mapping, stop_word = stop_word
+            assert(type(token_mapping) is dict)
+        else:
+            token_mapping = None
+
+        assert(type(stop_word) is str)
+
+        # token_mapping = {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""}
+        # For Gemma :)
+        if token_mapping is not None:
+
+            string_vocab = tokenizer._tokenizer.to_str()
+
+            for old_token, new_token in token_mapping.items():
+                old_count = string_vocab.count(f'""{old_token}""')
+                new_count = string_vocab.count(f'""{new_token}""')
+                if new_count != 0:
+                    print(f""{new_token} is already a token. Skipping."")
+                elif old_count == 0:
+                    raise RuntimeError(f""{old_token} was not part of the tokenizer!"")
+                else:
+                    string_vocab = string_vocab.replace(f'""{old_token}""', f'""{new_token}""')
+                pass
+            pass
+
+            logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
+            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)
+            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
+            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
+
+        elif stop_word != ""eos_token"":
             logger.warning_once(f""Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}."")
 
             # Replaces the old EOS token with a new one.
@@ -252,6 +312,7 @@ def get_chat_template(
             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)
             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)
         pass
+
     else:
         raise TypeError(
             f""Unsloth: `chat_template` must be a tuple of (your_template, eos_token,) or one of\n""\
@@ -318,6 +379,7 @@ def test_chat_templates():
         {""role"": ""user"", ""content"": ""  No it's 100% 5! ""},
     ]
 
+    # Zephyr
     from transformers import AutoTokenizer
     template = zephyr_template
     correct_tokenizer = AutoTokenizer.from_pretrained(""HuggingFaceH4/zephyr-7b-beta"")
@@ -326,6 +388,7 @@ def test_chat_templates():
     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
     assert(correct_prompt == our_prompt)
 
+    # Chatml
     template = chatml_template
     correct_tokenizer = AutoTokenizer.from_pretrained(""teknium/OpenHermes-2.5-Mistral-7B"")
     correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
@@ -333,6 +396,7 @@ def test_chat_templates():
     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
     assert(correct_prompt == our_prompt)
 
+    # Mistral
     template = mistral_template
     correct_tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"")
     correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
@@ -340,6 +404,7 @@ def test_chat_templates():
     our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
     assert(correct_prompt == our_prompt)
 
+    # Llama
     template = llama_template
     correct_tokenizer = AutoTokenizer.from_pretrained(""unsloth/llama-2-7b-chat"")
     correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
@@ -347,6 +412,7 @@ def test_chat_templates():
     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)
     assert(correct_prompt == our_prompt)
 
+    # Vicuna
     try:
         from fastchat.conversation import get_conv_template
     except:
@@ -381,4 +447,11 @@ def test_chat_templates():
     our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
     # We add </s> ourselves
     assert(correct_prompt == our_prompt.replace(""</s>"", """"))
+
+    # Gemma
+    correct_tokenizer = AutoTokenizer.from_pretrained(""unsloth/gemma-7b-it"")
+    correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
+    correct_tokenizer.chat_template = gemma_template
+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)
+    assert(our_prompt == correct_prompt)
 pass
diff --git a/unsloth/kernels/__init__.py b/unsloth/kernels/__init__.py
index f5db8fa..9c231e6 100644
--- a/unsloth/kernels/__init__.py
+++ b/unsloth/kernels/__init__.py
@@ -16,9 +16,11 @@ from .cross_entropy_loss import fast_cross_entropy_loss
 from .rms_layernorm import fast_rms_layernorm
 from .rope_embedding import fast_rope_embedding, inplace_rope_embedding
 from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
+from .geglu import geglu_forward_kernel, geglu_backward_kernel
 from .fast_lora import (
 	get_lora_parameters,
-	apply_lora_mlp,
+	apply_lora_mlp_swiglu,
+	apply_lora_mlp_geglu,
 	apply_lora_qkv,
 	apply_lora_o,
 )
diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 0a73a39..2605779 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -20,12 +20,14 @@ from transformers.models.llama.modeling_llama import logger
 
 
 @triton.jit
-def _cross_entropy_forward(logits_ptr, logits_row_stride,
-                           loss_ptr,
-                           lse_ptr,
-                           labels_ptr,
-                           n_cols,
-                           BLOCK_SIZE: tl.constexpr,):
+def _cross_entropy_forward(
+    logits_ptr, logits_row_stride,
+    loss_ptr,
+    logsumexp_ptr,
+    labels_ptr,
+    VOCAB_SIZE : tl.constexpr,
+    BLOCK_SIZE : tl.constexpr,
+):
     """"""
         Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]
         Pi = exp(xi) / sum(exp(xi))
@@ -34,40 +36,114 @@ def _cross_entropy_forward(logits_ptr, logits_row_stride,
              = y * (log[sum(exp(x))] - x)
         If y == 0: CE_i = 0
         If y == 1: CE_i = logsumexp - x
+
+        logsumexp is also stable
+        Take    y =         log[sum(exp(x))]
+           exp(y) =             sum(exp(x))
+           exp(y) =             sum(exp(x - c)*exp(c)) Since e^(x-c)*e^c = e^x
+           exp(y) =      exp(c)*sum(exp(x - c))
+               y  = log(exp(c)*sum(exp(x - c)))
+               y  = c + log[sum(exp(x - c))]
+        This means we can set c = max(x) to make sure
+        exp(x - c) always is exp(x - max(x)).
+        This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.
     """"""
     row_idx = tl.program_id(0)
-    logits_ptr += row_idx * logits_row_stride
-    loss_ptr   += row_idx
-    lse_ptr    += row_idx
-    labels_ptr += row_idx
+    logits_ptr    += row_idx * logits_row_stride.to(tl.int64)
+    loss_ptr      += row_idx
+    logsumexp_ptr += row_idx
+    labels_ptr    += row_idx
 
     col_offsets = tl.arange(0, BLOCK_SIZE)
-    mask = col_offsets < n_cols
+    mask = col_offsets < VOCAB_SIZE
 
-    # TODO: Fixup int32 locations to int64
     label_idx = tl.load(labels_ptr).to(tl.int32)
     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
-    max_logits = tl.max(logits, 0)
-    # Maximum stops overflow
-    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits
-    tl.store(lse_ptr, lse)
+    c = tl.max(logits, 0)
+    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
 
     if label_idx != -100:
-        logits_label = tl.load(logits_ptr + label_idx).to(tl.float32)
-        loss = lse - logits_label
+        x = tl.load(logits_ptr + label_idx).to(tl.float32)
+        loss = logsumexp - x
     else:
         loss = 0.0
+    tl.store(logsumexp_ptr, logsumexp)
     tl.store(loss_ptr, loss)
 pass
 
 
 @triton.jit
-def _cross_entropy_backward(logits_ptr, logits_row_stride,
-                            dloss_ptr,   dloss_row_stride,
-                            lse_ptr,
-                            labels_ptr,
-                            n_cols,
-                            BLOCK_SIZE: tl.constexpr,):
+def _chunked_cross_entropy_forward(
+    logits_ptr, logits_row_stride,
+    loss_ptr,
+    logsumexp_ptr,
+    labels_ptr,
+    VOCAB_SIZE : tl.constexpr,
+    N_CHUNKS   : tl.constexpr,
+    BLOCK_SIZE : tl.constexpr,
+):
+    """"""
+        256K vocab divided in 4 chunks
+
+        |-65536-| |-65536-| |-65536-| |-65536-|
+        |-------| |-------| |-------| |-------|
+        |-------| |-------| |-------| |-------|
+
+        If y == 0: CE_i = 0
+        If y == 1: CE_i = logsumexp - x
+
+        Notice we can do logsumexp for each chunk and then
+        logsumexp[chunk_sum(logsumexp)] == logsumexp
+
+        chunk_sum = log[chunk_sum(logsumexp)]
+                  = log[exp(logsumexp(a)) + ... + exp(logsumexp(z))]
+                  = log[exp(log[sum(exp(a))]) + ... + exp(log[sum(exp(z))])]
+                  = log[sum(exp(a)) + ... + sum(exp(z))]
+                  = logsumexp(x)
+
+        This means we can perform a logsumexp for each chunk, then do a
+        final logsumexp reduction!
+
+        Ie do: logsumexp(chunked_logsumexp) - x
+    """"""
+    row_idx   = tl.program_id(0)
+    chunk_idx = tl.program_id(1)
+    logits_ptr    += row_idx * logits_row_stride.to(tl.int64)
+    loss_ptr      += row_idx
+    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx
+    labels_ptr    += row_idx
+
+    col_offsets = chunk_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
+    mask = col_offsets < VOCAB_SIZE
+
+    label_idx = tl.load(labels_ptr).to(tl.int32)
+    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
+    c = tl.max(logits, 0)
+    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))
+
+    if chunk_idx == 0:
+        # logsumexp(chunked_logsumexp) - x
+        # Do the -x separately
+        if label_idx != -100:
+            x = tl.load(logits_ptr + label_idx).to(tl.float32)
+            loss = -1.0 * x
+        else:
+            loss = 0.0
+        tl.store(loss_ptr, loss)
+    pass
+    tl.store(logsumexp_ptr, logsumexp)
+pass
+
+
+@triton.jit
+def _cross_entropy_backward(
+    logits_ptr, logits_row_stride,
+    dloss_ptr,   dloss_row_stride,
+    logsumexp_ptr,
+    labels_ptr,
+    VOCAB_SIZE : tl.constexpr,
+    BLOCK_SIZE : tl.constexpr,
+):
     """"""
         CE_i = -y log(P) = y * (log[sum(exp(x))] - x)
         dC/dx = d/dx (y * log[sum(exp(x))] - x * y)
@@ -83,47 +159,80 @@ def _cross_entropy_backward(logits_ptr, logits_row_stride,
         If y == 1 and x == label: dC/dlabel = exp[x - logsumexp] - 1
         If y == 1 and x != label: dC/dx     = exp[x - logsumexp]
     """"""
-    row_idx = tl.program_id(0)
-    logits_ptr += row_idx * logits_row_stride
+    row_idx   = tl.program_id(0)
+    block_idx = tl.program_id(1)
+
+    logits_ptr += row_idx * logits_row_stride.to(tl.int64)
     dloss_ptr  += row_idx *  dloss_row_stride
-    col_offsets = tl.arange(0, BLOCK_SIZE)
-    mask = col_offsets < n_cols
-    # TODO: Fixup int32 locations to int64
+    col_offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
+    mask = col_offsets < VOCAB_SIZE
     label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)
 
     if label_idx != -100:
         dloss = tl.load(dloss_ptr)
     else:
         dloss = 0.0
-    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = 0).to(tl.float32)
-    lse = tl.load(lse_ptr + row_idx)
-    probs = tl.exp(logits - lse)
 
-    probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)
-    tl.store(logits_ptr + col_offsets, dloss * probs, mask = mask)
+    x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(""inf"")).to(tl.float32)
+    logsumexp = tl.load(logsumexp_ptr + row_idx)
+    y = tl.exp(x - logsumexp)
+    y = tl.where(
+        col_offsets == label_idx,
+        y - 1.0, # exp(x - logsumexp) - 1
+        y,       # exp(x - logsumexp)
+    )
+
+    # If y == 0: dC/dx = 0 ==> we already masked it to be = 0, so dloss = 0.
+    tl.store(logits_ptr + col_offsets, dloss * y, mask = mask)
 pass
 
 
+MAX_FUSED_SIZE = 65536 # 2**16
+
 class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
     def forward(ctx, logits, labels):
-        n_rows, n_cols = logits.shape
-        BLOCK_SIZE, num_warps = calculate_settings(n_cols)
-        losses    = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
-        logsumexp = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
-
-        _cross_entropy_forward[(n_rows,)](
-            logits, logits.stride(0),
-            losses,
-            logsumexp,
-            labels,
-            n_cols,
-            BLOCK_SIZE = BLOCK_SIZE,
-            num_warps  = num_warps,
-        )
+        n_rows, vocab_size = logits.shape
+
+        div, mod = divmod(vocab_size, MAX_FUSED_SIZE)
+        n_chunks = div + (mod != 0)
+        losses = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
+
+        if n_chunks == 1:
+            # For small vocabs <= 65336 like Llama, Mistral
+            BLOCK_SIZE, num_warps = calculate_settings(vocab_size)
+            logsumexp = torch.empty(n_rows, dtype = torch.float32, device = ""cuda"")
+
+            _cross_entropy_forward[(n_rows,)](
+                logits, logits.stride(0),
+                losses,
+                logsumexp,
+                labels,
+                VOCAB_SIZE = vocab_size,
+                BLOCK_SIZE = BLOCK_SIZE,
+                num_warps  = num_warps,
+            )
+        else:
+            # For large vocabs > 65336 like Gemma 256K
+            logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = ""cuda"")
+
+            _chunked_cross_entropy_forward[(n_rows, n_chunks,)](
+                logits, logits.stride(0),
+                losses,
+                logsumexp,
+                labels,
+                VOCAB_SIZE = vocab_size,
+                N_CHUNKS   = n_chunks,
+                BLOCK_SIZE = MAX_FUSED_SIZE,
+                num_warps  = 32,
+            )
+            # logsumexp(chunked_logsumexp) - x
+            # Do the -x separately
+            logsumexp = torch.logsumexp(logsumexp, dim = 1) # Row sum
+            losses += logsumexp
+            losses.masked_fill_(labels == -100, 0) # Don't forget to mask padding out!
+        pass
 
-        ctx.BLOCK_SIZE = BLOCK_SIZE
-        ctx.num_warps = num_warps
         ctx.save_for_backward(logits, logsumexp, labels)
         return losses
     pass
@@ -131,23 +240,26 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):
     @staticmethod
     def backward(ctx, dlosses):
         logits, logsumexp, labels = ctx.saved_tensors
-        n_rows, n_cols = logits.shape
+        n_rows, vocab_size = logits.shape
 
-        _cross_entropy_backward[(n_rows,)](
+        BLOCK_SIZE = 4096
+        div, mod = divmod(vocab_size, BLOCK_SIZE)
+        n_blocks = div + (mod != 0)
+
+        _cross_entropy_backward[(n_rows, n_blocks,)](
             logits,   logits.stride(0),
             dlosses, dlosses.stride(0),
             logsumexp,
             labels,
-            n_cols,
-            BLOCK_SIZE = ctx.BLOCK_SIZE,
-            num_warps  = ctx.num_warps,
+            VOCAB_SIZE = vocab_size,
+            BLOCK_SIZE = BLOCK_SIZE,
+            num_warps  = 8,
         )
         return logits, None, None,
     pass
 pass
 
 
-slow_cross_entropy_loss = torch.nn.functional.cross_entropy
 def fast_cross_entropy_loss(logits, labels):
     """"""
     Arguments:
@@ -159,25 +271,10 @@ def fast_cross_entropy_loss(logits, labels):
     batch, seq_len, d = logits.shape
     assert(labels.shape == (batch, seq_len))
 
-    # Prelim support Qwen, Deepseek other large vocab sizes > 2^16
-    if d > MAX_FUSED_SIZE:
-        logger.warning_once(
-            f""Unsloth: Vocab size of {d} exceeds the max CUDA blocksize of {MAX_FUSED_SIZE}.\n""\
-            ""For now, Unsloth will use Pytorch's CrossEntropyLoss, which will entail a\n""\
-            ""25% increase in memory usage and be slower. Make an issue on \n""\
-            ""Unsloth's Github page if you want a faster and more memory efficient kernel!""
-        )
-        loss = slow_cross_entropy_loss(
-            logits.float().view(batch*seq_len, d), # Must cast to float32 for numerical stability
-            labels.view(-1),
-        )
-        return loss
-    else:
-        loss = Fast_CrossEntropyLoss.apply(
-            logits.view(batch*seq_len, d),
-            labels.view(-1),
-        )
-        n_items = torch.count_nonzero(labels != -100)
-        return loss.sum() / n_items
-    pass
+    loss = Fast_CrossEntropyLoss.apply(
+        logits.view(batch*seq_len, d),
+        labels.view(-1),
+    )
+    n_items = torch.count_nonzero(labels != -100)
+    return loss.sum() / n_items
 pass
diff --git a/unsloth/kernels/fast_lora.py b/unsloth/kernels/fast_lora.py
index b3a1098..3ed0d3c 100644
--- a/unsloth/kernels/fast_lora.py
+++ b/unsloth/kernels/fast_lora.py
@@ -14,7 +14,6 @@
 
 import torch
 from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters
-from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
 
 
 def matmul_lora(X, W, W_quant, A, B, s, out = None):
@@ -85,20 +84,20 @@ class LoRA_MLP(torch.autograd.Function):
     def forward(ctx, X : torch.Tensor,
                 gateW, gateW_quant, gateA, gateB, gateS,
                   upW,   upW_quant, upA,   upB,   upS,
-                downW, downW_quant, downA, downB, downS):
+                downW, downW_quant, downA, downB, downS,
+                _forward_function, _backward_function,):
         dtype = X.dtype
 
         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)
         g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)
-        # f = torch.nn.functional.silu(e)
-        # h = f * g
-        h = swiglu_fg_kernel(e, g)
+        h = _forward_function(e, g)
         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)
 
         ctx.custom_saved_tensors = (
             gateW, gateW_quant, gateS,
             upW, upW_quant, upS,
             downW, downW_quant, downS,
+            _backward_function,
         )
         ctx.save_for_backward(gateA, gateB, upA, upB, downA, downB,
                               X, e, g)
@@ -109,8 +108,8 @@ class LoRA_MLP(torch.autograd.Function):
     @staticmethod
     @torch.cuda.amp.custom_bwd
     def backward(ctx, dY : torch.Tensor):
-        gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, = \
-            ctx.custom_saved_tensors
+        gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, \
+            _backward_function = ctx.custom_saved_tensors
         gateA, gateB, upA, upB, downA, downB, \
             X, e, g = ctx.saved_tensors
 
@@ -125,14 +124,7 @@ class LoRA_MLP(torch.autograd.Function):
         dtype = X.dtype
 
         DW = matmul_lora(dY, downW.t(), downW_quant, downB, downA, downS)
-        # e = e.float()
-        # se = 1.0 / (1.0 + torch.exp(-e))
-        # f = (se * e).to(dtype)
-        # h = f * g
-        # df = DW * f
-        # dg = DW * g
-        # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)
-        DW, e, g = swiglu_DWf_DW_dfg_kernel(DW, e, g)
+        DW, e, g = _backward_function(DW, e, g)
         h, df, de = DW, e, g
 
         # Down projection LoRA weights
@@ -155,7 +147,6 @@ class LoRA_MLP(torch.autograd.Function):
 
         # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)
         # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)
-
         upW = fast_dequantize(upW.t(), upW_quant)
         dX = torch.matmul(df, upW.t(), out = X)
         del upW
@@ -172,24 +163,36 @@ class LoRA_MLP(torch.autograd.Function):
         return dX.view(batch, seq_len, hd), \
             None, None, d_gateA.t(), d_gateB.t(), None, \
             None, None,   d_upA.t(),   d_upB.t(), None, \
-            None, None, d_downA.t(), d_downB.t(), None,
+            None, None, d_downA.t(), d_downB.t(), None, \
+            None, None, # _backward and _forward
     pass
 pass
 
 
-def apply_lora_mlp(self, X):
-    # gate = self.gate_proj(X)
-    # up   = self.  up_proj(X)
-    # h = torch.nn.functional.silu(gate) * up
-    # down = self.down_proj(h)
-    # return down
+from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel
+def apply_lora_mlp_swiglu(self, X):
+    gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
+    upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
+    downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
+    out = LoRA_MLP.apply(X,
+                         gateW, gateW_quant, gateA, gateB, gateS,
+                         upW,     upW_quant, upA,   upB,   upS,
+                         downW, downW_quant, downA, downB, downS,
+                         swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel,)
+    return out
+pass
+
+
+from .geglu import geglu_forward_kernel, geglu_backward_kernel
+def apply_lora_mlp_geglu(self, X):
     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)
     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)
     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)
     out = LoRA_MLP.apply(X,
                          gateW, gateW_quant, gateA, gateB, gateS,
                          upW,     upW_quant, upA,   upB,   upS,
-                         downW, downW_quant, downA, downB, downS)
+                         downW, downW_quant, downA, downB, downS,
+                         geglu_forward_kernel, geglu_backward_kernel,)
     return out
 pass
 
diff --git a/unsloth/kernels/geglu.py b/unsloth/kernels/geglu.py
new file mode 100644
index 0000000..7001b8f
--- /dev/null
+++ b/unsloth/kernels/geglu.py
@@ -0,0 +1,104 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import triton
+import triton.language as tl
+import torch
+from .utils import calculate_settings
+
+
+@triton.jit
+def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):
+    block_idx = tl.program_id(0)
+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
+    mask = offsets < n_elements
+
+    # f = 1/2 * e * (1 + erf(1/sqrt(2) * e))
+    # h = f * up
+    e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)
+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)
+
+    f_row = 0.5 * e_row * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)
+    f_row = f_row.to(g_row.dtype) # Exact copy from HF
+    h_row = f_row * g_row
+
+    # Store h
+    tl.store(h + offsets, h_row, mask = mask)
+pass
+
+
+def geglu_forward_kernel(gate, up):
+    batch, seq_len, hd = gate.shape
+    n_elements = gate.numel()
+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = ""cuda"")
+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
+    _forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)
+    return out
+pass
+
+
+@triton.jit
+def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):
+    """"""
+    f = 1/2 * e * (1 + erf(1/sqrt(2) * e))
+    h = f * up
+
+    df/de (with help of Wolfram :)
+    df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)
+
+    Reuse via
+    f =        1/2 * (1 + erf(1/sqrt(2) * e)) * e
+    """"""
+    block_idx = tl.program_id(0)
+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
+    mask = offsets < n_elements
+
+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)
+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)
+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)
+
+    # Break e_row away for re-use
+    # f = 1/2 * e * (1 + erf(1/sqrt(2) * e))
+    f_partial_row = 0.5 * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)
+    f_row = f_partial_row * e_row
+    
+    f_row = f_row.to(DW_row.dtype)
+    # h = f * g
+    h_row  =  f_row * g_row
+    # df = DW * f
+    df_row = DW_row * f_row
+    # dg = DW * g
+    dg_row = DW_row * g_row
+
+    # df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)
+    t = 0.3989422804014327 # 1/sqrt(2*pi)
+    df_de = f_partial_row + t * e_row * tl.exp(-0.5 * e_row * e_row)
+
+    de_row = dg_row.to(tl.float32) * df_de
+    de_row = de_row.to(DW_row.dtype)
+
+    # Store derivatives in buffers
+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g
+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f
+    tl.store(g  + offsets, de_row, mask = mask) # de
+pass
+
+
+def geglu_backward_kernel(DW, e, g):
+    batch_seq_len, hd = e.shape
+    n_elements = e.numel()
+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
+    _backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)
+    return DW, e, g
+pass
diff --git a/unsloth/kernels/rms_layernorm.py b/unsloth/kernels/rms_layernorm.py
index ec34880..ccd9f89 100644
--- a/unsloth/kernels/rms_layernorm.py
+++ b/unsloth/kernels/rms_layernorm.py
@@ -44,7 +44,7 @@ def _rms_layernorm_forward(
     W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)
 
     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols
-    inv_var = 1.0 / tl.sqrt(row_var + eps)
+    inv_var = tl.math.rsqrt(row_var + eps)
     tl.store(r, inv_var)
     normed = X_row * inv_var
     normed = normed.to(W_row.dtype) # Exact copy from HF
diff --git a/unsloth/models/gemma.py b/unsloth/models/gemma.py
new file mode 100644
index 0000000..4aa634a
--- /dev/null
+++ b/unsloth/models/gemma.py
@@ -0,0 +1,282 @@
+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from .llama import *
+from ._utils import __version__
+
+from transformers.models.gemma.modeling_gemma import (
+    GemmaAttention,
+    GemmaDecoderLayer,
+    GemmaModel,
+    GemmaForCausalLM,
+    GemmaRotaryEmbedding,
+    apply_rotary_pos_emb,
+    repeat_kv,
+)
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
+# For Pytorch 2.1.1
+try:
+    from transformers.models.gemma.modeling_gemma import (
+        GemmaSdpaAttention,
+        GemmaFlashAttention2,
+    )
+except:
+    GemmaSdpaAttention   = GemmaAttention
+    GemmaFlashAttention2 = GemmaAttention
+pass
+
+
+def fast_geglu_inference(self, X):
+    # gate = self.gate_proj(X)
+    # up   = self.up_proj(X)
+    bsz, _, hd = X.shape
+    mlp_size = self.config.intermediate_size
+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = ""cuda"")
+
+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])
+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])
+    gate = torch.nn.functional.gelu(gate)
+    gate *= up
+
+    # X = self.down_proj(gate)
+    down = fast_linear_forward(self.down_proj, gate, out = up[:,:,:hd])
+    return down
+pass
+
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590
+def GemmaDecoderLayer_fast_forward(
+    self,
+    hidden_states:        torch.Tensor,
+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,
+    attention_mask:       Optional[torch.Tensor] = None,
+    position_ids:         Optional[torch.LongTensor] = None,
+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,
+    output_attentions:    Optional[bool] = False,
+    use_cache:            Optional[bool] = False,
+    padding_mask:         Optional[torch.LongTensor] = None,
+    *args, **kwargs,
+):
+    if False:#past_key_value is not None:
+        do_prefill = not hasattr(self.self_attn, ""paged_attention"")
+
+        # Self Attention
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)
+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+            self.self_attn,
+            hidden_states,
+            past_key_value,
+            position_ids,
+            do_prefill = do_prefill,
+        )
+        hidden_states += residual
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
+        hidden_states = fast_geglu_inference(self.mlp, hidden_states)
+        hidden_states += residual
+    else:
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)
+        # hidden_states = self.input_layernorm(hidden_states)
+        hidden_states, self_attn_weights, present_key_value = self.self_attn(
+            hidden_states=hidden_states,
+            causal_mask=causal_mask,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            padding_mask=padding_mask,
+        )
+        hidden_states = residual + hidden_states
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)
+        # hidden_states = self.post_attention_layernorm(hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states
+    pass
+
+    outputs = (hidden_states,)
+
+    if output_attentions:
+        outputs += (self_attn_weights,)
+
+    if use_cache:
+        outputs += (present_key_value,)
+
+    return outputs
+pass
+
+
+from math import sqrt as math_sqrt
+
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825
+@torch.inference_mode
+def GemmaModel_fast_forward_inference(
+    self,
+    input_ids,
+    past_key_values,
+):
+    # Fix out of bounds tokenization
+    input_ids = input_ids[:,:self.max_seq_length]
+
+    hidden_states = self.embed_tokens(input_ids)
+    hidden_states *= math_sqrt(self.config.hidden_size)
+
+    next_decoder_cache = []
+    for idx, decoder_layer in enumerate(self.layers):
+        # Self Attention
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)
+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(
+            decoder_layer.self_attn,
+            hidden_states,
+            past_key_values[idx],
+            None,
+        )
+        hidden_states += residual
+
+        # Fully Connected
+        residual = hidden_states
+        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
+        hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)
+        hidden_states += residual
+
+        next_decoder_cache.append(present_key_value)
+    pass
+    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)
+
+    return BaseModelOutputWithPast(
+        last_hidden_state = hidden_states,
+        past_key_values   = next_decoder_cache,
+        hidden_states     = [],
+        attentions        = [],
+    )
+pass
+
+
+class FastGemmaModel(FastLlamaModel):
+
+    @staticmethod
+    def pre_patch():
+        GemmaAttention      .forward = LlamaAttention_fast_forward
+        GemmaSdpaAttention  .forward = LlamaAttention_fast_forward
+        GemmaFlashAttention2.forward = LlamaAttention_fast_forward
+        GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward
+        GemmaModel          .forward = LlamaModel_fast_forward
+        GemmaForCausalLM    .forward = LlamaForCausalLM_fast_forward
+        PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward
+        # Solves https://github.com/unslothai/unsloth/issues/168
+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.
+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.
+        # https://github.com/huggingface/transformers/pull/27931
+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py
+        import transformers.models.gemma.modeling_gemma
+        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = LlamaRotaryEmbedding
+        return
+    pass
+
+
+    @staticmethod
+    def post_patch(model):
+        # Patch model for Gemma
+        layers = model.model.layers
+
+        # Torch.compile fails on embedding matrix??
+        # Workaround randomnly fixes it for torch versions < 2.2
+        model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)
+        model.config.update({""unsloth_version"" : __version__})
+
+        # We also do this for the lm_head
+        lm_head = torch.nn.Linear(1, 1, bias = None)
+        del lm_head.weight
+        lm_head.weight = model.lm_head.weight
+        lm_head.in_features  = lm_head.weight.shape[1]
+        lm_head.out_features = lm_head.weight.shape[0]
+        model.lm_head = lm_head
+
+        # Gemma has tied weights! This means lm_head == embed_tokens
+        if model.model.embed_tokens.weight.data_ptr() != model.lm_head.weight.data_ptr():
+            lm_head = torch.nn.Linear(1, 1, bias = None)
+            del lm_head.weight
+            lm_head.weight = model.model.embed_tokens.weight
+            lm_head.in_features  = lm_head.weight.shape[1]
+            lm_head.out_features = lm_head.weight.shape[0]
+            model.lm_head = lm_head
+        pass
+
+        # Also patch all dtypes - BnB seems to not allocate the correct type?
+        # BnB default dtype seems to be float16!
+        correct_dtype = lm_head.weight.dtype
+
+        for name, module in model.named_modules():
+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):
+                weight = module.weight
+                quant_state = weight.quant_state
+
+                if type(quant_state) is list:
+                    # BnB seems to have float16 as default!
+                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype
+                else:
+                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files
+                    quant_state.dtype = correct_dtype
+                pass
+            pass
+            # Downcast RoPE embedding to correct data type
+            if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
+                and (module.cos_cached.dtype != correct_dtype):
+
+                module.cos_cached = module.cos_cached.to(correct_dtype)
+                module.sin_cached = module.sin_cached.to(correct_dtype)
+                pass
+            pass
+        pass
+
+        # Add 1 to weight
+        # return output * (1 + self.weight)
+        # https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py#L89
+        from transformers.models.gemma.modeling_gemma import GemmaRMSNorm
+
+        # Freeze all parameters except LoRA
+        # We do this first since += 1 seems to not be liked by requires_grad = True
+        for name, param in model.named_parameters():
+            if "".lora_A."" in name or "".lora_B."" in name:
+                param.requires_grad_(True)
+            else:
+                param.requires_grad_(False)
+        pass
+
+        # Patch RMS Layernorm
+        for name, module in model.named_modules():
+            if isinstance(module, GemmaRMSNorm):
+                module.weight += 1.0 # return output * (1 + self.weight)
+                if not hasattr(module, ""variance_epsilon""):
+                    module.variance_epsilon = module.eps # Gemma doesn't use variance_epsilon
+        pass
+
+        # Clear deleted GPU items
+        import gc
+        for _ in range(3):
+            gc.collect()
+            torch.cuda.empty_cache()
+        return model
+    pass
+pass
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 3ca6291..359761c 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -119,6 +119,7 @@ def LlamaAttention_fast_forward_inference(
     n_groups   = self.num_key_value_groups
     n_kv_heads = self.num_key_value_heads
     head_dim   = self.head_dim
+    attention_size = n_heads*head_dim
     # assert(n_kv_heads * n_groups == n_heads)
     seq_len = K1.shape[-2]
     kv_seq_len = seq_len + 1
@@ -131,7 +132,7 @@ def LlamaAttention_fast_forward_inference(
         self.paged_attention_V = self.paged_attention[:,1]
         self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)
         self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)
-        self.temp_QA = torch.empty((2, bsz, 1, hd), dtype = dtype, device = ""cuda"")
+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = ""cuda"")
         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = ""cuda"")
         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = ""cuda"")
         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = ""cuda"")
@@ -201,13 +202,13 @@ def LlamaAttention_fast_forward_inference(
     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)
     A = torch.matmul(A, Vnn, out = Qn)
     A = A.transpose(1, 2)
-    A = A.reshape(bsz, 1, self.hidden_size)
-    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1])
+    A = A.reshape(bsz, 1, attention_size)
+    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])
     return A, (Kn, Vn)
 pass
 
 
-def fast_mlp_inference(self, X):
+def fast_swiglu_inference(self, X):
     # gate = self.gate_proj(X)
     # up   = self.up_proj(X)
     bsz, _, hd = X.shape
@@ -339,7 +340,7 @@ def LlamaAttention_fast_forward(
         # Go back to (batch_size, seq_len, n_heads, head_dim)
         A = A.transpose(1, 2).contiguous()
     pass
-    attn_output = A.reshape(bsz, q_len, self.hidden_size)
+    attn_output = A.reshape(bsz, q_len, n_heads*head_dim)
     attn_output = self.apply_o(self, attn_output)
     attn_weights = None
     return attn_output, attn_weights, past_key_value
@@ -390,7 +391,7 @@ def LlamaDecoderLayer_fast_forward(
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)
-        hidden_states = fast_mlp_inference(self.mlp, hidden_states)
+        hidden_states = fast_swiglu_inference(self.mlp, hidden_states)
         hidden_states += residual
     else:
         residual = hidden_states
@@ -507,6 +508,14 @@ def LlamaModel_fast_forward(
     if inputs_embeds is None:
         inputs_embeds = self.embed_tokens(input_ids)
 
+    # Mormalized from Gemma
+    if self.config.model_type == ""gemma"":
+        inputs_requires_grad = inputs_embeds.requires_grad
+        if inputs_requires_grad: inputs_embeds.requires_grad_(False)
+        inputs_embeds *= math_sqrt(self.config.hidden_size)
+        if inputs_requires_grad: inputs_embeds.requires_grad_(True)
+    pass
+
     # Fix up attention mask by setting elements to 0
     # Specifically for DPO
     if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):
@@ -646,7 +655,7 @@ def LlamaModel_fast_forward_inference(
         # Fully Connected
         residual = hidden_states
         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)
-        hidden_states = fast_mlp_inference(decoder_layer.mlp, hidden_states)
+        hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)
         hidden_states += residual
 
         next_decoder_cache.append(present_key_value)
@@ -812,7 +821,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):
         self.register_buffer(""sin_cached"", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)
     pass
 
-    def forward(self, x, seq_len=None):
+    def forward(self, x, position_ids=None, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
         if seq_len > self.max_seq_len_cached:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
@@ -886,20 +895,22 @@ class FastLlamaModel:
         device_map     = ""sequential"",
         rope_scaling   = None,
         fix_tokenizer  = True,
+        model_patcher  = None,
         **kwargs,
     ):
+        if model_patcher is None: model_patcher = FastLlamaModel
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth: Fast Llama patching release {__version__}\n""\
+           f""==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
            f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
-        FastLlamaModel.pre_patch()
+        model_patcher.pre_patch()
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
@@ -955,7 +966,7 @@ class FastLlamaModel:
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
-        model = FastLlamaModel.post_patch(model)
+        model = model_patcher.post_patch(model)
 
         # Patch up QKV / O and MLP
         for idx, layer in enumerate(model.model.layers):
@@ -1159,6 +1170,14 @@ class FastLlamaModel:
                     quant_state.dtype = correct_dtype
                 pass
             pass
+            # Downcast RoPE embedding to correct data type
+            if (name.endswith(""rotary_emb"") or hasattr(module, ""cos_cached"")) \
+                and (module.cos_cached.dtype != correct_dtype):
+                
+                module.cos_cached = module.cos_cached.to(correct_dtype)
+                module.sin_cached = module.sin_cached.to(correct_dtype)
+                pass
+            pass
         pass
 
         # Clear deleted GPU items
@@ -1309,6 +1328,16 @@ class FastLlamaModel:
             )
         pass
 
+        # Get activation function
+        model_type = model.config.model_type
+
+        if   model_type == ""llama"":   apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""mistral"": apply_lora_mlp = apply_lora_mlp_swiglu
+        elif model_type == ""gemma"":   apply_lora_mlp = apply_lora_mlp_geglu
+        else:
+            raise NotImplementedError(f""Unsloth: {model_type} is not yet implemented!"")
+        pass
+
         model = prepare_model_for_kbit_training(
             model,
             use_gradient_checkpointing = use_gradient_checkpointing,
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index e4b3561..67a59c8 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -24,6 +24,9 @@ from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER
 major, minor = transformers_version.split(""."")[:2]
 major, minor = int(major), int(minor)
 SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)
+SUPPORTS_GEMMA   = (major > 4) or (major == 4 and minor >= 38)
+if SUPPORTS_GEMMA:
+    from .gemma import FastGemmaModel
 del major, minor
 
 
@@ -99,6 +102,15 @@ class FastLanguageModel(FastLlamaModel):
 
         if   model_type == ""llama"":   dispatch_model = FastLlamaModel
         elif model_type == ""mistral"": dispatch_model = FastMistralModel
+        elif model_type == ""gemma"":
+            if not SUPPORTS_GEMMA:
+                raise RuntimeError(
+                    f""Unsloth: Your transformers version of {transformers_version} does not support Gemma.\n""\
+                    f""The minimum required version is 4.38.\n""\
+                    f'Try `pip install --upgrade ""transformers>=4.38""`\n'\
+                    f""to obtain the latest transformers build, then restart this session.""\
+                )
+            dispatch_model = FastGemmaModel
         else:
             raise NotImplementedError(
                 f""Unsloth: {model_name} not supported yet!\n""\
@@ -115,6 +127,7 @@ class FastLanguageModel(FastLlamaModel):
             device_map     = device_map,
             rope_scaling   = rope_scaling,
             fix_tokenizer  = fix_tokenizer,
+            model_patcher  = dispatch_model,
             *args, **kwargs,
         )
 
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index 323358f..afcbdb7 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -74,6 +74,22 @@ __INT_TO_FLOAT_MAPPER = \
     ""unsloth/solar-10.7b-bnb-4bit"" : (
         ""upstage/SOLAR-10.7B-v1.0"",
     ),
+    ""unsloth/gemma-7b-bnb-4bit"" : (
+        ""unsloth/gemma-7b"",
+        ""google/gemma-7b"",
+    ),
+    ""unsloth/gemma-2b-bnb-4bit"" : (
+        ""unsloth/gemma-2b"",
+        ""google/gemma-2b"",
+    ),
+    ""unsloth/gemma-7b-it-bnb-4bit"" : (
+        ""unsloth/gemma-7b-it"",
+        ""google/gemma-7b-it"",
+    ),
+    ""unsloth/gemma-2b-bnb-4bit"" : (
+        ""unsloth/gemma-2b-it"",
+        ""google/gemma-2b-it"",
+    ),
 }
 
 INT_TO_FLOAT_MAPPER = {}
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 0e36023..6c9d9ec 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -293,8 +293,10 @@ class FastMistralModel(FastLlamaModel):
         device_map     = ""sequential"",
         rope_scaling   = None, # Mistral does not support RoPE scaling
         fix_tokenizer  = True,
+        model_patcher  = None,
         **kwargs,
     ):
+        if model_patcher is None: model_patcher = FastMistralModel
         # Mistral does NOT support RoPE Scaling!
         if rope_scaling is not None:
             logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
@@ -305,13 +307,13 @@ class FastMistralModel(FastLlamaModel):
         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
 
         statistics = \
-           f""==((====))==  Unsloth: Fast Mistral patching release {__version__}\n""\
+           f""==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\n""\
            f""   \\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\n""\
            f""O^O/ \_/ \\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\n""\
            f""\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\n""\
-           f' ""-____-""     Apache 2 free license: http://github.com/unslothai/unsloth'
+           f' ""-____-""     Free Apache license: http://github.com/unslothai/unsloth'
         print(statistics)
-        FastMistralModel.pre_patch()
+        model_patcher.pre_patch()
 
         if dtype is None:
             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16
@@ -360,7 +362,7 @@ class FastMistralModel(FastLlamaModel):
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
-        model = FastMistralModel.post_patch(model)
+        model = model_patcher.post_patch(model)
 
         # Patch up QKV / O and MLP
         for idx, layer in enumerate(model.model.layers):
diff --git a/unsloth/save.py b/unsloth/save.py
index 83e13bd..51ddeb3 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -369,6 +369,7 @@ def unsloth_save_model(
 
     # Switch to our fast saving modules if it's a slow PC!
     n_cpus = psutil.cpu_count(logical = False)
+    if n_cpus is None: n_cpus = 1
 
     if safe_serialization is None:
         safe_serialization = True
@@ -669,7 +670,9 @@ def save_to_gguf(
         pass
     pass
 
-    n_cpus = psutil.cpu_count()*2
+    n_cpus = psutil.cpu_count()
+    if n_cpus is None: n_cpus = 1
+    n_cpus *= 2
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
     
     final_location = f""./{model_directory}-unsloth.{first_conversion.upper()}.gguf""
"
"diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index 31733c2..1404be8 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -485,7 +485,7 @@ class FastBaseModel:
         full_finetuning = os.environ.get(""UNSLOTH_ENABLE_FULL_FINETUNING"", ""0"") == ""1""
 
         float32_mixed_precision = True
-        if _get_dtype(model.config.torch_dtype) == torch.bfloat16:
+        if _get_dtype(model.config.torch_dtype) == torch.bfloat16 and full_finetuning:
             # Use bfloat16 precision for full finetuning
             float32_mixed_precision = False
 
diff --git a/unsloth/save.py b/unsloth/save.py
index 4b2c012..3e720ce 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -2218,12 +2218,60 @@ pass
 
 
 from .models.loader_utils import get_model_name
-from unsloth_zoo.saving_utils import merge_and_overwrite_lora
+from unsloth_zoo.saving_utils import (
+    merge_and_overwrite_lora,
+    prepare_saving,
+)
 from unsloth_zoo.llama_cpp import (
     install_llama_cpp,
-    convert_to_gguf,
+    convert_to_gguf as _convert_to_gguf,
 )
 
+@torch.inference_mode
+def save_to_gguf_generic(
+    model,
+    save_directory,
+    quantization_type = ""Q8_0"",
+    repo_id = None,
+    token = None,
+):
+    if token is None and repo_id is not None: token = get_token()
+    if repo_id is not None and token is None:
+        raise RuntimeError(""Unsloth: Please specify a token for uploading!"")
+
+    if not os.path.exists(os.path.join(""llama.cpp"", ""unsloth_convert_hf_to_gguf.py"")):
+        install_llama_cpp(just_clone_repo = True)
+    pass
+
+    metadata = _convert_to_gguf(
+        save_directory,
+        print_output = True,
+        quantization_type = quantization_type,
+    )
+    if repo_id is not None:
+        prepare_saving(
+            model,
+            repo_id,
+            push_to_hub = True,
+            max_shard_size = ""50GB"",
+            private = True,
+            token = token,
+        )
+
+        from huggingface_hub import HfApi
+        api = HfApi(token = token)
+        api.upload_folder(
+            folder_path = save_directory,
+            repo_id = repo_id,
+            repo_type = ""model"",
+            allow_patterns = [""*.gguf""],
+            private = True,
+        )
+    pass
+    return metadata
+pass
+
+
 @torch.inference_mode
 def unsloth_generic_save(
     model,
@@ -2467,8 +2515,8 @@ def patch_saving_functions(model, vision = False):
         # Vision only 1 option
         model.push_to_hub_merged     = types.MethodType(unsloth_generic_push_to_hub_merged,     model)
         model.save_pretrained_merged = types.MethodType(unsloth_generic_save_pretrained_merged, model)
-        model.push_to_hub_gguf       = types.MethodType(not_implemented_save,                   model)
-        model.save_pretrained_gguf   = types.MethodType(not_implemented_save,                   model)
+        model.push_to_hub_gguf       = types.MethodType(save_to_gguf_generic,                   model)
+        model.save_pretrained_gguf   = types.MethodType(save_to_gguf_generic,                   model)
     pass
     return model
 pass
"
"diff --git a/unsloth/registry/registry.py b/unsloth/registry/registry.py
index bac0a26..d045f5b 100644
--- a/unsloth/registry/registry.py
+++ b/unsloth/registry/registry.py
@@ -13,13 +13,12 @@ BNB_QUANTIZED_TAG = ""bnb-4bit""
 UNSLOTH_DYNAMIC_QUANT_TAG = ""unsloth"" + ""-"" + BNB_QUANTIZED_TAG
 GGUF_TAG = ""GGUF""
 
-QUANT_TYPE_MAP = {
+QUANT_TAG_MAP = {
     QuantType.BNB: BNB_QUANTIZED_TAG,
     QuantType.UNSLOTH: UNSLOTH_DYNAMIC_QUANT_TAG,
     QuantType.GGUF: GGUF_TAG,
     QuantType.NONE: None,
 }
-QUANT_TYPES = list(QUANT_TYPE_MAP.keys())
 
 
 @dataclass
@@ -52,13 +51,8 @@ class ModelInfo:
     def append_quant_type(
         key: str, quant_type: QuantType = None
     ):
-        if quant_type:
-            if quant_type == ""bnb"":
-                key = ""-"".join([key, QUANT_TYPE_MAP[""bnb""]])
-            elif quant_type == ""unsloth"":
-                key = ""-"".join([key, QUANT_TYPE_MAP[""unsloth""]])
-            elif quant_type == ""GGUF"":
-                key = ""-"".join([key, QUANT_TYPE_MAP[""GGUF""]])
+        if quant_type != QuantType.NONE:
+            key = ""-"".join([key, QUANT_TAG_MAP[quant_type]])
         return key
 
     @classmethod
@@ -108,7 +102,7 @@ def register_model(
     key = f""{org}/{name}""
 
     if key in MODEL_REGISTRY:
-        raise ValueError(f""Model {key} already registered"")
+        raise ValueError(f""Model {key} already registered, current keys: {MODEL_REGISTRY.keys()}"")
 
     MODEL_REGISTRY[key] = model_info_cls(
         org=org,
"
"diff --git a/unsloth/kernels/cross_entropy_loss.py b/unsloth/kernels/cross_entropy_loss.py
index 1c8f8c8..57d07af 100644
--- a/unsloth/kernels/cross_entropy_loss.py
+++ b/unsloth/kernels/cross_entropy_loss.py
@@ -388,6 +388,13 @@ from transformers.models.llama.modeling_llama import (
     List,
     Tuple,
 )
+
+try:
+    from transformers.models.llama.modeling_llama import Unpack, KwargsForCausalLM
+except ImportError:
+    logger.warning(""Unsloth: Could not find Unpack, KwargsForCausalLM in LlamaForCausalLM. ""
+    ""This is expected if you are using an older version of Transformers (<4.46.0). "")
+
 import inspect, re
 function = inspect.getsource(LlamaForCausalLM.forward)
 function = function.split(""\n"")
"
"diff --git a/unsloth/models/dpo.py b/unsloth/models/dpo.py
index e7724c2..519914d 100644
--- a/unsloth/models/dpo.py
+++ b/unsloth/models/dpo.py
@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa
 pass
 
 
+def NotebookTrainingTracker_write_line(self, values):
+    """"""
+    Write the values in the inner table.
+
+    Args:
+        values (`Dict[str, float]`): The values to display.
+    """"""
+    if self.inner_table is None:
+        self.inner_table = [list(values.keys()), list(values.values())]
+    else:
+        columns = self.inner_table[0]
+        print(columns)
+        for key in values.keys():
+            if key not in columns:
+                columns.append(key)
+        self.inner_table[0] = columns
+        if len(self.inner_table) > 1:
+            last_values = self.inner_table[-1]
+            first_column = self.inner_table[0][0]
+            if last_values[0] != values[first_column]:
+                # write new line
+                self.inner_table.append([values[c] if c in values else ""No Log"" for c in columns])
+            else:
+                # update last line
+                new_values = values
+                for c in columns:
+                    if c not in new_values.keys():
+                        new_values[c] = last_values[columns.index(c)]
+                self.inner_table[-1] = [new_values[c] for c in columns]
+        else:
+            # Edit for evaluation purposes
+            self.inner_table.append([values[c] if c in values else 0 for c in columns])
+        pass
+    pass
+pass
+
+
 def PatchDPOTrainer():
     # Patch DPO notebook printing
+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line
     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK
     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin
     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 309625d..38b8f55 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -161,11 +161,12 @@ pass
 
 
 def fast_rms_layernorm_inference(self, X):
+    old_dtype = X.dtype
     X = X.to(torch.float32)
     variance = X.square().mean(-1, keepdim = True)
     variance += self.variance_epsilon
     X *= variance.rsqrt_()
-    X = X.to(residual.dtype)
+    X = X.to(old_dtype)
     X *= self.weight
     return X
 pass
@@ -660,14 +661,15 @@ class FastLlamaModel:
 
     @staticmethod
     def from_pretrained(
-        model_name = ""unsloth/llama-2-7b-bnb-4bit"",
+        model_name     = ""unsloth/llama-2-7b-bnb-4bit"",
         max_seq_length = 4096,
-        dtype = None,
-        load_in_4bit = True,
-        token = None,
-        device_map = ""sequential"",
-        rope_scaling = None,
-        fix_tokenizer = True,
+        dtype          = None,
+        load_in_4bit   = True,
+        token          = None,
+        device_map     = ""sequential"",
+        rope_scaling   = None,
+        fix_tokenizer  = True,
+        **kwargs,
     ):
         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()
         gpu_stats = torch.cuda.get_device_properties(0)
@@ -720,18 +722,19 @@ class FastLlamaModel:
         max_position_embeddings = max(max_seq_length, model_max_seq_length)
         model = AutoModelForCausalLM.from_pretrained(
             model_name,
-            device_map = device_map,
-            torch_dtype = dtype,
-            quantization_config = bnb_config,
-            token = token,
-            rope_scaling = rope_scaling,
+            device_map              = device_map,
+            torch_dtype             = dtype,
+            quantization_config     = bnb_config,
+            token                   = token,
+            rope_scaling            = rope_scaling,
             max_position_embeddings = max_position_embeddings,
+            **kwargs,
         )
         tokenizer = AutoTokenizer.from_pretrained(
             model_name,
             model_max_length = max_seq_length,
-            padding_side = ""right"",
-            token = token,
+            padding_side     = ""right"",
+            token            = token,
         )
 
         model, tokenizer = patch_tokenizer(model, tokenizer)
@@ -755,12 +758,12 @@ class FastLlamaModel:
         # We check the tokenizer first for errors
         if fix_tokenizer:
             tokenizer = check_tokenizer(
-                model = model,
-                tokenizer = tokenizer,
-                model_name = model_name,
+                model            = model,
+                tokenizer        = tokenizer,
+                model_name       = model_name,
                 model_max_length = max_seq_length,
-                padding_side = ""right"",
-                token = token,
+                padding_side     = ""right"",
+                token            = token,
             )
         pass
         patch_saving_functions(tokenizer)
@@ -828,20 +831,20 @@ class FastLlamaModel:
     @staticmethod
     def get_peft_model(
         model,
-        r = 16,
-        target_modules = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
-                          ""gate_proj"", ""up_proj"", ""down_proj""],
-        lora_alpha = 16,
-        lora_dropout = 0,
-        bias = ""none"",
+        r                   = 16,
+        target_modules      = [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"",
+                               ""gate_proj"", ""up_proj"", ""down_proj""],
+        lora_alpha          = 16,
+        lora_dropout        = 0,
+        bias                = ""none"",
         layers_to_transform = None,
-        layers_pattern = None,
+        layers_pattern      = None,
         use_gradient_checkpointing = True,
-        random_state = 3407,
-        max_seq_length = 2048, # not used anymore
-        use_rslora = False,
-        init_lora_weights = True,
-        loftq_config = None,
+        random_state        = 3407,
+        max_seq_length      = 2048, # not used anymore
+        use_rslora          = False,
+        init_lora_weights   = True,
+        loftq_config        = None,
         **kwargs,
     ):
         if isinstance(model, PeftModelForCausalLM):
@@ -909,12 +912,14 @@ class FastLlamaModel:
         assert(type(use_rslora) is bool)
         if use_rslora:
             if not SUPPORTS_RSLORA:
+                # We do it ourselves!
+                new_alpha = lora_alpha / (r**0.5)
                 import peft
-                raise RuntimeError(
-                    f""Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\n""\
-                    ""Please install PEFT 0.7.2 or higher.\n""\
-                    ""You can also install from source: `pip install git+https://github.com/huggingface/peft.git""
+                logger.warning_once(
+                    f""Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\n""\
+                    f""But, we do it ourselves by setting `alpha = {new_alpha}.`""
                 )
+                lora_alpha = new_alpha
             pass
         pass
 
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index b4ad3aa..d812d36 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -63,14 +63,14 @@ pass
 class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
-        model_name = ""unsloth/mistral-7b-bnb-4bit"",
+        model_name     = ""unsloth/mistral-7b-bnb-4bit"",
         max_seq_length = 4096,
-        dtype = None,
-        load_in_4bit = True,
-        token = None,
-        device_map = ""sequential"",
-        rope_scaling = None,
-        fix_tokenizer = True,
+        dtype          = None,
+        load_in_4bit   = True,
+        token          = None,
+        device_map     = ""sequential"",
+        rope_scaling   = None,
+        fix_tokenizer  = True,
         *args, **kwargs,
     ):
         old_model_name = model_name
@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):
         pass
 
         model, tokenizer = dispatch_model.from_pretrained(
-            model_name = model_name,
+            model_name     = model_name,
             max_seq_length = max_seq_length,
-            dtype = dtype,
-            load_in_4bit = load_in_4bit,
-            token = token,
-            device_map = device_map,
-            rope_scaling = rope_scaling,
-            fix_tokenizer = fix_tokenizer,
+            dtype          = dtype,
+            load_in_4bit   = load_in_4bit,
+            token          = token,
+            device_map     = device_map,
+            rope_scaling   = rope_scaling,
+            fix_tokenizer  = fix_tokenizer,
             *args, **kwargs,
         )
 
diff --git a/unsloth/models/mistral.py b/unsloth/models/mistral.py
index 17da5cc..eb9d105 100644
--- a/unsloth/models/mistral.py
+++ b/unsloth/models/mistral.py
@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):
 
     @staticmethod
     def from_pretrained(
-        model_name = ""unsloth/mistral-7b-bnb-4bit"",
+        model_name     = ""unsloth/mistral-7b-bnb-4bit"",
         max_seq_length = 4096,
-        dtype = None,
-        load_in_4bit = True,
-        token = None,
-        device_map = ""sequential"",
-        rope_scaling = None, # Mistral does not support RoPE scaling
-        fix_tokenizer = True,
+        dtype          = None,
+        load_in_4bit   = True,
+        token          = None,
+        device_map     = ""sequential"",
+        rope_scaling   = None, # Mistral does not support RoPE scaling
+        fix_tokenizer  = True,
+        **kwargs,
     ): 
         if rope_scaling is not None:
             logger.warning_once(""Unsloth: Mistral models do not support RoPE scaling."")
@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):
             quantization_config = bnb_config,
             token = token,
             # rope_scaling = rope_scaling,
+            **kwargs,
         )
         tokenizer = AutoTokenizer.from_pretrained(
             model_name,
diff --git a/unsloth/save.py b/unsloth/save.py
index 29175a1..a95fac1 100644
--- a/unsloth/save.py
+++ b/unsloth/save.py
@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):
     torch.save(
         shard,
         name,
-        pickle_module = pickle,
+        pickle_module   = pickle,
         pickle_protocol = pickle.HIGHEST_PROTOCOL,
     )
     return
@@ -106,7 +106,7 @@ def unsloth_save_model(
     model,
     tokenizer,
     save_directory       : Union[str, os.PathLike],
-    save_method          : str = ""lora"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    merge_method         : str = ""lora"", # [""lora"", ""16bit"", ""4bit""]
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
     is_main_process      : bool = True,
@@ -131,7 +131,7 @@ def unsloth_save_model(
     maximum_memory_usage : float = 0.9,
 ):
     save_pretrained_settings = dict(locals())
-    for deletion in (""model"", ""tokenizer"", ""save_method"", ""temporary_location"", ""maximum_memory_usage""):
+    for deletion in (""model"", ""tokenizer"", ""merge_method"", ""temporary_location"", ""maximum_memory_usage""):
         del save_pretrained_settings[deletion]
     pass
     import re
@@ -144,8 +144,8 @@ def unsloth_save_model(
         gc.collect()
     pass
 
-    save_method = save_method.lower().replace("" "", ""_"")
-    if save_method != ""lora"" and save_method != ""merged_16bit"" and save_method != ""merged_4bit"":
+    merge_method = merge_method.lower().replace("" "", ""_"")
+    if merge_method != ""lora"" and merge_method != ""16bit"" and merge_method != ""4bit"":
         raise RuntimeError(
             ""Unsloth: You must select one of 3 options when saving models:\n""\
             '""lora""         ==> This is the fastest and easiet. Just saves LoRA modules.\n'\
@@ -154,7 +154,7 @@ def unsloth_save_model(
         )
     pass
 
-    if save_method == ""merged_4bit"":
+    if merge_method == ""4bit"":
         print(""Unsloth: Merging 4bit and LoRA weights to 4bit..."")
         print(""This might take 5 minutes..."")
         model = model.merge_and_unload()
@@ -169,7 +169,7 @@ def unsloth_save_model(
     pass
     save_pretrained_settings[""tags""] = tags
 
-    if (save_method == ""lora"") and push_to_hub:
+    if (merge_method == ""lora"") and push_to_hub:
         if token is None:
             raise RuntimeError(
                 ""Unsloth: Pushing to HF requires a token. Pass `token = 'hf_....'`\n""\
@@ -222,7 +222,7 @@ def unsloth_save_model(
         save_directory = new_save_directory
     pass
     
-    if (save_method == ""merged_4bit"") or (save_method == ""lora"") or (
+    if (merge_method == ""4bit"") or (merge_method == ""lora"") or (
         not hasattr(model, ""model"") or \
         not hasattr(model.model, ""model"") or \
         not hasattr(model.model.model, ""layers"")
@@ -246,7 +246,7 @@ def unsloth_save_model(
             print()
 
         print(""Unsloth: Saving model..."", end = """")
-        if save_method != ""lora"": print("" This might take 10 minutes for Llama-7b..."", end = """")
+        if merge_method != ""lora"": print("" This might take 10 minutes for Llama-7b..."", end = """")
 
         model.save_pretrained(**save_pretrained_settings)
         print("" Done."")
@@ -434,19 +434,19 @@ pass
 
 
 def save_to_gguf(
-    model_directory     : str = ""unsloth_finetuned_model"",
-    quantization_method : str = ""fast_quantized"",
+    model_directory : str = ""unsloth_finetuned_model"",
+    quantization    : str = ""fast_quantized"",
     _run_installer = None, # Non blocking install of llama.cpp
 ):
     from transformers.models.llama.modeling_llama import logger
 
-    if   quantization_method == ""not_quantized"":  quantization_method = ""f16""
-    elif quantization_method == ""fast_quantized"": quantization_method = ""q8_0""
-    elif quantization_method == ""quantized"":      quantization_method = ""q4_k_m""
-    elif quantization_method is None:             quantization_method = ""q8_0""
+    if   quantization == ""not_quantized"":  quantization = ""f16""
+    elif quantization == ""fast_quantized"": quantization = ""q8_0""
+    elif quantization == ""quantized"":      quantization = ""q4_k_m""
+    elif quantization is None:             quantization = ""q8_0""
 
-    if quantization_method not in ALLOWED_QUANTS.keys():
-        error = f""Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\n""
+    if quantization not in ALLOWED_QUANTS.keys():
+        error = f""Unsloth: Quant method = [{quantization}] not supported. Choose from below:\n""
         for key, value in ALLOWED_QUANTS.items():
             error += f""[{key}] => {value}\n""
         raise RuntimeError(error)
@@ -456,7 +456,7 @@ def save_to_gguf(
         f""==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n""\
         f""   \\\   /|    [0] Installing llama.cpp will take 3 minutes.\n""\
         f""O^O/ \_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n""\
-        f""\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\n""\
+        f""\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\n""\
         f' ""-____-""     In total, you will have to wait around 26 minutes.\n'
     print(print_info)
 
@@ -469,9 +469,9 @@ def save_to_gguf(
 
     print(""Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes..."")
     first_conversion = ""f16""
-    if   quantization_method == ""f32"":  first_conversion = ""f32""
-    elif quantization_method == ""f16"":  first_conversion = ""f16""
-    elif quantization_method == ""q8_0"": first_conversion = ""q8_0""
+    if   quantization == ""f32"":  first_conversion = ""f32""
+    elif quantization == ""f16"":  first_conversion = ""f16""
+    elif quantization == ""q8_0"": first_conversion = ""q8_0""
 
     n_cpus = psutil.cpu_count()*2
     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model
@@ -489,13 +489,13 @@ def save_to_gguf(
 
     print(f""Unsloth: Conversion completed! Output location: {final_location}"")
 
-    if quantization_method != first_conversion:
+    if quantization != first_conversion:
         old_location = final_location
-        print(f""Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes..."")
-        final_location = f""./{model_directory}-unsloth.{quantization_method.upper()}.gguf""
+        print(f""Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes..."")
+        final_location = f""./{model_directory}-unsloth.{quantization.upper()}.gguf""
 
         command = f""./llama.cpp/quantize {old_location} ""\
-            f""{final_location} {quantization_method} {n_cpus}""
+            f""{final_location} {quantization} {n_cpus}""
         
         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:
             for line in sp.stdout:
@@ -511,7 +511,8 @@ pass
 def unsloth_save_pretrained_merged(
     self,
     save_directory       : Union[str, os.PathLike],
-    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    tokenizer            = None,
+    merge_method         : str = ""16bit"", # [""lora"", ""16bit"", ""4bit""]
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
     is_main_process      : bool = True,
@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(
         Same as .save_pretrained(...) except 4bit weights are auto
         converted to float16 with as few overhead as possible.
 
-        Choose for `save_method` to be either:
-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.
+        Choose for `merge_method` to be either:
+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
     """"""
+    if tokenizer is None:
+        logger.warning_once(
+            ""Unsloth: You're not saving a tokenizer as well?\n""\
+            ""You can do it separately via `tokenizer.save_pretrained(...)`""
+        )
+    pass
+
     arguments = dict(locals())
-    arguments[""model""]     = self
-    arguments[""tokenizer""] = None
+    arguments[""model""] = self
     del arguments[""self""]
     unsloth_save_model(**arguments)
     for _ in range(3):
@@ -547,7 +554,8 @@ pass
 def unsloth_push_to_hub_merged(
     self,
     repo_id              : str,
-    save_method          : str = ""merged_16bit"", # [""lora"", ""merged_16bit"", ""merged_4bit""]
+    tokenizer            = None,
+    merge_method         : str = ""16bit"", # [""lora"", ""16bit"", ""4bit""]
     use_temp_dir         : Optional[bool] = None,
     commit_message       : Optional[str] = None,
     private              : Optional[bool] = None,
@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(
         Same as .push_to_hub(...) except 4bit weights are auto
         converted to float16 with as few overhead as possible.
 
-        Choose for `save_method` to be either:
-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.
+        Choose for `merge_method` to be either:
+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.
+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.
+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.
     """"""
+    if tokenizer is None:
+        logger.warning_once(
+            ""Unsloth: You're not saving a tokenizer as well?\n""\
+            ""You can do it separately via `tokenizer.push_to_hub(...)`""
+        )
+    pass
+
     arguments = dict(locals())
     arguments[""model""]          = self
-    arguments[""tokenizer""]      = None
     arguments[""save_directory""] = repo_id
     arguments[""push_to_hub""]    = True
     del arguments[""self""]
@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(
     self,
     save_directory       : Union[str, os.PathLike],
     tokenizer            = None,
-    quantization_method  : str = ""fast_quantized"",
+    quantization         : str = ""fast_quantized"",
     push_to_hub          : bool = False,
     token                : Optional[Union[str, bool]] = None,
     is_main_process      : bool = True,
@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(
         Same as .save_pretrained(...) except 4bit weights are auto
         converted to float16 then converted to GGUF / llama.cpp format.
 
-        Choose for `quantization_method` to be:
+        Choose for `quantization` to be:
         ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
         ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
         ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(
         raise ValueError(""Unsloth: Saving to GGUF must have a tokenizer."")
 
     arguments = dict(locals())
-    arguments[""model""]       = self
-    arguments[""tokenizer""]   = tokenizer
-    arguments[""push_to_hub""] = False # We save ourselves
-    arguments[""save_method""] = ""merged_16bit"" # Must be 16bit
+    arguments[""model""]        = self
+    arguments[""tokenizer""]    = tokenizer
+    arguments[""push_to_hub""]  = False # We save ourselves
+    arguments[""merge_method""] = ""16bit"" # Must be 16bit
     del arguments[""self""]
-    del arguments[""quantization_method""]
+    del arguments[""quantization""]
 
     # Non blocking install GGUF first
     git_clone = install_llama_cpp_clone_non_blocking()
@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(
     for _ in range(3):
         gc.collect()
 
-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+    file_location = save_to_gguf(new_save_directory, quantization, makefile)
 
     # And save to HF
     if push_to_hub:
@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(
     self,
     repo_id              : str,
     tokenizer            = None,
-    quantization_method  : str = ""fast_quantized"",
+    quantization         : str = ""fast_quantized"",
     use_temp_dir         : Optional[bool] = None,
     commit_message       : Optional[str] = None,
     private              : Optional[bool] = None,
@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(
         Same as .push_to_hub(...) except 4bit weights are auto
         converted to float16 then converted to GGUF / llama.cpp format.
 
-        Choose for `quantization_method` to be:
+        Choose for `quantization` to be:
         ""not_quantized""  : ""Recommended. Fast conversion. Slow inference, big files."",
         ""fast_quantized"" : ""Recommended. Fast conversion. OK inference, OK file size."",
         ""quantized""      : ""Recommended. Slow conversion. Fast inference, small files."",
@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(
     arguments[""tokenizer""]      = tokenizer
     arguments[""save_directory""] = repo_id
     arguments[""push_to_hub""]    = False # We save ourselves
-    arguments[""save_method""]    = ""merged_16bit"" # Must be 16bit
+    arguments[""merge_method""]   = ""16bit"" # Must be 16bit
     del arguments[""self""]
     del arguments[""repo_id""]
-    del arguments[""quantization_method""]
+    del arguments[""quantization""]
 
     # Non blocking install GGUF first
     git_clone = install_llama_cpp_clone_non_blocking()
@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(
         gc.collect()
 
     python_install.wait()
-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)
+    file_location = save_to_gguf(new_save_directory, quantization, makefile)
 
     # Save to hub
     print(""Unsloth: Uploading GGUF to Huggingface Hub..."")
"
"diff --git a/unsloth/chat_templates.py b/unsloth/chat_templates.py
index 5785894..2c2e361 100644
--- a/unsloth/chat_templates.py
+++ b/unsloth/chat_templates.py
@@ -20,6 +20,7 @@ __all__ = [
 
     ""to_sharegpt"",
     ""standardize_sharegpt"",
+    ""standardize_data_formats"",
     ""apply_chat_template"",
     ""train_on_responses_only"",
 
@@ -37,7 +38,9 @@ from .models._utils import patch_tokenizer
 import re
 from unsloth_zoo.dataset_utils import (
     train_on_responses_only,
+    standardize_data_formats,
 )
+standardize_sharegpt = standardize_data_formats
 CHAT_TEMPLATES = {}
 DEFAULT_SYSTEM_MESSAGE = {}
 
@@ -934,6 +937,84 @@ DEFAULT_SYSTEM_MESSAGE[""phi-4""] = None # No system message in Phi-4
 pass
 
 
+# =========================================== Gemma-3
+# Obtained via
+# print(tokenizer.chat_template.replace(""}\n"", ""####"").replace(""\n"", ""\\n"").replace(""####"", ""}\n""))
+gemma3_template = \
+""""""{{ bos_token }}
+{%- if messages[0]['role'] == 'system' -%}
+    {%- if messages[0]['content'] is string -%}
+        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}
+    {%- else -%}
+        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}
+    {%- endif -%}
+    {%- set loop_messages = messages[1:] -%}
+{%- else -%}
+    {%- set first_user_prefix = """" -%}
+    {%- set loop_messages = messages -%}
+{%- endif -%}
+{%- for message in loop_messages -%}
+    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}
+        {{ raise_exception(""Conversation roles must alternate user/assistant/user/assistant/..."") }}
+    {%- endif -%}
+    {%- if (message['role'] == 'assistant') -%}
+        {%- set role = ""model"" -%}
+    {%- else -%}
+        {%- set role = message['role'] -%}
+    {%- endif -%}
+    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else """") }}
+    {%- if message['content'] is string -%}
+        {{ message['content'] | trim }}
+    {%- elif message['content'] is iterable -%}
+        {%- for item in message['content'] -%}
+            {%- if item['type'] == 'image' -%}
+                {{ '<start_of_image>' }}
+            {%- elif item['type'] == 'text' -%}
+                {{ item['text'] | trim }}
+            {%- endif -%}
+        {%- endfor -%}
+    {%- else -%}
+        {{ raise_exception(""Invalid content type"") }}
+    {%- endif -%}
+    {{ '<end_of_turn>\n' }}
+{%- endfor -%}
+{%- if add_generation_prompt -%}
+    {{ '<start_of_turn>model\n' }}
+{%- endif -%}
+""""""
+
+# Ollama from https://ollama.com/library/gemma3/blobs/e0a42594d802
+gemma3_ollama = \
+'''
+FROM {__FILE_LOCATION__}
+TEMPLATE """"""{{- range $i, $_ := .Messages }}
+{{- $last := eq (len (slice $.Messages $i)) 1 }}
+{{- if or (eq .Role ""user"") (eq .Role ""system"") }}<start_of_turn>user
+{{ .Content }}<end_of_turn>
+{{ if $last }}<start_of_turn>model
+{{ end }}
+{{- else if eq .Role ""assistant"" }}<start_of_turn>model
+{{ .Content }}{{ if not $last }}<end_of_turn>
+{{ end }}
+{{- end }}
+{{- end }}""""""
+PARAMETER stop ""<end_of_turn>""
+PARAMETER stop ""<eos>""
+PARAMETER temperature 0.1
+PARAMETER min_p 0.0
+PARAMETER top_k 64
+PARAMETER top_p 0.95
+PARAMETER num_predict 32768
+'''
+
+gemma3_template_eos_token = ""<end_of_turn>""
+CHAT_TEMPLATES[""gemma-3""] = (gemma3_template, gemma3_template_eos_token, False, gemma3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma-3""] = None # No system message in Gemma-3
+
+CHAT_TEMPLATES[""gemma3""] = (gemma3_template, gemma3_template_eos_token, False, gemma3_ollama,)
+DEFAULT_SYSTEM_MESSAGE[""gemma3""] = None # No system message in Gemma-3
+pass
+
 def _change_system_message(template: str, type_chat_template: str, system_message: str = None):
     system_message_pattern = r""\{system_message\}""
     
@@ -1033,11 +1114,12 @@ def get_chat_template(
 
         # Check fast tokenizer
         if not is_fast_tokenizer:
-            print(
-                ""Unsloth: Not a fast tokenizer, so can't process it as of yet :(\n""\
-                ""Please log a Github issue if you want this as a new feature!\n""\
-                ""Your chat template will still work, but it won't add or edit tokens.""
-            )
+            pass
+            # print(
+            #     ""Unsloth: Not a fast tokenizer, so can't process it as of yet :(\n""\
+            #     ""Please log a Github issue if you want this as a new feature!\n""\
+            #     ""Your chat template will still work, but it won't add or edit tokens.""
+            # )
 
         elif token_mapping is not None:
             # token_mapping = {""<start_of_turn>"" : ""<|im_start|>"", ""<end_of_turn>"" : ""<|im_end|>""}
@@ -1396,82 +1478,6 @@ def to_sharegpt(
 pass
 
 
-def standardize_sharegpt(
-    dataset,
-    aliases_for_system    = [""system"",],
-    aliases_for_user      = [""user"", ""human"", ""input"",],
-    aliases_for_assistant = [""gpt"", ""assistant"", ""output"",],
-):
-    """"""
-    Standardizes ShareGPT and other formats to user/assistant Hugging Face format.
-    
-    Get aliases for the system, user and assistant roles.
-    These shall map to ""system"", ""user"" and ""assistant"" respectively.
-    
-    aliases_for_system    = [""system"",],
-    aliases_for_user      = [""user"", ""human"", ""input"",],
-    aliases_for_assistant = [""gpt"", ""assistant"", ""output"",],
-    """"""
-    import collections
-    import itertools
-
-    convos = dataset[:10][""conversations""]
-    uniques = collections.defaultdict(list)
-    for convo in convos:
-        for message in convo:
-            for key, value in message.items():
-                uniques[key].append(value)
-    pass
-
-    # Must be only 2 entries
-    assert(len(uniques.keys()) == 2)
-
-    keys = list(uniques.keys())
-    length_first  = len(set(uniques[keys[0]]))
-    length_second = len(set(uniques[keys[1]]))
-
-    if length_first < length_second:
-        # Role is assigned to the first element
-        role_key    = keys[0]
-        content_key = keys[1]
-    else:
-        role_key    = keys[1]
-        content_key = keys[0]
-    pass
-
-    # Check roles are in aliases
-    all_aliases = set(aliases_for_system + aliases_for_user + aliases_for_assistant)
-    roles = set(uniques[role_key])
-    leftover_aliases = (all_aliases | roles) - all_aliases
-    if len(leftover_aliases) != 0:
-        raise TypeError(
-            f""Unsloth: {list(leftover_aliases)} are not in aliases. Please update aliases.""
-        )
-    pass
-
-    # Mapping for aliases
-    aliases_mapping = {}
-    for x in aliases_for_system:    aliases_mapping[x] = ""system""
-    for x in aliases_for_user:      aliases_mapping[x] = ""user""
-    for x in aliases_for_assistant: aliases_mapping[x] = ""assistant""
-
-    def _standardize_dataset(examples):
-        convos = examples[""conversations""]
-        all_convos = []
-        for convo in convos:
-            new_convo = [
-                { ""role"" : aliases_mapping[message[role_key]], ""content"" : message[content_key], }
-                for message in convo
-            ]
-            all_convos.append(new_convo)
-        pass
-        return { ""conversations"" : all_convos, }
-    pass
-
-    return dataset.map(_standardize_dataset, batched = True, desc = ""Standardizing format"")
-pass
-
-
 def get_ollama_eos_tokens(tokenizer, extra_eos_tokens = []):
     added_tokens_decoder = tokenizer.added_tokens_decoder.values()
     added_tokens_decoder = [str(x) for x in added_tokens_decoder]
@@ -1934,6 +1940,11 @@ extra_eos_tokens = None,
     tokenizer._ollama_modelfile = modelfile
     tokenizer._unsloth_input_part  = input_part
     tokenizer._unsloth_output_part = output_part
+    if hasattr(tokenizer, ""tokenizer""):
+        tokenizer.tokenizer.chat_template = jinja_template
+        tokenizer.tokenizer._ollama_modelfile = modelfile
+        tokenizer.tokenizer._unsloth_input_part  = input_part
+        tokenizer.tokenizer._unsloth_output_part = output_part
 
     return dataset.map(formatting_prompts_func, batched = True,)
 pass
diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 77bfa87..a3fc12f 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -71,6 +71,7 @@ from typing import Union, Optional, List, Any, Callable, Tuple
 from platform import system as platform_system
 platform_system = platform_system()
 import numpy as np
+import contextlib
 import warnings, subprocess, re, inspect, psutil, os, math
 from unsloth_zoo.utils import Version
 
@@ -113,6 +114,11 @@ from unsloth_zoo.compiler import (
 from unsloth_zoo.training_utils import (
     prepare_model_for_training,
 )
+from unsloth_zoo.temporary_patches import (
+    TEMPORARY_PATCHES,
+)
+for temporary_patch in TEMPORARY_PATCHES:
+    temporary_patch()
 
 # =============================================
 # Disable some warnings which can get annoying
@@ -981,7 +987,14 @@ def _unsloth_pre_compute_loss(self, model, inputs, *args, **kwargs):
             ""Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient""
         )
     pass
-    return self._old_compute_loss(model, inputs, *args, **kwargs)
+
+    if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""0"":
+        autocaster = contextlib.nullcontext()
+    else:
+        autocaster = torch.autocast(device_type = ""cuda"", dtype = torch.float32)
+    with autocaster:
+        outputs = self._old_compute_loss(model, inputs, *args, **kwargs)
+    return outputs
 pass
 
 
diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 7ae6e92..7000739 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -38,6 +38,7 @@ from ..kernels import *
 from ..tokenizer_utils import *
 if HAS_FLASH_ATTENTION:
     from flash_attn import flash_attn_func
+from .vision import FastBaseModel
 
 # Final patching code
 from transformers.models.llama.modeling_llama import (
@@ -1648,6 +1649,7 @@ class FastLlamaModel:
         disable_log_stats = False,
         **kwargs,
     ):
+        os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""0""
         if trust_remote_code:
             if fast_inference:
                 raise NotImplementedError(""Unsloth: Fast inference does not support `trust_remote_code` yet."")
@@ -2016,6 +2018,31 @@ class FastLlamaModel:
         temporary_location  = ""_unsloth_temporary_saved_buffers"",
         **kwargs,
     ):
+        if os.environ.get(""UNSLOTH_USE_NEW_MODEL"", ""0"") == ""1"":
+            return FastBaseModel.get_peft_model(
+                model                      = model,
+                r                          = r,
+                target_modules             = target_modules,
+                lora_alpha                 = lora_alpha,
+                lora_dropout               = lora_dropout,
+                bias                       = bias,
+                finetune_vision_layers     = False,
+                finetune_language_layers   = True,
+                finetune_attention_modules = True,
+                finetune_mlp_modules       = True,
+                layers_to_transform        = layers_to_transform,
+                layers_pattern             = layers_pattern,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+                random_state               = random_state,
+                max_seq_length             = max_seq_length,
+                use_rslora                 = use_rslora,
+                modules_to_save            = modules_to_save,
+                init_lora_weights          = init_lora_weights,
+                loftq_config               = loftq_config,
+                temporary_location         = temporary_location,
+                **kwargs,
+            )
+        pass
         if os.environ.get(""UNSLOTH_ENABLE_FULL_FINETUNING"", ""0"") == ""1"":
             print(""Unsloth: Full finetuning is enabled, so .get_peft_model has no effect"")
             return model
@@ -2435,6 +2462,12 @@ class FastLlamaModel:
         model,
         use_gradient_checkpointing = True,
     ):
+        if os.environ.get(""UNSLOTH_USE_NEW_MODEL"", ""0"") == ""1"":
+            return FastBaseModel.patch_peft_model(
+                model = model,
+                use_gradient_checkpointing = use_gradient_checkpointing,
+            )
+        pass
         if not isinstance(model, PeftModelForCausalLM):
             raise TypeError(
                 ""Unsloth: Your model needs to call `.get_peft_model` first!""
diff --git a/unsloth/models/loader.py b/unsloth/models/loader.py
index 92a166f..1b54c8c 100644
--- a/unsloth/models/loader.py
+++ b/unsloth/models/loader.py
@@ -70,7 +70,7 @@ class FastLanguageModel(FastLlamaModel):
     @staticmethod
     def from_pretrained(
         model_name                 = ""unsloth/Llama-3.2-1B-Instruct"",
-        max_seq_length             = None,
+        max_seq_length             = 2048,
         dtype                      = None,
         load_in_4bit               = True,
         load_in_8bit               = False,
@@ -96,7 +96,7 @@ class FastLanguageModel(FastLlamaModel):
         if load_in_8bit or full_finetuning:
             return FastModel.from_pretrained(
                 model_name                 = model_name,
-                max_seq_length             = max_seq_length, # [TODO] No effect
+                max_seq_length             = max_seq_length,
                 dtype                      = dtype,
                 load_in_4bit               = load_in_4bit,
                 load_in_8bit               = load_in_8bit,
@@ -295,7 +295,7 @@ class FastLanguageModel(FastLlamaModel):
         else:
             return FastModel.from_pretrained(
                 model_name                 = model_name,
-                max_seq_length             = max_seq_length, # [TODO] No effect
+                max_seq_length             = max_seq_length,
                 dtype                      = dtype,
                 load_in_4bit               = load_in_4bit,
                 load_in_8bit               = load_in_8bit,
@@ -442,7 +442,7 @@ class FastModel(FastBaseModel):
     @staticmethod
     def from_pretrained(
         model_name                 = ""unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"",
-        max_seq_length             = None, # [TODO] No effect
+        max_seq_length             = 2048,
         dtype                      = None,
         load_in_4bit               = True,
         load_in_8bit               = False,
@@ -500,6 +500,8 @@ class FastModel(FastBaseModel):
             raise RuntimeError(""Unsloth: Qwen 2.5 only works on transformers >= 4.49.0."" + LATEST)
         elif ""aya-vision"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
             raise RuntimeError(""Unsloth: Aya Vision only works on transformers >= 4.50.0."" + NIGHTLY)
+        elif ""gemma-3"" in model_name.lower() and transformers_version < Version(""4.50.0.dev0""):
+            raise RuntimeError(""Unsloth: Gemma 3 only works on transformers >= 4.50.0."" + NIGHTLY)
         pass
 
         if USE_MODELSCOPE and not os.path.exists(model_name):
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index b4facf7..cb0d73c 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -638,37 +638,45 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/QwQ-32B"",
         ""unsloth/QwQ-32B-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-1b-it-bnb-4bit"" : (
+    ""unsloth/gemma-3-1b-it-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-1b-it"",
         ""google/gemma-3-1b-it"",
+        ""unsloth/gemma-3-1b-it-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-4b-it-bnb-4bit"" : (
+    ""unsloth/gemma-3-4b-it-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-4b-it"",
         ""google/gemma-3-4b-it"",
+        ""unsloth/gemma-3-4b-it-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-12b-it-bnb-4bit"" : (
+    ""unsloth/gemma-3-12b-it-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-12b-it"",
         ""google/gemma-3-12b-it"",
+        ""unsloth/gemma-3-12b-it-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-27b-it-bnb-4bit"" : (
+    ""unsloth/gemma-3-27b-it-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-27b-it"",
         ""google/gemma-3-27b-it"",
+        ""unsloth/gemma-3-27b-it-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-1b-pt-bnb-4bit"" : (
+    ""unsloth/gemma-3-1b-pt-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-1b-pt"",
         ""google/gemma-3-1b-pt"",
+        ""unsloth/gemma-3-1b-pt-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-4b-pt-bnb-4bit"" : (
+    ""unsloth/gemma-3-4b-pt-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-4b-pt"",
         ""google/gemma-3-4b-pt"",
+        ""unsloth/gemma-3-4b-pt-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-12b-pt-bnb-4bit"" : (
+    ""unsloth/gemma-3-12b-pt-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-12b-pt"",
         ""google/gemma-3-12b-pt"",
+        ""unsloth/gemma-3-12b-pt-bnb-4bit"",
     ),
-    ""unsloth/gemma-3-27b-pt-bnb-4bit"" : (
+    ""unsloth/gemma-3-27b-pt-unsloth-bnb-4bit"" : (
         ""unsloth/gemma-3-27b-pt"",
         ""google/gemma-3-27b-pt"",
+        ""unsloth/gemma-3-27b-pt-bnb-4bit"",
     ),
 }
 
diff --git a/unsloth/models/rl.py b/unsloth/models/rl.py
index 86a174e..4e158f5 100644
--- a/unsloth/models/rl.py
+++ b/unsloth/models/rl.py
@@ -236,15 +236,24 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         mixed_precision = \
         ""use_bf16 = getattr(args, 'bf16', False)\n""\
         ""use_fp16 = getattr(args, 'fp16', False)\n""\
+        ""force_float32 = False\n""\
+        ""if os.environ.get('UNSLOTH_FORCE_FLOAT32', '0') == '1':\n""\
+        ""    if use_bf16 or use_fp16:\n""\
+        ""        print('Unsloth: Switching to float32 training since model cannot work with float16')\n""\
+        ""        force_float32 = True\n""\
         ""mixed_precision_dtype = os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32')\n""\
         ""dtype = getattr(model.config, 'torch_dtype', None)\n""\
         ""if dtype is None: dtype = model.get_input_embeddings().dtype\n""\
         ""from unsloth_zoo.utils import _get_dtype\n""\
         ""dtype = _get_dtype(dtype)\n""\
         ""float16 = dtype == torch.float16\n""\
-        ""if float16 and use_bf16: raise TypeError('Unsloth: Model is in float16 precision but you want to use bfloat16 precision. Set fp16 to `True` and bf16 to `False`')\n""\
-        ""if not float16 and use_fp16: raise TypeError('Unsloth: Model is in bfloat16 precision but you want to use float16 precision. Set fp16 to `False` and bf16 to `True`')\n""\
-        ""if (not use_bf16 and not use_fp16) and mixed_precision_dtype == 'float32':\n""\
+        ""if not force_float32 and (float16 and use_bf16): raise TypeError('Unsloth: Model is in float16 precision but you want to use bfloat16 precision. Set fp16 to `True` and bf16 to `False`')\n""\
+        ""if not force_float32 and (not float16 and use_fp16): raise TypeError('Unsloth: Model is in bfloat16 precision but you want to use float16 precision. Set fp16 to `False` and bf16 to `True`')\n""\
+        ""if force_float32:\n""\
+        ""    args.fp16 = False\n""\
+        ""    args.bf16 = False\n""\
+        ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'no'\n""\
+        ""elif (not use_bf16 and not use_fp16) and mixed_precision_dtype == 'float32':\n""\
         ""    args.fp16 = float16\n""\
         ""    args.bf16 = not float16\n""\
         ""    os.environ['ACCELERATE_MIXED_PRECISION'] = 'fp16' if float16 else 'bf16'\n""
@@ -287,7 +296,10 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
         ""bf16_full_eval = getattr(args, 'bf16_full_eval', False)\n""\
         ""if args.fp16 and bf16_full_eval: args.bf16_full_eval = False; args.fp16_full_eval = True\n""\
         ""if args.bf16 and fp16_full_eval: args.bf16_full_eval = True; args.fp16_full_eval = False\n""\
-        ""if os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32') == 'bfloat16':\n""\
+        ""if force_float32:\n""\
+        ""    args.bf16_full_eval = False\n""\
+        ""    args.fp16_full_eval = False\n""\
+        ""elif os.environ.get('UNSLOTH_MIXED_PRECISION', 'float32') == 'bfloat16':\n""\
         ""    args.bf16_full_eval = True\n""\
         ""    args.fp16_full_eval = False\n""\
         ""elif not bf16_full_eval and not fp16_full_eval:\n""\
@@ -343,11 +355,9 @@ def _patch_trl_rl_trainers(trainer_file = ""grpo_trainer""):
     if ""data_collator"" in call_args and ""train_dataset"" in call_args:
         data_collator_check = \
         ""if isinstance(data_collator, DataCollatorForSeq2Seq) and 'labels' not in train_dataset.column_names:\n""\
-        ""    print('Unsloth: Changing data collator to `DataCollatorForLanguageModeling` since `labels` not found.')\n""\
         ""    data_collator = DataCollatorForLanguageModeling(""\
         ""tokenizer = processing_class if 'processing_class' in locals() else tokenizer, mlm = False)\n""\
         ""elif isinstance(data_collator, DataCollatorForLanguageModeling) and 'labels' in train_dataset.column_names:\n""\
-        ""    print('Unsloth: Changing data collator to `DataCollatorForSeq2Seq` since `labels` found.')\n""\
         ""    data_collator = DataCollatorForSeq2Seq(""\
         ""tokenizer = processing_class if 'processing_class' in locals() else tokenizer)\n""
         extra_args += data_collator_check
diff --git a/unsloth/models/vision.py b/unsloth/models/vision.py
index fa5547e..2ef9d2e 100644
--- a/unsloth/models/vision.py
+++ b/unsloth/models/vision.py
@@ -25,29 +25,49 @@ try:
 except:
     from transformers import AutoModelForVision2Seq
 pass
-from .llama import *
 from ..kernels import (
     post_patch_loss_function,
 )
 from ._utils import __version__
+from ._utils import *
+from ..save import patch_saving_functions
 from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model
+from peft import PeftModelForCausalLM
 from transformers import set_seed as transformers_set_seed
 from unsloth_zoo.peft_utils import (
     get_peft_regex,
     SKIP_QUANTIZATION_MODULES,
     requires_grad_for_gradient_checkpointing,
 )
+from transformers.models.llama.modeling_llama import logger
+from transformers import __version__ as transformers_version
 from triton import __version__ as triton_version
 from unsloth_zoo.utils import _get_dtype
 from unsloth_zoo.patching_utils import patch_model_and_tokenizer
 from unsloth_zoo.training_utils import prepare_model_for_training
 import types
 import functools
+import os
+import gc
+import math
+import functools
+from typing import Optional, Tuple, List, Union
+import re, inspect, sys
+import types
+try:
+    from huggingface_hub.utils import get_token
+except:
+    # Old HF Hub versions <= 0.0.25
+    from huggingface_hub.utils._token import get_token
+pass
 
 __all__ = [
     ""FastBaseModel"",
 ]
 
+global FORCE_FLOAT32
+FORCE_FLOAT32 = [""gemma3""]
+
 
 def unsloth_base_fast_generate(
     self,
@@ -86,6 +106,7 @@ def unsloth_base_fast_generate(
     except: pass
 
     # Mixed precision autocast
+    if os.environ.get(""UNSLOTH_FORCE_FLOAT32"", ""0"") == ""1"": dtype = torch.float32
     with torch.inference_mode(), torch.autocast(device_type = ""cuda"", dtype = dtype):
         output = self._old_generate(*args, **kwargs)
     pass
@@ -100,7 +121,7 @@ class FastBaseModel:
     @staticmethod
     def from_pretrained(
         model_name        = ""unsloth/Llama-3.2-1B-Instruct"",
-        max_seq_length    = None,
+        max_seq_length    = 2048,
         dtype             = None,
         load_in_4bit      = True,
         load_in_8bit      = False,
@@ -114,6 +135,7 @@ class FastBaseModel:
         use_gradient_checkpointing = ""unsloth"",
         **kwargs,
     ):
+        os.environ[""UNSLOTH_USE_NEW_MODEL""] = ""1""
         if trust_remote_code:
             print(
                 ""Unsloth: WARNING `trust_remote_code` is True.\n""\
@@ -129,8 +151,12 @@ class FastBaseModel:
         try:    vllm_version = f"" vLLM: {importlib_version('vllm')}.""
         except: vllm_version = """"
 
+        model_type_arch = model_types[0]
+        if model_type_arch == ""siglip"" and len(model_types) != 1:
+            model_type_arch = model_types[1]
+
         statistics = \
-           f""==((====))==  Unsloth {__version__}: Fast {model_types[0].title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
+           f""==((====))==  Unsloth {__version__}: Fast {model_type_arch.title()} patching. Transformers: {transformers_version}.{vllm_version}\n""\
            f""   {chr(92)}{chr(92)}   /|    {gpu_stats.name}. Num GPUs = {torch.cuda.device_count()}. Max memory: {max_memory} GB. Platform: {platform_system}.\n""\
            f""O^O/ {chr(92)}_/ {chr(92)}    Torch: {torch.__version__}. CUDA: {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit: {torch.version.cuda}. Triton: {triton_version}\n""\
            f""{chr(92)}        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. FA [Xformers = {xformers_version}. FA2 = {HAS_FLASH_ATTENTION}]\n""\
@@ -156,6 +182,17 @@ class FastBaseModel:
 
         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)
 
+        global FORCE_FLOAT32
+        os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""0""
+        bnb_compute_dtype = dtype
+        for disable_name in FORCE_FLOAT32:
+            if disable_name.lower() == model_type_arch.lower() and dtype == torch.float16:
+                print(f""Unsloth: Using float16 precision for {model_type_arch} won't work! Using float32."")
+                os.environ[""UNSLOTH_FORCE_FLOAT32""] = ""1""
+                bnb_compute_dtype = torch.float32
+                break
+        pass
+
         bnb_config = None
         if full_finetuning and (load_in_4bit or load_in_8bit):
             print(""Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA."")
@@ -170,13 +207,13 @@ class FastBaseModel:
                 load_in_4bit              = True,
                 bnb_4bit_use_double_quant = True,
                 bnb_4bit_quant_type       = ""nf4"",
-                bnb_4bit_compute_dtype    = dtype,
-                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+                bnb_4bit_compute_dtype    = bnb_compute_dtype,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
             )
         elif load_in_8bit:
             bnb_config = BitsAndBytesConfig(
                 load_in_8bit              = True,
-                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
             )
         elif not load_in_4bit and not load_in_8bit and not full_finetuning:
             print(""Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA."")
@@ -185,8 +222,8 @@ class FastBaseModel:
                 load_in_4bit              = True,
                 bnb_4bit_use_double_quant = True,
                 bnb_4bit_quant_type       = ""nf4"",
-                bnb_4bit_compute_dtype    = dtype,
-                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES,
+                bnb_4bit_compute_dtype    = bnb_compute_dtype,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
             )
         pass
 
@@ -212,7 +249,7 @@ class FastBaseModel:
             # quantization_config   = bnb_config,
             token                   = token,
             trust_remote_code       = trust_remote_code,
-            # attn_implementation   = ""sdpa"", [TODO] Pixtral for eg fails
+            attn_implementation     = ""sdpa"", #[TODO] Pixtral for eg fails
             **kwargs,
         )
         # Return old flag
@@ -408,12 +445,7 @@ class FastBaseModel:
 
         from transformers.trainer import Trainer 
         if Trainer._inner_training_loop.__name__ != ""_fast_inner_training_loop"":
-            raise RuntimeError(
-                'Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so '\
-                'enabling it will require much more work, so we have to prioritize. Please understand!\n'\
-                'We do have a separate beta version, which you can contact us about!\n'\
-                'Thank you for your understanding and we appreciate it immensely!'
-            )
+            raise RuntimeError('Unsloth: Unsuccessfully patched inner_training_loop')
         pass
         patch_saving_functions(model, vision = True)
 
"
"diff --git a/unsloth/models/_utils.py b/unsloth/models/_utils.py
index 0f0d4c1..36e4f51 100644
--- a/unsloth/models/_utils.py
+++ b/unsloth/models/_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = ""2025.3.1""
+__version__ = ""2025.3.2""
 
 __all__ = [
     ""SUPPORTS_BFLOAT16"",
diff --git a/unsloth/tokenizer_utils.py b/unsloth/tokenizer_utils.py
index 9c5f825..de9f73d 100644
--- a/unsloth/tokenizer_utils.py
+++ b/unsloth/tokenizer_utils.py
@@ -859,6 +859,7 @@ pass
 
 import inspect
 from inspect import getsource
+import trl
 import trl.trainer.sft_trainer
 from trl.trainer.sft_trainer import *
 from transformers.trainer import *
@@ -1046,3 +1047,9 @@ pass
 
 # Finally patch TRL tokenizer things -> moved to RL
 # patch_sft_trainer_tokenizer()
+
+# Temporary measure to stop tokenizing data twice
+if hasattr(trl, ""data_utils""):
+    def maybe_apply_chat_template(example, *args, **kwargs): return example
+    trl.data_utils.maybe_apply_chat_template = maybe_apply_chat_template
+pass
"
"diff --git a/unsloth/models/llama.py b/unsloth/models/llama.py
index 2b07e5f..65c7334 100644
--- a/unsloth/models/llama.py
+++ b/unsloth/models/llama.py
@@ -23,6 +23,7 @@ from ._utils import __version__
 from torch.nn.functional import scaled_dot_product_attention
 from transformers import __version__ as transformers_version
 from unsloth_zoo.utils import Version, _get_dtype
+from unsloth_zoo.peft_utils import SKIP_QUANTIZATION_MODULES
 transformers_version = Version(transformers_version)
 # Transformers moved rotary embeddings out of all attention layers
 IS_ATTENTION_REFACTOR = transformers_version > Version(""4.47.1"")
@@ -1771,6 +1772,7 @@ class FastLlamaModel:
                 bnb_4bit_use_double_quant = True,
                 bnb_4bit_quant_type       = ""nf4"",
                 bnb_4bit_compute_dtype    = dtype,
+                llm_int8_skip_modules     = SKIP_QUANTIZATION_MODULES.copy(),
             )
         pass
 
diff --git a/unsloth/models/loader_utils.py b/unsloth/models/loader_utils.py
index e3eadd8..ee389b7 100644
--- a/unsloth/models/loader_utils.py
+++ b/unsloth/models/loader_utils.py
@@ -19,6 +19,12 @@ from transformers import __version__ as transformers_version
 transformers_version = Version(transformers_version)
 SUPPORTS_FOURBIT = transformers_version >= Version(""4.37"")
 
+BAD_MAPPINGS = \
+{
+    ""unsloth/qwen3-32B-unsloth-bnb-4bit""     : ""unsloth/Qwen3-32B-bnb-4bit"", # 32B dynamic quant is way too big
+    ""unsloth/qwen3-30B-A3B-unsloth-bnb-4bit"" : ""unsloth/qwen3-30B-A3B"",      # HF loads MoEs too slowly
+    ""unsloth/qwen3-30B-A3B-bnb-4bit""         : ""unsloth/qwen3-30B-A3B"",      # We rather do it on the fly
+}
 
 def __get_model_name(
     model_name,
@@ -102,6 +108,11 @@ def get_model_name(model_name, load_in_4bit = True):
         FLOAT_TO_INT_MAPPER  = FLOAT_TO_INT_MAPPER,
         MAP_TO_UNSLOTH_16bit = MAP_TO_UNSLOTH_16bit,
     )
+    # In the rare case, we convert bad model names to other names
+    # For eg too large dynamic quants or MoEs
+    if new_model_name.lower() in BAD_MAPPINGS:
+        new_model_name = BAD_MAPPINGS[new_model_name.lower()]
+
     if new_model_name is None and model_name.count(""/"") == 1 and model_name[0].isalnum():
         # Try checking if a new Unsloth version allows it!
         NEW_INT_TO_FLOAT_MAPPER, NEW_FLOAT_TO_INT_MAPPER, NEW_MAP_TO_UNSLOTH_16bit = _get_new_mapper()
diff --git a/unsloth/models/mapper.py b/unsloth/models/mapper.py
index b3a7834..d723fc4 100644
--- a/unsloth/models/mapper.py
+++ b/unsloth/models/mapper.py
@@ -763,9 +763,15 @@ __INT_TO_FLOAT_MAPPER = \
         ""Qwen/Qwen3-14B"",
         ""unsloth/Qwen3-14B-bnb-4bit"",
     ),
-    ""unsloth/Qwen3-32B-bnb-4bit"" : (
+    ""unsloth/Qwen3-32B-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen3-32B"",
         ""Qwen/Qwen3-32B"",
+        ""unsloth/Qwen3-32B-bnb-4bit"",
+    ),
+    ""unsloth/Qwen3-30B-A3B-unsloth-bnb-4bit"" : (
+        ""unsloth/Qwen3-30B-A3B"",
+        ""Qwen/Qwen3-30B-A3B"",
+        ""unsloth/Qwen3-30B-A3B-bnb-4bit"",
     ),
     ""unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit"" : (
         ""unsloth/Qwen3-0.6B-Base"",
"
