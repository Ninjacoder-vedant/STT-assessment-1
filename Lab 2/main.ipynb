{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ee3275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydriller\n",
    "pydriller.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5b733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a7a236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import radon\n",
    "radon.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d944d484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hash</th>\n",
       "      <th>Overall_diff</th>\n",
       "      <th>Message</th>\n",
       "      <th>Filename</th>\n",
       "      <th>New Rectified Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>@@ -1,23 +1,25 @@\\n &lt;div class=\"align-center\"&gt;...</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>['README.md', 'Discord.png', 'LAION 2GPU.png',...</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2d5d88487463e76f75002be3b2704267ec96e68a</td>\n",
       "      <td>@@ -19,6 +19,14 @@ import warnings\\n import gc...</td>\n",
       "      <td>tokenizer pad fix</td>\n",
       "      <td>['_utils.py', 'llama.py', 'mistral.py']</td>\n",
       "      <td>Fix tokenizer pad token issue and add new util...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>f380cc1170447800c112dc8568bdff3dd34c79a3</td>\n",
       "      <td>@@ -7,7 +7,7 @@\\n ## 2-5x faster 60% less memo...</td>\n",
       "      <td>Fix Mistral\\n\\nBlockDiagonalCausalMask fix cou...</td>\n",
       "      <td>['README.md', 'mistral.py']</td>\n",
       "      <td>Fix Mistral attention mask: replace deprecated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>de855c2afa68ada67d8b3caad4eb9bb592bf4a4f</td>\n",
       "      <td>@@ -33,7 +33,7 @@ If you trained a model with ...</td>\n",
       "      <td>Small fixes (#48)\\n\\n* Fix generation for GQA\\...</td>\n",
       "      <td>['README.md', 'unsloth made with love.png', 'u...</td>\n",
       "      <td>Small fixes (#48)\\n\\n* Fix generation for GQA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>51dd120e354cd2223df7ebe2240fb6d1a76108c5</td>\n",
       "      <td>@@ -369,6 +369,7 @@ def LlamaModel_fast_forwar...</td>\n",
       "      <td>Fix RoPE Scaling issues (#52)\\n\\n* Fix RoPE Sc...</td>\n",
       "      <td>['llama.py', 'mistral.py']</td>\n",
       "      <td>Fix RoPE Scaling issues by updating max_positi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>341</td>\n",
       "      <td>26601f9d42b4c416efa59a062665c858b94c8673</td>\n",
       "      <td>@@ -37,7 +37,7 @@ triton = [\\n ]\\n \\n huggingf...</td>\n",
       "      <td>Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>['pyproject.toml', '__init__.py', 'import_fixe...</td>\n",
       "      <td>Bug fixes:\\n\\n* Update unsloth_zoo version fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>342</td>\n",
       "      <td>e45ddb55a7193be9df23bd72f7396849a0089b2b</td>\n",
       "      <td>@@ -185,7 +185,15 @@ def Qwen3Attention_fast_f...</td>\n",
       "      <td>fix is casual for qwen3 (#3213)</td>\n",
       "      <td>['qwen3.py']</td>\n",
       "      <td>fix is_causal logic for Qwen3: add proper caus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>343</td>\n",
       "      <td>b753ec05c1ae49ab2fedc0e252f73c829e36b442</td>\n",
       "      <td>@@ -149,9 +149,6 @@ class FastLanguageModel(Fa...</td>\n",
       "      <td>GPT OSS Bug fixes (#3231)\\n\\n* Update rl.py\\n\\...</td>\n",
       "      <td>['loader.py', 'rl.py']</td>\n",
       "      <td>Fix gradient checkpointing and data collator p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>344</td>\n",
       "      <td>9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e</td>\n",
       "      <td>@@ -0,0 +1,18 @@\\n+#!/bin/bash\\n+set -e\\n+\\n+e...</td>\n",
       "      <td>tests for mxfp4 and quantized models merge fix...</td>\n",
       "      <td>['run_test.sh', 'test_merged_model.py', 'train...</td>\n",
       "      <td>Add training and inference scripts with proper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>12737e503d32fc1464f8e38536be96d92db4f7d8</td>\n",
       "      <td>@@ -415,7 +415,8 @@ sampling_params = Sampling...</td>\n",
       "      <td>Fix incorrect function call in test_qwen3_grpo...</td>\n",
       "      <td>['test_qwen3_grpo.py']</td>\n",
       "      <td>Fix incorrect function call in test_qwen3_grpo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                      Hash  \\\n",
       "0             0  4b97a810b509c93f44be4c037c7aa18fb8922884   \n",
       "1             1  2d5d88487463e76f75002be3b2704267ec96e68a   \n",
       "2             2  f380cc1170447800c112dc8568bdff3dd34c79a3   \n",
       "3             3  de855c2afa68ada67d8b3caad4eb9bb592bf4a4f   \n",
       "4             4  51dd120e354cd2223df7ebe2240fb6d1a76108c5   \n",
       "..          ...                                       ...   \n",
       "341         341  26601f9d42b4c416efa59a062665c858b94c8673   \n",
       "342         342  e45ddb55a7193be9df23bd72f7396849a0089b2b   \n",
       "343         343  b753ec05c1ae49ab2fedc0e252f73c829e36b442   \n",
       "344         344  9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e   \n",
       "345         345  12737e503d32fc1464f8e38536be96d92db4f7d8   \n",
       "\n",
       "                                          Overall_diff  \\\n",
       "0    @@ -1,23 +1,25 @@\\n <div class=\"align-center\">...   \n",
       "1    @@ -19,6 +19,14 @@ import warnings\\n import gc...   \n",
       "2    @@ -7,7 +7,7 @@\\n ## 2-5x faster 60% less memo...   \n",
       "3    @@ -33,7 +33,7 @@ If you trained a model with ...   \n",
       "4    @@ -369,6 +369,7 @@ def LlamaModel_fast_forwar...   \n",
       "..                                                 ...   \n",
       "341  @@ -37,7 +37,7 @@ triton = [\\n ]\\n \\n huggingf...   \n",
       "342  @@ -185,7 +185,15 @@ def Qwen3Attention_fast_f...   \n",
       "343  @@ -149,9 +149,6 @@ class FastLanguageModel(Fa...   \n",
       "344  @@ -0,0 +1,18 @@\\n+#!/bin/bash\\n+set -e\\n+\\n+e...   \n",
       "345  @@ -415,7 +415,8 @@ sampling_params = Sampling...   \n",
       "\n",
       "                                               Message  \\\n",
       "0    Pre-release 2023 December version (Mistral, Pr...   \n",
       "1                                    tokenizer pad fix   \n",
       "2    Fix Mistral\\n\\nBlockDiagonalCausalMask fix cou...   \n",
       "3    Small fixes (#48)\\n\\n* Fix generation for GQA\\...   \n",
       "4    Fix RoPE Scaling issues (#52)\\n\\n* Fix RoPE Sc...   \n",
       "..                                                 ...   \n",
       "341  Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...   \n",
       "342                    fix is casual for qwen3 (#3213)   \n",
       "343  GPT OSS Bug fixes (#3231)\\n\\n* Update rl.py\\n\\...   \n",
       "344  tests for mxfp4 and quantized models merge fix...   \n",
       "345  Fix incorrect function call in test_qwen3_grpo...   \n",
       "\n",
       "                                              Filename  \\\n",
       "0    ['README.md', 'Discord.png', 'LAION 2GPU.png',...   \n",
       "1              ['_utils.py', 'llama.py', 'mistral.py']   \n",
       "2                          ['README.md', 'mistral.py']   \n",
       "3    ['README.md', 'unsloth made with love.png', 'u...   \n",
       "4                           ['llama.py', 'mistral.py']   \n",
       "..                                                 ...   \n",
       "341  ['pyproject.toml', '__init__.py', 'import_fixe...   \n",
       "342                                       ['qwen3.py']   \n",
       "343                             ['loader.py', 'rl.py']   \n",
       "344  ['run_test.sh', 'test_merged_model.py', 'train...   \n",
       "345                             ['test_qwen3_grpo.py']   \n",
       "\n",
       "                                 New Rectified Message  \n",
       "0    Pre-release 2023 December version (Mistral, Pr...  \n",
       "1    Fix tokenizer pad token issue and add new util...  \n",
       "2    Fix Mistral attention mask: replace deprecated...  \n",
       "3    Small fixes (#48)\\n\\n* Fix generation for GQA ...  \n",
       "4    Fix RoPE Scaling issues by updating max_positi...  \n",
       "..                                                 ...  \n",
       "341  Bug fixes:\\n\\n* Update unsloth_zoo version fro...  \n",
       "342  fix is_causal logic for Qwen3: add proper caus...  \n",
       "343  Fix gradient checkpointing and data collator p...  \n",
       "344  Add training and inference scripts with proper...  \n",
       "345  Fix incorrect function call in test_qwen3_grpo...  \n",
       "\n",
       "[346 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('Final_rectification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model in 4-bit (bnb) without unsloth\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae37f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load tokenizer (CPU is fine for this)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "\n",
    "# Load model and move it to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "for i in tqdm(range(5, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_msgs = df[\"Message\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    batch_diffs = df[\"Overall_diff\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "\n",
    "    preds = predict_batch(model, tokenizer, batch_msgs, batch_diffs, device=\"cuda\")\n",
    "\n",
    "    df.loc[i:i+len(preds)-1, \"Rectified Message\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "580efd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, time\n",
    "\n",
    "def predict(model, input_text, device=\"cuda\"):\n",
    "    # Tokenize (truncate to 512 tokens)\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=5,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            max_length=64,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken:\", end_time - start_time, \"seconds.\")\n",
    "\n",
    "    # Move to CPU before decoding\n",
    "    prediction = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
    "\n",
    "    # Free memory\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177f273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"https://github.com/unslothai/unsloth\"\n",
    "L=[]\n",
    "for commit_hash in concat_df['Hash'].to_list():\n",
    "    commit_diff = get_commit_diff(repo_path, commit_hash)\n",
    "    L.append(commit_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
