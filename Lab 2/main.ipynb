{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:39:52.261052Z",
     "iopub.status.busy": "2025-08-22T16:39:52.260782Z",
     "iopub.status.idle": "2025-08-22T16:39:52.264715Z",
     "shell.execute_reply": "2025-08-22T16:39:52.264015Z",
     "shell.execute_reply.started": "2025-08-22T16:39:52.261029Z"
    }
   },
   "source": [
    "### Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:16:00.496658Z",
     "iopub.status.busy": "2025-09-03T20:16:00.496264Z",
     "iopub.status.idle": "2025-09-03T20:16:15.325925Z",
     "shell.execute_reply": "2025-09-03T20:16:15.325154Z",
     "shell.execute_reply.started": "2025-09-03T20:16:00.496636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydriller\n",
      "  Downloading pydriller-2.8-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from pydriller) (3.1.44)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from pydriller) (2025.2)\n",
      "Requirement already satisfied: types-pytz in /usr/local/lib/python3.11/dist-packages (from pydriller) (2025.2.0.20250516)\n",
      "Collecting lizard==1.17.10 (from pydriller)\n",
      "  Downloading lizard-1.17.10-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->pydriller) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->pydriller) (5.0.2)\n",
      "Downloading pydriller-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading lizard-1.17.10-py2.py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lizard, pydriller\n",
      "Successfully installed lizard-1.17.10 pydriller-2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pydriller\n",
    "import pandas as pd\n",
    "from pydriller import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:48:36.047256Z",
     "iopub.status.busy": "2025-09-05T08:48:36.046983Z",
     "iopub.status.idle": "2025-09-05T08:48:36.194759Z",
     "shell.execute_reply": "2025-09-05T08:48:36.193509Z",
     "shell.execute_reply.started": "2025-09-05T08:48:36.047226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.34.1\n"
     ]
    }
   ],
   "source": [
    "!git --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travering Repository using Pydriller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T18:37:51.610466Z",
     "iopub.status.busy": "2025-09-05T18:37:51.610149Z",
     "iopub.status.idle": "2025-09-05T18:37:51.615709Z",
     "shell.execute_reply": "2025-09-05T18:37:51.614805Z",
     "shell.execute_reply.started": "2025-09-05T18:37:51.610441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Keywords used to identify bug-fix commits\n",
    "keywords = [\"fixed\", \"bug\", \"fixes\", \"fix\", \"crash\", \"solves\", \"resolves\", \"issue\", \"regression\", \"fall back\", \"assertion\", \"coverity\", \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"failur\", \"fail\", \"npe\", \"except\",\n",
    "\"broken\", \"differential testing\", \"error\", \"hang\", \"test fix\", \"steps to reproduce\", \"crash\", \"assertion\", \"failure\", \"leak\", \"stack trace\", \"heap overflow\", \"freez\", \"problem\", \"overflow\", \"avoid\", \"workaround\", \"break\", \"stop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T18:38:26.652993Z",
     "iopub.status.busy": "2025-09-05T18:38:26.652715Z",
     "iopub.status.idle": "2025-09-05T18:38:37.891490Z",
     "shell.execute_reply": "2025-09-05T18:38:37.890452Z",
     "shell.execute_reply.started": "2025-09-05T18:38:26.652972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pydriller import Repository\n",
    "import pandas as pd\n",
    "bug_fix_df = []\n",
    "diff_analysis_df = []\n",
    "\n",
    "Repo_path = \"/kaggle/working/unsloth\"\n",
    "# Traversing commits using PyDriller\n",
    "for commit in Repository(Repo_path).traverse_commits():\n",
    "    for keyword in keywords:\n",
    "        if keyword in commit.msg:\n",
    "            bug_fix_df.append({\n",
    "                'Hash': commit.hash,\n",
    "                'Author': commit.author.name,\n",
    "                'Message': commit.msg,\n",
    "                'Hashes of parents': commit.parents,\n",
    "                'Is a merge commit?': len(commit.parents) > 1,\n",
    "                'List of modified files': [mod.filename for mod in commit.modified_files],\n",
    "            })\n",
    "\n",
    "            if commit.modified_files:  # normal case\n",
    "                for mod in commit.modified_files:\n",
    "                    diff_analysis_df.append({\n",
    "                        'Hash': commit.hash,\n",
    "                        'Author': commit.author.name,\n",
    "                        'Message': commit.msg,\n",
    "                        'Filename': mod.filename,\n",
    "                        'Change Type': mod.change_type.name,\n",
    "                        'Source Code (before)': mod.source_code_before,\n",
    "                        'Source Code (current)': mod.source_code,\n",
    "                        'Diff': mod.diff\n",
    "                    })\n",
    "            break\n",
    "# Create DataFrames\n",
    "bug_fix_df = pd.DataFrame(bug_fix_df)\n",
    "diff_analysis_df = pd.DataFrame(diff_analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T18:38:40.317157Z",
     "iopub.status.busy": "2025-09-05T18:38:40.316716Z",
     "iopub.status.idle": "2025-09-05T18:38:40.357249Z",
     "shell.execute_reply": "2025-09-05T18:38:40.356322Z",
     "shell.execute_reply.started": "2025-09-05T18:38:40.317128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Hashes of parents</th>\n",
       "      <th>Is a merge commit?</th>\n",
       "      <th>List of modified files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>[3aa16bb452ab82d7a2b2987ec3bfb47c6812582c]</td>\n",
       "      <td>False</td>\n",
       "      <td>[README.md, Discord.png, LAION 2GPU.png, LAION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d5d88487463e76f75002be3b2704267ec96e68a</td>\n",
       "      <td>Daniel Han-Chen</td>\n",
       "      <td>tokenizer pad fix</td>\n",
       "      <td>[28f3b971d21e469fb985f384db10a03982c4ce12]</td>\n",
       "      <td>False</td>\n",
       "      <td>[_utils.py, llama.py, mistral.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f380cc1170447800c112dc8568bdff3dd34c79a3</td>\n",
       "      <td>Daniel Han-Chen</td>\n",
       "      <td>Fix Mistral\\n\\nBlockDiagonalCausalMask fix cou...</td>\n",
       "      <td>[399f8ed56f40df0919208d1ffdee64a31a1b22c8]</td>\n",
       "      <td>False</td>\n",
       "      <td>[README.md, mistral.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de855c2afa68ada67d8b3caad4eb9bb592bf4a4f</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Small fixes (#48)\\n\\n* Fix generation for GQA\\...</td>\n",
       "      <td>[725e581539a0755beb23aaff684608db4e42160a]</td>\n",
       "      <td>False</td>\n",
       "      <td>[README.md, unsloth made with love.png, unslot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51dd120e354cd2223df7ebe2240fb6d1a76108c5</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Fix RoPE Scaling issues (#52)\\n\\n* Fix RoPE Sc...</td>\n",
       "      <td>[627acc4bb37d5a0354a86c0783be20162f0940b2]</td>\n",
       "      <td>False</td>\n",
       "      <td>[llama.py, mistral.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>91b2a22548b99e034f89c2a76ca5b4eb59f41f76</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Update import_fixes.py</td>\n",
       "      <td>[a656a3df6e467556f19631e4b25da4b61b2a01fd]</td>\n",
       "      <td>False</td>\n",
       "      <td>[import_fixes.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>7ce3e3b2ce78bfa0074143b852eb9ab0decbafb7</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>fixed save_pretrained_torchao and associated t...</td>\n",
       "      <td>[91b2a22548b99e034f89c2a76ca5b4eb59f41f76]</td>\n",
       "      <td>False</td>\n",
       "      <td>[test_unsloth_save.py, mapper.py, save.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>978f73ed862ac1c10954cb27eb30e543f77d0421</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>[cd71b1bb964098b0a2d8451eac48be53e08568b5]</td>\n",
       "      <td>False</td>\n",
       "      <td>[pyproject.toml, __init__.py, llama.py, mistra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>ff6cbb03d0c2d11fe15c844bfc635eb24f533243</td>\n",
       "      <td>DoubleMathew</td>\n",
       "      <td>llama vision inference fix (#3270)\\n\\n* llama ...</td>\n",
       "      <td>[c19a002a779ab08f207e0316013b16ea32885b3c]</td>\n",
       "      <td>False</td>\n",
       "      <td>[vision.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>551bc65235aaf4ce1d00b48fed79eb48d759507b</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>Add TorchAO quantization tests with FP16 model...</td>\n",
       "      <td>[ff6cbb03d0c2d11fe15c844bfc635eb24f533243]</td>\n",
       "      <td>False</td>\n",
       "      <td>[test_unsloth_save.py]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Hash           Author  \\\n",
       "0    4b97a810b509c93f44be4c037c7aa18fb8922884       Daniel Han   \n",
       "1    2d5d88487463e76f75002be3b2704267ec96e68a  Daniel Han-Chen   \n",
       "2    f380cc1170447800c112dc8568bdff3dd34c79a3  Daniel Han-Chen   \n",
       "3    de855c2afa68ada67d8b3caad4eb9bb592bf4a4f       Daniel Han   \n",
       "4    51dd120e354cd2223df7ebe2240fb6d1a76108c5       Daniel Han   \n",
       "..                                        ...              ...   \n",
       "364  91b2a22548b99e034f89c2a76ca5b4eb59f41f76       Daniel Han   \n",
       "365  7ce3e3b2ce78bfa0074143b852eb9ab0decbafb7   Roland Tannous   \n",
       "366  978f73ed862ac1c10954cb27eb30e543f77d0421       Daniel Han   \n",
       "367  ff6cbb03d0c2d11fe15c844bfc635eb24f533243     DoubleMathew   \n",
       "368  551bc65235aaf4ce1d00b48fed79eb48d759507b   Roland Tannous   \n",
       "\n",
       "                                               Message  \\\n",
       "0    Pre-release 2023 December version (Mistral, Pr...   \n",
       "1                                    tokenizer pad fix   \n",
       "2    Fix Mistral\\n\\nBlockDiagonalCausalMask fix cou...   \n",
       "3    Small fixes (#48)\\n\\n* Fix generation for GQA\\...   \n",
       "4    Fix RoPE Scaling issues (#52)\\n\\n* Fix RoPE Sc...   \n",
       "..                                                 ...   \n",
       "364                             Update import_fixes.py   \n",
       "365  fixed save_pretrained_torchao and associated t...   \n",
       "366  Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...   \n",
       "367  llama vision inference fix (#3270)\\n\\n* llama ...   \n",
       "368  Add TorchAO quantization tests with FP16 model...   \n",
       "\n",
       "                              Hashes of parents  Is a merge commit?  \\\n",
       "0    [3aa16bb452ab82d7a2b2987ec3bfb47c6812582c]               False   \n",
       "1    [28f3b971d21e469fb985f384db10a03982c4ce12]               False   \n",
       "2    [399f8ed56f40df0919208d1ffdee64a31a1b22c8]               False   \n",
       "3    [725e581539a0755beb23aaff684608db4e42160a]               False   \n",
       "4    [627acc4bb37d5a0354a86c0783be20162f0940b2]               False   \n",
       "..                                          ...                 ...   \n",
       "364  [a656a3df6e467556f19631e4b25da4b61b2a01fd]               False   \n",
       "365  [91b2a22548b99e034f89c2a76ca5b4eb59f41f76]               False   \n",
       "366  [cd71b1bb964098b0a2d8451eac48be53e08568b5]               False   \n",
       "367  [c19a002a779ab08f207e0316013b16ea32885b3c]               False   \n",
       "368  [ff6cbb03d0c2d11fe15c844bfc635eb24f533243]               False   \n",
       "\n",
       "                                List of modified files  \n",
       "0    [README.md, Discord.png, LAION 2GPU.png, LAION...  \n",
       "1                    [_utils.py, llama.py, mistral.py]  \n",
       "2                              [README.md, mistral.py]  \n",
       "3    [README.md, unsloth made with love.png, unslot...  \n",
       "4                               [llama.py, mistral.py]  \n",
       "..                                                 ...  \n",
       "364                                  [import_fixes.py]  \n",
       "365         [test_unsloth_save.py, mapper.py, save.py]  \n",
       "366  [pyproject.toml, __init__.py, llama.py, mistra...  \n",
       "367                                        [vision.py]  \n",
       "368                             [test_unsloth_save.py]  \n",
       "\n",
       "[369 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_fix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T19:02:28.108824Z",
     "iopub.status.busy": "2025-09-05T19:02:28.108465Z",
     "iopub.status.idle": "2025-09-05T19:02:28.132798Z",
     "shell.execute_reply": "2025-09-05T19:02:28.131375Z",
     "shell.execute_reply.started": "2025-09-05T19:02:28.108802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Change Type</th>\n",
       "      <th>Source Code (before)</th>\n",
       "      <th>Source Code (current)</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>README.md</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>@@ -1,23 +1,25 @@\\n &lt;div class=\"align-center\"&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>Discord.png</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0006\u0000\u0000\u0001\b\u0006\u0000\u0000\u0000\u000e\u0006\u0000\u0000\u0000\u0001sRGB\u0000\u001c\u0000\u0000\u0000\u0004...</td>\n",
       "      <td>PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003}\u0000\u0000\u0001\u0004\b\u0006\u0000\u0000\u00008]\u0000\u0000 \u0000IDATx\\t\u0014...</td>\n",
       "      <td>Binary files a/images/Discord.png and b/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.png</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003\u0000\u0000\u0002A\b\u0006\u0000\u0000\u0001\u0011\u0000\u0000ȫIDATx\u0001\u000b\\k]\\...</td>\n",
       "      <td>Binary files /dev/null and b/images/LAION 2GPU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>None</td>\n",
       "      <td>@@ -1,1518 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>SlimOrca 1GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>None</td>\n",
       "      <td>@@ -1,1424 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>978f73ed862ac1c10954cb27eb30e543f77d0421</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>llama.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -1236,7 +1236,7 @@ def CausalLM_fast_forwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>978f73ed862ac1c10954cb27eb30e543f77d0421</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>mistral.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -300,17 +300,30 @@ def MistralForCausalLM_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>978f73ed862ac1c10954cb27eb30e543f77d0421</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>rl.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -513,7 +513,7 @@ def _patch_trl_rl_trainers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>ff6cbb03d0c2d11fe15c844bfc635eb24f533243</td>\n",
       "      <td>DoubleMathew</td>\n",
       "      <td>llama vision inference fix (#3270)\\n\\n* llama ...</td>\n",
       "      <td>vision.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -206,7 +206,7 @@ def unsloth_base_fast_gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>551bc65235aaf4ce1d00b48fed79eb48d759507b</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>Add TorchAO quantization tests with FP16 model...</td>\n",
       "      <td>test_unsloth_save.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>import json\\nimport os\\nimport shutil\\nimport ...</td>\n",
       "      <td>import json\\nimport os\\nimport shutil\\nimport ...</td>\n",
       "      <td>@@ -22,6 +22,15 @@ model_to_test = [\\n     \"un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1247 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Hash          Author  \\\n",
       "0     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "1     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "2     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "3     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "4     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "...                                        ...             ...   \n",
       "1242  978f73ed862ac1c10954cb27eb30e543f77d0421      Daniel Han   \n",
       "1243  978f73ed862ac1c10954cb27eb30e543f77d0421      Daniel Han   \n",
       "1244  978f73ed862ac1c10954cb27eb30e543f77d0421      Daniel Han   \n",
       "1245  ff6cbb03d0c2d11fe15c844bfc635eb24f533243    DoubleMathew   \n",
       "1246  551bc65235aaf4ce1d00b48fed79eb48d759507b  Roland Tannous   \n",
       "\n",
       "                                                Message              Filename  \\\n",
       "0     Pre-release 2023 December version (Mistral, Pr...             README.md   \n",
       "1     Pre-release 2023 December version (Mistral, Pr...           Discord.png   \n",
       "2     Pre-release 2023 December version (Mistral, Pr...        LAION 2GPU.png   \n",
       "3     Pre-release 2023 December version (Mistral, Pr...        LAION 2GPU.svg   \n",
       "4     Pre-release 2023 December version (Mistral, Pr...     SlimOrca 1GPU.svg   \n",
       "...                                                 ...                   ...   \n",
       "1242  Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...              llama.py   \n",
       "1243  Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...            mistral.py   \n",
       "1244  Bug fixes (#3266)\\n\\n* Fix mamba\\n\\n* Update l...                 rl.py   \n",
       "1245  llama vision inference fix (#3270)\\n\\n* llama ...             vision.py   \n",
       "1246  Add TorchAO quantization tests with FP16 model...  test_unsloth_save.py   \n",
       "\n",
       "     Change Type                               Source Code (before)  \\\n",
       "0         MODIFY  <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1         MODIFY  PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0006\u0000\u0000\u0001\b\u0006\u0000\u0000\u0000\u000e\u0006\u0000\u0000\u0000\u0001sRGB\u0000\n",
       "\u0000\u0000\u0000\u0004...   \n",
       "2            ADD                                               None   \n",
       "3         DELETE  <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "4         DELETE  <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "...          ...                                                ...   \n",
       "1242      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1243      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1244      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1245      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1246      MODIFY  import json\\nimport os\\nimport shutil\\nimport ...   \n",
       "\n",
       "                                  Source Code (current)  \\\n",
       "0     <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1     PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003}\u0000\u0000\u0001\u0004\b\u0006\u0000\u0000\u00008]\u0000\u0000 \u0000IDATx\\t\u0014...   \n",
       "2     PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003\u0000\u0000\u0002A\b\u0006\u0000\u0000\u0001\u0011\u0000\u0000ȫIDATx\u0001\n",
       "\\k]\\...   \n",
       "3                                                  None   \n",
       "4                                                  None   \n",
       "...                                                 ...   \n",
       "1242  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1243  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1244  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1245  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1246  import json\\nimport os\\nimport shutil\\nimport ...   \n",
       "\n",
       "                                                   Diff  \n",
       "0     @@ -1,23 +1,25 @@\\n <div class=\"align-center\">...  \n",
       "1     Binary files a/images/Discord.png and b/images...  \n",
       "2     Binary files /dev/null and b/images/LAION 2GPU...  \n",
       "3     @@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encod...  \n",
       "4     @@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encod...  \n",
       "...                                                 ...  \n",
       "1242  @@ -1236,7 +1236,7 @@ def CausalLM_fast_forwar...  \n",
       "1243  @@ -300,17 +300,30 @@ def MistralForCausalLM_f...  \n",
       "1244  @@ -513,7 +513,7 @@ def _patch_trl_rl_trainers...  \n",
       "1245  @@ -206,7 +206,7 @@ def unsloth_base_fast_gene...  \n",
       "1246  @@ -22,6 +22,15 @@ model_to_test = [\\n     \"un...  \n",
       "\n",
       "[1247 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T18:39:33.264025Z",
     "iopub.status.busy": "2025-09-05T18:39:33.263713Z",
     "iopub.status.idle": "2025-09-05T18:39:52.537955Z",
     "shell.execute_reply": "2025-09-05T18:39:52.536394Z",
     "shell.execute_reply.started": "2025-09-05T18:39:33.264006Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652a122b49f2429b8fc2dcbd8b194e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20b09ddbe9243808392c4d45a830d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2215e4643db487db463c4ab3e1aa9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14962bbcf3e844e3958d9ca19e46dd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b75adba21024ca58992350c06d2ddbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc538e991f574b26996e2f0d65b766db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 18:39:51.613955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757097591.828956      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757097591.888682      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1200847653.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using device:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mamiksik/CommitPredictorT5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mTokenClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_LAYERNORM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_pruneable_heads_and_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_linear_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m from ...utils import (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mverify_tp_plan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m from .pytorch_utils import (  # noqa: F401\n\u001b[1;32m     75\u001b[0m     \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_d_fine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDFineForObjectDetectionLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_d_fine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .loss_for_object_detection import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbox_iou\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_for_object_detection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_KerasLazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mself_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/platform/self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# incompatibilities before we trigger them (which would typically result in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# SIGILL).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_cpu_feature_guard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0m_pywrap_cpu_feature_guard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInfoAboutUnusedCPUFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "\n",
    "# Loading model and moving it to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, time\n",
    "def predict_batch(model, texts):\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)   \n",
    "\n",
    "    print(\"Batch size:\", len(texts))\n",
    "    print(\"Tokenized shape:\", inputs['input_ids'].shape)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=5,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            max_length=64,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken:\", end_time - start_time, \"seconds.\")\n",
    "\n",
    "    predictions = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
    "\n",
    "    # cleanup\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:17:11.023432Z",
     "iopub.status.busy": "2025-09-03T20:17:11.022947Z",
     "iopub.status.idle": "2025-09-03T20:23:10.353989Z",
     "shell.execute_reply": "2025-09-03T20:23:10.353155Z",
     "shell.execute_reply.started": "2025-09-03T20:17:11.023407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 20 : 5.36007833480835 seconds.\n",
      "Time taken for batch of 20 : 5.279646873474121 seconds.\n",
      "Time taken for batch of 20 : 4.7002716064453125 seconds.\n",
      "Time taken for batch of 20 : 5.556906461715698 seconds.\n",
      "Time taken for batch of 20 : 5.836158752441406 seconds.\n",
      "Time taken for batch of 20 : 5.273885726928711 seconds.\n",
      "Time taken for batch of 20 : 5.291773557662964 seconds.\n",
      "Time taken for batch of 20 : 5.861851692199707 seconds.\n",
      "Time taken for batch of 20 : 5.017362356185913 seconds.\n",
      "Time taken for batch of 20 : 4.471691370010376 seconds.\n",
      "Time taken for batch of 20 : 5.034398794174194 seconds.\n",
      "Time taken for batch of 20 : 5.0324387550354 seconds.\n",
      "Time taken for batch of 20 : 5.047799587249756 seconds.\n",
      "Time taken for batch of 20 : 5.031895637512207 seconds.\n",
      "Time taken for batch of 20 : 5.311582088470459 seconds.\n",
      "Time taken for batch of 20 : 5.620614051818848 seconds.\n",
      "Time taken for batch of 20 : 5.081935167312622 seconds.\n",
      "Time taken for batch of 20 : 5.056572675704956 seconds.\n",
      "Time taken for batch of 20 : 4.79361629486084 seconds.\n",
      "Time taken for batch of 20 : 5.073583364486694 seconds.\n",
      "Time taken for batch of 20 : 5.079817295074463 seconds.\n",
      "Time taken for batch of 20 : 6.203255891799927 seconds.\n",
      "Time taken for batch of 20 : 5.094135522842407 seconds.\n",
      "Time taken for batch of 20 : 5.6601951122283936 seconds.\n",
      "Time taken for batch of 20 : 6.787935495376587 seconds.\n",
      "Time taken for batch of 20 : 4.552196502685547 seconds.\n",
      "Time taken for batch of 20 : 5.38364052772522 seconds.\n",
      "Time taken for batch of 20 : 6.265027761459351 seconds.\n",
      "Time taken for batch of 20 : 5.1344568729400635 seconds.\n",
      "Time taken for batch of 20 : 6.263135194778442 seconds.\n",
      "Time taken for batch of 20 : 4.862339019775391 seconds.\n",
      "Time taken for batch of 20 : 5.700581789016724 seconds.\n",
      "Time taken for batch of 20 : 4.844019412994385 seconds.\n",
      "Time taken for batch of 20 : 5.411619186401367 seconds.\n",
      "Time taken for batch of 20 : 4.841052055358887 seconds.\n",
      "Time taken for batch of 20 : 5.6445534229278564 seconds.\n",
      "Time taken for batch of 20 : 5.6668806076049805 seconds.\n",
      "Time taken for batch of 20 : 4.827131748199463 seconds.\n",
      "Time taken for batch of 20 : 5.3986780643463135 seconds.\n",
      "Time taken for batch of 20 : 4.842013835906982 seconds.\n",
      "Time taken for batch of 20 : 5.657012224197388 seconds.\n",
      "Time taken for batch of 20 : 5.113498687744141 seconds.\n",
      "Time taken for batch of 20 : 5.695874214172363 seconds.\n",
      "Time taken for batch of 20 : 5.7062530517578125 seconds.\n",
      "Time taken for batch of 20 : 4.851161479949951 seconds.\n",
      "Time taken for batch of 20 : 5.132807731628418 seconds.\n",
      "Time taken for batch of 20 : 5.418392181396484 seconds.\n",
      "Time taken for batch of 20 : 4.839756011962891 seconds.\n",
      "Time taken for batch of 20 : 4.847575902938843 seconds.\n",
      "Time taken for batch of 20 : 6.51946759223938 seconds.\n",
      "Time taken for batch of 20 : 5.670523405075073 seconds.\n",
      "Time taken for batch of 20 : 5.669216871261597 seconds.\n",
      "Time taken for batch of 20 : 5.928407669067383 seconds.\n",
      "Time taken for batch of 20 : 6.245543003082275 seconds.\n",
      "Time taken for batch of 20 : 4.8553078174591064 seconds.\n",
      "Time taken for batch of 20 : 5.1147894859313965 seconds.\n",
      "Time taken for batch of 20 : 5.416203022003174 seconds.\n",
      "Time taken for batch of 20 : 5.387255907058716 seconds.\n",
      "Time taken for batch of 20 : 5.114590167999268 seconds.\n",
      "Time taken for batch of 20 : 7.913918733596802 seconds.\n",
      "Time taken for batch of 20 : 5.399469614028931 seconds.\n",
      "Time taken for batch of 15 : 4.302406549453735 seconds.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20 # tune based on GPU memory\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(0, len(diff_analysis_df), BATCH_SIZE):\n",
    "    batch_texts = diff_analysis_df[\"Diff\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    preds = predict_batch(model, batch_texts, device=\"cuda\")\n",
    "    all_predictions.extend(preds)\n",
    "\n",
    "# Adding as a new column\n",
    "diff_analysis_df[\"LLM_inference\"] = all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:28:07.449707Z",
     "iopub.status.busy": "2025-09-03T20:28:07.448634Z",
     "iopub.status.idle": "2025-09-03T20:28:07.479642Z",
     "shell.execute_reply": "2025-09-03T20:28:07.478916Z",
     "shell.execute_reply.started": "2025-09-03T20:28:07.449678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Change Type</th>\n",
       "      <th>Source Code (before)</th>\n",
       "      <th>Source Code (current)</th>\n",
       "      <th>Diff</th>\n",
       "      <th>LLM_inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>README.md</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>@@ -1,23 +1,25 @@\\n &lt;div class=\"align-center\"&gt;...</td>\n",
       "      <td>add more info about nvidia gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>Discord.png</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0006\u0000\u0000\u0001\b\u0006\u0000\u0000\u0000\u000e\u0006\u0000\u0000\u0000\u0001sRGB\u0000\u001c\u0000\u0000\u0000\u0004...</td>\n",
       "      <td>PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003}\u0000\u0000\u0001\u0004\b\u0006\u0000\u0000\u00008]\u0000\u0000 \u0000IDATx\\t\u0014...</td>\n",
       "      <td>Binary files a/images/Discord.png and b/images...</td>\n",
       "      <td>distro binary files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.png</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003\u0000\u0000\u0002A\b\u0006\u0000\u0000\u0001\u0011\u0000\u0000ȫIDATx\u0001\u000b\\k]\\...</td>\n",
       "      <td>Binary files /dev/null and b/images/LAION 2GPU...</td>\n",
       "      <td>distinguish 2gpu image from null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>None</td>\n",
       "      <td>@@ -1,1518 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "      <td>add missing missing nodes in skeleton skeleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>SlimOrca 1GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>None</td>\n",
       "      <td>@@ -1,1424 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "      <td>add missing missing svg elements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>tests for mxfp4 and quantized models merge fix...</td>\n",
       "      <td>run_test.sh</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>#!/bin/bash\\nset -e\\n\\necho \"=================...</td>\n",
       "      <td>@@ -0,0 +1,18 @@\\n+#!/bin/bash\\n+set -e\\n+\\n+e...</td>\n",
       "      <td>add missing line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>tests for mxfp4 and quantized models merge fix...</td>\n",
       "      <td>test_merged_model.py</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td># inference_on_merged.py\\nfrom unsloth import ...</td>\n",
       "      <td>@@ -0,0 +1,55 @@\\n+# inference_on_merged.py\\n+...</td>\n",
       "      <td>add test for merge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>tests for mxfp4 and quantized models merge fix...</td>\n",
       "      <td>train_and_merge.py</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td># train_and_merge.py\\nfrom unsloth import Fast...</td>\n",
       "      <td>@@ -0,0 +1,71 @@\\n+# train_and_merge.py\\n+from...</td>\n",
       "      <td>add examples to train_and_merge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e</td>\n",
       "      <td>Roland Tannous</td>\n",
       "      <td>tests for mxfp4 and quantized models merge fix...</td>\n",
       "      <td>test_merge_4bit_validation.py</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>from unsloth import FastLanguageModel\\nfrom un...</td>\n",
       "      <td>@@ -0,0 +1,223 @@\\n+from unsloth import FastLa...</td>\n",
       "      <td>add test case for formatting prompts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>12737e503d32fc1464f8e38536be96d92db4f7d8</td>\n",
       "      <td>stevenxdavis</td>\n",
       "      <td>Fix incorrect function call in test_qwen3_grpo...</td>\n",
       "      <td>test_qwen3_grpo.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>from unsloth import FastLanguageModel\\nimport ...</td>\n",
       "      <td>from unsloth import FastLanguageModel\\nimport ...</td>\n",
       "      <td>@@ -415,7 +415,8 @@ sampling_params = Sampling...</td>\n",
       "      <td>update sample_params.rb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1235 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Hash          Author  \\\n",
       "0     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "1     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "2     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "3     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "4     4b97a810b509c93f44be4c037c7aa18fb8922884      Daniel Han   \n",
       "...                                        ...             ...   \n",
       "1230  9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e  Roland Tannous   \n",
       "1231  9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e  Roland Tannous   \n",
       "1232  9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e  Roland Tannous   \n",
       "1233  9050d6f5278e2373e66d17fc9ac8c2ec4f986e7e  Roland Tannous   \n",
       "1234  12737e503d32fc1464f8e38536be96d92db4f7d8    stevenxdavis   \n",
       "\n",
       "                                                Message  \\\n",
       "0     Pre-release 2023 December version (Mistral, Pr...   \n",
       "1     Pre-release 2023 December version (Mistral, Pr...   \n",
       "2     Pre-release 2023 December version (Mistral, Pr...   \n",
       "3     Pre-release 2023 December version (Mistral, Pr...   \n",
       "4     Pre-release 2023 December version (Mistral, Pr...   \n",
       "...                                                 ...   \n",
       "1230  tests for mxfp4 and quantized models merge fix...   \n",
       "1231  tests for mxfp4 and quantized models merge fix...   \n",
       "1232  tests for mxfp4 and quantized models merge fix...   \n",
       "1233  tests for mxfp4 and quantized models merge fix...   \n",
       "1234  Fix incorrect function call in test_qwen3_grpo...   \n",
       "\n",
       "                           Filename Change Type  \\\n",
       "0                         README.md      MODIFY   \n",
       "1                       Discord.png      MODIFY   \n",
       "2                    LAION 2GPU.png         ADD   \n",
       "3                    LAION 2GPU.svg      DELETE   \n",
       "4                 SlimOrca 1GPU.svg      DELETE   \n",
       "...                             ...         ...   \n",
       "1230                    run_test.sh         ADD   \n",
       "1231           test_merged_model.py         ADD   \n",
       "1232             train_and_merge.py         ADD   \n",
       "1233  test_merge_4bit_validation.py         ADD   \n",
       "1234             test_qwen3_grpo.py      MODIFY   \n",
       "\n",
       "                                   Source Code (before)  \\\n",
       "0     <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1     PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0006\u0000\u0000\u0001\b\u0006\u0000\u0000\u0000\u000e\u0006\u0000\u0000\u0000\u0001sRGB\u0000\n",
       "\u0000\u0000\u0000\u0004...   \n",
       "2                                                  None   \n",
       "3     <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "4     <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "...                                                 ...   \n",
       "1230                                               None   \n",
       "1231                                               None   \n",
       "1232                                               None   \n",
       "1233                                               None   \n",
       "1234  from unsloth import FastLanguageModel\\nimport ...   \n",
       "\n",
       "                                  Source Code (current)  \\\n",
       "0     <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1     PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003}\u0000\u0000\u0001\u0004\b\u0006\u0000\u0000\u00008]\u0000\u0000 \u0000IDATx\\t\u0014...   \n",
       "2     PNG\\r\\n\u001a\\n\u0000\u0000\u0000\\rIHDR\u0000\u0000\u0003\u0000\u0000\u0002A\b\u0006\u0000\u0000\u0001\u0011\u0000\u0000ȫIDATx\u0001\n",
       "\\k]\\...   \n",
       "3                                                  None   \n",
       "4                                                  None   \n",
       "...                                                 ...   \n",
       "1230  #!/bin/bash\\nset -e\\n\\necho \"=================...   \n",
       "1231  # inference_on_merged.py\\nfrom unsloth import ...   \n",
       "1232  # train_and_merge.py\\nfrom unsloth import Fast...   \n",
       "1233  from unsloth import FastLanguageModel\\nfrom un...   \n",
       "1234  from unsloth import FastLanguageModel\\nimport ...   \n",
       "\n",
       "                                                   Diff  \\\n",
       "0     @@ -1,23 +1,25 @@\\n <div class=\"align-center\">...   \n",
       "1     Binary files a/images/Discord.png and b/images...   \n",
       "2     Binary files /dev/null and b/images/LAION 2GPU...   \n",
       "3     @@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encod...   \n",
       "4     @@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encod...   \n",
       "...                                                 ...   \n",
       "1230  @@ -0,0 +1,18 @@\\n+#!/bin/bash\\n+set -e\\n+\\n+e...   \n",
       "1231  @@ -0,0 +1,55 @@\\n+# inference_on_merged.py\\n+...   \n",
       "1232  @@ -0,0 +1,71 @@\\n+# train_and_merge.py\\n+from...   \n",
       "1233  @@ -0,0 +1,223 @@\\n+from unsloth import FastLa...   \n",
       "1234  @@ -415,7 +415,8 @@ sampling_params = Sampling...   \n",
       "\n",
       "                                       LLM_inference  \n",
       "0                    add more info about nvidia gpus  \n",
       "1                                distro binary files  \n",
       "2                   distinguish 2gpu image from null  \n",
       "3     add missing missing nodes in skeleton skeleton  \n",
       "4                   add missing missing svg elements  \n",
       "...                                              ...  \n",
       "1230                                add missing line  \n",
       "1231                              add test for merge  \n",
       "1232                 add examples to train_and_merge  \n",
       "1233            add test case for formatting prompts  \n",
       "1234                         update sample_params.rb  \n",
       "\n",
       "[1235 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:28:27.587352Z",
     "iopub.status.busy": "2025-09-03T20:28:27.586420Z",
     "iopub.status.idle": "2025-09-03T20:28:27.621219Z",
     "shell.execute_reply": "2025-09-03T20:28:27.620639Z",
     "shell.execute_reply.started": "2025-09-03T20:28:27.587323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df = diff_analysis_df.groupby('Hash',sort=False)['Diff'].apply(list).reset_index()\n",
    "diffs = []\n",
    "for i in range(len(new_df)):\n",
    "    diff = '\\n'.join(new_df.iloc[i]['Diff'])\n",
    "    diffs.append(diff)\n",
    "new_df['Overall_diff'] = diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:30:53.593472Z",
     "iopub.status.busy": "2025-09-06T06:30:53.593188Z",
     "iopub.status.idle": "2025-09-06T06:31:55.554166Z",
     "shell.execute_reply": "2025-09-06T06:31:55.553380Z",
     "shell.execute_reply.started": "2025-09-06T06:30:53.593444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d336e742eed045a286c44672b2fe7803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02c0740e7434e628b50f3082e3fb2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b42bc19cb047fbb2748f55253479b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4ffdcfc9c8430681055e4019f3c267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ba22d796a941cabca65d0b177a582f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5926c219171d4a939fcf0463b9319635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531462a3fa6e4f519a0e076058b884b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaade0f068747ddb8f3aa363e3c37a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 06:31:06.368850: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757140266.616327      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757140266.684917      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775fe4bc37c64b02bc464b297be9c0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9761ceaa36db49f2b34ac6865257004c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I need help with a problem.\n",
      "\n",
      "Of course! I'm doing great, thanks for asking. Please go ahead and share the problem you need help with — I'm here to help! 😊\n",
      "\n",
      "(Also, if you're working on a math\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model in 4-bit (bnb) without unsloth\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",        # automatically places on GPU\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Quick test\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T07:08:23.245196Z",
     "iopub.status.busy": "2025-09-06T07:08:23.244704Z",
     "iopub.status.idle": "2025-09-06T07:08:23.253852Z",
     "shell.execute_reply": "2025-09-06T07:08:23.253215Z",
     "shell.execute_reply.started": "2025-09-06T07:08:23.245174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, gc, time\n",
    "\n",
    "def predict_batch(\n",
    "    model, tokenizer, commit_msgs, diffs, device=\"cuda\",\n",
    "    max_commit_tokens=64, max_diff_tokens=1024\n",
    "):\n",
    "    system_prompt = (\n",
    "    \"You are given a code diff and the original developer commit message. \"\n",
    "    \"Write a clear, concise, and grammatically correct commit message that best describes \"\n",
    "    \"the changes in the diff. \"\n",
    "    \"You may use the original developer message as a reference, but do not rely on it if it is unclear, incomplete, or misleading. \"\n",
    "    \"Output only the final commit message, without any extra text, labels, or explanation.\"\n",
    "    )\n",
    "\n",
    "    texts = []\n",
    "    for commit_msg, diff in zip(commit_msgs, diffs):\n",
    "        # Tokenize and truncate commit message\n",
    "        commit_tokens = tokenizer.encode(commit_msg, add_special_tokens=False)\n",
    "        if len(commit_tokens) > max_commit_tokens:\n",
    "            commit_tokens = commit_tokens[:max_commit_tokens]\n",
    "        commit_msg_trunc = tokenizer.decode(commit_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize and truncate diff\n",
    "        diff_tokens = tokenizer.encode(diff, add_special_tokens=False)\n",
    "        if len(diff_tokens) > max_diff_tokens:\n",
    "            diff_tokens = diff_tokens[:max_diff_tokens]\n",
    "        diff_trunc = tokenizer.decode(diff_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Build conversation\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Diff:\\n{diff_trunc}\\n\\nOriginal commit message: {commit_msg_trunc}\"}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize batch (handles padding + truncation for full prompt)\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.80,\n",
    "            top_k=20,\n",
    "            max_new_tokens=64\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    print(\"Time taken for batch of\", len(commit_msgs), \":\", round(end_time - start_time, 2), \"seconds\")\n",
    "\n",
    "    # Decode only the new tokens after the input length\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "        pred = tokenizer.decode(output[input_len:], skip_special_tokens=True).strip()\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Free memory\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T07:10:57.783618Z",
     "iopub.status.busy": "2025-09-06T07:10:57.782865Z",
     "iopub.status.idle": "2025-09-06T07:48:26.946611Z",
     "shell.execute_reply": "2025-09-06T07:48:26.945991Z",
     "shell.execute_reply.started": "2025-09-06T07:10:57.783592Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 68.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 1/35 [01:14<42:12, 74.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 2/35 [02:22<38:55, 70.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▊         | 3/35 [03:29<36:53, 69.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█▏        | 4/35 [04:36<35:18, 68.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 63.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 5/35 [05:41<33:24, 66.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 65.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 6/35 [06:47<32:13, 66.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 7/35 [07:54<31:13, 66.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 8/35 [09:02<30:10, 67.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.4 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 9/35 [10:10<29:10, 67.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▊       | 10/35 [11:17<28:04, 67.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███▏      | 11/35 [12:25<26:56, 67.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 12/35 [13:32<25:51, 67.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 13/35 [14:39<24:39, 67.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 14/35 [15:46<23:33, 67.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.3 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|████▎     | 15/35 [16:54<22:29, 67.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  46%|████▌     | 16/35 [18:01<21:19, 67.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  49%|████▊     | 17/35 [19:09<20:11, 67.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 54.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████▏    | 18/35 [20:03<18:00, 63.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|█████▍    | 19/35 [21:10<17:13, 64.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  57%|█████▋    | 20/35 [22:18<16:20, 65.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  60%|██████    | 21/35 [23:25<15:22, 65.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  63%|██████▎   | 22/35 [24:32<14:22, 66.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 60.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  66%|██████▌   | 23/35 [25:33<12:55, 64.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 58.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  69%|██████▊   | 24/35 [26:31<11:30, 62.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 55.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  71%|███████▏  | 25/35 [27:27<10:06, 60.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 63.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  74%|███████▍  | 26/35 [28:31<09:14, 61.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  77%|███████▋  | 27/35 [29:38<08:26, 63.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|████████  | 28/35 [30:45<07:30, 64.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%|████████▎ | 29/35 [31:52<06:30, 65.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  86%|████████▌ | 30/35 [32:59<05:29, 65.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 60.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  89%|████████▊ | 31/35 [34:00<04:17, 64.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 67.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  91%|█████████▏| 32/35 [35:07<03:15, 65.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  94%|█████████▍| 33/35 [36:14<02:11, 65.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 10 : 66.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|█████████▋| 34/35 [37:21<01:06, 66.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch of 1 : 7.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 35/35 [37:29<00:00, 64.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "for i in tqdm(range(5, len(new_df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_msgs = new_df[\"Message\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    batch_diffs = new_df[\"Overall_diff\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "\n",
    "    preds = predict_batch(model, tokenizer, batch_msgs, batch_diffs, device=\"cuda\")\n",
    "\n",
    "    new_df.loc[i:i+len(preds)-1, \"Rectified Message\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\",device_map=\"auto\")\n",
    "\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling over token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def evaluate_candidates_with_threshold(df, threshold):\n",
    "    hit_counts = {\"Message\": 0, \"LLM_inference\": 0, \"Rectified Message\": 0}\n",
    "    total = len(df)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating commits\"):\n",
    "        diff_embedding = get_embedding(row[\"Overall_diff\"], tokenizer, model)\n",
    "        \n",
    "        for col in [\"Message\",\"LLM_inference\",\"Rectified Message\"]:\n",
    "            msg_embedding = get_embedding(str(row[col]), tokenizer, model)\n",
    "            score = cosine_similarity([diff_embedding], [msg_embedding])[0][0]\n",
    "\n",
    "            # Similarity score\n",
    "            df.at[idx, f\"{col}_score\"] = score\n",
    "            # Boolean precision flag\n",
    "            df.at[idx, f\"{col}_precise\"] = score >= threshold\n",
    "\n",
    "            if score >= threshold:\n",
    "                hit_counts[col] += 1\n",
    "\n",
    "    hit_rates = {k: v/total for k,v in hit_counts.items()}\n",
    "\n",
    "    return df, hit_rates\n",
    "\n",
    "df, hit_rates = evaluate_candidates_with_threshold(new_df, threshold=0.85)\n",
    "print(\"Hit rates:\", hit_rates)\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8181547,
     "sourceId": 12977521,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
