{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31bb8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb7a0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keywords used to identify bug-fix commits\n",
    "keywords = [\"fixed\", \"bug\", \"fixes\", \"fix\", \"crash\", \"solves\", \"resolves\", \"issue\", \"regression\", \"fall back\", \"assertion\", \"coverity\", \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"failur\", \"fail\" \"npe\", \"except\", \"broken\", \"differential testing\", \"error\", \"hang\", \"test fix\", \"steps to reproduce\", \"crash\", \"assertion\", \"failure\", \"leak\", \"stack trace\", \"heap overflow\", \"freez\", \"problem\", \"overflow\", \"avoid\", \"workaround\", \"break\", \"stop\"]\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import Repository\n",
    "import pandas as pd\n",
    "bug_fix_df = []\n",
    "diff_analysis_df = []\n",
    "\n",
    "Repo_path = \"/kaggle/working/unsloth\"\n",
    "\n",
    "# Traversing commits using PyDriller\n",
    "for commit in Repository(Repo_path).traverse_commits():\n",
    "    for keyword in keywords:\n",
    "        if keyword in commit.msg:\n",
    "            bug_fix_df.append({\n",
    "                'Hash': commit.hash,\n",
    "                'Author': commit.author.name,\n",
    "                'Message': commit.msg,\n",
    "                'Hashes of parents': commit.parents,\n",
    "                'Is a merge commit?': len(commit.parents) > 1,\n",
    "                'List of modified files': [mod.filename for mod in commit.modified_files],\n",
    "            })\n",
    "\n",
    "            if commit.modified_files:  # normal case\n",
    "                for mod in commit.modified_files:\n",
    "                    diff_analysis_df.append({\n",
    "                        'Hash': commit.hash,\n",
    "                        'Author': commit.author.name,\n",
    "                        'Message': commit.msg,\n",
    "                        'Filename': mod.filename,\n",
    "                        'Change Type': mod.change_type.name,\n",
    "                        'Source Code (before)': mod.source_code_before,\n",
    "                        'Source Code (current)': mod.source_code,\n",
    "                        'Diff': mod.diff\n",
    "                    })\n",
    "            break\n",
    "# Create DataFrames\n",
    "bug_fix_df = pd.DataFrame(bug_fix_df)\n",
    "diff_analysis_df = pd.DataFrame(diff_analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\",device_map=\"auto\")\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling over token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\",device_map=\"auto\")\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling over token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "def evaluate_candidates_with_threshold(df, threshold):\n",
    "    hit_counts = {\"Message\": 0, \"LLM_inference\": 0, \"Rectified Message\": 0}\n",
    "    total = len(df)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating commits\"):\n",
    "        diff_embedding = get_embedding(row[\"Overall_diff\"], tokenizer, model)\n",
    "        \n",
    "        for col in [\"Message\",\"LLM_inference\",\"Rectified Message\"]:\n",
    "            msg_embedding = get_embedding(str(row[col]), tokenizer, model)\n",
    "            score = cosine_similarity([diff_embedding], [msg_embedding])[0][0]\n",
    "\n",
    "            # Similarity score\n",
    "            df.at[idx, f\"{col}_score\"] = score\n",
    "            # Boolean precision flag\n",
    "            df.at[idx, f\"{col}_precise\"] = score >= threshold\n",
    "\n",
    "            if score >= threshold:\n",
    "                hit_counts[col] += 1\n",
    "\n",
    "    hit_rates = {k: v/total for k,v in hit_counts.items()}\n",
    "\n",
    "    return df, hit_rates\n",
    "\n",
    "df, hit_rates = evaluate_candidates_with_threshold(final_df, threshold=0.85)\n",
    "print(\"Hit rates:\", hit_rates)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eb0977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Change Type</th>\n",
       "      <th>Source Code (before)</th>\n",
       "      <th>Source Code (current)</th>\n",
       "      <th>Diff</th>\n",
       "      <th>LLM_inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>README.md</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>@@ -1,23 +1,25 @@\\n &lt;div class=\"align-center\"&gt;...</td>\n",
       "      <td>add link to nvidia gpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>Discord.png</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>PNG\\r\\n\u001a\\n</td>\n",
       "      <td>PNG\\r\\n\u001a\\n</td>\n",
       "      <td>Binary files a/images/Discord.png and b/images...</td>\n",
       "      <td>distro binary files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.png</td>\n",
       "      <td>ADD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PNG\\r\\n\u001a\\n</td>\n",
       "      <td>Binary files /dev/null and b/images/LAION 2GPU...</td>\n",
       "      <td>distro image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1,1518 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "      <td>add missing missing nodes in skeleton skeleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>SlimOrca 1GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1,1424 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "      <td>add missing nodes in skeleton skeleton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>1222</td>\n",
       "      <td>26601f9d42b4c416efa59a062665c858b94c8673</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>pyproject.toml</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>[build-system]\\nrequires = [\"setuptools\", \"set...</td>\n",
       "      <td>[build-system]\\nrequires = [\"setuptools\", \"set...</td>\n",
       "      <td>@@ -37,7 +37,7 @@ triton = [\\n ]\\n \\n huggingf...</td>\n",
       "      <td>add more huggingfaces and colab versions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>1223</td>\n",
       "      <td>26601f9d42b4c416efa59a062665c858b94c8673</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>__init__.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -17,6 +17,10 @@ from packaging.version impo...</td>\n",
       "      <td>add missing fix for unsloth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>1224</td>\n",
       "      <td>26601f9d42b4c416efa59a062665c858b94c8673</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>import_fixes.py</td>\n",
       "      <td>ADD</td>\n",
       "      <td>NaN</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -0,0 +1,119 @@\\n+# Copyright 2023-present D...</td>\n",
       "      <td>add fixup for google.protobuf.message_factory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>1225</td>\n",
       "      <td>26601f9d42b4c416efa59a062665c858b94c8673</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>_utils.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -12,7 +12,7 @@\\n # See the License for the ...</td>\n",
       "      <td>add missing version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>1226</td>\n",
       "      <td>26601f9d42b4c416efa59a062665c858b94c8673</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...</td>\n",
       "      <td>loader.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -618,9 +618,6 @@ class FastModel(FastBaseMo...</td>\n",
       "      <td>add missing env variables</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1227 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      Hash      Author  \\\n",
       "0              0  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "1              1  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "2              2  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "3              3  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "4              4  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "...          ...                                       ...         ...   \n",
       "1222        1222  26601f9d42b4c416efa59a062665c858b94c8673  Daniel Han   \n",
       "1223        1223  26601f9d42b4c416efa59a062665c858b94c8673  Daniel Han   \n",
       "1224        1224  26601f9d42b4c416efa59a062665c858b94c8673  Daniel Han   \n",
       "1225        1225  26601f9d42b4c416efa59a062665c858b94c8673  Daniel Han   \n",
       "1226        1226  26601f9d42b4c416efa59a062665c858b94c8673  Daniel Han   \n",
       "\n",
       "                                                Message           Filename  \\\n",
       "0     Pre-release 2023 December version (Mistral, Pr...          README.md   \n",
       "1     Pre-release 2023 December version (Mistral, Pr...        Discord.png   \n",
       "2     Pre-release 2023 December version (Mistral, Pr...     LAION 2GPU.png   \n",
       "3     Pre-release 2023 December version (Mistral, Pr...     LAION 2GPU.svg   \n",
       "4     Pre-release 2023 December version (Mistral, Pr...  SlimOrca 1GPU.svg   \n",
       "...                                                 ...                ...   \n",
       "1222  Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...     pyproject.toml   \n",
       "1223  Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...        __init__.py   \n",
       "1224  Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...    import_fixes.py   \n",
       "1225  Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...          _utils.py   \n",
       "1226  Bug fixes (#3195)\\n\\n* Fix mamba\\n\\n* Update l...          loader.py   \n",
       "\n",
       "     Change Type                               Source Code (before)  \\\n",
       "0         MODIFY  <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1         MODIFY                                         PNG\\r\\n\u001a\\n   \n",
       "2            ADD                                                NaN   \n",
       "3         DELETE  <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "4         DELETE  <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "...          ...                                                ...   \n",
       "1222      MODIFY  [build-system]\\nrequires = [\"setuptools\", \"set...   \n",
       "1223      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1224         ADD                                                NaN   \n",
       "1225      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1226      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "\n",
       "                                  Source Code (current)  \\\n",
       "0     <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1                                            PNG\\r\\n\u001a\\n   \n",
       "2                                            PNG\\r\\n\u001a\\n   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1222  [build-system]\\nrequires = [\"setuptools\", \"set...   \n",
       "1223  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1224  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1225  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "1226  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "\n",
       "                                                   Diff  \\\n",
       "0     @@ -1,23 +1,25 @@\\n <div class=\"align-center\">...   \n",
       "1     Binary files a/images/Discord.png and b/images...   \n",
       "2     Binary files /dev/null and b/images/LAION 2GPU...   \n",
       "3     @@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encod...   \n",
       "4     @@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encod...   \n",
       "...                                                 ...   \n",
       "1222  @@ -37,7 +37,7 @@ triton = [\\n ]\\n \\n huggingf...   \n",
       "1223  @@ -17,6 +17,10 @@ from packaging.version impo...   \n",
       "1224  @@ -0,0 +1,119 @@\\n+# Copyright 2023-present D...   \n",
       "1225  @@ -12,7 +12,7 @@\\n # See the License for the ...   \n",
       "1226  @@ -618,9 +618,6 @@ class FastModel(FastBaseMo...   \n",
       "\n",
       "                                       LLM_inference  \n",
       "0                             add link to nvidia gpu  \n",
       "1                                distro binary files  \n",
       "2                                       distro image  \n",
       "3     add missing missing nodes in skeleton skeleton  \n",
       "4             add missing nodes in skeleton skeleton  \n",
       "...                                              ...  \n",
       "1222        add more huggingfaces and colab versions  \n",
       "1223                     add missing fix for unsloth  \n",
       "1224   add fixup for google.protobuf.message_factory  \n",
       "1225                             add missing version  \n",
       "1226                       add missing env variables  \n",
       "\n",
       "[1227 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('assgn2_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c794ef82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Change Type</th>\n",
       "      <th>Source Code (before)</th>\n",
       "      <th>Source Code (current)</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>README.md</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>&lt;div class=\"align-center\"&gt;\\n  &lt;img src=\"./imag...</td>\n",
       "      <td>@@ -1,23 +1,25 @@\\n &lt;div class=\"align-center\"&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>Discord.png</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>PNG\\r\\n\u001a\\n</td>\n",
       "      <td>PNG\\r\\n\u001a\\n</td>\n",
       "      <td>Binary files a/images/Discord.png and b/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.png</td>\n",
       "      <td>ADD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PNG\\r\\n\u001a\\n</td>\n",
       "      <td>Binary files /dev/null and b/images/LAION 2GPU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>LAION 2GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1,1518 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>SlimOrca 1GPU.svg</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalon...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1,1424 +0,0 @@\\n-&lt;?xml version=\"1.0\" encod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>4537</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>loader.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -591,20 +591,12 @@ class FastModel(FastBase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>4538</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>vision.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -365,8 +365,10 @@ class FastBaseModel:\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539</th>\n",
       "      <td>4539</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>_utils.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -76,6 +76,7 @@ platform_system = platform_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>4540</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>loader.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -591,20 +591,12 @@ class FastModel(FastBase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>4541</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>vision.py</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td># Copyright 2023-present Daniel Han-Chen &amp; the...</td>\n",
       "      <td>@@ -365,8 +365,10 @@ class FastBaseModel:\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4542 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      Hash      Author  \\\n",
       "0              0  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "1              1  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "2              2  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "3              3  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "4              4  4b97a810b509c93f44be4c037c7aa18fb8922884  Daniel Han   \n",
       "...          ...                                       ...         ...   \n",
       "4537        4537  f1508c9259f91e33f5c7fdf95d971a309196471c  Daniel Han   \n",
       "4538        4538  f1508c9259f91e33f5c7fdf95d971a309196471c  Daniel Han   \n",
       "4539        4539  f1508c9259f91e33f5c7fdf95d971a309196471c  Daniel Han   \n",
       "4540        4540  f1508c9259f91e33f5c7fdf95d971a309196471c  Daniel Han   \n",
       "4541        4541  f1508c9259f91e33f5c7fdf95d971a309196471c  Daniel Han   \n",
       "\n",
       "                                                Message           Filename  \\\n",
       "0     Pre-release 2023 December version (Mistral, Pr...          README.md   \n",
       "1     Pre-release 2023 December version (Mistral, Pr...        Discord.png   \n",
       "2     Pre-release 2023 December version (Mistral, Pr...     LAION 2GPU.png   \n",
       "3     Pre-release 2023 December version (Mistral, Pr...     LAION 2GPU.svg   \n",
       "4     Pre-release 2023 December version (Mistral, Pr...  SlimOrca 1GPU.svg   \n",
       "...                                                 ...                ...   \n",
       "4537                                      GPT OSS fixes          loader.py   \n",
       "4538                                      GPT OSS fixes          vision.py   \n",
       "4539                                      GPT OSS fixes          _utils.py   \n",
       "4540                                      GPT OSS fixes          loader.py   \n",
       "4541                                      GPT OSS fixes          vision.py   \n",
       "\n",
       "     Change Type                               Source Code (before)  \\\n",
       "0         MODIFY  <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1         MODIFY                                         PNG\\r\\n\u001a\\n   \n",
       "2            ADD                                                NaN   \n",
       "3         DELETE  <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "4         DELETE  <?xml version=\"1.0\" encoding=\"utf-8\" standalon...   \n",
       "...          ...                                                ...   \n",
       "4537      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4538      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4539      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4540      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4541      MODIFY  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "\n",
       "                                  Source Code (current)  \\\n",
       "0     <div class=\"align-center\">\\n  <img src=\"./imag...   \n",
       "1                                            PNG\\r\\n\u001a\\n   \n",
       "2                                            PNG\\r\\n\u001a\\n   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "4537  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4538  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4539  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4540  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "4541  # Copyright 2023-present Daniel Han-Chen & the...   \n",
       "\n",
       "                                                   Diff  \n",
       "0     @@ -1,23 +1,25 @@\\n <div class=\"align-center\">...  \n",
       "1     Binary files a/images/Discord.png and b/images...  \n",
       "2     Binary files /dev/null and b/images/LAION 2GPU...  \n",
       "3     @@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encod...  \n",
       "4     @@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encod...  \n",
       "...                                                 ...  \n",
       "4537  @@ -591,20 +591,12 @@ class FastModel(FastBase...  \n",
       "4538  @@ -365,8 +365,10 @@ class FastBaseModel:\\n   ...  \n",
       "4539  @@ -76,6 +76,7 @@ platform_system = platform_s...  \n",
       "4540  @@ -591,20 +591,12 @@ class FastModel(FastBase...  \n",
       "4541  @@ -365,8 +365,10 @@ class FastBaseModel:\\n   ...  \n",
       "\n",
       "[4542 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbdf5660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Daniel Han', 'Daniel Han-Chen', 'Michael Han', 'Z', 'XiaoYang',\n",
       "       'moontidef', 'Nazim Ali', 'vo1d-ai', 'Edd', 'Datta Nimmaturi',\n",
       "       'Uday Girish Maradana', 'Zewen Shen', 'Kareem', 'tastelikefeet',\n",
       "       'Gennadii Manzhos', 'Nino Risteski', 'Jyotin Goel', 'Igor Kilbas',\n",
       "       'Charles London', 'Mohamed Mekkouri', 'Mukkesh Ganesh',\n",
       "       'Isaac Breen', 'lurf21', 'naliazheli', 'jeromeku', 'Etherll',\n",
       "       'Richi', 'Erland366', 'Dattu Sharma', 'Mathew Mathew',\n",
       "       'Roland Tannous', 'feng lui', 'David Dobolyi', 'Emmanuel Ferdman',\n",
       "       'DoubleMathew', 'RunFMe', 'Salpingopharyngeus', 'pluesclues',\n",
       "       'Lei Zhenyuan', 'Dhia Eddine Rhaiem', 'billishyahao',\n",
       "       'Muzammil Khan', 'Sekinal'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('files_fixed.csv')\n",
    "df['Author'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f7189",
   "metadata": {},
   "source": [
    "### Number of files, Number of commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9190a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commits: 309\n",
      "Number of files: 113\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of commits:\",df['Hash'].nunique())\n",
    "print(\"Number of files:\", df['Filename'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e29ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f39f8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32103, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32103, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32103, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1535d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2539e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_text):\n",
    "    # Tokenize and generate \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode generated tokens\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4271a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def predict_batch(input_texts, batch_size=1000):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_texts), batch_size):\n",
    "            batch = input_texts[i:i+batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            # Move to same device as model\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(**inputs)\n",
    "\n",
    "            # Decode each prediction\n",
    "            batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend(batch_preds)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydriller import Repository\n",
    "# Keywords used to identify bug-fix commits\n",
    "keywords = [\"fixed\", \"bug\", \"fixes\", \"fix\", \"crash\", \"solves\", \"resolves\", \"issue\", \"regression\", \"fall back\", \n",
    "            \"assertion\", \"coverity\", \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"failur\", \n",
    "            \"fail\" \"npe\", \"except\", \"broken\", \"differential testing\", \"error\", \"hang\", \"test fix\", \n",
    "            \"steps to reproduce\", \"crash\", \"assertion\", \"failure\", \"leak\", \"stack trace\", \"heap overflow\", \n",
    "            \"freez\", \"problem\", \"overflow\", \"avoid\", \"workaround\", \"break\", \"stop\"]\n",
    "\n",
    "rows = []\n",
    "rows2 = []\n",
    "for commit in Repository(\"https://github.com/unslothai/unsloth\").traverse_commits():\n",
    "    for keyword in keywords:\n",
    "        if keyword in commit.msg:\n",
    "            rows.append({\n",
    "                'Hash': commit.hash,\n",
    "                'Author' : commit.author.name,\n",
    "                'Message': commit.msg,\n",
    "                'Hashes of parents': commit.parents,\n",
    "                'Is a merge commit?': len(commit.parents) > 1,\n",
    "                'List of modified files': [mod.filename for mod in commit.modified_files],\n",
    "            })\n",
    "            for mod in commit.modified_files:\n",
    "                rows2.append({\n",
    "                    'Hash': commit.hash,\n",
    "                    'Author' : commit.author.name,\n",
    "                    'Message': commit.msg,\n",
    "                    'Filename' : mod.filename,\n",
    "                    'Change Type': mod.change_type.name,\n",
    "                    'Source Code (before)' : mod.source_code_before,\n",
    "                    'Source Code (current)' : mod.source_code,\n",
    "                    'Diff' : mod.diff\n",
    "                })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df2 = pd.DataFrame(rows2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f73066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e07942e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"files_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52861acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a47d8b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@@ -1,23 +1,25 @@\\n <div class=\"align-center\">\\n   <img src=\"./images/unsloth new logo.png\" width=\"400\" />\\n   <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord.png\" width=\"180\"></a>\\n+  <a href=\"https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing\"><img src=\"./images/try live demo green.png\" width=\"130\"></a>\\n </div>\\n \\n-\\n-## 80% faster 50% less memory local QLoRA finetuning\\n+## 2-5x faster 50% less memory local LLM finetuning\\n * Manual autograd engine - hand derived backprop steps.\\n-* QLoRA / LoRA 80% faster, 50% less memory.\\n-* All kernels written in OpenAI\\'s Triton language.\\n+* 2x to 5x faster than QLoRA. 50% less memory usage.\\n+* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language.\\n * 0% loss in accuracy - no approximation methods - all exact.\\n-* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+. Tesla T4, RTX 20, 30, 40 series, A100, H100s\\n-* Flash Attention support via Xformers.\\n-* Supports 4bit and 16bit LoRA finetuning.\\n+* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)\\n+* [Flash Attention v2](https://github.com/Dao-AILab/flash-attention) support via [Xformers](https://github.com/facebookresearch/xformers).\\n+* **NEW!** Works on **Linux** and **Windows** via WSL.\\n+* **NEW!** Experimental support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)!\\n+* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n * Train Slim Orca **fully locally in 260 hours from 1301 hours (5x faster).**\\n * Open source version trains 5x faster or you can check out [Unsloth Pro and Max](https://unsloth.ai/) codepaths for **30x faster training**!\\n   \\n <div class=\"align-center\">\\n   <img src=\"./images/Slim Orca 2GPUs.png\" width=\"400\" />\\n-  <img src=\"./images/LAION%202GPU.svg\" width=\"400\" />\\n+  <img src=\"./images/LAION 2GPU.png\" width=\"400\" />\\n </div>\\n \\n 1. Try our Colab examples for [the Alpaca 52K dataset](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) or [the Slim Orca 518K dataset](https://colab.research.google.com/drive/1VNqLARpE8N8eYwNrUSDoHVjtbR9W0_c7?usp=sharing).\\n@@ -49,7 +51,13 @@ pip install --upgrade --force-reinstall --no-cache-dir torch triton \\\\\\n ```\\n Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n \\n-# Alpaca Example\\n+4. If you get errors, try the below first, then go back to step 1:\\n+```\\n+pip install --upgrade pip\\n+```\\n+\\n+# Documentation\\n+We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n ```\\n from unsloth import FastLlamaModel\\n import torch\\n@@ -59,7 +67,7 @@ load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False\\n \\n # Load Llama model\\n model, tokenizer = FastLlamaModel.from_pretrained(\\n-    model_name = \"unsloth/llama-2-7b\", # Supports any llama model\\n+    model_name = \"unsloth/llama-2-7b\", # Supports any llama model eg meta-llama/Llama-2-7b-hf\\n     max_seq_length = max_seq_length,\\n     dtype = dtype,\\n     load_in_4bit = load_in_4bit,\\n@@ -80,12 +88,16 @@ model = FastLlamaModel.get_peft_model(\\n     max_seq_length = max_seq_length,\\n )\\n \\n-trainer = .... Use Huggingface\\'s Trainer and dataset loading\\n+trainer = .... Use Huggingface\\'s Trainer and dataset loading (TRL, transformers etc)\\n ```\\n \\n If you trained a model with Unsloth, we made a cool sticker!!\\n <img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n \\n+# DPO (Direct Preference Optimization) Experimental support\\n+[152334H](https://github.com/152334H) hacked Unsloth to work with DPO via TRL!\\n+1. Hack the model\\'s `config.json` to be llama model. [Example gist](https://gist.github.com/152334H/d8a68b51b83bac008a02e69ecc81d5c1).\\n+2. Use Unsloth for DPO for both base and reference models. [Example gist](https://gist.github.com/152334H/4847f3a8cca12894877e6b30698b0b64).\\n \\n # Future Milestones and limitations\\n 1. Support sqrt gradient checkpointing which further slashes memory usage by 25%.\\n@@ -94,6 +106,9 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n # Performance comparisons on 1 Tesla T4 GPU:\\n **Time taken for 1 epoch**\\n \\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n | System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n | --- | --- | --- | --- | --- | --- |\\n | Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n@@ -113,19 +128,28 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n # Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n **Time taken for 1 epoch**\\n \\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n | --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n \\n **Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n \\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n | --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+\\n+### For replication of timings:\\n+* [Huggingface LAION DDP reference implementation](https://www.kaggle.com/code/danielhanchen/huggingface-original-laion-oig) 60 steps on DDP Kaggle 2 Tesla T4 GPUs takes 40 minutes and 46 seconds\\n+* [Unsloth LAION DDP fast implementation](https://www.kaggle.com/code/danielhanchen/unsloth-laion-chip2-kaggle) 60 steps on DDP Kaggle 2 Tesla T4 GPUs - **Unsloth only uses 1 GPU whilst Pro plans use more.** takes 4 minutes and 34 seconds **(8.64x speedup)**\\n \\n # Troubleshooting\\n 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n@@ -136,4 +160,8 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n \\n 3. If it doesn\\'t install - maybe try updating `pip`.\\n \\n+# Credits\\n+1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n+2. [152334H](https://github.com/152334H) for experimental DPO support\\n+\\n <img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files a/images/Discord.png and b/images/Discord.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/LAION 2GPU.png differ\\n',\n",
       " '@@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\n-  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n- <metadata>\\n-  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n-   <cc:Work>\\n-    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\\n-    <dc:date>2023-11-30T13:10:10.501490</dc:date>\\n-    <dc:format>image/svg+xml</dc:format>\\n-    <dc:creator>\\n-     <cc:Agent>\\n-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\\n-     </cc:Agent>\\n-    </dc:creator>\\n-   </cc:Work>\\n-  </rdf:RDF>\\n- </metadata>\\n- <defs>\\n-  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\\n- </defs>\\n- <g id=\"figure_1\">\\n-  <g id=\"patch_1\">\\n-   <path d=\"M 0 432 \\n-L 720 432 \\n-L 720 0 \\n-L 0 0 \\n-L 0 432 \\n-z\\n-\" style=\"fill: none\"/>\\n-  </g>\\n-  <g id=\"axes_1\">\\n-   <g id=\"patch_2\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-L 648 51.84 \\n-L 90 51.84 \\n-L 90 384.48 \\n-z\\n-\" style=\"fill: none\"/>\\n-   </g>\\n-   <g id=\"patch_3\">\\n-    <path d=\"M 90 369.36 \\n-L 621.428571 369.36 \\n-L 621.428571 305.696842 \\n-L 90 305.696842 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_4\">\\n-    <path d=\"M 90 289.781053 \\n-L 202.147218 289.781053 \\n-L 202.147218 226.117895 \\n-L 90 226.117895 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_5\">\\n-    <path d=\"M 90 210.202105 \\n-L 108.547009 210.202105 \\n-L 108.547009 146.538947 \\n-L 90 146.538947 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_6\">\\n-    <path d=\"M 90 130.623158 \\n-L 106.978894 130.623158 \\n-L 106.978894 66.96 \\n-L 90 66.96 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"matplotlib.axis_1\">\\n-    <g id=\"xtick_1\">\\n-     <g id=\"line2d_1\">\\n-      <defs>\\n-       <path id=\"m7f9e7b8ac6\" d=\"M 0 0 \\n-L 0 3.5 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"90\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_1\">\\n-      <!-- 0 -->\\n-      <g transform=\"translate(86.81875 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \\n-Q 1547 4250 1301 3770 \\n-Q 1056 3291 1056 2328 \\n-Q 1056 1369 1301 889 \\n-Q 1547 409 2034 409 \\n-Q 2525 409 2770 889 \\n-Q 3016 1369 3016 2328 \\n-Q 3016 3291 2770 3770 \\n-Q 2525 4250 2034 4250 \\n-z\\n-M 2034 4750 \\n-Q 2819 4750 3233 4129 \\n-Q 3647 3509 3647 2328 \\n-Q 3647 1150 3233 529 \\n-Q 2819 -91 2034 -91 \\n-Q 1250 -91 836 529 \\n-Q 422 1150 422 2328 \\n-Q 422 3509 836 4129 \\n-Q 1250 4750 2034 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-30\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_2\">\\n-     <g id=\"line2d_2\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"154.887493\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_2\">\\n-      <!-- 20 -->\\n-      <g transform=\"translate(148.524993 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \\n-L 3431 531 \\n-L 3431 0 \\n-L 469 0 \\n-L 469 531 \\n-Q 828 903 1448 1529 \\n-Q 2069 2156 2228 2338 \\n-Q 2531 2678 2651 2914 \\n-Q 2772 3150 2772 3378 \\n-Q 2772 3750 2511 3984 \\n-Q 2250 4219 1831 4219 \\n-Q 1534 4219 1204 4116 \\n-Q 875 4013 500 3803 \\n-L 500 4441 \\n-Q 881 4594 1212 4672 \\n-Q 1544 4750 1819 4750 \\n-Q 2544 4750 2975 4387 \\n-Q 3406 4025 3406 3419 \\n-Q 3406 3131 3298 2873 \\n-Q 3191 2616 2906 2266 \\n-Q 2828 2175 2409 1742 \\n-Q 1991 1309 1228 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_3\">\\n-     <g id=\"line2d_3\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"219.774987\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_3\">\\n-      <!-- 40 -->\\n-      <g transform=\"translate(213.412487 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \\n-L 825 1625 \\n-L 2419 1625 \\n-L 2419 4116 \\n-z\\n-M 2253 4666 \\n-L 3047 4666 \\n-L 3047 1625 \\n-L 3713 1625 \\n-L 3713 1100 \\n-L 3047 1100 \\n-L 3047 0 \\n-L 2419 0 \\n-L 2419 1100 \\n-L 313 1100 \\n-L 313 1709 \\n-L 2253 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-34\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_4\">\\n-     <g id=\"line2d_4\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"284.66248\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_4\">\\n-      <!-- 60 -->\\n-      <g transform=\"translate(278.29998 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \\n-Q 1688 2584 1439 2293 \\n-Q 1191 2003 1191 1497 \\n-Q 1191 994 1439 701 \\n-Q 1688 409 2113 409 \\n-Q 2538 409 2786 701 \\n-Q 3034 994 3034 1497 \\n-Q 3034 2003 2786 2293 \\n-Q 2538 2584 2113 2584 \\n-z\\n-M 3366 4563 \\n-L 3366 3988 \\n-Q 3128 4100 2886 4159 \\n-Q 2644 4219 2406 4219 \\n-Q 1781 4219 1451 3797 \\n-Q 1122 3375 1075 2522 \\n-Q 1259 2794 1537 2939 \\n-Q 1816 3084 2150 3084 \\n-Q 2853 3084 3261 2657 \\n-Q 3669 2231 3669 1497 \\n-Q 3669 778 3244 343 \\n-Q 2819 -91 2113 -91 \\n-Q 1303 -91 875 529 \\n-Q 447 1150 447 2328 \\n-Q 447 3434 972 4092 \\n-Q 1497 4750 2381 4750 \\n-Q 2619 4750 2861 4703 \\n-Q 3103 4656 3366 4563 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-36\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_5\">\\n-     <g id=\"line2d_5\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"349.549974\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_5\">\\n-      <!-- 80 -->\\n-      <g transform=\"translate(343.187474 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \\n-Q 1584 2216 1326 1975 \\n-Q 1069 1734 1069 1313 \\n-Q 1069 891 1326 650 \\n-Q 1584 409 2034 409 \\n-Q 2484 409 2743 651 \\n-Q 3003 894 3003 1313 \\n-Q 3003 1734 2745 1975 \\n-Q 2488 2216 2034 2216 \\n-z\\n-M 1403 2484 \\n-Q 997 2584 770 2862 \\n-Q 544 3141 544 3541 \\n-Q 544 4100 942 4425 \\n-Q 1341 4750 2034 4750 \\n-Q 2731 4750 3128 4425 \\n-Q 3525 4100 3525 3541 \\n-Q 3525 3141 3298 2862 \\n-Q 3072 2584 2669 2484 \\n-Q 3125 2378 3379 2068 \\n-Q 3634 1759 3634 1313 \\n-Q 3634 634 3220 271 \\n-Q 2806 -91 2034 -91 \\n-Q 1263 -91 848 271 \\n-Q 434 634 434 1313 \\n-Q 434 1759 690 2068 \\n-Q 947 2378 1403 2484 \\n-z\\n-M 1172 3481 \\n-Q 1172 3119 1398 2916 \\n-Q 1625 2713 2034 2713 \\n-Q 2441 2713 2670 2916 \\n-Q 2900 3119 2900 3481 \\n-Q 2900 3844 2670 4047 \\n-Q 2441 4250 2034 4250 \\n-Q 1625 4250 1398 4047 \\n-Q 1172 3844 1172 3481 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-38\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_6\">\\n-     <g id=\"line2d_6\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"414.437467\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_6\">\\n-      <!-- 100 -->\\n-      <g transform=\"translate(404.893717 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-31\" d=\"M 794 531 \\n-L 1825 531 \\n-L 1825 4091 \\n-L 703 3866 \\n-L 703 4441 \\n-L 1819 4666 \\n-L 2450 4666 \\n-L 2450 531 \\n-L 3481 531 \\n-L 3481 0 \\n-L 794 0 \\n-L 794 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_7\">\\n-     <g id=\"line2d_7\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"479.324961\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_7\">\\n-      <!-- 120 -->\\n-      <g transform=\"translate(469.781211 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_8\">\\n-     <g id=\"line2d_8\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"544.212454\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_8\">\\n-      <!-- 140 -->\\n-      <g transform=\"translate(534.668704 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_9\">\\n-     <g id=\"line2d_9\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"609.099948\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_9\">\\n-      <!-- 160 -->\\n-      <g transform=\"translate(599.556198 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"text_10\">\\n-     <!-- Time taken (lower is better). * Unsloth Open uses 1 GPU only. -->\\n-     <g transform=\"translate(184.611563 414.27625) scale(0.12 -0.12)\">\\n-      <defs>\\n-       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \\n-L 3928 4666 \\n-L 3928 4134 \\n-L 2272 4134 \\n-L 2272 0 \\n-L 1638 0 \\n-L 1638 4134 \\n-L -19 4134 \\n-L -19 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \\n-L 1178 3500 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 3500 \\n-z\\n-M 603 4863 \\n-L 1178 4863 \\n-L 1178 4134 \\n-L 603 4134 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \\n-Q 3544 3216 3844 3400 \\n-Q 4144 3584 4550 3584 \\n-Q 5097 3584 5394 3201 \\n-Q 5691 2819 5691 2113 \\n-L 5691 0 \\n-L 5113 0 \\n-L 5113 2094 \\n-Q 5113 2597 4934 2840 \\n-Q 4756 3084 4391 3084 \\n-Q 3944 3084 3684 2787 \\n-Q 3425 2491 3425 1978 \\n-L 3425 0 \\n-L 2847 0 \\n-L 2847 2094 \\n-Q 2847 2600 2669 2842 \\n-Q 2491 3084 2119 3084 \\n-Q 1678 3084 1418 2786 \\n-Q 1159 2488 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1356 3278 1631 3431 \\n-Q 1906 3584 2284 3584 \\n-Q 2666 3584 2933 3390 \\n-Q 3200 3197 3328 2828 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \\n-L 3597 1613 \\n-L 953 1613 \\n-Q 991 1019 1311 708 \\n-Q 1631 397 2203 397 \\n-Q 2534 397 2845 478 \\n-Q 3156 559 3463 722 \\n-L 3463 178 \\n-Q 3153 47 2828 -22 \\n-Q 2503 -91 2169 -91 \\n-Q 1331 -91 842 396 \\n-Q 353 884 353 1716 \\n-Q 353 2575 817 3079 \\n-Q 1281 3584 2069 3584 \\n-Q 2775 3584 3186 3129 \\n-Q 3597 2675 3597 1894 \\n-z\\n-M 3022 2063 \\n-Q 3016 2534 2758 2815 \\n-Q 2500 3097 2075 3097 \\n-Q 1594 3097 1305 2825 \\n-Q 1016 2553 972 2059 \\n-L 3022 2063 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \\n-L 1172 3500 \\n-L 2356 3500 \\n-L 2356 3053 \\n-L 1172 3053 \\n-L 1172 1153 \\n-Q 1172 725 1289 603 \\n-Q 1406 481 1766 481 \\n-L 2356 481 \\n-L 2356 0 \\n-L 1766 0 \\n-Q 1100 0 847 248 \\n-Q 594 497 594 1153 \\n-L 594 3053 \\n-L 172 3053 \\n-L 172 3500 \\n-L 594 3500 \\n-L 594 4494 \\n-L 1172 4494 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \\n-Q 1497 1759 1228 1600 \\n-Q 959 1441 959 1056 \\n-Q 959 750 1161 570 \\n-Q 1363 391 1709 391 \\n-Q 2188 391 2477 730 \\n-Q 2766 1069 2766 1631 \\n-L 2766 1759 \\n-L 2194 1759 \\n-z\\n-M 3341 1997 \\n-L 3341 0 \\n-L 2766 0 \\n-L 2766 531 \\n-Q 2569 213 2275 61 \\n-Q 1981 -91 1556 -91 \\n-Q 1019 -91 701 211 \\n-Q 384 513 384 1019 \\n-Q 384 1609 779 1909 \\n-Q 1175 2209 1959 2209 \\n-L 2766 2209 \\n-L 2766 2266 \\n-Q 2766 2663 2505 2880 \\n-Q 2244 3097 1772 3097 \\n-Q 1472 3097 1187 3025 \\n-Q 903 2953 641 2809 \\n-L 641 3341 \\n-Q 956 3463 1253 3523 \\n-Q 1550 3584 1831 3584 \\n-Q 2591 3584 2966 3190 \\n-Q 3341 2797 3341 1997 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \\n-L 1159 4863 \\n-L 1159 1991 \\n-L 2875 3500 \\n-L 3609 3500 \\n-L 1753 1863 \\n-L 3688 0 \\n-L 2938 0 \\n-L 1159 1709 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \\n-Q 1566 4138 1362 3434 \\n-Q 1159 2731 1159 2009 \\n-Q 1159 1288 1364 580 \\n-Q 1569 -128 1984 -844 \\n-L 1484 -844 \\n-Q 1016 -109 783 600 \\n-Q 550 1309 550 2009 \\n-Q 550 2706 781 3412 \\n-Q 1013 4119 1484 4856 \\n-L 1984 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \\n-L 1178 4863 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \\n-Q 1497 3097 1228 2736 \\n-Q 959 2375 959 1747 \\n-Q 959 1119 1226 758 \\n-Q 1494 397 1959 397 \\n-Q 2419 397 2687 759 \\n-Q 2956 1122 2956 1747 \\n-Q 2956 2369 2687 2733 \\n-Q 2419 3097 1959 3097 \\n-z\\n-M 1959 3584 \\n-Q 2709 3584 3137 3096 \\n-Q 3566 2609 3566 1747 \\n-Q 3566 888 3137 398 \\n-Q 2709 -91 1959 -91 \\n-Q 1206 -91 779 398 \\n-Q 353 888 353 1747 \\n-Q 353 2609 779 3096 \\n-Q 1206 3584 1959 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \\n-L 844 3500 \\n-L 1563 769 \\n-L 2278 3500 \\n-L 2956 3500 \\n-L 3675 769 \\n-L 4391 3500 \\n-L 4966 3500 \\n-L 4050 0 \\n-L 3372 0 \\n-L 2619 2869 \\n-L 1863 0 \\n-L 1184 0 \\n-L 269 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \\n-Q 2534 3019 2420 3045 \\n-Q 2306 3072 2169 3072 \\n-Q 1681 3072 1420 2755 \\n-Q 1159 2438 1159 1844 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1341 3275 1631 3429 \\n-Q 1922 3584 2338 3584 \\n-Q 2397 3584 2469 3576 \\n-Q 2541 3569 2628 3553 \\n-L 2631 2963 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \\n-L 2834 2853 \\n-Q 2591 2978 2328 3040 \\n-Q 2066 3103 1784 3103 \\n-Q 1356 3103 1142 2972 \\n-Q 928 2841 928 2578 \\n-Q 928 2378 1081 2264 \\n-Q 1234 2150 1697 2047 \\n-L 1894 2003 \\n-Q 2506 1872 2764 1633 \\n-Q 3022 1394 3022 966 \\n-Q 3022 478 2636 193 \\n-Q 2250 -91 1575 -91 \\n-Q 1294 -91 989 -36 \\n-Q 684 19 347 128 \\n-L 347 722 \\n-Q 666 556 975 473 \\n-Q 1284 391 1588 391 \\n-Q 1994 391 2212 530 \\n-Q 2431 669 2431 922 \\n-Q 2431 1156 2273 1281 \\n-Q 2116 1406 1581 1522 \\n-L 1381 1569 \\n-Q 847 1681 609 1914 \\n-Q 372 2147 372 2553 \\n-Q 372 3047 722 3315 \\n-Q 1072 3584 1716 3584 \\n-Q 2034 3584 2315 3537 \\n-Q 2597 3491 2834 3397 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-M 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2969 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \\n-L 1013 4856 \\n-Q 1481 4119 1714 3412 \\n-Q 1947 2706 1947 2009 \\n-Q 1947 1309 1714 600 \\n-Q 1481 -109 1013 -844 \\n-L 513 -844 \\n-Q 928 -128 1133 580 \\n-Q 1338 1288 1338 2009 \\n-Q 1338 2731 1133 3434 \\n-Q 928 4138 513 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2e\" d=\"M 684 794 \\n-L 1344 794 \\n-L 1344 0 \\n-L 684 0 \\n-L 684 794 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2a\" d=\"M 3009 3897 \\n-L 1888 3291 \\n-L 3009 2681 \\n-L 2828 2375 \\n-L 1778 3009 \\n-L 1778 1831 \\n-L 1422 1831 \\n-L 1422 3009 \\n-L 372 2375 \\n-L 191 2681 \\n-L 1313 3291 \\n-L 191 3897 \\n-L 372 4206 \\n-L 1422 3572 \\n-L 1422 4750 \\n-L 1778 4750 \\n-L 1778 3572 \\n-L 2828 4206 \\n-L 3009 3897 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-55\" d=\"M 556 4666 \\n-L 1191 4666 \\n-L 1191 1831 \\n-Q 1191 1081 1462 751 \\n-Q 1734 422 2344 422 \\n-Q 2950 422 3222 751 \\n-Q 3494 1081 3494 1831 \\n-L 3494 4666 \\n-L 4128 4666 \\n-L 4128 1753 \\n-Q 4128 841 3676 375 \\n-Q 3225 -91 2344 -91 \\n-Q 1459 -91 1007 375 \\n-Q 556 841 556 1753 \\n-L 556 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-4f\" d=\"M 2522 4238 \\n-Q 1834 4238 1429 3725 \\n-Q 1025 3213 1025 2328 \\n-Q 1025 1447 1429 934 \\n-Q 1834 422 2522 422 \\n-Q 3209 422 3611 934 \\n-Q 4013 1447 4013 2328 \\n-Q 4013 3213 3611 3725 \\n-Q 3209 4238 2522 4238 \\n-z\\n-M 2522 4750 \\n-Q 3503 4750 4090 4092 \\n-Q 4678 3434 4678 2328 \\n-Q 4678 1225 4090 567 \\n-Q 3503 -91 2522 -91 \\n-Q 1538 -91 948 565 \\n-Q 359 1222 359 2328 \\n-Q 359 3434 948 4092 \\n-Q 1538 4750 2522 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \\n-L 1159 -1331 \\n-L 581 -1331 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-z\\n-M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \\n-L 544 3500 \\n-L 1119 3500 \\n-L 1119 1403 \\n-Q 1119 906 1312 657 \\n-Q 1506 409 1894 409 \\n-Q 2359 409 2629 706 \\n-Q 2900 1003 2900 1516 \\n-L 2900 3500 \\n-L 3475 3500 \\n-L 3475 0 \\n-L 2900 0 \\n-L 2900 538 \\n-Q 2691 219 2414 64 \\n-Q 2138 -91 1772 -91 \\n-Q 1169 -91 856 284 \\n-Q 544 659 544 1381 \\n-z\\n-M 1991 3584 \\n-L 1991 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \\n-L 3809 1919 \\n-L 2778 1919 \\n-L 2778 2438 \\n-L 4434 2438 \\n-L 4434 434 \\n-Q 4069 175 3628 42 \\n-Q 3188 -91 2688 -91 \\n-Q 1594 -91 976 548 \\n-Q 359 1188 359 2328 \\n-Q 359 3472 976 4111 \\n-Q 1594 4750 2688 4750 \\n-Q 3144 4750 3555 4637 \\n-Q 3966 4525 4313 4306 \\n-L 4313 3634 \\n-Q 3963 3931 3569 4081 \\n-Q 3175 4231 2741 4231 \\n-Q 1884 4231 1454 3753 \\n-Q 1025 3275 1025 2328 \\n-Q 1025 1384 1454 906 \\n-Q 1884 428 2741 428 \\n-Q 3075 428 3337 486 \\n-Q 3600 544 3809 666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \\n-L 1259 2394 \\n-L 2053 2394 \\n-Q 2494 2394 2734 2622 \\n-Q 2975 2850 2975 3272 \\n-Q 2975 3691 2734 3919 \\n-Q 2494 4147 2053 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 2053 4666 \\n-Q 2838 4666 3239 4311 \\n-Q 3641 3956 3641 3272 \\n-Q 3641 2581 3239 2228 \\n-Q 2838 1875 2053 1875 \\n-L 1259 1875 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \\n-Q 1816 -950 1584 -1140 \\n-Q 1353 -1331 966 -1331 \\n-L 506 -1331 \\n-L 506 -850 \\n-L 844 -850 \\n-Q 1081 -850 1212 -737 \\n-Q 1344 -625 1503 -206 \\n-L 1606 56 \\n-L 191 3500 \\n-L 800 3500 \\n-L 1894 763 \\n-L 2988 3500 \\n-L 3597 3500 \\n-L 2059 -325 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      </defs>\\n-      <use xlink:href=\"#DejaVuSans-54\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"57.958984\"/>\\n-      <use xlink:href=\"#DejaVuSans-6d\" x=\"85.742188\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"183.154297\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"244.677734\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"276.464844\"/>\\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"315.673828\"/>\\n-      <use xlink:href=\"#DejaVuSans-6b\" x=\"376.953125\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.238281\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"492.761719\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"556.140625\"/>\\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"587.927734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"626.941406\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"654.724609\"/>\\n-      <use xlink:href=\"#DejaVuSans-77\" x=\"715.90625\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"797.693359\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"859.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"900.330078\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"932.117188\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"959.900391\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1012\"/>\\n-      <use xlink:href=\"#DejaVuSans-62\" x=\"1043.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1107.263672\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1168.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1207.996094\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1247.205078\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"1308.728516\"/>\\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"1349.841797\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"1388.855469\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1420.642578\"/>\\n-      <use xlink:href=\"#DejaVuSans-2a\" x=\"1452.429688\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1502.429688\"/>\\n-      <use xlink:href=\"#DejaVuSans-55\" x=\"1534.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"1607.410156\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"1670.789062\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"1722.888672\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"1750.671875\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1811.853516\"/>\\n-      <use xlink:href=\"#DejaVuSans-68\" x=\"1851.0625\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1914.441406\"/>\\n-      <use xlink:href=\"#DejaVuSans-4f\" x=\"1946.228516\"/>\\n-      <use xlink:href=\"#DejaVuSans-70\" x=\"2024.939453\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"2088.416016\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"2149.939453\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2213.318359\"/>\\n-      <use xlink:href=\"#DejaVuSans-75\" x=\"2245.105469\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"2308.484375\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"2360.583984\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"2422.107422\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2474.207031\"/>\\n-      <use xlink:href=\"#DejaVuSans-31\" x=\"2505.994141\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2569.617188\"/>\\n-      <use xlink:href=\"#DejaVuSans-47\" x=\"2601.404297\"/>\\n-      <use xlink:href=\"#DejaVuSans-50\" x=\"2678.894531\"/>\\n-      <use xlink:href=\"#DejaVuSans-55\" x=\"2739.197266\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2812.390625\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"2844.177734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"2905.359375\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"2968.738281\"/>\\n-      <use xlink:href=\"#DejaVuSans-79\" x=\"2996.521484\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"3041.451172\"/>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"matplotlib.axis_2\">\\n-    <g id=\"ytick_1\">\\n-     <g id=\"line2d_10\">\\n-      <defs>\\n-       <path id=\"me3cc6245ad\" d=\"M 0 0 \\n-L -3.5 0 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"337.528421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_11\">\\n-      <!-- Huggingface -->\\n-      <g transform=\"translate(19.68125 341.32764) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2753 \\n-L 3553 2753 \\n-L 3553 4666 \\n-L 4184 4666 \\n-L 4184 0 \\n-L 3553 0 \\n-L 3553 2222 \\n-L 1259 2222 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \\n-Q 2906 2416 2648 2759 \\n-Q 2391 3103 1925 3103 \\n-Q 1463 3103 1205 2759 \\n-Q 947 2416 947 1791 \\n-Q 947 1169 1205 825 \\n-Q 1463 481 1925 481 \\n-Q 2391 481 2648 825 \\n-Q 2906 1169 2906 1791 \\n-z\\n-M 3481 434 \\n-Q 3481 -459 3084 -895 \\n-Q 2688 -1331 1869 -1331 \\n-Q 1566 -1331 1297 -1286 \\n-Q 1028 -1241 775 -1147 \\n-L 775 -588 \\n-Q 1028 -725 1275 -790 \\n-Q 1522 -856 1778 -856 \\n-Q 2344 -856 2625 -561 \\n-Q 2906 -266 2906 331 \\n-L 2906 616 \\n-Q 2728 306 2450 153 \\n-Q 2172 0 1784 0 \\n-Q 1141 0 747 490 \\n-Q 353 981 353 1791 \\n-Q 353 2603 747 3093 \\n-Q 1141 3584 1784 3584 \\n-Q 2172 3584 2450 3431 \\n-Q 2728 3278 2906 2969 \\n-L 2906 3500 \\n-L 3481 3500 \\n-L 3481 434 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \\n-L 2375 4384 \\n-L 1825 4384 \\n-Q 1516 4384 1395 4259 \\n-Q 1275 4134 1275 3809 \\n-L 1275 3500 \\n-L 2222 3500 \\n-L 2222 3053 \\n-L 1275 3053 \\n-L 1275 0 \\n-L 697 0 \\n-L 697 3053 \\n-L 147 3053 \\n-L 147 3500 \\n-L 697 3500 \\n-L 697 3744 \\n-Q 697 4328 969 4595 \\n-Q 1241 4863 1831 4863 \\n-L 2375 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \\n-L 3122 2828 \\n-Q 2878 2963 2633 3030 \\n-Q 2388 3097 2138 3097 \\n-Q 1578 3097 1268 2742 \\n-Q 959 2388 959 1747 \\n-Q 959 1106 1268 751 \\n-Q 1578 397 2138 397 \\n-Q 2388 397 2633 464 \\n-Q 2878 531 3122 666 \\n-L 3122 134 \\n-Q 2881 22 2623 -34 \\n-Q 2366 -91 2075 -91 \\n-Q 1284 -91 818 406 \\n-Q 353 903 353 1747 \\n-Q 353 2603 823 3093 \\n-Q 1294 3584 2113 3584 \\n-Q 2378 3584 2631 3529 \\n-Q 2884 3475 3122 3366 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-48\"/>\\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"138.574219\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"202.050781\"/>\\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"265.527344\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.310547\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"356.689453\"/>\\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"420.166016\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"455.371094\"/>\\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"516.650391\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"571.630859\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_2\">\\n-     <g id=\"line2d_11\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"257.949474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_12\">\\n-      <!-- Unsloth Open* -->\\n-      <g transform=\"translate(10.090625 261.748692) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4f\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"490.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"554.199219\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"615.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-2a\" x=\"679.101562\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_3\">\\n-     <g id=\"line2d_12\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"178.370526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_13\">\\n-      <!-- Unsloth Pro -->\\n-      <g transform=\"translate(25.942187 182.169745) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-50\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"470.564453\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"509.427734\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_4\">\\n-     <g id=\"line2d_13\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"98.791579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_14\">\\n-      <!-- Unsloth Max -->\\n-      <g transform=\"translate(21.126562 102.590798) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \\n-L 1569 4666 \\n-L 2759 1491 \\n-L 3956 4666 \\n-L 4897 4666 \\n-L 4897 0 \\n-L 4281 0 \\n-L 4281 4097 \\n-L 3078 897 \\n-L 2444 897 \\n-L 1241 4097 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \\n-L 2247 1797 \\n-L 3578 0 \\n-L 2900 0 \\n-L 1881 1375 \\n-L 863 0 \\n-L 184 0 \\n-L 1544 1831 \\n-L 300 3500 \\n-L 978 3500 \\n-L 1906 2253 \\n-L 2834 3500 \\n-L 3513 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4d\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"498.291016\"/>\\n-       <use xlink:href=\"#DejaVuSans-78\" x=\"559.570312\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"patch_7\">\\n-    <path d=\"M 90 384.48 \\n-L 90 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_8\">\\n-    <path d=\"M 648 384.48 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_9\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_10\">\\n-    <path d=\"M 90 51.84 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"text_15\">\\n-    <!-- 163.80 hrs (1.0X) -->\\n-    <g transform=\"translate(491.653585 340.839671) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \\n-Q 3050 2419 3304 2112 \\n-Q 3559 1806 3559 1356 \\n-Q 3559 666 3084 287 \\n-Q 2609 -91 1734 -91 \\n-Q 1441 -91 1130 -33 \\n-Q 819 25 488 141 \\n-L 488 750 \\n-Q 750 597 1062 519 \\n-Q 1375 441 1716 441 \\n-Q 2309 441 2620 675 \\n-Q 2931 909 2931 1356 \\n-Q 2931 1769 2642 2001 \\n-Q 2353 2234 1838 2234 \\n-L 1294 2234 \\n-L 1294 2753 \\n-L 1863 2753 \\n-Q 2328 2753 2575 2939 \\n-Q 2822 3125 2822 3475 \\n-Q 2822 3834 2567 4026 \\n-Q 2313 4219 1838 4219 \\n-Q 1578 4219 1281 4162 \\n-Q 984 4106 628 3988 \\n-L 628 4550 \\n-Q 988 4650 1302 4700 \\n-Q 1616 4750 1894 4750 \\n-Q 2613 4750 3031 4423 \\n-Q 3450 4097 3450 3541 \\n-Q 3450 3153 3228 2886 \\n-Q 3006 2619 2597 2516 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-58\" d=\"M 403 4666 \\n-L 1081 4666 \\n-L 2241 2931 \\n-L 3406 4666 \\n-L 4084 4666 \\n-L 2584 2425 \\n-L 4184 0 \\n-L 3506 0 \\n-L 2194 1984 \\n-L 872 0 \\n-L 191 0 \\n-L 1856 2491 \\n-L 403 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-31\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_16\">\\n-    <!-- 34.57 hrs (4.7X) -->\\n-    <g transform=\"translate(205.391593 261.260724) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-35\" d=\"M 691 4666 \\n-L 3169 4666 \\n-L 3169 4134 \\n-L 1269 4134 \\n-L 1269 2991 \\n-Q 1406 3038 1543 3061 \\n-Q 1681 3084 1819 3084 \\n-Q 2600 3084 3056 2656 \\n-Q 3513 2228 3513 1497 \\n-Q 3513 744 3044 326 \\n-Q 2575 -91 1722 -91 \\n-Q 1428 -91 1123 -41 \\n-Q 819 9 494 109 \\n-L 494 744 \\n-Q 775 591 1075 516 \\n-Q 1375 441 1709 441 \\n-Q 2250 441 2565 725 \\n-Q 2881 1009 2881 1497 \\n-Q 2881 1984 2565 2268 \\n-Q 2250 2553 1709 2553 \\n-Q 1456 2553 1204 2497 \\n-Q 953 2441 691 2322 \\n-L 691 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \\n-L 3525 4666 \\n-L 3525 4397 \\n-L 1831 0 \\n-L 1172 0 \\n-L 2766 4134 \\n-L 525 4134 \\n-L 525 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-33\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"318.066406\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"381.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"422.558594\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"474.658203\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"506.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_17\">\\n-    <!-- 5.72 hrs (28.7X) -->\\n-    <g transform=\"translate(111.791383 181.681776) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"254.443359\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"317.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"358.935547\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"411.035156\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"442.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"481.835938\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_18\">\\n-    <!-- 5.23 hrs (31.3X) -->\\n-    <g transform=\"translate(110.223269 102.102829) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"254.443359\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"317.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"358.935547\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"411.035156\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"442.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"481.835938\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_19\">\\n-    <!-- LAION Chip2 210K (1 epoch on 2 T4 GPUs DDP) -->\\n-    <g transform=\"translate(178.88375 45.84) scale(0.16 -0.16)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 531 \\n-L 3531 531 \\n-L 3531 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \\n-L 1331 1722 \\n-L 3047 1722 \\n-L 2188 4044 \\n-z\\n-M 1831 4666 \\n-L 2547 4666 \\n-L 4325 0 \\n-L 3669 0 \\n-L 3244 1197 \\n-L 1141 1197 \\n-L 716 0 \\n-L 50 0 \\n-L 1831 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-49\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \\n-L 1478 4666 \\n-L 3547 763 \\n-L 3547 4666 \\n-L 4159 4666 \\n-L 4159 0 \\n-L 3309 0 \\n-L 1241 3903 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \\n-L 4122 3641 \\n-Q 3803 3938 3442 4084 \\n-Q 3081 4231 2675 4231 \\n-Q 1875 4231 1450 3742 \\n-Q 1025 3253 1025 2328 \\n-Q 1025 1406 1450 917 \\n-Q 1875 428 2675 428 \\n-Q 3081 428 3442 575 \\n-Q 3803 722 4122 1019 \\n-L 4122 359 \\n-Q 3791 134 3420 21 \\n-Q 3050 -91 2638 -91 \\n-Q 1578 -91 968 557 \\n-Q 359 1206 359 2328 \\n-Q 359 3453 968 4101 \\n-Q 1578 4750 2638 4750 \\n-Q 3056 4750 3426 4639 \\n-Q 3797 4528 4122 4306 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2694 \\n-L 3353 4666 \\n-L 4166 4666 \\n-L 1850 2491 \\n-L 4331 0 \\n-L 3500 0 \\n-L 1259 2247 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \\n-L 1259 519 \\n-L 2022 519 \\n-Q 2988 519 3436 956 \\n-Q 3884 1394 3884 2338 \\n-Q 3884 3275 3436 3711 \\n-Q 2988 4147 2022 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 1925 4666 \\n-Q 3281 4666 3915 4102 \\n-Q 4550 3538 4550 2338 \\n-Q 4550 1131 3912 565 \\n-Q 3275 0 1925 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-4c\"/>\\n-     <use xlink:href=\"#DejaVuSans-41\" x=\"57.962891\"/>\\n-     <use xlink:href=\"#DejaVuSans-49\" x=\"126.371094\"/>\\n-     <use xlink:href=\"#DejaVuSans-4f\" x=\"155.863281\"/>\\n-     <use xlink:href=\"#DejaVuSans-4e\" x=\"234.574219\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"309.378906\"/>\\n-     <use xlink:href=\"#DejaVuSans-43\" x=\"341.166016\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"410.990234\"/>\\n-     <use xlink:href=\"#DejaVuSans-69\" x=\"474.369141\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"502.152344\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"565.628906\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"629.251953\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"661.039062\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"724.662109\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"788.285156\"/>\\n-     <use xlink:href=\"#DejaVuSans-4b\" x=\"851.908203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"917.484375\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"949.271484\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"988.285156\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1051.908203\"/>\\n-     <use xlink:href=\"#DejaVuSans-65\" x=\"1083.695312\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"1145.21875\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1208.695312\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"1269.876953\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"1324.857422\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1388.236328\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1420.023438\"/>\\n-     <use xlink:href=\"#DejaVuSans-6e\" x=\"1481.205078\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1544.583984\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"1576.371094\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1639.994141\"/>\\n-     <use xlink:href=\"#DejaVuSans-54\" x=\"1671.78125\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"1732.865234\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1796.488281\"/>\\n-     <use xlink:href=\"#DejaVuSans-47\" x=\"1828.275391\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"1905.765625\"/>\\n-     <use xlink:href=\"#DejaVuSans-55\" x=\"1966.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"2039.261719\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"2091.361328\"/>\\n-     <use xlink:href=\"#DejaVuSans-44\" x=\"2123.148438\"/>\\n-     <use xlink:href=\"#DejaVuSans-44\" x=\"2200.150391\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"2277.152344\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"2337.455078\"/>\\n-    </g>\\n-   </g>\\n-  </g>\\n- </g>\\n- <defs>\\n-  <clipPath id=\"p42f2634aae\">\\n-   <rect x=\"90\" y=\"51.84\" width=\"558\" height=\"332.64\"/>\\n-  </clipPath>\\n- </defs>\\n-</svg>\\n',\n",
       " '@@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\n-  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n- <metadata>\\n-  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n-   <cc:Work>\\n-    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\\n-    <dc:date>2023-11-30T13:15:28.212061</dc:date>\\n-    <dc:format>image/svg+xml</dc:format>\\n-    <dc:creator>\\n-     <cc:Agent>\\n-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\\n-     </cc:Agent>\\n-    </dc:creator>\\n-   </cc:Work>\\n-  </rdf:RDF>\\n- </metadata>\\n- <defs>\\n-  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\\n- </defs>\\n- <g id=\"figure_1\">\\n-  <g id=\"patch_1\">\\n-   <path d=\"M 0 432 \\n-L 720 432 \\n-L 720 0 \\n-L 0 0 \\n-L 0 432 \\n-z\\n-\" style=\"fill: none\"/>\\n-  </g>\\n-  <g id=\"axes_1\">\\n-   <g id=\"patch_2\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-L 648 51.84 \\n-L 90 51.84 \\n-L 90 384.48 \\n-z\\n-\" style=\"fill: none\"/>\\n-   </g>\\n-   <g id=\"patch_3\">\\n-    <path d=\"M 90 369.36 \\n-L 621.428571 369.36 \\n-L 621.428571 305.696842 \\n-L 90 305.696842 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_4\">\\n-    <path d=\"M 90 289.781053 \\n-L 415.717933 289.781053 \\n-L 415.717933 226.117895 \\n-L 90 226.117895 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_5\">\\n-    <path d=\"M 90 210.202105 \\n-L 287.004626 210.202105 \\n-L 287.004626 146.538947 \\n-L 90 146.538947 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_6\">\\n-    <path d=\"M 90 130.623158 \\n-L 159.87423 130.623158 \\n-L 159.87423 66.96 \\n-L 90 66.96 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"matplotlib.axis_1\">\\n-    <g id=\"xtick_1\">\\n-     <g id=\"line2d_1\">\\n-      <defs>\\n-       <path id=\"m7e152711a6\" d=\"M 0 0 \\n-L 0 3.5 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"90\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_1\">\\n-      <!-- 0 -->\\n-      <g transform=\"translate(86.81875 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \\n-Q 1547 4250 1301 3770 \\n-Q 1056 3291 1056 2328 \\n-Q 1056 1369 1301 889 \\n-Q 1547 409 2034 409 \\n-Q 2525 409 2770 889 \\n-Q 3016 1369 3016 2328 \\n-Q 3016 3291 2770 3770 \\n-Q 2525 4250 2034 4250 \\n-z\\n-M 2034 4750 \\n-Q 2819 4750 3233 4129 \\n-Q 3647 3509 3647 2328 \\n-Q 3647 1150 3233 529 \\n-Q 2819 -91 2034 -91 \\n-Q 1250 -91 836 529 \\n-Q 422 1150 422 2328 \\n-Q 422 3509 836 4129 \\n-Q 1250 4750 2034 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-30\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_2\">\\n-     <g id=\"line2d_2\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"157.839059\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_2\">\\n-      <!-- 50 -->\\n-      <g transform=\"translate(151.476559 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \\n-L 3169 4666 \\n-L 3169 4134 \\n-L 1269 4134 \\n-L 1269 2991 \\n-Q 1406 3038 1543 3061 \\n-Q 1681 3084 1819 3084 \\n-Q 2600 3084 3056 2656 \\n-Q 3513 2228 3513 1497 \\n-Q 3513 744 3044 326 \\n-Q 2575 -91 1722 -91 \\n-Q 1428 -91 1123 -41 \\n-Q 819 9 494 109 \\n-L 494 744 \\n-Q 775 591 1075 516 \\n-Q 1375 441 1709 441 \\n-Q 2250 441 2565 725 \\n-Q 2881 1009 2881 1497 \\n-Q 2881 1984 2565 2268 \\n-Q 2250 2553 1709 2553 \\n-Q 1456 2553 1204 2497 \\n-Q 953 2441 691 2322 \\n-L 691 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-35\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_3\">\\n-     <g id=\"line2d_3\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"225.678117\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_3\">\\n-      <!-- 100 -->\\n-      <g transform=\"translate(216.134367 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-31\" d=\"M 794 531 \\n-L 1825 531 \\n-L 1825 4091 \\n-L 703 3866 \\n-L 703 4441 \\n-L 1819 4666 \\n-L 2450 4666 \\n-L 2450 531 \\n-L 3481 531 \\n-L 3481 0 \\n-L 794 0 \\n-L 794 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_4\">\\n-     <g id=\"line2d_4\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"293.517176\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_4\">\\n-      <!-- 150 -->\\n-      <g transform=\"translate(283.973426 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_5\">\\n-     <g id=\"line2d_5\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"361.356234\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_5\">\\n-      <!-- 200 -->\\n-      <g transform=\"translate(351.812484 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \\n-L 3431 531 \\n-L 3431 0 \\n-L 469 0 \\n-L 469 531 \\n-Q 828 903 1448 1529 \\n-Q 2069 2156 2228 2338 \\n-Q 2531 2678 2651 2914 \\n-Q 2772 3150 2772 3378 \\n-Q 2772 3750 2511 3984 \\n-Q 2250 4219 1831 4219 \\n-Q 1534 4219 1204 4116 \\n-Q 875 4013 500 3803 \\n-L 500 4441 \\n-Q 881 4594 1212 4672 \\n-Q 1544 4750 1819 4750 \\n-Q 2544 4750 2975 4387 \\n-Q 3406 4025 3406 3419 \\n-Q 3406 3131 3298 2873 \\n-Q 3191 2616 2906 2266 \\n-Q 2828 2175 2409 1742 \\n-Q 1991 1309 1228 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_6\">\\n-     <g id=\"line2d_6\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"429.195293\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_6\">\\n-      <!-- 250 -->\\n-      <g transform=\"translate(419.651543 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_7\">\\n-     <g id=\"line2d_7\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"497.034351\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_7\">\\n-      <!-- 300 -->\\n-      <g transform=\"translate(487.490601 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \\n-Q 3050 2419 3304 2112 \\n-Q 3559 1806 3559 1356 \\n-Q 3559 666 3084 287 \\n-Q 2609 -91 1734 -91 \\n-Q 1441 -91 1130 -33 \\n-Q 819 25 488 141 \\n-L 488 750 \\n-Q 750 597 1062 519 \\n-Q 1375 441 1716 441 \\n-Q 2309 441 2620 675 \\n-Q 2931 909 2931 1356 \\n-Q 2931 1769 2642 2001 \\n-Q 2353 2234 1838 2234 \\n-L 1294 2234 \\n-L 1294 2753 \\n-L 1863 2753 \\n-Q 2328 2753 2575 2939 \\n-Q 2822 3125 2822 3475 \\n-Q 2822 3834 2567 4026 \\n-Q 2313 4219 1838 4219 \\n-Q 1578 4219 1281 4162 \\n-Q 984 4106 628 3988 \\n-L 628 4550 \\n-Q 988 4650 1302 4700 \\n-Q 1616 4750 1894 4750 \\n-Q 2613 4750 3031 4423 \\n-Q 3450 4097 3450 3541 \\n-Q 3450 3153 3228 2886 \\n-Q 3006 2619 2597 2516 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-33\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_8\">\\n-     <g id=\"line2d_8\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"564.87341\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_8\">\\n-      <!-- 350 -->\\n-      <g transform=\"translate(555.32966 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-33\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_9\">\\n-     <g id=\"line2d_9\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"632.712468\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_9\">\\n-      <!-- 400 -->\\n-      <g transform=\"translate(623.168718 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \\n-L 825 1625 \\n-L 2419 1625 \\n-L 2419 4116 \\n-z\\n-M 2253 4666 \\n-L 3047 4666 \\n-L 3047 1625 \\n-L 3713 1625 \\n-L 3713 1100 \\n-L 3047 1100 \\n-L 3047 0 \\n-L 2419 0 \\n-L 2419 1100 \\n-L 313 1100 \\n-L 313 1709 \\n-L 2253 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-34\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"text_10\">\\n-     <!-- Time taken (lower is better). -->\\n-     <g transform=\"translate(283.763438 414.27625) scale(0.12 -0.12)\">\\n-      <defs>\\n-       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \\n-L 3928 4666 \\n-L 3928 4134 \\n-L 2272 4134 \\n-L 2272 0 \\n-L 1638 0 \\n-L 1638 4134 \\n-L -19 4134 \\n-L -19 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \\n-L 1178 3500 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 3500 \\n-z\\n-M 603 4863 \\n-L 1178 4863 \\n-L 1178 4134 \\n-L 603 4134 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \\n-Q 3544 3216 3844 3400 \\n-Q 4144 3584 4550 3584 \\n-Q 5097 3584 5394 3201 \\n-Q 5691 2819 5691 2113 \\n-L 5691 0 \\n-L 5113 0 \\n-L 5113 2094 \\n-Q 5113 2597 4934 2840 \\n-Q 4756 3084 4391 3084 \\n-Q 3944 3084 3684 2787 \\n-Q 3425 2491 3425 1978 \\n-L 3425 0 \\n-L 2847 0 \\n-L 2847 2094 \\n-Q 2847 2600 2669 2842 \\n-Q 2491 3084 2119 3084 \\n-Q 1678 3084 1418 2786 \\n-Q 1159 2488 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1356 3278 1631 3431 \\n-Q 1906 3584 2284 3584 \\n-Q 2666 3584 2933 3390 \\n-Q 3200 3197 3328 2828 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \\n-L 3597 1613 \\n-L 953 1613 \\n-Q 991 1019 1311 708 \\n-Q 1631 397 2203 397 \\n-Q 2534 397 2845 478 \\n-Q 3156 559 3463 722 \\n-L 3463 178 \\n-Q 3153 47 2828 -22 \\n-Q 2503 -91 2169 -91 \\n-Q 1331 -91 842 396 \\n-Q 353 884 353 1716 \\n-Q 353 2575 817 3079 \\n-Q 1281 3584 2069 3584 \\n-Q 2775 3584 3186 3129 \\n-Q 3597 2675 3597 1894 \\n-z\\n-M 3022 2063 \\n-Q 3016 2534 2758 2815 \\n-Q 2500 3097 2075 3097 \\n-Q 1594 3097 1305 2825 \\n-Q 1016 2553 972 2059 \\n-L 3022 2063 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \\n-L 1172 3500 \\n-L 2356 3500 \\n-L 2356 3053 \\n-L 1172 3053 \\n-L 1172 1153 \\n-Q 1172 725 1289 603 \\n-Q 1406 481 1766 481 \\n-L 2356 481 \\n-L 2356 0 \\n-L 1766 0 \\n-Q 1100 0 847 248 \\n-Q 594 497 594 1153 \\n-L 594 3053 \\n-L 172 3053 \\n-L 172 3500 \\n-L 594 3500 \\n-L 594 4494 \\n-L 1172 4494 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \\n-Q 1497 1759 1228 1600 \\n-Q 959 1441 959 1056 \\n-Q 959 750 1161 570 \\n-Q 1363 391 1709 391 \\n-Q 2188 391 2477 730 \\n-Q 2766 1069 2766 1631 \\n-L 2766 1759 \\n-L 2194 1759 \\n-z\\n-M 3341 1997 \\n-L 3341 0 \\n-L 2766 0 \\n-L 2766 531 \\n-Q 2569 213 2275 61 \\n-Q 1981 -91 1556 -91 \\n-Q 1019 -91 701 211 \\n-Q 384 513 384 1019 \\n-Q 384 1609 779 1909 \\n-Q 1175 2209 1959 2209 \\n-L 2766 2209 \\n-L 2766 2266 \\n-Q 2766 2663 2505 2880 \\n-Q 2244 3097 1772 3097 \\n-Q 1472 3097 1187 3025 \\n-Q 903 2953 641 2809 \\n-L 641 3341 \\n-Q 956 3463 1253 3523 \\n-Q 1550 3584 1831 3584 \\n-Q 2591 3584 2966 3190 \\n-Q 3341 2797 3341 1997 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \\n-L 1159 4863 \\n-L 1159 1991 \\n-L 2875 3500 \\n-L 3609 3500 \\n-L 1753 1863 \\n-L 3688 0 \\n-L 2938 0 \\n-L 1159 1709 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \\n-Q 1566 4138 1362 3434 \\n-Q 1159 2731 1159 2009 \\n-Q 1159 1288 1364 580 \\n-Q 1569 -128 1984 -844 \\n-L 1484 -844 \\n-Q 1016 -109 783 600 \\n-Q 550 1309 550 2009 \\n-Q 550 2706 781 3412 \\n-Q 1013 4119 1484 4856 \\n-L 1984 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \\n-L 1178 4863 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \\n-Q 1497 3097 1228 2736 \\n-Q 959 2375 959 1747 \\n-Q 959 1119 1226 758 \\n-Q 1494 397 1959 397 \\n-Q 2419 397 2687 759 \\n-Q 2956 1122 2956 1747 \\n-Q 2956 2369 2687 2733 \\n-Q 2419 3097 1959 3097 \\n-z\\n-M 1959 3584 \\n-Q 2709 3584 3137 3096 \\n-Q 3566 2609 3566 1747 \\n-Q 3566 888 3137 398 \\n-Q 2709 -91 1959 -91 \\n-Q 1206 -91 779 398 \\n-Q 353 888 353 1747 \\n-Q 353 2609 779 3096 \\n-Q 1206 3584 1959 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \\n-L 844 3500 \\n-L 1563 769 \\n-L 2278 3500 \\n-L 2956 3500 \\n-L 3675 769 \\n-L 4391 3500 \\n-L 4966 3500 \\n-L 4050 0 \\n-L 3372 0 \\n-L 2619 2869 \\n-L 1863 0 \\n-L 1184 0 \\n-L 269 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \\n-Q 2534 3019 2420 3045 \\n-Q 2306 3072 2169 3072 \\n-Q 1681 3072 1420 2755 \\n-Q 1159 2438 1159 1844 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1341 3275 1631 3429 \\n-Q 1922 3584 2338 3584 \\n-Q 2397 3584 2469 3576 \\n-Q 2541 3569 2628 3553 \\n-L 2631 2963 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \\n-L 2834 2853 \\n-Q 2591 2978 2328 3040 \\n-Q 2066 3103 1784 3103 \\n-Q 1356 3103 1142 2972 \\n-Q 928 2841 928 2578 \\n-Q 928 2378 1081 2264 \\n-Q 1234 2150 1697 2047 \\n-L 1894 2003 \\n-Q 2506 1872 2764 1633 \\n-Q 3022 1394 3022 966 \\n-Q 3022 478 2636 193 \\n-Q 2250 -91 1575 -91 \\n-Q 1294 -91 989 -36 \\n-Q 684 19 347 128 \\n-L 347 722 \\n-Q 666 556 975 473 \\n-Q 1284 391 1588 391 \\n-Q 1994 391 2212 530 \\n-Q 2431 669 2431 922 \\n-Q 2431 1156 2273 1281 \\n-Q 2116 1406 1581 1522 \\n-L 1381 1569 \\n-Q 847 1681 609 1914 \\n-Q 372 2147 372 2553 \\n-Q 372 3047 722 3315 \\n-Q 1072 3584 1716 3584 \\n-Q 2034 3584 2315 3537 \\n-Q 2597 3491 2834 3397 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-M 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2969 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \\n-L 1013 4856 \\n-Q 1481 4119 1714 3412 \\n-Q 1947 2706 1947 2009 \\n-Q 1947 1309 1714 600 \\n-Q 1481 -109 1013 -844 \\n-L 513 -844 \\n-Q 928 -128 1133 580 \\n-Q 1338 1288 1338 2009 \\n-Q 1338 2731 1133 3434 \\n-Q 928 4138 513 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2e\" d=\"M 684 794 \\n-L 1344 794 \\n-L 1344 0 \\n-L 684 0 \\n-L 684 794 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      </defs>\\n-      <use xlink:href=\"#DejaVuSans-54\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"57.958984\"/>\\n-      <use xlink:href=\"#DejaVuSans-6d\" x=\"85.742188\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"183.154297\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"244.677734\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"276.464844\"/>\\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"315.673828\"/>\\n-      <use xlink:href=\"#DejaVuSans-6b\" x=\"376.953125\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.238281\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"492.761719\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"556.140625\"/>\\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"587.927734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"626.941406\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"654.724609\"/>\\n-      <use xlink:href=\"#DejaVuSans-77\" x=\"715.90625\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"797.693359\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"859.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"900.330078\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"932.117188\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"959.900391\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1012\"/>\\n-      <use xlink:href=\"#DejaVuSans-62\" x=\"1043.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1107.263672\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1168.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1207.996094\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1247.205078\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"1308.728516\"/>\\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"1349.841797\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"1388.855469\"/>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"matplotlib.axis_2\">\\n-    <g id=\"ytick_1\">\\n-     <g id=\"line2d_10\">\\n-      <defs>\\n-       <path id=\"m45234ecef3\" d=\"M 0 0 \\n-L -3.5 0 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"337.528421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_11\">\\n-      <!-- Huggingface -->\\n-      <g transform=\"translate(19.68125 341.32764) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2753 \\n-L 3553 2753 \\n-L 3553 4666 \\n-L 4184 4666 \\n-L 4184 0 \\n-L 3553 0 \\n-L 3553 2222 \\n-L 1259 2222 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-75\" d=\"M 544 1381 \\n-L 544 3500 \\n-L 1119 3500 \\n-L 1119 1403 \\n-Q 1119 906 1312 657 \\n-Q 1506 409 1894 409 \\n-Q 2359 409 2629 706 \\n-Q 2900 1003 2900 1516 \\n-L 2900 3500 \\n-L 3475 3500 \\n-L 3475 0 \\n-L 2900 0 \\n-L 2900 538 \\n-Q 2691 219 2414 64 \\n-Q 2138 -91 1772 -91 \\n-Q 1169 -91 856 284 \\n-Q 544 659 544 1381 \\n-z\\n-M 1991 3584 \\n-L 1991 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \\n-Q 2906 2416 2648 2759 \\n-Q 2391 3103 1925 3103 \\n-Q 1463 3103 1205 2759 \\n-Q 947 2416 947 1791 \\n-Q 947 1169 1205 825 \\n-Q 1463 481 1925 481 \\n-Q 2391 481 2648 825 \\n-Q 2906 1169 2906 1791 \\n-z\\n-M 3481 434 \\n-Q 3481 -459 3084 -895 \\n-Q 2688 -1331 1869 -1331 \\n-Q 1566 -1331 1297 -1286 \\n-Q 1028 -1241 775 -1147 \\n-L 775 -588 \\n-Q 1028 -725 1275 -790 \\n-Q 1522 -856 1778 -856 \\n-Q 2344 -856 2625 -561 \\n-Q 2906 -266 2906 331 \\n-L 2906 616 \\n-Q 2728 306 2450 153 \\n-Q 2172 0 1784 0 \\n-Q 1141 0 747 490 \\n-Q 353 981 353 1791 \\n-Q 353 2603 747 3093 \\n-Q 1141 3584 1784 3584 \\n-Q 2172 3584 2450 3431 \\n-Q 2728 3278 2906 2969 \\n-L 2906 3500 \\n-L 3481 3500 \\n-L 3481 434 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \\n-L 2375 4384 \\n-L 1825 4384 \\n-Q 1516 4384 1395 4259 \\n-Q 1275 4134 1275 3809 \\n-L 1275 3500 \\n-L 2222 3500 \\n-L 2222 3053 \\n-L 1275 3053 \\n-L 1275 0 \\n-L 697 0 \\n-L 697 3053 \\n-L 147 3053 \\n-L 147 3500 \\n-L 697 3500 \\n-L 697 3744 \\n-Q 697 4328 969 4595 \\n-Q 1241 4863 1831 4863 \\n-L 2375 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \\n-L 3122 2828 \\n-Q 2878 2963 2633 3030 \\n-Q 2388 3097 2138 3097 \\n-Q 1578 3097 1268 2742 \\n-Q 959 2388 959 1747 \\n-Q 959 1106 1268 751 \\n-Q 1578 397 2138 397 \\n-Q 2388 397 2633 464 \\n-Q 2878 531 3122 666 \\n-L 3122 134 \\n-Q 2881 22 2623 -34 \\n-Q 2366 -91 2075 -91 \\n-Q 1284 -91 818 406 \\n-Q 353 903 353 1747 \\n-Q 353 2603 823 3093 \\n-Q 1294 3584 2113 3584 \\n-Q 2378 3584 2631 3529 \\n-Q 2884 3475 3122 3366 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-48\"/>\\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"138.574219\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"202.050781\"/>\\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"265.527344\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.310547\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"356.689453\"/>\\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"420.166016\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"455.371094\"/>\\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"516.650391\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"571.630859\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_2\">\\n-     <g id=\"line2d_11\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"257.949474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_12\">\\n-      <!-- Unsloth Open -->\\n-      <g transform=\"translate(15.090625 261.748692) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-55\" d=\"M 556 4666 \\n-L 1191 4666 \\n-L 1191 1831 \\n-Q 1191 1081 1462 751 \\n-Q 1734 422 2344 422 \\n-Q 2950 422 3222 751 \\n-Q 3494 1081 3494 1831 \\n-L 3494 4666 \\n-L 4128 4666 \\n-L 4128 1753 \\n-Q 4128 841 3676 375 \\n-Q 3225 -91 2344 -91 \\n-Q 1459 -91 1007 375 \\n-Q 556 841 556 1753 \\n-L 556 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-4f\" d=\"M 2522 4238 \\n-Q 1834 4238 1429 3725 \\n-Q 1025 3213 1025 2328 \\n-Q 1025 1447 1429 934 \\n-Q 1834 422 2522 422 \\n-Q 3209 422 3611 934 \\n-Q 4013 1447 4013 2328 \\n-Q 4013 3213 3611 3725 \\n-Q 3209 4238 2522 4238 \\n-z\\n-M 2522 4750 \\n-Q 3503 4750 4090 4092 \\n-Q 4678 3434 4678 2328 \\n-Q 4678 1225 4090 567 \\n-Q 3503 -91 2522 -91 \\n-Q 1538 -91 948 565 \\n-Q 359 1222 359 2328 \\n-Q 359 3434 948 4092 \\n-Q 1538 4750 2522 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \\n-L 1159 -1331 \\n-L 581 -1331 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-z\\n-M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4f\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"490.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"554.199219\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"615.722656\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_3\">\\n-     <g id=\"line2d_12\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"178.370526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_13\">\\n-      <!-- Unsloth Pro -->\\n-      <g transform=\"translate(25.942187 182.169745) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \\n-L 1259 2394 \\n-L 2053 2394 \\n-Q 2494 2394 2734 2622 \\n-Q 2975 2850 2975 3272 \\n-Q 2975 3691 2734 3919 \\n-Q 2494 4147 2053 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 2053 4666 \\n-Q 2838 4666 3239 4311 \\n-Q 3641 3956 3641 3272 \\n-Q 3641 2581 3239 2228 \\n-Q 2838 1875 2053 1875 \\n-L 1259 1875 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-50\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"470.564453\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"509.427734\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_4\">\\n-     <g id=\"line2d_13\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"98.791579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_14\">\\n-      <!-- Unsloth Max -->\\n-      <g transform=\"translate(21.126562 102.590798) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \\n-L 1569 4666 \\n-L 2759 1491 \\n-L 3956 4666 \\n-L 4897 4666 \\n-L 4897 0 \\n-L 4281 0 \\n-L 4281 4097 \\n-L 3078 897 \\n-L 2444 897 \\n-L 1241 4097 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \\n-L 2247 1797 \\n-L 3578 0 \\n-L 2900 0 \\n-L 1881 1375 \\n-L 863 0 \\n-L 184 0 \\n-L 1544 1831 \\n-L 300 3500 \\n-L 978 3500 \\n-L 1906 2253 \\n-L 2834 3500 \\n-L 3513 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4d\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"498.291016\"/>\\n-       <use xlink:href=\"#DejaVuSans-78\" x=\"559.570312\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"patch_7\">\\n-    <path d=\"M 90 384.48 \\n-L 90 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_8\">\\n-    <path d=\"M 648 384.48 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_9\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_10\">\\n-    <path d=\"M 90 51.84 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"text_15\">\\n-    <!-- 391.68 hrs (1.0X) -->\\n-    <g transform=\"translate(512.886078 340.839671) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-39\" d=\"M 703 97 \\n-L 703 672 \\n-Q 941 559 1184 500 \\n-Q 1428 441 1663 441 \\n-Q 2288 441 2617 861 \\n-Q 2947 1281 2994 2138 \\n-Q 2813 1869 2534 1725 \\n-Q 2256 1581 1919 1581 \\n-Q 1219 1581 811 2004 \\n-Q 403 2428 403 3163 \\n-Q 403 3881 828 4315 \\n-Q 1253 4750 1959 4750 \\n-Q 2769 4750 3195 4129 \\n-Q 3622 3509 3622 2328 \\n-Q 3622 1225 3098 567 \\n-Q 2575 -91 1691 -91 \\n-Q 1453 -91 1209 -44 \\n-Q 966 3 703 97 \\n-z\\n-M 1959 2075 \\n-Q 2384 2075 2632 2365 \\n-Q 2881 2656 2881 3163 \\n-Q 2881 3666 2632 3958 \\n-Q 2384 4250 1959 4250 \\n-Q 1534 4250 1286 3958 \\n-Q 1038 3666 1038 3163 \\n-Q 1038 2656 1286 2365 \\n-Q 1534 2075 1959 2075 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \\n-Q 1688 2584 1439 2293 \\n-Q 1191 2003 1191 1497 \\n-Q 1191 994 1439 701 \\n-Q 1688 409 2113 409 \\n-Q 2538 409 2786 701 \\n-Q 3034 994 3034 1497 \\n-Q 3034 2003 2786 2293 \\n-Q 2538 2584 2113 2584 \\n-z\\n-M 3366 4563 \\n-L 3366 3988 \\n-Q 3128 4100 2886 4159 \\n-Q 2644 4219 2406 4219 \\n-Q 1781 4219 1451 3797 \\n-Q 1122 3375 1075 2522 \\n-Q 1259 2794 1537 2939 \\n-Q 1816 3084 2150 3084 \\n-Q 2853 3084 3261 2657 \\n-Q 3669 2231 3669 1497 \\n-Q 3669 778 3244 343 \\n-Q 2819 -91 2113 -91 \\n-Q 1303 -91 875 529 \\n-Q 447 1150 447 2328 \\n-Q 447 3434 972 4092 \\n-Q 1497 4750 2381 4750 \\n-Q 2619 4750 2861 4703 \\n-Q 3103 4656 3366 4563 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \\n-Q 1584 2216 1326 1975 \\n-Q 1069 1734 1069 1313 \\n-Q 1069 891 1326 650 \\n-Q 1584 409 2034 409 \\n-Q 2484 409 2743 651 \\n-Q 3003 894 3003 1313 \\n-Q 3003 1734 2745 1975 \\n-Q 2488 2216 2034 2216 \\n-z\\n-M 1403 2484 \\n-Q 997 2584 770 2862 \\n-Q 544 3141 544 3541 \\n-Q 544 4100 942 4425 \\n-Q 1341 4750 2034 4750 \\n-Q 2731 4750 3128 4425 \\n-Q 3525 4100 3525 3541 \\n-Q 3525 3141 3298 2862 \\n-Q 3072 2584 2669 2484 \\n-Q 3125 2378 3379 2068 \\n-Q 3634 1759 3634 1313 \\n-Q 3634 634 3220 271 \\n-Q 2806 -91 2034 -91 \\n-Q 1263 -91 848 271 \\n-Q 434 634 434 1313 \\n-Q 434 1759 690 2068 \\n-Q 947 2378 1403 2484 \\n-z\\n-M 1172 3481 \\n-Q 1172 3119 1398 2916 \\n-Q 1625 2713 2034 2713 \\n-Q 2441 2713 2670 2916 \\n-Q 2900 3119 2900 3481 \\n-Q 2900 3844 2670 4047 \\n-Q 2441 4250 2034 4250 \\n-Q 1625 4250 1398 4047 \\n-Q 1172 3844 1172 3481 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-58\" d=\"M 403 4666 \\n-L 1081 4666 \\n-L 2241 2931 \\n-L 3406 4666 \\n-L 4084 4666 \\n-L 2584 2425 \\n-L 4184 0 \\n-L 3506 0 \\n-L 2194 1984 \\n-L 872 0 \\n-L 191 0 \\n-L 1856 2491 \\n-L 403 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-33\"/>\\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_16\">\\n-    <!-- 240.07 hrs (1.6X) -->\\n-    <g transform=\"translate(417.074714 261.260724) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \\n-L 3525 4666 \\n-L 3525 4397 \\n-L 1831 0 \\n-L 1172 0 \\n-L 2766 4134 \\n-L 525 4134 \\n-L 525 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-32\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_17\">\\n-    <!-- 145.20 hrs (2.7X) -->\\n-    <g transform=\"translate(288.361407 181.681776) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-31\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_18\">\\n-    <!-- 51.50 hrs (7.6X) -->\\n-    <g transform=\"translate(161.231011 102.102829) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"318.066406\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"381.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"422.558594\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"474.658203\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"506.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_19\">\\n-    <!-- SlimOrca 518K (1 epoch on 1 T4 GPU) -->\\n-    <g transform=\"translate(217.05625 45.84) scale(0.16 -0.16)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \\n-L 3425 3897 \\n-Q 3066 4069 2747 4153 \\n-Q 2428 4238 2131 4238 \\n-Q 1616 4238 1336 4038 \\n-Q 1056 3838 1056 3469 \\n-Q 1056 3159 1242 3001 \\n-Q 1428 2844 1947 2747 \\n-L 2328 2669 \\n-Q 3034 2534 3370 2195 \\n-Q 3706 1856 3706 1288 \\n-Q 3706 609 3251 259 \\n-Q 2797 -91 1919 -91 \\n-Q 1588 -91 1214 -16 \\n-Q 841 59 441 206 \\n-L 441 856 \\n-Q 825 641 1194 531 \\n-Q 1563 422 1919 422 \\n-Q 2459 422 2753 634 \\n-Q 3047 847 3047 1241 \\n-Q 3047 1584 2836 1778 \\n-Q 2625 1972 2144 2069 \\n-L 1759 2144 \\n-Q 1053 2284 737 2584 \\n-Q 422 2884 422 3419 \\n-Q 422 4038 858 4394 \\n-Q 1294 4750 2059 4750 \\n-Q 2388 4750 2728 4690 \\n-Q 3069 4631 3425 4513 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2694 \\n-L 3353 4666 \\n-L 4166 4666 \\n-L 1850 2491 \\n-L 4331 0 \\n-L 3500 0 \\n-L 1259 2247 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-47\" d=\"M 3809 666 \\n-L 3809 1919 \\n-L 2778 1919 \\n-L 2778 2438 \\n-L 4434 2438 \\n-L 4434 434 \\n-Q 4069 175 3628 42 \\n-Q 3188 -91 2688 -91 \\n-Q 1594 -91 976 548 \\n-Q 359 1188 359 2328 \\n-Q 359 3472 976 4111 \\n-Q 1594 4750 2688 4750 \\n-Q 3144 4750 3555 4637 \\n-Q 3966 4525 4313 4306 \\n-L 4313 3634 \\n-Q 3963 3931 3569 4081 \\n-Q 3175 4231 2741 4231 \\n-Q 1884 4231 1454 3753 \\n-Q 1025 3275 1025 2328 \\n-Q 1025 1384 1454 906 \\n-Q 1884 428 2741 428 \\n-Q 3075 428 3337 486 \\n-Q 3600 544 3809 666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-53\"/>\\n-     <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\\n-     <use xlink:href=\"#DejaVuSans-69\" x=\"91.259766\"/>\\n-     <use xlink:href=\"#DejaVuSans-6d\" x=\"119.042969\"/>\\n-     <use xlink:href=\"#DejaVuSans-4f\" x=\"216.455078\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"295.166016\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"334.029297\"/>\\n-     <use xlink:href=\"#DejaVuSans-61\" x=\"389.009766\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"450.289062\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"482.076172\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"545.699219\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"609.322266\"/>\\n-     <use xlink:href=\"#DejaVuSans-4b\" x=\"672.945312\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"738.521484\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"770.308594\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"809.322266\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"872.945312\"/>\\n-     <use xlink:href=\"#DejaVuSans-65\" x=\"904.732422\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"966.255859\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1029.732422\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"1090.914062\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"1145.894531\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1209.273438\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1241.060547\"/>\\n-     <use xlink:href=\"#DejaVuSans-6e\" x=\"1302.242188\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1365.621094\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"1397.408203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1461.03125\"/>\\n-     <use xlink:href=\"#DejaVuSans-54\" x=\"1492.818359\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"1553.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1617.525391\"/>\\n-     <use xlink:href=\"#DejaVuSans-47\" x=\"1649.3125\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"1726.802734\"/>\\n-     <use xlink:href=\"#DejaVuSans-55\" x=\"1787.105469\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"1860.298828\"/>\\n-    </g>\\n-   </g>\\n-  </g>\\n- </g>\\n- <defs>\\n-  <clipPath id=\"p682a0e8bed\">\\n-   <rect x=\"90\" y=\"51.84\" width=\"558\" height=\"332.64\"/>\\n-  </clipPath>\\n- </defs>\\n-</svg>\\n',\n",
       " 'Binary files /dev/null and b/images/try live demo green.png differ\\n',\n",
       " '@@ -33,7 +33,7 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-\\t\"transformers\",\\n+    \"transformers\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n@@ -70,4 +70,4 @@ colab = [\\n [project.urls]\\n homepage = \"http://www.unsloth.ai\"\\n documentation = \"https://github.com/unslothai/unsloth\"\\n-repository = \"https://github.com/unslothai/unsloth\"\\n\\\\ No newline at end of file\\n+repository = \"https://github.com/unslothai/unsloth\"\\n',\n",
       " '@@ -11,7 +11,7 @@\\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n-__version__ = \"2023.11\"\\n+__version__ = \"2023.12\"\\n import os\\n import warnings\\n import importlib\\n@@ -35,7 +35,7 @@ if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n         )\\n         os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n else:\\n-    warnings.warn(\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is not set. We shall set it ourselves.\")\\n+    # warnings.warn(\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is not set. We shall set it ourselves.\")\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n',\n",
       " '@@ -43,7 +43,6 @@ def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n     mask = col_offsets < n_cols\\n \\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     label_idx = tl.load(labels_ptr).to(tl.int32)\\n     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(\"inf\")).to(tl.float32)\\n     max_logits = tl.max(logits, 0)\\n@@ -88,7 +87,6 @@ def _cross_entropy_backward(logits_ptr, logits_row_stride,\\n     col_offsets = tl.arange(0, BLOCK_SIZE)\\n     mask = col_offsets < n_cols\\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\\n \\n     if label_idx != -100:\\n',\n",
       " '@@ -35,7 +35,6 @@ def _rope_embedding(\\n     mask = col_offsets < half_head_dim\\n \\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     rot_position = row_position % seqlen\\n \\n     Q   += row_position*  Q_row_stride + head_position*head_dim\\n@@ -48,8 +47,6 @@ def _rope_embedding(\\n \\n     Q2   = tl.load(Q   + half_head_dim*1 + col_offsets, mask = mask, other = 0)\\n     # RoPE repeats sin and cos so 128 = [64, 64].\\n-    # sin2 = tl.load(sin + half_head_dim*1, mask = mask, other = 0)\\n-    # cos2 = tl.load(cos + half_head_dim*1, mask = mask, other = 0)\\n \\n     if BACKWARD_PASS:\\n         \"\"\"\\n@@ -62,11 +59,8 @@ def _rope_embedding(\\n             where R.T is again the same  [ 0, -I]\\n             but the minus is transposed. [ I,  0]\\n         \"\"\"\\n-        # sin1, sin2 = -sin1, -sin2\\n         sin1 = -sin1\\n-\\n-    # tl.store(Q + half_head_dim*0, Q1*cos1 - Q2*sin1, mask = mask)\\n-    # tl.store(Q + half_head_dim*1, Q2*cos2 + Q1*sin2, mask = mask)\\n+    \\n     # RoPE repeats sin and cos so 128 = [64, 64].\\n     tl.store(Q + half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)\\n     tl.store(Q + half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)\\n',\n",
       " '@@ -13,12 +13,12 @@\\n # limitations under the License.\\n \\n import triton\\n-MAX_FUSED_SIZE = 65535 # 2**16 - 1\\n+MAX_FUSED_SIZE = 65536 # 2**16 Solves https://github.com/unslothai/unsloth/issues/7\\n next_power_of_2 = triton.next_power_of_2\\n \\n def calculate_settings(n):\\n     BLOCK_SIZE = next_power_of_2(n)\\n-    # CUDA only supports 65535 - 2^16-1 threads per block\\n+    # CUDA only supports 65536 - 2^16 threads per block\\n     if BLOCK_SIZE > MAX_FUSED_SIZE:\\n         raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\\\\n                            f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\\n',\n",
       " '@@ -16,14 +16,15 @@ import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n from transformers.models.llama.modeling_llama import (\\n-    # apply_rotary_pos_emb,\\n-    # repeat_kv,\\n-    # _prepare_4d_causal_attention_mask,\\n+    _prepare_4d_causal_attention_mask,\\n     logger,\\n     BaseModelOutputWithPast,\\n     CausalLMOutputWithPast,\\n )\\n from ..kernels import *\\n+from ._utils import (\\n+    prepare_model_for_kbit_training,\\n+)\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -37,7 +38,6 @@ else:\\n     # Tri Dao\\'s benchmark shows xformers is faster for now.\\n     HAS_FLASH_ATTENTION = False\\n pass\\n-\\n import xformers.ops.fmha as xformers\\n xformers_attention = xformers.memory_efficient_attention\\n \\n@@ -55,12 +55,9 @@ import bitsandbytes as bnb\\n import numpy as np\\n import types\\n \\n-from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n-from ._utils import (\\n-    prepare_model_for_kbit_training,\\n-)\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -92,10 +89,6 @@ def LlamaAttention_fast_forward(\\n ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\\n     \\n     bsz, q_len, _ = hidden_states.size()\\n-\\n-    # Q = self.q_proj(hidden_states)\\n-    # K = self.k_proj(hidden_states)\\n-    # V = self.v_proj(hidden_states)\\n     Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n     n_heads    = self.num_heads\\n@@ -112,8 +105,6 @@ def LlamaAttention_fast_forward(\\n     if past_key_value is not None:\\n         kv_seq_len += past_key_value[0].shape[-2]\\n \\n-    # cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)\\n-    # Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)\\n     if position_ids is None:\\n         cos = self.rotary_emb.cos_cached\\n         sin = self.rotary_emb.sin_cached\\n@@ -130,10 +121,9 @@ def LlamaAttention_fast_forward(\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n-    # no_attention_mask = attention_mask is None\\n-    # Ignore attention_mask\\n-\\n-    if (not HAS_FLASH_ATTENTION): #and no_attention_mask:\\n+    # Xformers doesnt support backward pass for GQA (yet)\\n+    # TEMP fix\\n+    if (n_groups == 1) and (not HAS_FLASH_ATTENTION):\\n         # Xformers memory efficient attention\\n         # Also has Flash Attention v2 dispatching\\n         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)\\n@@ -143,18 +133,17 @@ def LlamaAttention_fast_forward(\\n \\n         # Grouped query attention\\n         if n_groups != 1:\\n-            Q = Q.reshape(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n-\\n-            K = K.reshape(bsz, q_len, n_groups,          1, head_dim)\\n-            V = V.reshape(bsz, q_len, n_groups,          1, head_dim)\\n-            K = K .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n-            V = V .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n+            Q = Q.reshape(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            K = K.reshape(bsz, q_len, n_kv_heads,        1, head_dim)\\n+            V = V.reshape(bsz, q_len, n_kv_heads,        1, head_dim)\\n+            K = K .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            V = V .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n         A = A.view(bsz, q_len, n_heads, head_dim)\\n \\n-    elif HAS_FLASH_ATTENTION:# and no_attention_mask:\\n+    elif HAS_FLASH_ATTENTION:\\n         # Flash Attention\\n         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)\\n         Q = Q.transpose(1, 2)\\n@@ -163,37 +152,22 @@ def LlamaAttention_fast_forward(\\n \\n         # Flash Attention v2 auto supports grouped query attention\\n         A = flash_attn_func(Q, K, V, causal = True)\\n-\\n     else:\\n-        # Uses Pytorch\\'s scaled dot product attention\\n-        if attention_mask is not None:\\n-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\\n-                raise ValueError(\\n-                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\\n-                )\\n-        pass\\n-\\n         # Grouped query attention\\n-        # K = repeat_kv(K, n_groups)\\n-        # V = repeat_kv(V, n_groups)\\n         if n_groups != 1:\\n             K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n             V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n             K = K.reshape(bsz, n_heads, q_len, head_dim)\\n             V = V.reshape(bsz, n_heads, q_len, head_dim)\\n         pass\\n-\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n-        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = attention_mask is None)\\n+        A = scaled_dot_product_attention(Q, K, V, attn_mask = None, is_causal = True)\\n         # Go back to (batch_size, seq_len, n_heads, head_dim)\\n         A = A.transpose(1, 2)\\n     pass\\n     attn_output = A.reshape(bsz, q_len, self.hidden_size)\\n-\\n-    # attn_output = self.o_proj(attn_output)\\n     attn_output = self.apply_o(self, attn_output)\\n-\\n     attn_weights = None\\n     return attn_output, attn_weights, past_key_value\\n pass\\n@@ -227,7 +201,6 @@ def LlamaDecoderLayer_fast_forward(\\n     \"\"\"\\n     residual = hidden_states\\n \\n-    # hidden_states = self.input_layernorm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n \\n     # Self Attention\\n@@ -245,7 +218,6 @@ def LlamaDecoderLayer_fast_forward(\\n \\n     # Fully Connected\\n     residual = hidden_states\\n-    # hidden_states = self.post_attention_layernorm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n     hidden_states = self.mlp(hidden_states)\\n     hidden_states = residual + hidden_states\\n@@ -308,7 +280,7 @@ def LlamaModel_fast_forward(\\n     if (past_key_values_length != 0):\\n         position_ids = torch.arange(\\n             past_key_values_length, seq_length + past_key_values_length,\\n-            dtype  = torch.int32,#dtype=torch.long,\\n+            dtype  = torch.int32,\\n             device = \"cuda\",\\n         )\\n         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\\n@@ -326,11 +298,7 @@ def LlamaModel_fast_forward(\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n     # Ignore attention_mask\\n-    if True:\\n-    # if attention_mask is None:\\n-        # attention_mask = torch.ones(\\n-        #     (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\\n-        # )\\n+    if attention_mask is None:\\n         padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n@@ -339,7 +307,7 @@ def LlamaModel_fast_forward(\\n             padding_mask = None\\n \\n         attention_mask = _prepare_4d_causal_attention_mask(\\n-            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\\n+            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length,\\n         )\\n     pass\\n \\n@@ -403,7 +371,6 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    # hidden_states = self.norm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n \\n     # add hidden states from the last decoder layer\\n@@ -466,19 +433,13 @@ def LlamaForCausalLM_fast_forward(\\n \\n     loss = None\\n     if labels is not None:\\n-        # logits = logits.float()\\n-        # shift_logits = logits[..., :-1, :].contiguous()\\n-        # shift_labels = labels[..., 1:].contiguous()\\n-        # shift_labels = shift_labels.view(-1)\\n-        # shift_logits = shift_logits.view(-1, self.config.vocab_size)\\n         shift_logits = logits\\n+        if not hasattr(self, \"extra_ignored_labels\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/10\\n+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+        pass\\n+        \\n         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-\\n-        # loss_fct = torch.nn.CrossEntropyLoss(\\n-        #     ignore_index = self.ignore_index,\\n-        #     label_smoothing = self.label_smoothing,\\n-        # )\\n-        # loss = loss_fct(shift_logits, shift_labels)\\n         loss = fast_cross_entropy_loss(\\n             logits = shift_logits,\\n             labels = shift_labels,\\n@@ -547,13 +508,14 @@ class FastLlamaModel:\\n         load_in_4bit = True,\\n         token = None,\\n         device_map = \"sequential\",\\n+        rope_scaling = None,\\n     ):\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n \\n         statistics = \\\\\\n-            \"==((====))==  Unsloth: Fast Llama patching release 23.11\\\\n\"\\\\\\n+            \"==((====))==  Unsloth: Fast Llama patching release 2023.12\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n            f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n            f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n@@ -570,9 +532,20 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # [TODO]: Determine RoPE scaling\\n-        # https://github.com/huggingface/transformers/pull/24653\\n-        assert(max_seq_length <= 4096)\\n+        # RoPE scaling\\n+        model_max_seq_length = \\\\\\n+            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+\\n+        if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+            rope_scaling = max_seq_length / model_max_seq_length\\n+            logger.warning_once(\\n+                f\"Unsloth: {model_name} can only handle sequence lengths of of most \"\\\\\\n+                f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n+                f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n+                f\"{max_seq_length}!\"\\n+            )\\n+            rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+        pass\\n \\n         bnb_config = None\\n         if load_in_4bit:\\n@@ -589,6 +562,7 @@ class FastLlamaModel:\\n             torch_dtype = dtype,\\n             quantization_config = bnb_config,\\n             token = token,\\n+            rope_scaling = rope_scaling,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n@@ -596,9 +570,22 @@ class FastLlamaModel:\\n             padding_side = \"right\",\\n             token = token,\\n         )\\n-        tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token});\\n-        tokenizer.pad_token = tokenizer.unk_token\\n-        config = model.config.update({\"pad_token_id\" : tokenizer.unk_token_id});\\n+\\n+        if not hasattr(tokenizer, \"pad_token\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/5\\n+            if hasattr(tokenizer, \"unk_token\"):\\n+                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n+                tokenizer.pad_token = tokenizer.unk_token\\n+            else:\\n+                logger.warning_one(\\n+                    f\"{model_name} does not have a padding or unknown token!\\\\n\"\\\\\\n+                    f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+                )\\n+                assert(hasattr(tokenizer, \"eos_token\"))\\n+                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n+                tokenizer.pad_token = tokenizer.eos_token\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+        pass\\n \\n         model = FastLlamaModel.post_patch(model)\\n \\n@@ -607,6 +594,8 @@ class FastLlamaModel:\\n             layer.self_attn.apply_qkv = original_apply_qkv\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n+\\n+        model.max_seq_length = max_seq_length\\n         return model, tokenizer\\n     pass\\n \\n@@ -668,6 +657,8 @@ class FastLlamaModel:\\n         random_state = 3407,\\n         max_seq_length = 2048,\\n     ):\\n+        assert(max_seq_length <= model.max_seq_length)\\n+\\n         if lora_dropout != 0:\\n             raise TypeError(\"Unsloth: Fast Llama patching only works with dropout = 0.\")\\n         if bias != \"none\":\\n@@ -727,8 +718,14 @@ class FastLlamaModel:\\n         pass\\n \\n         # Patch cross entropy loss labels\\n-        model.model.extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n-        \\n+        # Fixes https://github.com/unslothai/unsloth/issues/10\\n+        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n+        model.model.extra_ignored_labels = extra_ignored_labels\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model.max_seq_length = max_seq_length\\n+            internal_model = internal_model.model\\n+        pass\\n         return model\\n     pass\\n pass\\n',\n",
       " '@@ -1,23 +1,25 @@\\n <div class=\"align-center\">\\n   <img src=\"./images/unsloth new logo.png\" width=\"400\" />\\n   <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord.png\" width=\"180\"></a>\\n+  <a href=\"https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing\"><img src=\"./images/try live demo green.png\" width=\"130\"></a>\\n </div>\\n \\n-\\n-## 80% faster 50% less memory local QLoRA finetuning\\n+## 2-5x faster 50% less memory local LLM finetuning\\n * Manual autograd engine - hand derived backprop steps.\\n-* QLoRA / LoRA 80% faster, 50% less memory.\\n-* All kernels written in OpenAI\\'s Triton language.\\n+* 2x to 5x faster than QLoRA. 50% less memory usage.\\n+* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language.\\n * 0% loss in accuracy - no approximation methods - all exact.\\n-* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+. Tesla T4, RTX 20, 30, 40 series, A100, H100s\\n-* Flash Attention support via Xformers.\\n-* Supports 4bit and 16bit LoRA finetuning.\\n+* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)\\n+* [Flash Attention v2](https://github.com/Dao-AILab/flash-attention) support via [Xformers](https://github.com/facebookresearch/xformers).\\n+* **NEW!** Works on **Linux** and **Windows** via WSL.\\n+* **NEW!** Experimental support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)!\\n+* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n * Train Slim Orca **fully locally in 260 hours from 1301 hours (5x faster).**\\n * Open source version trains 5x faster or you can check out [Unsloth Pro and Max](https://unsloth.ai/) codepaths for **30x faster training**!\\n   \\n <div class=\"align-center\">\\n   <img src=\"./images/Slim Orca 2GPUs.png\" width=\"400\" />\\n-  <img src=\"./images/LAION%202GPU.svg\" width=\"400\" />\\n+  <img src=\"./images/LAION 2GPU.png\" width=\"400\" />\\n </div>\\n \\n 1. Try our Colab examples for [the Alpaca 52K dataset](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) or [the Slim Orca 518K dataset](https://colab.research.google.com/drive/1VNqLARpE8N8eYwNrUSDoHVjtbR9W0_c7?usp=sharing).\\n@@ -49,7 +51,13 @@ pip install --upgrade --force-reinstall --no-cache-dir torch triton \\\\\\n ```\\n Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n \\n-# Alpaca Example\\n+4. If you get errors, try the below first, then go back to step 1:\\n+```\\n+pip install --upgrade pip\\n+```\\n+\\n+# Documentation\\n+We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n ```\\n from unsloth import FastLlamaModel\\n import torch\\n@@ -59,7 +67,7 @@ load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False\\n \\n # Load Llama model\\n model, tokenizer = FastLlamaModel.from_pretrained(\\n-    model_name = \"unsloth/llama-2-7b\", # Supports any llama model\\n+    model_name = \"unsloth/llama-2-7b\", # Supports any llama model eg meta-llama/Llama-2-7b-hf\\n     max_seq_length = max_seq_length,\\n     dtype = dtype,\\n     load_in_4bit = load_in_4bit,\\n@@ -80,12 +88,16 @@ model = FastLlamaModel.get_peft_model(\\n     max_seq_length = max_seq_length,\\n )\\n \\n-trainer = .... Use Huggingface\\'s Trainer and dataset loading\\n+trainer = .... Use Huggingface\\'s Trainer and dataset loading (TRL, transformers etc)\\n ```\\n \\n If you trained a model with Unsloth, we made a cool sticker!!\\n <img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n \\n+# DPO (Direct Preference Optimization) Experimental support\\n+[152334H](https://github.com/152334H) hacked Unsloth to work with DPO via TRL!\\n+1. Hack the model\\'s `config.json` to be llama model. [Example gist](https://gist.github.com/152334H/d8a68b51b83bac008a02e69ecc81d5c1).\\n+2. Use Unsloth for DPO for both base and reference models. [Example gist](https://gist.github.com/152334H/4847f3a8cca12894877e6b30698b0b64).\\n \\n # Future Milestones and limitations\\n 1. Support sqrt gradient checkpointing which further slashes memory usage by 25%.\\n@@ -94,6 +106,9 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n # Performance comparisons on 1 Tesla T4 GPU:\\n **Time taken for 1 epoch**\\n \\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n | System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n | --- | --- | --- | --- | --- | --- |\\n | Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n@@ -113,19 +128,28 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n # Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n **Time taken for 1 epoch**\\n \\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n | --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n \\n **Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n \\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n | --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+\\n+### For replication of timings:\\n+* [Huggingface LAION DDP reference implementation](https://www.kaggle.com/code/danielhanchen/huggingface-original-laion-oig) 60 steps on DDP Kaggle 2 Tesla T4 GPUs takes 40 minutes and 46 seconds\\n+* [Unsloth LAION DDP fast implementation](https://www.kaggle.com/code/danielhanchen/unsloth-laion-chip2-kaggle) 60 steps on DDP Kaggle 2 Tesla T4 GPUs - **Unsloth only uses 1 GPU whilst Pro plans use more.** takes 4 minutes and 34 seconds **(8.64x speedup)**\\n \\n # Troubleshooting\\n 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n@@ -136,4 +160,8 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n \\n 3. If it doesn\\'t install - maybe try updating `pip`.\\n \\n+# Credits\\n+1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n+2. [152334H](https://github.com/152334H) for experimental DPO support\\n+\\n <img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files a/images/Discord.png and b/images/Discord.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/LAION 2GPU.png differ\\n',\n",
       " '@@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\n-  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n- <metadata>\\n-  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n-   <cc:Work>\\n-    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\\n-    <dc:date>2023-11-30T13:10:10.501490</dc:date>\\n-    <dc:format>image/svg+xml</dc:format>\\n-    <dc:creator>\\n-     <cc:Agent>\\n-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\\n-     </cc:Agent>\\n-    </dc:creator>\\n-   </cc:Work>\\n-  </rdf:RDF>\\n- </metadata>\\n- <defs>\\n-  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\\n- </defs>\\n- <g id=\"figure_1\">\\n-  <g id=\"patch_1\">\\n-   <path d=\"M 0 432 \\n-L 720 432 \\n-L 720 0 \\n-L 0 0 \\n-L 0 432 \\n-z\\n-\" style=\"fill: none\"/>\\n-  </g>\\n-  <g id=\"axes_1\">\\n-   <g id=\"patch_2\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-L 648 51.84 \\n-L 90 51.84 \\n-L 90 384.48 \\n-z\\n-\" style=\"fill: none\"/>\\n-   </g>\\n-   <g id=\"patch_3\">\\n-    <path d=\"M 90 369.36 \\n-L 621.428571 369.36 \\n-L 621.428571 305.696842 \\n-L 90 305.696842 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_4\">\\n-    <path d=\"M 90 289.781053 \\n-L 202.147218 289.781053 \\n-L 202.147218 226.117895 \\n-L 90 226.117895 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_5\">\\n-    <path d=\"M 90 210.202105 \\n-L 108.547009 210.202105 \\n-L 108.547009 146.538947 \\n-L 90 146.538947 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_6\">\\n-    <path d=\"M 90 130.623158 \\n-L 106.978894 130.623158 \\n-L 106.978894 66.96 \\n-L 90 66.96 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"matplotlib.axis_1\">\\n-    <g id=\"xtick_1\">\\n-     <g id=\"line2d_1\">\\n-      <defs>\\n-       <path id=\"m7f9e7b8ac6\" d=\"M 0 0 \\n-L 0 3.5 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"90\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_1\">\\n-      <!-- 0 -->\\n-      <g transform=\"translate(86.81875 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \\n-Q 1547 4250 1301 3770 \\n-Q 1056 3291 1056 2328 \\n-Q 1056 1369 1301 889 \\n-Q 1547 409 2034 409 \\n-Q 2525 409 2770 889 \\n-Q 3016 1369 3016 2328 \\n-Q 3016 3291 2770 3770 \\n-Q 2525 4250 2034 4250 \\n-z\\n-M 2034 4750 \\n-Q 2819 4750 3233 4129 \\n-Q 3647 3509 3647 2328 \\n-Q 3647 1150 3233 529 \\n-Q 2819 -91 2034 -91 \\n-Q 1250 -91 836 529 \\n-Q 422 1150 422 2328 \\n-Q 422 3509 836 4129 \\n-Q 1250 4750 2034 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-30\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_2\">\\n-     <g id=\"line2d_2\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"154.887493\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_2\">\\n-      <!-- 20 -->\\n-      <g transform=\"translate(148.524993 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \\n-L 3431 531 \\n-L 3431 0 \\n-L 469 0 \\n-L 469 531 \\n-Q 828 903 1448 1529 \\n-Q 2069 2156 2228 2338 \\n-Q 2531 2678 2651 2914 \\n-Q 2772 3150 2772 3378 \\n-Q 2772 3750 2511 3984 \\n-Q 2250 4219 1831 4219 \\n-Q 1534 4219 1204 4116 \\n-Q 875 4013 500 3803 \\n-L 500 4441 \\n-Q 881 4594 1212 4672 \\n-Q 1544 4750 1819 4750 \\n-Q 2544 4750 2975 4387 \\n-Q 3406 4025 3406 3419 \\n-Q 3406 3131 3298 2873 \\n-Q 3191 2616 2906 2266 \\n-Q 2828 2175 2409 1742 \\n-Q 1991 1309 1228 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_3\">\\n-     <g id=\"line2d_3\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"219.774987\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_3\">\\n-      <!-- 40 -->\\n-      <g transform=\"translate(213.412487 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \\n-L 825 1625 \\n-L 2419 1625 \\n-L 2419 4116 \\n-z\\n-M 2253 4666 \\n-L 3047 4666 \\n-L 3047 1625 \\n-L 3713 1625 \\n-L 3713 1100 \\n-L 3047 1100 \\n-L 3047 0 \\n-L 2419 0 \\n-L 2419 1100 \\n-L 313 1100 \\n-L 313 1709 \\n-L 2253 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-34\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_4\">\\n-     <g id=\"line2d_4\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"284.66248\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_4\">\\n-      <!-- 60 -->\\n-      <g transform=\"translate(278.29998 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \\n-Q 1688 2584 1439 2293 \\n-Q 1191 2003 1191 1497 \\n-Q 1191 994 1439 701 \\n-Q 1688 409 2113 409 \\n-Q 2538 409 2786 701 \\n-Q 3034 994 3034 1497 \\n-Q 3034 2003 2786 2293 \\n-Q 2538 2584 2113 2584 \\n-z\\n-M 3366 4563 \\n-L 3366 3988 \\n-Q 3128 4100 2886 4159 \\n-Q 2644 4219 2406 4219 \\n-Q 1781 4219 1451 3797 \\n-Q 1122 3375 1075 2522 \\n-Q 1259 2794 1537 2939 \\n-Q 1816 3084 2150 3084 \\n-Q 2853 3084 3261 2657 \\n-Q 3669 2231 3669 1497 \\n-Q 3669 778 3244 343 \\n-Q 2819 -91 2113 -91 \\n-Q 1303 -91 875 529 \\n-Q 447 1150 447 2328 \\n-Q 447 3434 972 4092 \\n-Q 1497 4750 2381 4750 \\n-Q 2619 4750 2861 4703 \\n-Q 3103 4656 3366 4563 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-36\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_5\">\\n-     <g id=\"line2d_5\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"349.549974\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_5\">\\n-      <!-- 80 -->\\n-      <g transform=\"translate(343.187474 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \\n-Q 1584 2216 1326 1975 \\n-Q 1069 1734 1069 1313 \\n-Q 1069 891 1326 650 \\n-Q 1584 409 2034 409 \\n-Q 2484 409 2743 651 \\n-Q 3003 894 3003 1313 \\n-Q 3003 1734 2745 1975 \\n-Q 2488 2216 2034 2216 \\n-z\\n-M 1403 2484 \\n-Q 997 2584 770 2862 \\n-Q 544 3141 544 3541 \\n-Q 544 4100 942 4425 \\n-Q 1341 4750 2034 4750 \\n-Q 2731 4750 3128 4425 \\n-Q 3525 4100 3525 3541 \\n-Q 3525 3141 3298 2862 \\n-Q 3072 2584 2669 2484 \\n-Q 3125 2378 3379 2068 \\n-Q 3634 1759 3634 1313 \\n-Q 3634 634 3220 271 \\n-Q 2806 -91 2034 -91 \\n-Q 1263 -91 848 271 \\n-Q 434 634 434 1313 \\n-Q 434 1759 690 2068 \\n-Q 947 2378 1403 2484 \\n-z\\n-M 1172 3481 \\n-Q 1172 3119 1398 2916 \\n-Q 1625 2713 2034 2713 \\n-Q 2441 2713 2670 2916 \\n-Q 2900 3119 2900 3481 \\n-Q 2900 3844 2670 4047 \\n-Q 2441 4250 2034 4250 \\n-Q 1625 4250 1398 4047 \\n-Q 1172 3844 1172 3481 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-38\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_6\">\\n-     <g id=\"line2d_6\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"414.437467\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_6\">\\n-      <!-- 100 -->\\n-      <g transform=\"translate(404.893717 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-31\" d=\"M 794 531 \\n-L 1825 531 \\n-L 1825 4091 \\n-L 703 3866 \\n-L 703 4441 \\n-L 1819 4666 \\n-L 2450 4666 \\n-L 2450 531 \\n-L 3481 531 \\n-L 3481 0 \\n-L 794 0 \\n-L 794 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_7\">\\n-     <g id=\"line2d_7\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"479.324961\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_7\">\\n-      <!-- 120 -->\\n-      <g transform=\"translate(469.781211 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_8\">\\n-     <g id=\"line2d_8\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"544.212454\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_8\">\\n-      <!-- 140 -->\\n-      <g transform=\"translate(534.668704 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_9\">\\n-     <g id=\"line2d_9\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"609.099948\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_9\">\\n-      <!-- 160 -->\\n-      <g transform=\"translate(599.556198 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"text_10\">\\n-     <!-- Time taken (lower is better). * Unsloth Open uses 1 GPU only. -->\\n-     <g transform=\"translate(184.611563 414.27625) scale(0.12 -0.12)\">\\n-      <defs>\\n-       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \\n-L 3928 4666 \\n-L 3928 4134 \\n-L 2272 4134 \\n-L 2272 0 \\n-L 1638 0 \\n-L 1638 4134 \\n-L -19 4134 \\n-L -19 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \\n-L 1178 3500 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 3500 \\n-z\\n-M 603 4863 \\n-L 1178 4863 \\n-L 1178 4134 \\n-L 603 4134 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \\n-Q 3544 3216 3844 3400 \\n-Q 4144 3584 4550 3584 \\n-Q 5097 3584 5394 3201 \\n-Q 5691 2819 5691 2113 \\n-L 5691 0 \\n-L 5113 0 \\n-L 5113 2094 \\n-Q 5113 2597 4934 2840 \\n-Q 4756 3084 4391 3084 \\n-Q 3944 3084 3684 2787 \\n-Q 3425 2491 3425 1978 \\n-L 3425 0 \\n-L 2847 0 \\n-L 2847 2094 \\n-Q 2847 2600 2669 2842 \\n-Q 2491 3084 2119 3084 \\n-Q 1678 3084 1418 2786 \\n-Q 1159 2488 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1356 3278 1631 3431 \\n-Q 1906 3584 2284 3584 \\n-Q 2666 3584 2933 3390 \\n-Q 3200 3197 3328 2828 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \\n-L 3597 1613 \\n-L 953 1613 \\n-Q 991 1019 1311 708 \\n-Q 1631 397 2203 397 \\n-Q 2534 397 2845 478 \\n-Q 3156 559 3463 722 \\n-L 3463 178 \\n-Q 3153 47 2828 -22 \\n-Q 2503 -91 2169 -91 \\n-Q 1331 -91 842 396 \\n-Q 353 884 353 1716 \\n-Q 353 2575 817 3079 \\n-Q 1281 3584 2069 3584 \\n-Q 2775 3584 3186 3129 \\n-Q 3597 2675 3597 1894 \\n-z\\n-M 3022 2063 \\n-Q 3016 2534 2758 2815 \\n-Q 2500 3097 2075 3097 \\n-Q 1594 3097 1305 2825 \\n-Q 1016 2553 972 2059 \\n-L 3022 2063 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \\n-L 1172 3500 \\n-L 2356 3500 \\n-L 2356 3053 \\n-L 1172 3053 \\n-L 1172 1153 \\n-Q 1172 725 1289 603 \\n-Q 1406 481 1766 481 \\n-L 2356 481 \\n-L 2356 0 \\n-L 1766 0 \\n-Q 1100 0 847 248 \\n-Q 594 497 594 1153 \\n-L 594 3053 \\n-L 172 3053 \\n-L 172 3500 \\n-L 594 3500 \\n-L 594 4494 \\n-L 1172 4494 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \\n-Q 1497 1759 1228 1600 \\n-Q 959 1441 959 1056 \\n-Q 959 750 1161 570 \\n-Q 1363 391 1709 391 \\n-Q 2188 391 2477 730 \\n-Q 2766 1069 2766 1631 \\n-L 2766 1759 \\n-L 2194 1759 \\n-z\\n-M 3341 1997 \\n-L 3341 0 \\n-L 2766 0 \\n-L 2766 531 \\n-Q 2569 213 2275 61 \\n-Q 1981 -91 1556 -91 \\n-Q 1019 -91 701 211 \\n-Q 384 513 384 1019 \\n-Q 384 1609 779 1909 \\n-Q 1175 2209 1959 2209 \\n-L 2766 2209 \\n-L 2766 2266 \\n-Q 2766 2663 2505 2880 \\n-Q 2244 3097 1772 3097 \\n-Q 1472 3097 1187 3025 \\n-Q 903 2953 641 2809 \\n-L 641 3341 \\n-Q 956 3463 1253 3523 \\n-Q 1550 3584 1831 3584 \\n-Q 2591 3584 2966 3190 \\n-Q 3341 2797 3341 1997 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \\n-L 1159 4863 \\n-L 1159 1991 \\n-L 2875 3500 \\n-L 3609 3500 \\n-L 1753 1863 \\n-L 3688 0 \\n-L 2938 0 \\n-L 1159 1709 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \\n-Q 1566 4138 1362 3434 \\n-Q 1159 2731 1159 2009 \\n-Q 1159 1288 1364 580 \\n-Q 1569 -128 1984 -844 \\n-L 1484 -844 \\n-Q 1016 -109 783 600 \\n-Q 550 1309 550 2009 \\n-Q 550 2706 781 3412 \\n-Q 1013 4119 1484 4856 \\n-L 1984 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \\n-L 1178 4863 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \\n-Q 1497 3097 1228 2736 \\n-Q 959 2375 959 1747 \\n-Q 959 1119 1226 758 \\n-Q 1494 397 1959 397 \\n-Q 2419 397 2687 759 \\n-Q 2956 1122 2956 1747 \\n-Q 2956 2369 2687 2733 \\n-Q 2419 3097 1959 3097 \\n-z\\n-M 1959 3584 \\n-Q 2709 3584 3137 3096 \\n-Q 3566 2609 3566 1747 \\n-Q 3566 888 3137 398 \\n-Q 2709 -91 1959 -91 \\n-Q 1206 -91 779 398 \\n-Q 353 888 353 1747 \\n-Q 353 2609 779 3096 \\n-Q 1206 3584 1959 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \\n-L 844 3500 \\n-L 1563 769 \\n-L 2278 3500 \\n-L 2956 3500 \\n-L 3675 769 \\n-L 4391 3500 \\n-L 4966 3500 \\n-L 4050 0 \\n-L 3372 0 \\n-L 2619 2869 \\n-L 1863 0 \\n-L 1184 0 \\n-L 269 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \\n-Q 2534 3019 2420 3045 \\n-Q 2306 3072 2169 3072 \\n-Q 1681 3072 1420 2755 \\n-Q 1159 2438 1159 1844 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1341 3275 1631 3429 \\n-Q 1922 3584 2338 3584 \\n-Q 2397 3584 2469 3576 \\n-Q 2541 3569 2628 3553 \\n-L 2631 2963 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \\n-L 2834 2853 \\n-Q 2591 2978 2328 3040 \\n-Q 2066 3103 1784 3103 \\n-Q 1356 3103 1142 2972 \\n-Q 928 2841 928 2578 \\n-Q 928 2378 1081 2264 \\n-Q 1234 2150 1697 2047 \\n-L 1894 2003 \\n-Q 2506 1872 2764 1633 \\n-Q 3022 1394 3022 966 \\n-Q 3022 478 2636 193 \\n-Q 2250 -91 1575 -91 \\n-Q 1294 -91 989 -36 \\n-Q 684 19 347 128 \\n-L 347 722 \\n-Q 666 556 975 473 \\n-Q 1284 391 1588 391 \\n-Q 1994 391 2212 530 \\n-Q 2431 669 2431 922 \\n-Q 2431 1156 2273 1281 \\n-Q 2116 1406 1581 1522 \\n-L 1381 1569 \\n-Q 847 1681 609 1914 \\n-Q 372 2147 372 2553 \\n-Q 372 3047 722 3315 \\n-Q 1072 3584 1716 3584 \\n-Q 2034 3584 2315 3537 \\n-Q 2597 3491 2834 3397 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-M 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2969 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \\n-L 1013 4856 \\n-Q 1481 4119 1714 3412 \\n-Q 1947 2706 1947 2009 \\n-Q 1947 1309 1714 600 \\n-Q 1481 -109 1013 -844 \\n-L 513 -844 \\n-Q 928 -128 1133 580 \\n-Q 1338 1288 1338 2009 \\n-Q 1338 2731 1133 3434 \\n-Q 928 4138 513 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2e\" d=\"M 684 794 \\n-L 1344 794 \\n-L 1344 0 \\n-L 684 0 \\n-L 684 794 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2a\" d=\"M 3009 3897 \\n-L 1888 3291 \\n-L 3009 2681 \\n-L 2828 2375 \\n-L 1778 3009 \\n-L 1778 1831 \\n-L 1422 1831 \\n-L 1422 3009 \\n-L 372 2375 \\n-L 191 2681 \\n-L 1313 3291 \\n-L 191 3897 \\n-L 372 4206 \\n-L 1422 3572 \\n-L 1422 4750 \\n-L 1778 4750 \\n-L 1778 3572 \\n-L 2828 4206 \\n-L 3009 3897 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-55\" d=\"M 556 4666 \\n-L 1191 4666 \\n-L 1191 1831 \\n-Q 1191 1081 1462 751 \\n-Q 1734 422 2344 422 \\n-Q 2950 422 3222 751 \\n-Q 3494 1081 3494 1831 \\n-L 3494 4666 \\n-L 4128 4666 \\n-L 4128 1753 \\n-Q 4128 841 3676 375 \\n-Q 3225 -91 2344 -91 \\n-Q 1459 -91 1007 375 \\n-Q 556 841 556 1753 \\n-L 556 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-4f\" d=\"M 2522 4238 \\n-Q 1834 4238 1429 3725 \\n-Q 1025 3213 1025 2328 \\n-Q 1025 1447 1429 934 \\n-Q 1834 422 2522 422 \\n-Q 3209 422 3611 934 \\n-Q 4013 1447 4013 2328 \\n-Q 4013 3213 3611 3725 \\n-Q 3209 4238 2522 4238 \\n-z\\n-M 2522 4750 \\n-Q 3503 4750 4090 4092 \\n-Q 4678 3434 4678 2328 \\n-Q 4678 1225 4090 567 \\n-Q 3503 -91 2522 -91 \\n-Q 1538 -91 948 565 \\n-Q 359 1222 359 2328 \\n-Q 359 3434 948 4092 \\n-Q 1538 4750 2522 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \\n-L 1159 -1331 \\n-L 581 -1331 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-z\\n-M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \\n-L 544 3500 \\n-L 1119 3500 \\n-L 1119 1403 \\n-Q 1119 906 1312 657 \\n-Q 1506 409 1894 409 \\n-Q 2359 409 2629 706 \\n-Q 2900 1003 2900 1516 \\n-L 2900 3500 \\n-L 3475 3500 \\n-L 3475 0 \\n-L 2900 0 \\n-L 2900 538 \\n-Q 2691 219 2414 64 \\n-Q 2138 -91 1772 -91 \\n-Q 1169 -91 856 284 \\n-Q 544 659 544 1381 \\n-z\\n-M 1991 3584 \\n-L 1991 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \\n-L 3809 1919 \\n-L 2778 1919 \\n-L 2778 2438 \\n-L 4434 2438 \\n-L 4434 434 \\n-Q 4069 175 3628 42 \\n-Q 3188 -91 2688 -91 \\n-Q 1594 -91 976 548 \\n-Q 359 1188 359 2328 \\n-Q 359 3472 976 4111 \\n-Q 1594 4750 2688 4750 \\n-Q 3144 4750 3555 4637 \\n-Q 3966 4525 4313 4306 \\n-L 4313 3634 \\n-Q 3963 3931 3569 4081 \\n-Q 3175 4231 2741 4231 \\n-Q 1884 4231 1454 3753 \\n-Q 1025 3275 1025 2328 \\n-Q 1025 1384 1454 906 \\n-Q 1884 428 2741 428 \\n-Q 3075 428 3337 486 \\n-Q 3600 544 3809 666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \\n-L 1259 2394 \\n-L 2053 2394 \\n-Q 2494 2394 2734 2622 \\n-Q 2975 2850 2975 3272 \\n-Q 2975 3691 2734 3919 \\n-Q 2494 4147 2053 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 2053 4666 \\n-Q 2838 4666 3239 4311 \\n-Q 3641 3956 3641 3272 \\n-Q 3641 2581 3239 2228 \\n-Q 2838 1875 2053 1875 \\n-L 1259 1875 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \\n-Q 1816 -950 1584 -1140 \\n-Q 1353 -1331 966 -1331 \\n-L 506 -1331 \\n-L 506 -850 \\n-L 844 -850 \\n-Q 1081 -850 1212 -737 \\n-Q 1344 -625 1503 -206 \\n-L 1606 56 \\n-L 191 3500 \\n-L 800 3500 \\n-L 1894 763 \\n-L 2988 3500 \\n-L 3597 3500 \\n-L 2059 -325 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      </defs>\\n-      <use xlink:href=\"#DejaVuSans-54\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"57.958984\"/>\\n-      <use xlink:href=\"#DejaVuSans-6d\" x=\"85.742188\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"183.154297\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"244.677734\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"276.464844\"/>\\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"315.673828\"/>\\n-      <use xlink:href=\"#DejaVuSans-6b\" x=\"376.953125\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.238281\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"492.761719\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"556.140625\"/>\\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"587.927734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"626.941406\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"654.724609\"/>\\n-      <use xlink:href=\"#DejaVuSans-77\" x=\"715.90625\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"797.693359\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"859.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"900.330078\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"932.117188\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"959.900391\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1012\"/>\\n-      <use xlink:href=\"#DejaVuSans-62\" x=\"1043.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1107.263672\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1168.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1207.996094\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1247.205078\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"1308.728516\"/>\\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"1349.841797\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"1388.855469\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1420.642578\"/>\\n-      <use xlink:href=\"#DejaVuSans-2a\" x=\"1452.429688\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1502.429688\"/>\\n-      <use xlink:href=\"#DejaVuSans-55\" x=\"1534.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"1607.410156\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"1670.789062\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"1722.888672\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"1750.671875\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1811.853516\"/>\\n-      <use xlink:href=\"#DejaVuSans-68\" x=\"1851.0625\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1914.441406\"/>\\n-      <use xlink:href=\"#DejaVuSans-4f\" x=\"1946.228516\"/>\\n-      <use xlink:href=\"#DejaVuSans-70\" x=\"2024.939453\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"2088.416016\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"2149.939453\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2213.318359\"/>\\n-      <use xlink:href=\"#DejaVuSans-75\" x=\"2245.105469\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"2308.484375\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"2360.583984\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"2422.107422\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2474.207031\"/>\\n-      <use xlink:href=\"#DejaVuSans-31\" x=\"2505.994141\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2569.617188\"/>\\n-      <use xlink:href=\"#DejaVuSans-47\" x=\"2601.404297\"/>\\n-      <use xlink:href=\"#DejaVuSans-50\" x=\"2678.894531\"/>\\n-      <use xlink:href=\"#DejaVuSans-55\" x=\"2739.197266\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2812.390625\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"2844.177734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"2905.359375\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"2968.738281\"/>\\n-      <use xlink:href=\"#DejaVuSans-79\" x=\"2996.521484\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"3041.451172\"/>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"matplotlib.axis_2\">\\n-    <g id=\"ytick_1\">\\n-     <g id=\"line2d_10\">\\n-      <defs>\\n-       <path id=\"me3cc6245ad\" d=\"M 0 0 \\n-L -3.5 0 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"337.528421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_11\">\\n-      <!-- Huggingface -->\\n-      <g transform=\"translate(19.68125 341.32764) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2753 \\n-L 3553 2753 \\n-L 3553 4666 \\n-L 4184 4666 \\n-L 4184 0 \\n-L 3553 0 \\n-L 3553 2222 \\n-L 1259 2222 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \\n-Q 2906 2416 2648 2759 \\n-Q 2391 3103 1925 3103 \\n-Q 1463 3103 1205 2759 \\n-Q 947 2416 947 1791 \\n-Q 947 1169 1205 825 \\n-Q 1463 481 1925 481 \\n-Q 2391 481 2648 825 \\n-Q 2906 1169 2906 1791 \\n-z\\n-M 3481 434 \\n-Q 3481 -459 3084 -895 \\n-Q 2688 -1331 1869 -1331 \\n-Q 1566 -1331 1297 -1286 \\n-Q 1028 -1241 775 -1147 \\n-L 775 -588 \\n-Q 1028 -725 1275 -790 \\n-Q 1522 -856 1778 -856 \\n-Q 2344 -856 2625 -561 \\n-Q 2906 -266 2906 331 \\n-L 2906 616 \\n-Q 2728 306 2450 153 \\n-Q 2172 0 1784 0 \\n-Q 1141 0 747 490 \\n-Q 353 981 353 1791 \\n-Q 353 2603 747 3093 \\n-Q 1141 3584 1784 3584 \\n-Q 2172 3584 2450 3431 \\n-Q 2728 3278 2906 2969 \\n-L 2906 3500 \\n-L 3481 3500 \\n-L 3481 434 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \\n-L 2375 4384 \\n-L 1825 4384 \\n-Q 1516 4384 1395 4259 \\n-Q 1275 4134 1275 3809 \\n-L 1275 3500 \\n-L 2222 3500 \\n-L 2222 3053 \\n-L 1275 3053 \\n-L 1275 0 \\n-L 697 0 \\n-L 697 3053 \\n-L 147 3053 \\n-L 147 3500 \\n-L 697 3500 \\n-L 697 3744 \\n-Q 697 4328 969 4595 \\n-Q 1241 4863 1831 4863 \\n-L 2375 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \\n-L 3122 2828 \\n-Q 2878 2963 2633 3030 \\n-Q 2388 3097 2138 3097 \\n-Q 1578 3097 1268 2742 \\n-Q 959 2388 959 1747 \\n-Q 959 1106 1268 751 \\n-Q 1578 397 2138 397 \\n-Q 2388 397 2633 464 \\n-Q 2878 531 3122 666 \\n-L 3122 134 \\n-Q 2881 22 2623 -34 \\n-Q 2366 -91 2075 -91 \\n-Q 1284 -91 818 406 \\n-Q 353 903 353 1747 \\n-Q 353 2603 823 3093 \\n-Q 1294 3584 2113 3584 \\n-Q 2378 3584 2631 3529 \\n-Q 2884 3475 3122 3366 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-48\"/>\\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"138.574219\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"202.050781\"/>\\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"265.527344\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.310547\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"356.689453\"/>\\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"420.166016\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"455.371094\"/>\\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"516.650391\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"571.630859\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_2\">\\n-     <g id=\"line2d_11\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"257.949474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_12\">\\n-      <!-- Unsloth Open* -->\\n-      <g transform=\"translate(10.090625 261.748692) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4f\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"490.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"554.199219\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"615.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-2a\" x=\"679.101562\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_3\">\\n-     <g id=\"line2d_12\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"178.370526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_13\">\\n-      <!-- Unsloth Pro -->\\n-      <g transform=\"translate(25.942187 182.169745) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-50\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"470.564453\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"509.427734\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_4\">\\n-     <g id=\"line2d_13\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"98.791579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_14\">\\n-      <!-- Unsloth Max -->\\n-      <g transform=\"translate(21.126562 102.590798) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \\n-L 1569 4666 \\n-L 2759 1491 \\n-L 3956 4666 \\n-L 4897 4666 \\n-L 4897 0 \\n-L 4281 0 \\n-L 4281 4097 \\n-L 3078 897 \\n-L 2444 897 \\n-L 1241 4097 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \\n-L 2247 1797 \\n-L 3578 0 \\n-L 2900 0 \\n-L 1881 1375 \\n-L 863 0 \\n-L 184 0 \\n-L 1544 1831 \\n-L 300 3500 \\n-L 978 3500 \\n-L 1906 2253 \\n-L 2834 3500 \\n-L 3513 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4d\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"498.291016\"/>\\n-       <use xlink:href=\"#DejaVuSans-78\" x=\"559.570312\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"patch_7\">\\n-    <path d=\"M 90 384.48 \\n-L 90 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_8\">\\n-    <path d=\"M 648 384.48 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_9\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_10\">\\n-    <path d=\"M 90 51.84 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"text_15\">\\n-    <!-- 163.80 hrs (1.0X) -->\\n-    <g transform=\"translate(491.653585 340.839671) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \\n-Q 3050 2419 3304 2112 \\n-Q 3559 1806 3559 1356 \\n-Q 3559 666 3084 287 \\n-Q 2609 -91 1734 -91 \\n-Q 1441 -91 1130 -33 \\n-Q 819 25 488 141 \\n-L 488 750 \\n-Q 750 597 1062 519 \\n-Q 1375 441 1716 441 \\n-Q 2309 441 2620 675 \\n-Q 2931 909 2931 1356 \\n-Q 2931 1769 2642 2001 \\n-Q 2353 2234 1838 2234 \\n-L 1294 2234 \\n-L 1294 2753 \\n-L 1863 2753 \\n-Q 2328 2753 2575 2939 \\n-Q 2822 3125 2822 3475 \\n-Q 2822 3834 2567 4026 \\n-Q 2313 4219 1838 4219 \\n-Q 1578 4219 1281 4162 \\n-Q 984 4106 628 3988 \\n-L 628 4550 \\n-Q 988 4650 1302 4700 \\n-Q 1616 4750 1894 4750 \\n-Q 2613 4750 3031 4423 \\n-Q 3450 4097 3450 3541 \\n-Q 3450 3153 3228 2886 \\n-Q 3006 2619 2597 2516 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-58\" d=\"M 403 4666 \\n-L 1081 4666 \\n-L 2241 2931 \\n-L 3406 4666 \\n-L 4084 4666 \\n-L 2584 2425 \\n-L 4184 0 \\n-L 3506 0 \\n-L 2194 1984 \\n-L 872 0 \\n-L 191 0 \\n-L 1856 2491 \\n-L 403 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-31\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_16\">\\n-    <!-- 34.57 hrs (4.7X) -->\\n-    <g transform=\"translate(205.391593 261.260724) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-35\" d=\"M 691 4666 \\n-L 3169 4666 \\n-L 3169 4134 \\n-L 1269 4134 \\n-L 1269 2991 \\n-Q 1406 3038 1543 3061 \\n-Q 1681 3084 1819 3084 \\n-Q 2600 3084 3056 2656 \\n-Q 3513 2228 3513 1497 \\n-Q 3513 744 3044 326 \\n-Q 2575 -91 1722 -91 \\n-Q 1428 -91 1123 -41 \\n-Q 819 9 494 109 \\n-L 494 744 \\n-Q 775 591 1075 516 \\n-Q 1375 441 1709 441 \\n-Q 2250 441 2565 725 \\n-Q 2881 1009 2881 1497 \\n-Q 2881 1984 2565 2268 \\n-Q 2250 2553 1709 2553 \\n-Q 1456 2553 1204 2497 \\n-Q 953 2441 691 2322 \\n-L 691 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \\n-L 3525 4666 \\n-L 3525 4397 \\n-L 1831 0 \\n-L 1172 0 \\n-L 2766 4134 \\n-L 525 4134 \\n-L 525 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-33\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"318.066406\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"381.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"422.558594\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"474.658203\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"506.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_17\">\\n-    <!-- 5.72 hrs (28.7X) -->\\n-    <g transform=\"translate(111.791383 181.681776) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"254.443359\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"317.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"358.935547\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"411.035156\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"442.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"481.835938\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_18\">\\n-    <!-- 5.23 hrs (31.3X) -->\\n-    <g transform=\"translate(110.223269 102.102829) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"254.443359\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"317.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"358.935547\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"411.035156\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"442.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"481.835938\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_19\">\\n-    <!-- LAION Chip2 210K (1 epoch on 2 T4 GPUs DDP) -->\\n-    <g transform=\"translate(178.88375 45.84) scale(0.16 -0.16)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 531 \\n-L 3531 531 \\n-L 3531 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \\n-L 1331 1722 \\n-L 3047 1722 \\n-L 2188 4044 \\n-z\\n-M 1831 4666 \\n-L 2547 4666 \\n-L 4325 0 \\n-L 3669 0 \\n-L 3244 1197 \\n-L 1141 1197 \\n-L 716 0 \\n-L 50 0 \\n-L 1831 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-49\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \\n-L 1478 4666 \\n-L 3547 763 \\n-L 3547 4666 \\n-L 4159 4666 \\n-L 4159 0 \\n-L 3309 0 \\n-L 1241 3903 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \\n-L 4122 3641 \\n-Q 3803 3938 3442 4084 \\n-Q 3081 4231 2675 4231 \\n-Q 1875 4231 1450 3742 \\n-Q 1025 3253 1025 2328 \\n-Q 1025 1406 1450 917 \\n-Q 1875 428 2675 428 \\n-Q 3081 428 3442 575 \\n-Q 3803 722 4122 1019 \\n-L 4122 359 \\n-Q 3791 134 3420 21 \\n-Q 3050 -91 2638 -91 \\n-Q 1578 -91 968 557 \\n-Q 359 1206 359 2328 \\n-Q 359 3453 968 4101 \\n-Q 1578 4750 2638 4750 \\n-Q 3056 4750 3426 4639 \\n-Q 3797 4528 4122 4306 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2694 \\n-L 3353 4666 \\n-L 4166 4666 \\n-L 1850 2491 \\n-L 4331 0 \\n-L 3500 0 \\n-L 1259 2247 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \\n-L 1259 519 \\n-L 2022 519 \\n-Q 2988 519 3436 956 \\n-Q 3884 1394 3884 2338 \\n-Q 3884 3275 3436 3711 \\n-Q 2988 4147 2022 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 1925 4666 \\n-Q 3281 4666 3915 4102 \\n-Q 4550 3538 4550 2338 \\n-Q 4550 1131 3912 565 \\n-Q 3275 0 1925 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-4c\"/>\\n-     <use xlink:href=\"#DejaVuSans-41\" x=\"57.962891\"/>\\n-     <use xlink:href=\"#DejaVuSans-49\" x=\"126.371094\"/>\\n-     <use xlink:href=\"#DejaVuSans-4f\" x=\"155.863281\"/>\\n-     <use xlink:href=\"#DejaVuSans-4e\" x=\"234.574219\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"309.378906\"/>\\n-     <use xlink:href=\"#DejaVuSans-43\" x=\"341.166016\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"410.990234\"/>\\n-     <use xlink:href=\"#DejaVuSans-69\" x=\"474.369141\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"502.152344\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"565.628906\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"629.251953\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"661.039062\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"724.662109\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"788.285156\"/>\\n-     <use xlink:href=\"#DejaVuSans-4b\" x=\"851.908203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"917.484375\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"949.271484\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"988.285156\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1051.908203\"/>\\n-     <use xlink:href=\"#DejaVuSans-65\" x=\"1083.695312\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"1145.21875\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1208.695312\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"1269.876953\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"1324.857422\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1388.236328\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1420.023438\"/>\\n-     <use xlink:href=\"#DejaVuSans-6e\" x=\"1481.205078\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1544.583984\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"1576.371094\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1639.994141\"/>\\n-     <use xlink:href=\"#DejaVuSans-54\" x=\"1671.78125\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"1732.865234\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1796.488281\"/>\\n-     <use xlink:href=\"#DejaVuSans-47\" x=\"1828.275391\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"1905.765625\"/>\\n-     <use xlink:href=\"#DejaVuSans-55\" x=\"1966.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"2039.261719\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"2091.361328\"/>\\n-     <use xlink:href=\"#DejaVuSans-44\" x=\"2123.148438\"/>\\n-     <use xlink:href=\"#DejaVuSans-44\" x=\"2200.150391\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"2277.152344\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"2337.455078\"/>\\n-    </g>\\n-   </g>\\n-  </g>\\n- </g>\\n- <defs>\\n-  <clipPath id=\"p42f2634aae\">\\n-   <rect x=\"90\" y=\"51.84\" width=\"558\" height=\"332.64\"/>\\n-  </clipPath>\\n- </defs>\\n-</svg>\\n',\n",
       " '@@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\n-  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n- <metadata>\\n-  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n-   <cc:Work>\\n-    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\\n-    <dc:date>2023-11-30T13:15:28.212061</dc:date>\\n-    <dc:format>image/svg+xml</dc:format>\\n-    <dc:creator>\\n-     <cc:Agent>\\n-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\\n-     </cc:Agent>\\n-    </dc:creator>\\n-   </cc:Work>\\n-  </rdf:RDF>\\n- </metadata>\\n- <defs>\\n-  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\\n- </defs>\\n- <g id=\"figure_1\">\\n-  <g id=\"patch_1\">\\n-   <path d=\"M 0 432 \\n-L 720 432 \\n-L 720 0 \\n-L 0 0 \\n-L 0 432 \\n-z\\n-\" style=\"fill: none\"/>\\n-  </g>\\n-  <g id=\"axes_1\">\\n-   <g id=\"patch_2\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-L 648 51.84 \\n-L 90 51.84 \\n-L 90 384.48 \\n-z\\n-\" style=\"fill: none\"/>\\n-   </g>\\n-   <g id=\"patch_3\">\\n-    <path d=\"M 90 369.36 \\n-L 621.428571 369.36 \\n-L 621.428571 305.696842 \\n-L 90 305.696842 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_4\">\\n-    <path d=\"M 90 289.781053 \\n-L 415.717933 289.781053 \\n-L 415.717933 226.117895 \\n-L 90 226.117895 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_5\">\\n-    <path d=\"M 90 210.202105 \\n-L 287.004626 210.202105 \\n-L 287.004626 146.538947 \\n-L 90 146.538947 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_6\">\\n-    <path d=\"M 90 130.623158 \\n-L 159.87423 130.623158 \\n-L 159.87423 66.96 \\n-L 90 66.96 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"matplotlib.axis_1\">\\n-    <g id=\"xtick_1\">\\n-     <g id=\"line2d_1\">\\n-      <defs>\\n-       <path id=\"m7e152711a6\" d=\"M 0 0 \\n-L 0 3.5 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"90\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_1\">\\n-      <!-- 0 -->\\n-      <g transform=\"translate(86.81875 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \\n-Q 1547 4250 1301 3770 \\n-Q 1056 3291 1056 2328 \\n-Q 1056 1369 1301 889 \\n-Q 1547 409 2034 409 \\n-Q 2525 409 2770 889 \\n-Q 3016 1369 3016 2328 \\n-Q 3016 3291 2770 3770 \\n-Q 2525 4250 2034 4250 \\n-z\\n-M 2034 4750 \\n-Q 2819 4750 3233 4129 \\n-Q 3647 3509 3647 2328 \\n-Q 3647 1150 3233 529 \\n-Q 2819 -91 2034 -91 \\n-Q 1250 -91 836 529 \\n-Q 422 1150 422 2328 \\n-Q 422 3509 836 4129 \\n-Q 1250 4750 2034 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-30\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_2\">\\n-     <g id=\"line2d_2\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"157.839059\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_2\">\\n-      <!-- 50 -->\\n-      <g transform=\"translate(151.476559 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \\n-L 3169 4666 \\n-L 3169 4134 \\n-L 1269 4134 \\n-L 1269 2991 \\n-Q 1406 3038 1543 3061 \\n-Q 1681 3084 1819 3084 \\n-Q 2600 3084 3056 2656 \\n-Q 3513 2228 3513 1497 \\n-Q 3513 744 3044 326 \\n-Q 2575 -91 1722 -91 \\n-Q 1428 -91 1123 -41 \\n-Q 819 9 494 109 \\n-L 494 744 \\n-Q 775 591 1075 516 \\n-Q 1375 441 1709 441 \\n-Q 2250 441 2565 725 \\n-Q 2881 1009 2881 1497 \\n-Q 2881 1984 2565 2268 \\n-Q 2250 2553 1709 2553 \\n-Q 1456 2553 1204 2497 \\n-Q 953 2441 691 2322 \\n-L 691 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-35\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_3\">\\n-     <g id=\"line2d_3\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"225.678117\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_3\">\\n-      <!-- 100 -->\\n-      <g transform=\"translate(216.134367 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-31\" d=\"M 794 531 \\n-L 1825 531 \\n-L 1825 4091 \\n-L 703 3866 \\n-L 703 4441 \\n-L 1819 4666 \\n-L 2450 4666 \\n-L 2450 531 \\n-L 3481 531 \\n-L 3481 0 \\n-L 794 0 \\n-L 794 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_4\">\\n-     <g id=\"line2d_4\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"293.517176\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_4\">\\n-      <!-- 150 -->\\n-      <g transform=\"translate(283.973426 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_5\">\\n-     <g id=\"line2d_5\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"361.356234\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_5\">\\n-      <!-- 200 -->\\n-      <g transform=\"translate(351.812484 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \\n-L 3431 531 \\n-L 3431 0 \\n-L 469 0 \\n-L 469 531 \\n-Q 828 903 1448 1529 \\n-Q 2069 2156 2228 2338 \\n-Q 2531 2678 2651 2914 \\n-Q 2772 3150 2772 3378 \\n-Q 2772 3750 2511 3984 \\n-Q 2250 4219 1831 4219 \\n-Q 1534 4219 1204 4116 \\n-Q 875 4013 500 3803 \\n-L 500 4441 \\n-Q 881 4594 1212 4672 \\n-Q 1544 4750 1819 4750 \\n-Q 2544 4750 2975 4387 \\n-Q 3406 4025 3406 3419 \\n-Q 3406 3131 3298 2873 \\n-Q 3191 2616 2906 2266 \\n-Q 2828 2175 2409 1742 \\n-Q 1991 1309 1228 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_6\">\\n-     <g id=\"line2d_6\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"429.195293\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_6\">\\n-      <!-- 250 -->\\n-      <g transform=\"translate(419.651543 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_7\">\\n-     <g id=\"line2d_7\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"497.034351\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_7\">\\n-      <!-- 300 -->\\n-      <g transform=\"translate(487.490601 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \\n-Q 3050 2419 3304 2112 \\n-Q 3559 1806 3559 1356 \\n-Q 3559 666 3084 287 \\n-Q 2609 -91 1734 -91 \\n-Q 1441 -91 1130 -33 \\n-Q 819 25 488 141 \\n-L 488 750 \\n-Q 750 597 1062 519 \\n-Q 1375 441 1716 441 \\n-Q 2309 441 2620 675 \\n-Q 2931 909 2931 1356 \\n-Q 2931 1769 2642 2001 \\n-Q 2353 2234 1838 2234 \\n-L 1294 2234 \\n-L 1294 2753 \\n-L 1863 2753 \\n-Q 2328 2753 2575 2939 \\n-Q 2822 3125 2822 3475 \\n-Q 2822 3834 2567 4026 \\n-Q 2313 4219 1838 4219 \\n-Q 1578 4219 1281 4162 \\n-Q 984 4106 628 3988 \\n-L 628 4550 \\n-Q 988 4650 1302 4700 \\n-Q 1616 4750 1894 4750 \\n-Q 2613 4750 3031 4423 \\n-Q 3450 4097 3450 3541 \\n-Q 3450 3153 3228 2886 \\n-Q 3006 2619 2597 2516 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-33\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_8\">\\n-     <g id=\"line2d_8\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"564.87341\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_8\">\\n-      <!-- 350 -->\\n-      <g transform=\"translate(555.32966 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-33\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_9\">\\n-     <g id=\"line2d_9\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"632.712468\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_9\">\\n-      <!-- 400 -->\\n-      <g transform=\"translate(623.168718 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \\n-L 825 1625 \\n-L 2419 1625 \\n-L 2419 4116 \\n-z\\n-M 2253 4666 \\n-L 3047 4666 \\n-L 3047 1625 \\n-L 3713 1625 \\n-L 3713 1100 \\n-L 3047 1100 \\n-L 3047 0 \\n-L 2419 0 \\n-L 2419 1100 \\n-L 313 1100 \\n-L 313 1709 \\n-L 2253 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-34\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"text_10\">\\n-     <!-- Time taken (lower is better). -->\\n-     <g transform=\"translate(283.763438 414.27625) scale(0.12 -0.12)\">\\n-      <defs>\\n-       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \\n-L 3928 4666 \\n-L 3928 4134 \\n-L 2272 4134 \\n-L 2272 0 \\n-L 1638 0 \\n-L 1638 4134 \\n-L -19 4134 \\n-L -19 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \\n-L 1178 3500 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 3500 \\n-z\\n-M 603 4863 \\n-L 1178 4863 \\n-L 1178 4134 \\n-L 603 4134 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \\n-Q 3544 3216 3844 3400 \\n-Q 4144 3584 4550 3584 \\n-Q 5097 3584 5394 3201 \\n-Q 5691 2819 5691 2113 \\n-L 5691 0 \\n-L 5113 0 \\n-L 5113 2094 \\n-Q 5113 2597 4934 2840 \\n-Q 4756 3084 4391 3084 \\n-Q 3944 3084 3684 2787 \\n-Q 3425 2491 3425 1978 \\n-L 3425 0 \\n-L 2847 0 \\n-L 2847 2094 \\n-Q 2847 2600 2669 2842 \\n-Q 2491 3084 2119 3084 \\n-Q 1678 3084 1418 2786 \\n-Q 1159 2488 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1356 3278 1631 3431 \\n-Q 1906 3584 2284 3584 \\n-Q 2666 3584 2933 3390 \\n-Q 3200 3197 3328 2828 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \\n-L 3597 1613 \\n-L 953 1613 \\n-Q 991 1019 1311 708 \\n-Q 1631 397 2203 397 \\n-Q 2534 397 2845 478 \\n-Q 3156 559 3463 722 \\n-L 3463 178 \\n-Q 3153 47 2828 -22 \\n-Q 2503 -91 2169 -91 \\n-Q 1331 -91 842 396 \\n-Q 353 884 353 1716 \\n-Q 353 2575 817 3079 \\n-Q 1281 3584 2069 3584 \\n-Q 2775 3584 3186 3129 \\n-Q 3597 2675 3597 1894 \\n-z\\n-M 3022 2063 \\n-Q 3016 2534 2758 2815 \\n-Q 2500 3097 2075 3097 \\n-Q 1594 3097 1305 2825 \\n-Q 1016 2553 972 2059 \\n-L 3022 2063 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \\n-L 1172 3500 \\n-L 2356 3500 \\n-L 2356 3053 \\n-L 1172 3053 \\n-L 1172 1153 \\n-Q 1172 725 1289 603 \\n-Q 1406 481 1766 481 \\n-L 2356 481 \\n-L 2356 0 \\n-L 1766 0 \\n-Q 1100 0 847 248 \\n-Q 594 497 594 1153 \\n-L 594 3053 \\n-L 172 3053 \\n-L 172 3500 \\n-L 594 3500 \\n-L 594 4494 \\n-L 1172 4494 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \\n-Q 1497 1759 1228 1600 \\n-Q 959 1441 959 1056 \\n-Q 959 750 1161 570 \\n-Q 1363 391 1709 391 \\n-Q 2188 391 2477 730 \\n-Q 2766 1069 2766 1631 \\n-L 2766 1759 \\n-L 2194 1759 \\n-z\\n-M 3341 1997 \\n-L 3341 0 \\n-L 2766 0 \\n-L 2766 531 \\n-Q 2569 213 2275 61 \\n-Q 1981 -91 1556 -91 \\n-Q 1019 -91 701 211 \\n-Q 384 513 384 1019 \\n-Q 384 1609 779 1909 \\n-Q 1175 2209 1959 2209 \\n-L 2766 2209 \\n-L 2766 2266 \\n-Q 2766 2663 2505 2880 \\n-Q 2244 3097 1772 3097 \\n-Q 1472 3097 1187 3025 \\n-Q 903 2953 641 2809 \\n-L 641 3341 \\n-Q 956 3463 1253 3523 \\n-Q 1550 3584 1831 3584 \\n-Q 2591 3584 2966 3190 \\n-Q 3341 2797 3341 1997 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \\n-L 1159 4863 \\n-L 1159 1991 \\n-L 2875 3500 \\n-L 3609 3500 \\n-L 1753 1863 \\n-L 3688 0 \\n-L 2938 0 \\n-L 1159 1709 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \\n-Q 1566 4138 1362 3434 \\n-Q 1159 2731 1159 2009 \\n-Q 1159 1288 1364 580 \\n-Q 1569 -128 1984 -844 \\n-L 1484 -844 \\n-Q 1016 -109 783 600 \\n-Q 550 1309 550 2009 \\n-Q 550 2706 781 3412 \\n-Q 1013 4119 1484 4856 \\n-L 1984 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \\n-L 1178 4863 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \\n-Q 1497 3097 1228 2736 \\n-Q 959 2375 959 1747 \\n-Q 959 1119 1226 758 \\n-Q 1494 397 1959 397 \\n-Q 2419 397 2687 759 \\n-Q 2956 1122 2956 1747 \\n-Q 2956 2369 2687 2733 \\n-Q 2419 3097 1959 3097 \\n-z\\n-M 1959 3584 \\n-Q 2709 3584 3137 3096 \\n-Q 3566 2609 3566 1747 \\n-Q 3566 888 3137 398 \\n-Q 2709 -91 1959 -91 \\n-Q 1206 -91 779 398 \\n-Q 353 888 353 1747 \\n-Q 353 2609 779 3096 \\n-Q 1206 3584 1959 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \\n-L 844 3500 \\n-L 1563 769 \\n-L 2278 3500 \\n-L 2956 3500 \\n-L 3675 769 \\n-L 4391 3500 \\n-L 4966 3500 \\n-L 4050 0 \\n-L 3372 0 \\n-L 2619 2869 \\n-L 1863 0 \\n-L 1184 0 \\n-L 269 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \\n-Q 2534 3019 2420 3045 \\n-Q 2306 3072 2169 3072 \\n-Q 1681 3072 1420 2755 \\n-Q 1159 2438 1159 1844 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1341 3275 1631 3429 \\n-Q 1922 3584 2338 3584 \\n-Q 2397 3584 2469 3576 \\n-Q 2541 3569 2628 3553 \\n-L 2631 2963 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \\n-L 2834 2853 \\n-Q 2591 2978 2328 3040 \\n-Q 2066 3103 1784 3103 \\n-Q 1356 3103 1142 2972 \\n-Q 928 2841 928 2578 \\n-Q 928 2378 1081 2264 \\n-Q 1234 2150 1697 2047 \\n-L 1894 2003 \\n-Q 2506 1872 2764 1633 \\n-Q 3022 1394 3022 966 \\n-Q 3022 478 2636 193 \\n-Q 2250 -91 1575 -91 \\n-Q 1294 -91 989 -36 \\n-Q 684 19 347 128 \\n-L 347 722 \\n-Q 666 556 975 473 \\n-Q 1284 391 1588 391 \\n-Q 1994 391 2212 530 \\n-Q 2431 669 2431 922 \\n-Q 2431 1156 2273 1281 \\n-Q 2116 1406 1581 1522 \\n-L 1381 1569 \\n-Q 847 1681 609 1914 \\n-Q 372 2147 372 2553 \\n-Q 372 3047 722 3315 \\n-Q 1072 3584 1716 3584 \\n-Q 2034 3584 2315 3537 \\n-Q 2597 3491 2834 3397 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-M 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2969 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \\n-L 1013 4856 \\n-Q 1481 4119 1714 3412 \\n-Q 1947 2706 1947 2009 \\n-Q 1947 1309 1714 600 \\n-Q 1481 -109 1013 -844 \\n-L 513 -844 \\n-Q 928 -128 1133 580 \\n-Q 1338 1288 1338 2009 \\n-Q 1338 2731 1133 3434 \\n-Q 928 4138 513 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2e\" d=\"M 684 794 \\n-L 1344 794 \\n-L 1344 0 \\n-L 684 0 \\n-L 684 794 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      </defs>\\n-      <use xlink:href=\"#DejaVuSans-54\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"57.958984\"/>\\n-      <use xlink:href=\"#DejaVuSans-6d\" x=\"85.742188\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"183.154297\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"244.677734\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"276.464844\"/>\\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"315.673828\"/>\\n-      <use xlink:href=\"#DejaVuSans-6b\" x=\"376.953125\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.238281\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"492.761719\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"556.140625\"/>\\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"587.927734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"626.941406\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"654.724609\"/>\\n-      <use xlink:href=\"#DejaVuSans-77\" x=\"715.90625\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"797.693359\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"859.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"900.330078\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"932.117188\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"959.900391\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1012\"/>\\n-      <use xlink:href=\"#DejaVuSans-62\" x=\"1043.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1107.263672\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1168.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1207.996094\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1247.205078\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"1308.728516\"/>\\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"1349.841797\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"1388.855469\"/>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"matplotlib.axis_2\">\\n-    <g id=\"ytick_1\">\\n-     <g id=\"line2d_10\">\\n-      <defs>\\n-       <path id=\"m45234ecef3\" d=\"M 0 0 \\n-L -3.5 0 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"337.528421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_11\">\\n-      <!-- Huggingface -->\\n-      <g transform=\"translate(19.68125 341.32764) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2753 \\n-L 3553 2753 \\n-L 3553 4666 \\n-L 4184 4666 \\n-L 4184 0 \\n-L 3553 0 \\n-L 3553 2222 \\n-L 1259 2222 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-75\" d=\"M 544 1381 \\n-L 544 3500 \\n-L 1119 3500 \\n-L 1119 1403 \\n-Q 1119 906 1312 657 \\n-Q 1506 409 1894 409 \\n-Q 2359 409 2629 706 \\n-Q 2900 1003 2900 1516 \\n-L 2900 3500 \\n-L 3475 3500 \\n-L 3475 0 \\n-L 2900 0 \\n-L 2900 538 \\n-Q 2691 219 2414 64 \\n-Q 2138 -91 1772 -91 \\n-Q 1169 -91 856 284 \\n-Q 544 659 544 1381 \\n-z\\n-M 1991 3584 \\n-L 1991 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \\n-Q 2906 2416 2648 2759 \\n-Q 2391 3103 1925 3103 \\n-Q 1463 3103 1205 2759 \\n-Q 947 2416 947 1791 \\n-Q 947 1169 1205 825 \\n-Q 1463 481 1925 481 \\n-Q 2391 481 2648 825 \\n-Q 2906 1169 2906 1791 \\n-z\\n-M 3481 434 \\n-Q 3481 -459 3084 -895 \\n-Q 2688 -1331 1869 -1331 \\n-Q 1566 -1331 1297 -1286 \\n-Q 1028 -1241 775 -1147 \\n-L 775 -588 \\n-Q 1028 -725 1275 -790 \\n-Q 1522 -856 1778 -856 \\n-Q 2344 -856 2625 -561 \\n-Q 2906 -266 2906 331 \\n-L 2906 616 \\n-Q 2728 306 2450 153 \\n-Q 2172 0 1784 0 \\n-Q 1141 0 747 490 \\n-Q 353 981 353 1791 \\n-Q 353 2603 747 3093 \\n-Q 1141 3584 1784 3584 \\n-Q 2172 3584 2450 3431 \\n-Q 2728 3278 2906 2969 \\n-L 2906 3500 \\n-L 3481 3500 \\n-L 3481 434 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \\n-L 2375 4384 \\n-L 1825 4384 \\n-Q 1516 4384 1395 4259 \\n-Q 1275 4134 1275 3809 \\n-L 1275 3500 \\n-L 2222 3500 \\n-L 2222 3053 \\n-L 1275 3053 \\n-L 1275 0 \\n-L 697 0 \\n-L 697 3053 \\n-L 147 3053 \\n-L 147 3500 \\n-L 697 3500 \\n-L 697 3744 \\n-Q 697 4328 969 4595 \\n-Q 1241 4863 1831 4863 \\n-L 2375 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \\n-L 3122 2828 \\n-Q 2878 2963 2633 3030 \\n-Q 2388 3097 2138 3097 \\n-Q 1578 3097 1268 2742 \\n-Q 959 2388 959 1747 \\n-Q 959 1106 1268 751 \\n-Q 1578 397 2138 397 \\n-Q 2388 397 2633 464 \\n-Q 2878 531 3122 666 \\n-L 3122 134 \\n-Q 2881 22 2623 -34 \\n-Q 2366 -91 2075 -91 \\n-Q 1284 -91 818 406 \\n-Q 353 903 353 1747 \\n-Q 353 2603 823 3093 \\n-Q 1294 3584 2113 3584 \\n-Q 2378 3584 2631 3529 \\n-Q 2884 3475 3122 3366 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-48\"/>\\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"138.574219\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"202.050781\"/>\\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"265.527344\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.310547\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"356.689453\"/>\\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"420.166016\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"455.371094\"/>\\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"516.650391\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"571.630859\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_2\">\\n-     <g id=\"line2d_11\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"257.949474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_12\">\\n-      <!-- Unsloth Open -->\\n-      <g transform=\"translate(15.090625 261.748692) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-55\" d=\"M 556 4666 \\n-L 1191 4666 \\n-L 1191 1831 \\n-Q 1191 1081 1462 751 \\n-Q 1734 422 2344 422 \\n-Q 2950 422 3222 751 \\n-Q 3494 1081 3494 1831 \\n-L 3494 4666 \\n-L 4128 4666 \\n-L 4128 1753 \\n-Q 4128 841 3676 375 \\n-Q 3225 -91 2344 -91 \\n-Q 1459 -91 1007 375 \\n-Q 556 841 556 1753 \\n-L 556 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-4f\" d=\"M 2522 4238 \\n-Q 1834 4238 1429 3725 \\n-Q 1025 3213 1025 2328 \\n-Q 1025 1447 1429 934 \\n-Q 1834 422 2522 422 \\n-Q 3209 422 3611 934 \\n-Q 4013 1447 4013 2328 \\n-Q 4013 3213 3611 3725 \\n-Q 3209 4238 2522 4238 \\n-z\\n-M 2522 4750 \\n-Q 3503 4750 4090 4092 \\n-Q 4678 3434 4678 2328 \\n-Q 4678 1225 4090 567 \\n-Q 3503 -91 2522 -91 \\n-Q 1538 -91 948 565 \\n-Q 359 1222 359 2328 \\n-Q 359 3434 948 4092 \\n-Q 1538 4750 2522 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \\n-L 1159 -1331 \\n-L 581 -1331 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-z\\n-M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4f\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"490.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"554.199219\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"615.722656\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_3\">\\n-     <g id=\"line2d_12\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"178.370526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_13\">\\n-      <!-- Unsloth Pro -->\\n-      <g transform=\"translate(25.942187 182.169745) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \\n-L 1259 2394 \\n-L 2053 2394 \\n-Q 2494 2394 2734 2622 \\n-Q 2975 2850 2975 3272 \\n-Q 2975 3691 2734 3919 \\n-Q 2494 4147 2053 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 2053 4666 \\n-Q 2838 4666 3239 4311 \\n-Q 3641 3956 3641 3272 \\n-Q 3641 2581 3239 2228 \\n-Q 2838 1875 2053 1875 \\n-L 1259 1875 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-50\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"470.564453\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"509.427734\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_4\">\\n-     <g id=\"line2d_13\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"98.791579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_14\">\\n-      <!-- Unsloth Max -->\\n-      <g transform=\"translate(21.126562 102.590798) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \\n-L 1569 4666 \\n-L 2759 1491 \\n-L 3956 4666 \\n-L 4897 4666 \\n-L 4897 0 \\n-L 4281 0 \\n-L 4281 4097 \\n-L 3078 897 \\n-L 2444 897 \\n-L 1241 4097 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \\n-L 2247 1797 \\n-L 3578 0 \\n-L 2900 0 \\n-L 1881 1375 \\n-L 863 0 \\n-L 184 0 \\n-L 1544 1831 \\n-L 300 3500 \\n-L 978 3500 \\n-L 1906 2253 \\n-L 2834 3500 \\n-L 3513 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4d\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"498.291016\"/>\\n-       <use xlink:href=\"#DejaVuSans-78\" x=\"559.570312\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"patch_7\">\\n-    <path d=\"M 90 384.48 \\n-L 90 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_8\">\\n-    <path d=\"M 648 384.48 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_9\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_10\">\\n-    <path d=\"M 90 51.84 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"text_15\">\\n-    <!-- 391.68 hrs (1.0X) -->\\n-    <g transform=\"translate(512.886078 340.839671) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-39\" d=\"M 703 97 \\n-L 703 672 \\n-Q 941 559 1184 500 \\n-Q 1428 441 1663 441 \\n-Q 2288 441 2617 861 \\n-Q 2947 1281 2994 2138 \\n-Q 2813 1869 2534 1725 \\n-Q 2256 1581 1919 1581 \\n-Q 1219 1581 811 2004 \\n-Q 403 2428 403 3163 \\n-Q 403 3881 828 4315 \\n-Q 1253 4750 1959 4750 \\n-Q 2769 4750 3195 4129 \\n-Q 3622 3509 3622 2328 \\n-Q 3622 1225 3098 567 \\n-Q 2575 -91 1691 -91 \\n-Q 1453 -91 1209 -44 \\n-Q 966 3 703 97 \\n-z\\n-M 1959 2075 \\n-Q 2384 2075 2632 2365 \\n-Q 2881 2656 2881 3163 \\n-Q 2881 3666 2632 3958 \\n-Q 2384 4250 1959 4250 \\n-Q 1534 4250 1286 3958 \\n-Q 1038 3666 1038 3163 \\n-Q 1038 2656 1286 2365 \\n-Q 1534 2075 1959 2075 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \\n-Q 1688 2584 1439 2293 \\n-Q 1191 2003 1191 1497 \\n-Q 1191 994 1439 701 \\n-Q 1688 409 2113 409 \\n-Q 2538 409 2786 701 \\n-Q 3034 994 3034 1497 \\n-Q 3034 2003 2786 2293 \\n-Q 2538 2584 2113 2584 \\n-z\\n-M 3366 4563 \\n-L 3366 3988 \\n-Q 3128 4100 2886 4159 \\n-Q 2644 4219 2406 4219 \\n-Q 1781 4219 1451 3797 \\n-Q 1122 3375 1075 2522 \\n-Q 1259 2794 1537 2939 \\n-Q 1816 3084 2150 3084 \\n-Q 2853 3084 3261 2657 \\n-Q 3669 2231 3669 1497 \\n-Q 3669 778 3244 343 \\n-Q 2819 -91 2113 -91 \\n-Q 1303 -91 875 529 \\n-Q 447 1150 447 2328 \\n-Q 447 3434 972 4092 \\n-Q 1497 4750 2381 4750 \\n-Q 2619 4750 2861 4703 \\n-Q 3103 4656 3366 4563 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \\n-Q 1584 2216 1326 1975 \\n-Q 1069 1734 1069 1313 \\n-Q 1069 891 1326 650 \\n-Q 1584 409 2034 409 \\n-Q 2484 409 2743 651 \\n-Q 3003 894 3003 1313 \\n-Q 3003 1734 2745 1975 \\n-Q 2488 2216 2034 2216 \\n-z\\n-M 1403 2484 \\n-Q 997 2584 770 2862 \\n-Q 544 3141 544 3541 \\n-Q 544 4100 942 4425 \\n-Q 1341 4750 2034 4750 \\n-Q 2731 4750 3128 4425 \\n-Q 3525 4100 3525 3541 \\n-Q 3525 3141 3298 2862 \\n-Q 3072 2584 2669 2484 \\n-Q 3125 2378 3379 2068 \\n-Q 3634 1759 3634 1313 \\n-Q 3634 634 3220 271 \\n-Q 2806 -91 2034 -91 \\n-Q 1263 -91 848 271 \\n-Q 434 634 434 1313 \\n-Q 434 1759 690 2068 \\n-Q 947 2378 1403 2484 \\n-z\\n-M 1172 3481 \\n-Q 1172 3119 1398 2916 \\n-Q 1625 2713 2034 2713 \\n-Q 2441 2713 2670 2916 \\n-Q 2900 3119 2900 3481 \\n-Q 2900 3844 2670 4047 \\n-Q 2441 4250 2034 4250 \\n-Q 1625 4250 1398 4047 \\n-Q 1172 3844 1172 3481 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-58\" d=\"M 403 4666 \\n-L 1081 4666 \\n-L 2241 2931 \\n-L 3406 4666 \\n-L 4084 4666 \\n-L 2584 2425 \\n-L 4184 0 \\n-L 3506 0 \\n-L 2194 1984 \\n-L 872 0 \\n-L 191 0 \\n-L 1856 2491 \\n-L 403 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-33\"/>\\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_16\">\\n-    <!-- 240.07 hrs (1.6X) -->\\n-    <g transform=\"translate(417.074714 261.260724) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \\n-L 3525 4666 \\n-L 3525 4397 \\n-L 1831 0 \\n-L 1172 0 \\n-L 2766 4134 \\n-L 525 4134 \\n-L 525 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-32\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_17\">\\n-    <!-- 145.20 hrs (2.7X) -->\\n-    <g transform=\"translate(288.361407 181.681776) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-31\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_18\">\\n-    <!-- 51.50 hrs (7.6X) -->\\n-    <g transform=\"translate(161.231011 102.102829) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"318.066406\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"381.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"422.558594\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"474.658203\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"506.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_19\">\\n-    <!-- SlimOrca 518K (1 epoch on 1 T4 GPU) -->\\n-    <g transform=\"translate(217.05625 45.84) scale(0.16 -0.16)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \\n-L 3425 3897 \\n-Q 3066 4069 2747 4153 \\n-Q 2428 4238 2131 4238 \\n-Q 1616 4238 1336 4038 \\n-Q 1056 3838 1056 3469 \\n-Q 1056 3159 1242 3001 \\n-Q 1428 2844 1947 2747 \\n-L 2328 2669 \\n-Q 3034 2534 3370 2195 \\n-Q 3706 1856 3706 1288 \\n-Q 3706 609 3251 259 \\n-Q 2797 -91 1919 -91 \\n-Q 1588 -91 1214 -16 \\n-Q 841 59 441 206 \\n-L 441 856 \\n-Q 825 641 1194 531 \\n-Q 1563 422 1919 422 \\n-Q 2459 422 2753 634 \\n-Q 3047 847 3047 1241 \\n-Q 3047 1584 2836 1778 \\n-Q 2625 1972 2144 2069 \\n-L 1759 2144 \\n-Q 1053 2284 737 2584 \\n-Q 422 2884 422 3419 \\n-Q 422 4038 858 4394 \\n-Q 1294 4750 2059 4750 \\n-Q 2388 4750 2728 4690 \\n-Q 3069 4631 3425 4513 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2694 \\n-L 3353 4666 \\n-L 4166 4666 \\n-L 1850 2491 \\n-L 4331 0 \\n-L 3500 0 \\n-L 1259 2247 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-47\" d=\"M 3809 666 \\n-L 3809 1919 \\n-L 2778 1919 \\n-L 2778 2438 \\n-L 4434 2438 \\n-L 4434 434 \\n-Q 4069 175 3628 42 \\n-Q 3188 -91 2688 -91 \\n-Q 1594 -91 976 548 \\n-Q 359 1188 359 2328 \\n-Q 359 3472 976 4111 \\n-Q 1594 4750 2688 4750 \\n-Q 3144 4750 3555 4637 \\n-Q 3966 4525 4313 4306 \\n-L 4313 3634 \\n-Q 3963 3931 3569 4081 \\n-Q 3175 4231 2741 4231 \\n-Q 1884 4231 1454 3753 \\n-Q 1025 3275 1025 2328 \\n-Q 1025 1384 1454 906 \\n-Q 1884 428 2741 428 \\n-Q 3075 428 3337 486 \\n-Q 3600 544 3809 666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-53\"/>\\n-     <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\\n-     <use xlink:href=\"#DejaVuSans-69\" x=\"91.259766\"/>\\n-     <use xlink:href=\"#DejaVuSans-6d\" x=\"119.042969\"/>\\n-     <use xlink:href=\"#DejaVuSans-4f\" x=\"216.455078\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"295.166016\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"334.029297\"/>\\n-     <use xlink:href=\"#DejaVuSans-61\" x=\"389.009766\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"450.289062\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"482.076172\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"545.699219\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"609.322266\"/>\\n-     <use xlink:href=\"#DejaVuSans-4b\" x=\"672.945312\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"738.521484\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"770.308594\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"809.322266\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"872.945312\"/>\\n-     <use xlink:href=\"#DejaVuSans-65\" x=\"904.732422\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"966.255859\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1029.732422\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"1090.914062\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"1145.894531\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1209.273438\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1241.060547\"/>\\n-     <use xlink:href=\"#DejaVuSans-6e\" x=\"1302.242188\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1365.621094\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"1397.408203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1461.03125\"/>\\n-     <use xlink:href=\"#DejaVuSans-54\" x=\"1492.818359\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"1553.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1617.525391\"/>\\n-     <use xlink:href=\"#DejaVuSans-47\" x=\"1649.3125\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"1726.802734\"/>\\n-     <use xlink:href=\"#DejaVuSans-55\" x=\"1787.105469\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"1860.298828\"/>\\n-    </g>\\n-   </g>\\n-  </g>\\n- </g>\\n- <defs>\\n-  <clipPath id=\"p682a0e8bed\">\\n-   <rect x=\"90\" y=\"51.84\" width=\"558\" height=\"332.64\"/>\\n-  </clipPath>\\n- </defs>\\n-</svg>\\n',\n",
       " 'Binary files /dev/null and b/images/try live demo green.png differ\\n',\n",
       " '@@ -33,7 +33,7 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-\\t\"transformers\",\\n+    \"transformers\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n@@ -70,4 +70,4 @@ colab = [\\n [project.urls]\\n homepage = \"http://www.unsloth.ai\"\\n documentation = \"https://github.com/unslothai/unsloth\"\\n-repository = \"https://github.com/unslothai/unsloth\"\\n\\\\ No newline at end of file\\n+repository = \"https://github.com/unslothai/unsloth\"\\n',\n",
       " '@@ -11,7 +11,7 @@\\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n-__version__ = \"2023.11\"\\n+__version__ = \"2023.12\"\\n import os\\n import warnings\\n import importlib\\n@@ -35,7 +35,7 @@ if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n         )\\n         os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n else:\\n-    warnings.warn(\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is not set. We shall set it ourselves.\")\\n+    # warnings.warn(\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is not set. We shall set it ourselves.\")\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n',\n",
       " '@@ -43,7 +43,6 @@ def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n     mask = col_offsets < n_cols\\n \\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     label_idx = tl.load(labels_ptr).to(tl.int32)\\n     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(\"inf\")).to(tl.float32)\\n     max_logits = tl.max(logits, 0)\\n@@ -88,7 +87,6 @@ def _cross_entropy_backward(logits_ptr, logits_row_stride,\\n     col_offsets = tl.arange(0, BLOCK_SIZE)\\n     mask = col_offsets < n_cols\\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\\n \\n     if label_idx != -100:\\n',\n",
       " '@@ -35,7 +35,6 @@ def _rope_embedding(\\n     mask = col_offsets < half_head_dim\\n \\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     rot_position = row_position % seqlen\\n \\n     Q   += row_position*  Q_row_stride + head_position*head_dim\\n@@ -48,8 +47,6 @@ def _rope_embedding(\\n \\n     Q2   = tl.load(Q   + half_head_dim*1 + col_offsets, mask = mask, other = 0)\\n     # RoPE repeats sin and cos so 128 = [64, 64].\\n-    # sin2 = tl.load(sin + half_head_dim*1, mask = mask, other = 0)\\n-    # cos2 = tl.load(cos + half_head_dim*1, mask = mask, other = 0)\\n \\n     if BACKWARD_PASS:\\n         \"\"\"\\n@@ -62,11 +59,8 @@ def _rope_embedding(\\n             where R.T is again the same  [ 0, -I]\\n             but the minus is transposed. [ I,  0]\\n         \"\"\"\\n-        # sin1, sin2 = -sin1, -sin2\\n         sin1 = -sin1\\n-\\n-    # tl.store(Q + half_head_dim*0, Q1*cos1 - Q2*sin1, mask = mask)\\n-    # tl.store(Q + half_head_dim*1, Q2*cos2 + Q1*sin2, mask = mask)\\n+    \\n     # RoPE repeats sin and cos so 128 = [64, 64].\\n     tl.store(Q + half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)\\n     tl.store(Q + half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)\\n',\n",
       " '@@ -13,12 +13,12 @@\\n # limitations under the License.\\n \\n import triton\\n-MAX_FUSED_SIZE = 65535 # 2**16 - 1\\n+MAX_FUSED_SIZE = 65536 # 2**16 Solves https://github.com/unslothai/unsloth/issues/7\\n next_power_of_2 = triton.next_power_of_2\\n \\n def calculate_settings(n):\\n     BLOCK_SIZE = next_power_of_2(n)\\n-    # CUDA only supports 65535 - 2^16-1 threads per block\\n+    # CUDA only supports 65536 - 2^16 threads per block\\n     if BLOCK_SIZE > MAX_FUSED_SIZE:\\n         raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\\\\n                            f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\\n',\n",
       " '@@ -16,14 +16,15 @@ import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n from transformers.models.llama.modeling_llama import (\\n-    # apply_rotary_pos_emb,\\n-    # repeat_kv,\\n-    # _prepare_4d_causal_attention_mask,\\n+    _prepare_4d_causal_attention_mask,\\n     logger,\\n     BaseModelOutputWithPast,\\n     CausalLMOutputWithPast,\\n )\\n from ..kernels import *\\n+from ._utils import (\\n+    prepare_model_for_kbit_training,\\n+)\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -37,7 +38,6 @@ else:\\n     # Tri Dao\\'s benchmark shows xformers is faster for now.\\n     HAS_FLASH_ATTENTION = False\\n pass\\n-\\n import xformers.ops.fmha as xformers\\n xformers_attention = xformers.memory_efficient_attention\\n \\n@@ -55,12 +55,9 @@ import bitsandbytes as bnb\\n import numpy as np\\n import types\\n \\n-from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n-from ._utils import (\\n-    prepare_model_for_kbit_training,\\n-)\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -92,10 +89,6 @@ def LlamaAttention_fast_forward(\\n ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\\n     \\n     bsz, q_len, _ = hidden_states.size()\\n-\\n-    # Q = self.q_proj(hidden_states)\\n-    # K = self.k_proj(hidden_states)\\n-    # V = self.v_proj(hidden_states)\\n     Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n     n_heads    = self.num_heads\\n@@ -112,8 +105,6 @@ def LlamaAttention_fast_forward(\\n     if past_key_value is not None:\\n         kv_seq_len += past_key_value[0].shape[-2]\\n \\n-    # cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)\\n-    # Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)\\n     if position_ids is None:\\n         cos = self.rotary_emb.cos_cached\\n         sin = self.rotary_emb.sin_cached\\n@@ -130,10 +121,9 @@ def LlamaAttention_fast_forward(\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n-    # no_attention_mask = attention_mask is None\\n-    # Ignore attention_mask\\n-\\n-    if (not HAS_FLASH_ATTENTION): #and no_attention_mask:\\n+    # Xformers doesnt support backward pass for GQA (yet)\\n+    # TEMP fix\\n+    if (n_groups == 1) and (not HAS_FLASH_ATTENTION):\\n         # Xformers memory efficient attention\\n         # Also has Flash Attention v2 dispatching\\n         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)\\n@@ -143,18 +133,17 @@ def LlamaAttention_fast_forward(\\n \\n         # Grouped query attention\\n         if n_groups != 1:\\n-            Q = Q.reshape(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n-\\n-            K = K.reshape(bsz, q_len, n_groups,          1, head_dim)\\n-            V = V.reshape(bsz, q_len, n_groups,          1, head_dim)\\n-            K = K .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n-            V = V .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n+            Q = Q.reshape(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            K = K.reshape(bsz, q_len, n_kv_heads,        1, head_dim)\\n+            V = V.reshape(bsz, q_len, n_kv_heads,        1, head_dim)\\n+            K = K .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            V = V .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n         A = A.view(bsz, q_len, n_heads, head_dim)\\n \\n-    elif HAS_FLASH_ATTENTION:# and no_attention_mask:\\n+    elif HAS_FLASH_ATTENTION:\\n         # Flash Attention\\n         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)\\n         Q = Q.transpose(1, 2)\\n@@ -163,37 +152,22 @@ def LlamaAttention_fast_forward(\\n \\n         # Flash Attention v2 auto supports grouped query attention\\n         A = flash_attn_func(Q, K, V, causal = True)\\n-\\n     else:\\n-        # Uses Pytorch\\'s scaled dot product attention\\n-        if attention_mask is not None:\\n-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\\n-                raise ValueError(\\n-                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\\n-                )\\n-        pass\\n-\\n         # Grouped query attention\\n-        # K = repeat_kv(K, n_groups)\\n-        # V = repeat_kv(V, n_groups)\\n         if n_groups != 1:\\n             K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n             V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n             K = K.reshape(bsz, n_heads, q_len, head_dim)\\n             V = V.reshape(bsz, n_heads, q_len, head_dim)\\n         pass\\n-\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n-        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = attention_mask is None)\\n+        A = scaled_dot_product_attention(Q, K, V, attn_mask = None, is_causal = True)\\n         # Go back to (batch_size, seq_len, n_heads, head_dim)\\n         A = A.transpose(1, 2)\\n     pass\\n     attn_output = A.reshape(bsz, q_len, self.hidden_size)\\n-\\n-    # attn_output = self.o_proj(attn_output)\\n     attn_output = self.apply_o(self, attn_output)\\n-\\n     attn_weights = None\\n     return attn_output, attn_weights, past_key_value\\n pass\\n@@ -227,7 +201,6 @@ def LlamaDecoderLayer_fast_forward(\\n     \"\"\"\\n     residual = hidden_states\\n \\n-    # hidden_states = self.input_layernorm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n \\n     # Self Attention\\n@@ -245,7 +218,6 @@ def LlamaDecoderLayer_fast_forward(\\n \\n     # Fully Connected\\n     residual = hidden_states\\n-    # hidden_states = self.post_attention_layernorm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n     hidden_states = self.mlp(hidden_states)\\n     hidden_states = residual + hidden_states\\n@@ -308,7 +280,7 @@ def LlamaModel_fast_forward(\\n     if (past_key_values_length != 0):\\n         position_ids = torch.arange(\\n             past_key_values_length, seq_length + past_key_values_length,\\n-            dtype  = torch.int32,#dtype=torch.long,\\n+            dtype  = torch.int32,\\n             device = \"cuda\",\\n         )\\n         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\\n@@ -326,11 +298,7 @@ def LlamaModel_fast_forward(\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n     # Ignore attention_mask\\n-    if True:\\n-    # if attention_mask is None:\\n-        # attention_mask = torch.ones(\\n-        #     (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\\n-        # )\\n+    if attention_mask is None:\\n         padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n@@ -339,7 +307,7 @@ def LlamaModel_fast_forward(\\n             padding_mask = None\\n \\n         attention_mask = _prepare_4d_causal_attention_mask(\\n-            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\\n+            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length,\\n         )\\n     pass\\n \\n@@ -403,7 +371,6 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    # hidden_states = self.norm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n \\n     # add hidden states from the last decoder layer\\n@@ -466,19 +433,13 @@ def LlamaForCausalLM_fast_forward(\\n \\n     loss = None\\n     if labels is not None:\\n-        # logits = logits.float()\\n-        # shift_logits = logits[..., :-1, :].contiguous()\\n-        # shift_labels = labels[..., 1:].contiguous()\\n-        # shift_labels = shift_labels.view(-1)\\n-        # shift_logits = shift_logits.view(-1, self.config.vocab_size)\\n         shift_logits = logits\\n+        if not hasattr(self, \"extra_ignored_labels\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/10\\n+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+        pass\\n+        \\n         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-\\n-        # loss_fct = torch.nn.CrossEntropyLoss(\\n-        #     ignore_index = self.ignore_index,\\n-        #     label_smoothing = self.label_smoothing,\\n-        # )\\n-        # loss = loss_fct(shift_logits, shift_labels)\\n         loss = fast_cross_entropy_loss(\\n             logits = shift_logits,\\n             labels = shift_labels,\\n@@ -547,13 +508,14 @@ class FastLlamaModel:\\n         load_in_4bit = True,\\n         token = None,\\n         device_map = \"sequential\",\\n+        rope_scaling = None,\\n     ):\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n \\n         statistics = \\\\\\n-            \"==((====))==  Unsloth: Fast Llama patching release 23.11\\\\n\"\\\\\\n+            \"==((====))==  Unsloth: Fast Llama patching release 2023.12\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n            f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n            f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n@@ -570,9 +532,20 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # [TODO]: Determine RoPE scaling\\n-        # https://github.com/huggingface/transformers/pull/24653\\n-        assert(max_seq_length <= 4096)\\n+        # RoPE scaling\\n+        model_max_seq_length = \\\\\\n+            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+\\n+        if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+            rope_scaling = max_seq_length / model_max_seq_length\\n+            logger.warning_once(\\n+                f\"Unsloth: {model_name} can only handle sequence lengths of of most \"\\\\\\n+                f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n+                f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n+                f\"{max_seq_length}!\"\\n+            )\\n+            rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+        pass\\n \\n         bnb_config = None\\n         if load_in_4bit:\\n@@ -589,6 +562,7 @@ class FastLlamaModel:\\n             torch_dtype = dtype,\\n             quantization_config = bnb_config,\\n             token = token,\\n+            rope_scaling = rope_scaling,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n@@ -596,9 +570,22 @@ class FastLlamaModel:\\n             padding_side = \"right\",\\n             token = token,\\n         )\\n-        tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token});\\n-        tokenizer.pad_token = tokenizer.unk_token\\n-        config = model.config.update({\"pad_token_id\" : tokenizer.unk_token_id});\\n+\\n+        if not hasattr(tokenizer, \"pad_token\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/5\\n+            if hasattr(tokenizer, \"unk_token\"):\\n+                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n+                tokenizer.pad_token = tokenizer.unk_token\\n+            else:\\n+                logger.warning_one(\\n+                    f\"{model_name} does not have a padding or unknown token!\\\\n\"\\\\\\n+                    f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+                )\\n+                assert(hasattr(tokenizer, \"eos_token\"))\\n+                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n+                tokenizer.pad_token = tokenizer.eos_token\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+        pass\\n \\n         model = FastLlamaModel.post_patch(model)\\n \\n@@ -607,6 +594,8 @@ class FastLlamaModel:\\n             layer.self_attn.apply_qkv = original_apply_qkv\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n+\\n+        model.max_seq_length = max_seq_length\\n         return model, tokenizer\\n     pass\\n \\n@@ -668,6 +657,8 @@ class FastLlamaModel:\\n         random_state = 3407,\\n         max_seq_length = 2048,\\n     ):\\n+        assert(max_seq_length <= model.max_seq_length)\\n+\\n         if lora_dropout != 0:\\n             raise TypeError(\"Unsloth: Fast Llama patching only works with dropout = 0.\")\\n         if bias != \"none\":\\n@@ -727,8 +718,14 @@ class FastLlamaModel:\\n         pass\\n \\n         # Patch cross entropy loss labels\\n-        model.model.extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n-        \\n+        # Fixes https://github.com/unslothai/unsloth/issues/10\\n+        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n+        model.model.extra_ignored_labels = extra_ignored_labels\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model.max_seq_length = max_seq_length\\n+            internal_model = internal_model.model\\n+        pass\\n         return model\\n     pass\\n pass\\n',\n",
       " '@@ -1,23 +1,25 @@\\n <div class=\"align-center\">\\n   <img src=\"./images/unsloth new logo.png\" width=\"400\" />\\n   <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord.png\" width=\"180\"></a>\\n+  <a href=\"https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing\"><img src=\"./images/try live demo green.png\" width=\"130\"></a>\\n </div>\\n \\n-\\n-## 80% faster 50% less memory local QLoRA finetuning\\n+## 2-5x faster 50% less memory local LLM finetuning\\n * Manual autograd engine - hand derived backprop steps.\\n-* QLoRA / LoRA 80% faster, 50% less memory.\\n-* All kernels written in OpenAI\\'s Triton language.\\n+* 2x to 5x faster than QLoRA. 50% less memory usage.\\n+* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language.\\n * 0% loss in accuracy - no approximation methods - all exact.\\n-* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+. Tesla T4, RTX 20, 30, 40 series, A100, H100s\\n-* Flash Attention support via Xformers.\\n-* Supports 4bit and 16bit LoRA finetuning.\\n+* No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)\\n+* [Flash Attention v2](https://github.com/Dao-AILab/flash-attention) support via [Xformers](https://github.com/facebookresearch/xformers).\\n+* **NEW!** Works on **Linux** and **Windows** via WSL.\\n+* **NEW!** Experimental support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)!\\n+* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n * Train Slim Orca **fully locally in 260 hours from 1301 hours (5x faster).**\\n * Open source version trains 5x faster or you can check out [Unsloth Pro and Max](https://unsloth.ai/) codepaths for **30x faster training**!\\n   \\n <div class=\"align-center\">\\n   <img src=\"./images/Slim Orca 2GPUs.png\" width=\"400\" />\\n-  <img src=\"./images/LAION%202GPU.svg\" width=\"400\" />\\n+  <img src=\"./images/LAION 2GPU.png\" width=\"400\" />\\n </div>\\n \\n 1. Try our Colab examples for [the Alpaca 52K dataset](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) or [the Slim Orca 518K dataset](https://colab.research.google.com/drive/1VNqLARpE8N8eYwNrUSDoHVjtbR9W0_c7?usp=sharing).\\n@@ -49,7 +51,13 @@ pip install --upgrade --force-reinstall --no-cache-dir torch triton \\\\\\n ```\\n Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n \\n-# Alpaca Example\\n+4. If you get errors, try the below first, then go back to step 1:\\n+```\\n+pip install --upgrade pip\\n+```\\n+\\n+# Documentation\\n+We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n ```\\n from unsloth import FastLlamaModel\\n import torch\\n@@ -59,7 +67,7 @@ load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False\\n \\n # Load Llama model\\n model, tokenizer = FastLlamaModel.from_pretrained(\\n-    model_name = \"unsloth/llama-2-7b\", # Supports any llama model\\n+    model_name = \"unsloth/llama-2-7b\", # Supports any llama model eg meta-llama/Llama-2-7b-hf\\n     max_seq_length = max_seq_length,\\n     dtype = dtype,\\n     load_in_4bit = load_in_4bit,\\n@@ -80,12 +88,16 @@ model = FastLlamaModel.get_peft_model(\\n     max_seq_length = max_seq_length,\\n )\\n \\n-trainer = .... Use Huggingface\\'s Trainer and dataset loading\\n+trainer = .... Use Huggingface\\'s Trainer and dataset loading (TRL, transformers etc)\\n ```\\n \\n If you trained a model with Unsloth, we made a cool sticker!!\\n <img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n \\n+# DPO (Direct Preference Optimization) Experimental support\\n+[152334H](https://github.com/152334H) hacked Unsloth to work with DPO via TRL!\\n+1. Hack the model\\'s `config.json` to be llama model. [Example gist](https://gist.github.com/152334H/d8a68b51b83bac008a02e69ecc81d5c1).\\n+2. Use Unsloth for DPO for both base and reference models. [Example gist](https://gist.github.com/152334H/4847f3a8cca12894877e6b30698b0b64).\\n \\n # Future Milestones and limitations\\n 1. Support sqrt gradient checkpointing which further slashes memory usage by 25%.\\n@@ -94,6 +106,9 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n # Performance comparisons on 1 Tesla T4 GPU:\\n **Time taken for 1 epoch**\\n \\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n | System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n | --- | --- | --- | --- | --- | --- |\\n | Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n@@ -113,19 +128,28 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n # Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n **Time taken for 1 epoch**\\n \\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n | --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n \\n **Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n \\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n | --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+\\n+### For replication of timings:\\n+* [Huggingface LAION DDP reference implementation](https://www.kaggle.com/code/danielhanchen/huggingface-original-laion-oig) 60 steps on DDP Kaggle 2 Tesla T4 GPUs takes 40 minutes and 46 seconds\\n+* [Unsloth LAION DDP fast implementation](https://www.kaggle.com/code/danielhanchen/unsloth-laion-chip2-kaggle) 60 steps on DDP Kaggle 2 Tesla T4 GPUs - **Unsloth only uses 1 GPU whilst Pro plans use more.** takes 4 minutes and 34 seconds **(8.64x speedup)**\\n \\n # Troubleshooting\\n 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n@@ -136,4 +160,8 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n \\n 3. If it doesn\\'t install - maybe try updating `pip`.\\n \\n+# Credits\\n+1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n+2. [152334H](https://github.com/152334H) for experimental DPO support\\n+\\n <img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files a/images/Discord.png and b/images/Discord.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/LAION 2GPU.png differ\\n',\n",
       " '@@ -1,1518 +0,0 @@\\n-<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\n-  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n- <metadata>\\n-  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n-   <cc:Work>\\n-    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\\n-    <dc:date>2023-11-30T13:10:10.501490</dc:date>\\n-    <dc:format>image/svg+xml</dc:format>\\n-    <dc:creator>\\n-     <cc:Agent>\\n-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\\n-     </cc:Agent>\\n-    </dc:creator>\\n-   </cc:Work>\\n-  </rdf:RDF>\\n- </metadata>\\n- <defs>\\n-  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\\n- </defs>\\n- <g id=\"figure_1\">\\n-  <g id=\"patch_1\">\\n-   <path d=\"M 0 432 \\n-L 720 432 \\n-L 720 0 \\n-L 0 0 \\n-L 0 432 \\n-z\\n-\" style=\"fill: none\"/>\\n-  </g>\\n-  <g id=\"axes_1\">\\n-   <g id=\"patch_2\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-L 648 51.84 \\n-L 90 51.84 \\n-L 90 384.48 \\n-z\\n-\" style=\"fill: none\"/>\\n-   </g>\\n-   <g id=\"patch_3\">\\n-    <path d=\"M 90 369.36 \\n-L 621.428571 369.36 \\n-L 621.428571 305.696842 \\n-L 90 305.696842 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_4\">\\n-    <path d=\"M 90 289.781053 \\n-L 202.147218 289.781053 \\n-L 202.147218 226.117895 \\n-L 90 226.117895 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_5\">\\n-    <path d=\"M 90 210.202105 \\n-L 108.547009 210.202105 \\n-L 108.547009 146.538947 \\n-L 90 146.538947 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_6\">\\n-    <path d=\"M 90 130.623158 \\n-L 106.978894 130.623158 \\n-L 106.978894 66.96 \\n-L 90 66.96 \\n-z\\n-\" clip-path=\"url(#p42f2634aae)\" style=\"fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"matplotlib.axis_1\">\\n-    <g id=\"xtick_1\">\\n-     <g id=\"line2d_1\">\\n-      <defs>\\n-       <path id=\"m7f9e7b8ac6\" d=\"M 0 0 \\n-L 0 3.5 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"90\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_1\">\\n-      <!-- 0 -->\\n-      <g transform=\"translate(86.81875 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \\n-Q 1547 4250 1301 3770 \\n-Q 1056 3291 1056 2328 \\n-Q 1056 1369 1301 889 \\n-Q 1547 409 2034 409 \\n-Q 2525 409 2770 889 \\n-Q 3016 1369 3016 2328 \\n-Q 3016 3291 2770 3770 \\n-Q 2525 4250 2034 4250 \\n-z\\n-M 2034 4750 \\n-Q 2819 4750 3233 4129 \\n-Q 3647 3509 3647 2328 \\n-Q 3647 1150 3233 529 \\n-Q 2819 -91 2034 -91 \\n-Q 1250 -91 836 529 \\n-Q 422 1150 422 2328 \\n-Q 422 3509 836 4129 \\n-Q 1250 4750 2034 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-30\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_2\">\\n-     <g id=\"line2d_2\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"154.887493\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_2\">\\n-      <!-- 20 -->\\n-      <g transform=\"translate(148.524993 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \\n-L 3431 531 \\n-L 3431 0 \\n-L 469 0 \\n-L 469 531 \\n-Q 828 903 1448 1529 \\n-Q 2069 2156 2228 2338 \\n-Q 2531 2678 2651 2914 \\n-Q 2772 3150 2772 3378 \\n-Q 2772 3750 2511 3984 \\n-Q 2250 4219 1831 4219 \\n-Q 1534 4219 1204 4116 \\n-Q 875 4013 500 3803 \\n-L 500 4441 \\n-Q 881 4594 1212 4672 \\n-Q 1544 4750 1819 4750 \\n-Q 2544 4750 2975 4387 \\n-Q 3406 4025 3406 3419 \\n-Q 3406 3131 3298 2873 \\n-Q 3191 2616 2906 2266 \\n-Q 2828 2175 2409 1742 \\n-Q 1991 1309 1228 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_3\">\\n-     <g id=\"line2d_3\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"219.774987\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_3\">\\n-      <!-- 40 -->\\n-      <g transform=\"translate(213.412487 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \\n-L 825 1625 \\n-L 2419 1625 \\n-L 2419 4116 \\n-z\\n-M 2253 4666 \\n-L 3047 4666 \\n-L 3047 1625 \\n-L 3713 1625 \\n-L 3713 1100 \\n-L 3047 1100 \\n-L 3047 0 \\n-L 2419 0 \\n-L 2419 1100 \\n-L 313 1100 \\n-L 313 1709 \\n-L 2253 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-34\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_4\">\\n-     <g id=\"line2d_4\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"284.66248\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_4\">\\n-      <!-- 60 -->\\n-      <g transform=\"translate(278.29998 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \\n-Q 1688 2584 1439 2293 \\n-Q 1191 2003 1191 1497 \\n-Q 1191 994 1439 701 \\n-Q 1688 409 2113 409 \\n-Q 2538 409 2786 701 \\n-Q 3034 994 3034 1497 \\n-Q 3034 2003 2786 2293 \\n-Q 2538 2584 2113 2584 \\n-z\\n-M 3366 4563 \\n-L 3366 3988 \\n-Q 3128 4100 2886 4159 \\n-Q 2644 4219 2406 4219 \\n-Q 1781 4219 1451 3797 \\n-Q 1122 3375 1075 2522 \\n-Q 1259 2794 1537 2939 \\n-Q 1816 3084 2150 3084 \\n-Q 2853 3084 3261 2657 \\n-Q 3669 2231 3669 1497 \\n-Q 3669 778 3244 343 \\n-Q 2819 -91 2113 -91 \\n-Q 1303 -91 875 529 \\n-Q 447 1150 447 2328 \\n-Q 447 3434 972 4092 \\n-Q 1497 4750 2381 4750 \\n-Q 2619 4750 2861 4703 \\n-Q 3103 4656 3366 4563 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-36\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_5\">\\n-     <g id=\"line2d_5\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"349.549974\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_5\">\\n-      <!-- 80 -->\\n-      <g transform=\"translate(343.187474 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \\n-Q 1584 2216 1326 1975 \\n-Q 1069 1734 1069 1313 \\n-Q 1069 891 1326 650 \\n-Q 1584 409 2034 409 \\n-Q 2484 409 2743 651 \\n-Q 3003 894 3003 1313 \\n-Q 3003 1734 2745 1975 \\n-Q 2488 2216 2034 2216 \\n-z\\n-M 1403 2484 \\n-Q 997 2584 770 2862 \\n-Q 544 3141 544 3541 \\n-Q 544 4100 942 4425 \\n-Q 1341 4750 2034 4750 \\n-Q 2731 4750 3128 4425 \\n-Q 3525 4100 3525 3541 \\n-Q 3525 3141 3298 2862 \\n-Q 3072 2584 2669 2484 \\n-Q 3125 2378 3379 2068 \\n-Q 3634 1759 3634 1313 \\n-Q 3634 634 3220 271 \\n-Q 2806 -91 2034 -91 \\n-Q 1263 -91 848 271 \\n-Q 434 634 434 1313 \\n-Q 434 1759 690 2068 \\n-Q 947 2378 1403 2484 \\n-z\\n-M 1172 3481 \\n-Q 1172 3119 1398 2916 \\n-Q 1625 2713 2034 2713 \\n-Q 2441 2713 2670 2916 \\n-Q 2900 3119 2900 3481 \\n-Q 2900 3844 2670 4047 \\n-Q 2441 4250 2034 4250 \\n-Q 1625 4250 1398 4047 \\n-Q 1172 3844 1172 3481 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-38\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_6\">\\n-     <g id=\"line2d_6\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"414.437467\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_6\">\\n-      <!-- 100 -->\\n-      <g transform=\"translate(404.893717 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-31\" d=\"M 794 531 \\n-L 1825 531 \\n-L 1825 4091 \\n-L 703 3866 \\n-L 703 4441 \\n-L 1819 4666 \\n-L 2450 4666 \\n-L 2450 531 \\n-L 3481 531 \\n-L 3481 0 \\n-L 794 0 \\n-L 794 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_7\">\\n-     <g id=\"line2d_7\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"479.324961\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_7\">\\n-      <!-- 120 -->\\n-      <g transform=\"translate(469.781211 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_8\">\\n-     <g id=\"line2d_8\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"544.212454\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_8\">\\n-      <!-- 140 -->\\n-      <g transform=\"translate(534.668704 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_9\">\\n-     <g id=\"line2d_9\">\\n-      <g>\\n-       <use xlink:href=\"#m7f9e7b8ac6\" x=\"609.099948\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_9\">\\n-      <!-- 160 -->\\n-      <g transform=\"translate(599.556198 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"text_10\">\\n-     <!-- Time taken (lower is better). * Unsloth Open uses 1 GPU only. -->\\n-     <g transform=\"translate(184.611563 414.27625) scale(0.12 -0.12)\">\\n-      <defs>\\n-       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \\n-L 3928 4666 \\n-L 3928 4134 \\n-L 2272 4134 \\n-L 2272 0 \\n-L 1638 0 \\n-L 1638 4134 \\n-L -19 4134 \\n-L -19 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \\n-L 1178 3500 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 3500 \\n-z\\n-M 603 4863 \\n-L 1178 4863 \\n-L 1178 4134 \\n-L 603 4134 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \\n-Q 3544 3216 3844 3400 \\n-Q 4144 3584 4550 3584 \\n-Q 5097 3584 5394 3201 \\n-Q 5691 2819 5691 2113 \\n-L 5691 0 \\n-L 5113 0 \\n-L 5113 2094 \\n-Q 5113 2597 4934 2840 \\n-Q 4756 3084 4391 3084 \\n-Q 3944 3084 3684 2787 \\n-Q 3425 2491 3425 1978 \\n-L 3425 0 \\n-L 2847 0 \\n-L 2847 2094 \\n-Q 2847 2600 2669 2842 \\n-Q 2491 3084 2119 3084 \\n-Q 1678 3084 1418 2786 \\n-Q 1159 2488 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1356 3278 1631 3431 \\n-Q 1906 3584 2284 3584 \\n-Q 2666 3584 2933 3390 \\n-Q 3200 3197 3328 2828 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \\n-L 3597 1613 \\n-L 953 1613 \\n-Q 991 1019 1311 708 \\n-Q 1631 397 2203 397 \\n-Q 2534 397 2845 478 \\n-Q 3156 559 3463 722 \\n-L 3463 178 \\n-Q 3153 47 2828 -22 \\n-Q 2503 -91 2169 -91 \\n-Q 1331 -91 842 396 \\n-Q 353 884 353 1716 \\n-Q 353 2575 817 3079 \\n-Q 1281 3584 2069 3584 \\n-Q 2775 3584 3186 3129 \\n-Q 3597 2675 3597 1894 \\n-z\\n-M 3022 2063 \\n-Q 3016 2534 2758 2815 \\n-Q 2500 3097 2075 3097 \\n-Q 1594 3097 1305 2825 \\n-Q 1016 2553 972 2059 \\n-L 3022 2063 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \\n-L 1172 3500 \\n-L 2356 3500 \\n-L 2356 3053 \\n-L 1172 3053 \\n-L 1172 1153 \\n-Q 1172 725 1289 603 \\n-Q 1406 481 1766 481 \\n-L 2356 481 \\n-L 2356 0 \\n-L 1766 0 \\n-Q 1100 0 847 248 \\n-Q 594 497 594 1153 \\n-L 594 3053 \\n-L 172 3053 \\n-L 172 3500 \\n-L 594 3500 \\n-L 594 4494 \\n-L 1172 4494 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \\n-Q 1497 1759 1228 1600 \\n-Q 959 1441 959 1056 \\n-Q 959 750 1161 570 \\n-Q 1363 391 1709 391 \\n-Q 2188 391 2477 730 \\n-Q 2766 1069 2766 1631 \\n-L 2766 1759 \\n-L 2194 1759 \\n-z\\n-M 3341 1997 \\n-L 3341 0 \\n-L 2766 0 \\n-L 2766 531 \\n-Q 2569 213 2275 61 \\n-Q 1981 -91 1556 -91 \\n-Q 1019 -91 701 211 \\n-Q 384 513 384 1019 \\n-Q 384 1609 779 1909 \\n-Q 1175 2209 1959 2209 \\n-L 2766 2209 \\n-L 2766 2266 \\n-Q 2766 2663 2505 2880 \\n-Q 2244 3097 1772 3097 \\n-Q 1472 3097 1187 3025 \\n-Q 903 2953 641 2809 \\n-L 641 3341 \\n-Q 956 3463 1253 3523 \\n-Q 1550 3584 1831 3584 \\n-Q 2591 3584 2966 3190 \\n-Q 3341 2797 3341 1997 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \\n-L 1159 4863 \\n-L 1159 1991 \\n-L 2875 3500 \\n-L 3609 3500 \\n-L 1753 1863 \\n-L 3688 0 \\n-L 2938 0 \\n-L 1159 1709 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \\n-Q 1566 4138 1362 3434 \\n-Q 1159 2731 1159 2009 \\n-Q 1159 1288 1364 580 \\n-Q 1569 -128 1984 -844 \\n-L 1484 -844 \\n-Q 1016 -109 783 600 \\n-Q 550 1309 550 2009 \\n-Q 550 2706 781 3412 \\n-Q 1013 4119 1484 4856 \\n-L 1984 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \\n-L 1178 4863 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \\n-Q 1497 3097 1228 2736 \\n-Q 959 2375 959 1747 \\n-Q 959 1119 1226 758 \\n-Q 1494 397 1959 397 \\n-Q 2419 397 2687 759 \\n-Q 2956 1122 2956 1747 \\n-Q 2956 2369 2687 2733 \\n-Q 2419 3097 1959 3097 \\n-z\\n-M 1959 3584 \\n-Q 2709 3584 3137 3096 \\n-Q 3566 2609 3566 1747 \\n-Q 3566 888 3137 398 \\n-Q 2709 -91 1959 -91 \\n-Q 1206 -91 779 398 \\n-Q 353 888 353 1747 \\n-Q 353 2609 779 3096 \\n-Q 1206 3584 1959 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \\n-L 844 3500 \\n-L 1563 769 \\n-L 2278 3500 \\n-L 2956 3500 \\n-L 3675 769 \\n-L 4391 3500 \\n-L 4966 3500 \\n-L 4050 0 \\n-L 3372 0 \\n-L 2619 2869 \\n-L 1863 0 \\n-L 1184 0 \\n-L 269 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \\n-Q 2534 3019 2420 3045 \\n-Q 2306 3072 2169 3072 \\n-Q 1681 3072 1420 2755 \\n-Q 1159 2438 1159 1844 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1341 3275 1631 3429 \\n-Q 1922 3584 2338 3584 \\n-Q 2397 3584 2469 3576 \\n-Q 2541 3569 2628 3553 \\n-L 2631 2963 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \\n-L 2834 2853 \\n-Q 2591 2978 2328 3040 \\n-Q 2066 3103 1784 3103 \\n-Q 1356 3103 1142 2972 \\n-Q 928 2841 928 2578 \\n-Q 928 2378 1081 2264 \\n-Q 1234 2150 1697 2047 \\n-L 1894 2003 \\n-Q 2506 1872 2764 1633 \\n-Q 3022 1394 3022 966 \\n-Q 3022 478 2636 193 \\n-Q 2250 -91 1575 -91 \\n-Q 1294 -91 989 -36 \\n-Q 684 19 347 128 \\n-L 347 722 \\n-Q 666 556 975 473 \\n-Q 1284 391 1588 391 \\n-Q 1994 391 2212 530 \\n-Q 2431 669 2431 922 \\n-Q 2431 1156 2273 1281 \\n-Q 2116 1406 1581 1522 \\n-L 1381 1569 \\n-Q 847 1681 609 1914 \\n-Q 372 2147 372 2553 \\n-Q 372 3047 722 3315 \\n-Q 1072 3584 1716 3584 \\n-Q 2034 3584 2315 3537 \\n-Q 2597 3491 2834 3397 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-M 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2969 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \\n-L 1013 4856 \\n-Q 1481 4119 1714 3412 \\n-Q 1947 2706 1947 2009 \\n-Q 1947 1309 1714 600 \\n-Q 1481 -109 1013 -844 \\n-L 513 -844 \\n-Q 928 -128 1133 580 \\n-Q 1338 1288 1338 2009 \\n-Q 1338 2731 1133 3434 \\n-Q 928 4138 513 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2e\" d=\"M 684 794 \\n-L 1344 794 \\n-L 1344 0 \\n-L 684 0 \\n-L 684 794 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2a\" d=\"M 3009 3897 \\n-L 1888 3291 \\n-L 3009 2681 \\n-L 2828 2375 \\n-L 1778 3009 \\n-L 1778 1831 \\n-L 1422 1831 \\n-L 1422 3009 \\n-L 372 2375 \\n-L 191 2681 \\n-L 1313 3291 \\n-L 191 3897 \\n-L 372 4206 \\n-L 1422 3572 \\n-L 1422 4750 \\n-L 1778 4750 \\n-L 1778 3572 \\n-L 2828 4206 \\n-L 3009 3897 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-55\" d=\"M 556 4666 \\n-L 1191 4666 \\n-L 1191 1831 \\n-Q 1191 1081 1462 751 \\n-Q 1734 422 2344 422 \\n-Q 2950 422 3222 751 \\n-Q 3494 1081 3494 1831 \\n-L 3494 4666 \\n-L 4128 4666 \\n-L 4128 1753 \\n-Q 4128 841 3676 375 \\n-Q 3225 -91 2344 -91 \\n-Q 1459 -91 1007 375 \\n-Q 556 841 556 1753 \\n-L 556 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-4f\" d=\"M 2522 4238 \\n-Q 1834 4238 1429 3725 \\n-Q 1025 3213 1025 2328 \\n-Q 1025 1447 1429 934 \\n-Q 1834 422 2522 422 \\n-Q 3209 422 3611 934 \\n-Q 4013 1447 4013 2328 \\n-Q 4013 3213 3611 3725 \\n-Q 3209 4238 2522 4238 \\n-z\\n-M 2522 4750 \\n-Q 3503 4750 4090 4092 \\n-Q 4678 3434 4678 2328 \\n-Q 4678 1225 4090 567 \\n-Q 3503 -91 2522 -91 \\n-Q 1538 -91 948 565 \\n-Q 359 1222 359 2328 \\n-Q 359 3434 948 4092 \\n-Q 1538 4750 2522 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \\n-L 1159 -1331 \\n-L 581 -1331 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-z\\n-M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \\n-L 544 3500 \\n-L 1119 3500 \\n-L 1119 1403 \\n-Q 1119 906 1312 657 \\n-Q 1506 409 1894 409 \\n-Q 2359 409 2629 706 \\n-Q 2900 1003 2900 1516 \\n-L 2900 3500 \\n-L 3475 3500 \\n-L 3475 0 \\n-L 2900 0 \\n-L 2900 538 \\n-Q 2691 219 2414 64 \\n-Q 2138 -91 1772 -91 \\n-Q 1169 -91 856 284 \\n-Q 544 659 544 1381 \\n-z\\n-M 1991 3584 \\n-L 1991 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \\n-L 3809 1919 \\n-L 2778 1919 \\n-L 2778 2438 \\n-L 4434 2438 \\n-L 4434 434 \\n-Q 4069 175 3628 42 \\n-Q 3188 -91 2688 -91 \\n-Q 1594 -91 976 548 \\n-Q 359 1188 359 2328 \\n-Q 359 3472 976 4111 \\n-Q 1594 4750 2688 4750 \\n-Q 3144 4750 3555 4637 \\n-Q 3966 4525 4313 4306 \\n-L 4313 3634 \\n-Q 3963 3931 3569 4081 \\n-Q 3175 4231 2741 4231 \\n-Q 1884 4231 1454 3753 \\n-Q 1025 3275 1025 2328 \\n-Q 1025 1384 1454 906 \\n-Q 1884 428 2741 428 \\n-Q 3075 428 3337 486 \\n-Q 3600 544 3809 666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \\n-L 1259 2394 \\n-L 2053 2394 \\n-Q 2494 2394 2734 2622 \\n-Q 2975 2850 2975 3272 \\n-Q 2975 3691 2734 3919 \\n-Q 2494 4147 2053 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 2053 4666 \\n-Q 2838 4666 3239 4311 \\n-Q 3641 3956 3641 3272 \\n-Q 3641 2581 3239 2228 \\n-Q 2838 1875 2053 1875 \\n-L 1259 1875 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \\n-Q 1816 -950 1584 -1140 \\n-Q 1353 -1331 966 -1331 \\n-L 506 -1331 \\n-L 506 -850 \\n-L 844 -850 \\n-Q 1081 -850 1212 -737 \\n-Q 1344 -625 1503 -206 \\n-L 1606 56 \\n-L 191 3500 \\n-L 800 3500 \\n-L 1894 763 \\n-L 2988 3500 \\n-L 3597 3500 \\n-L 2059 -325 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      </defs>\\n-      <use xlink:href=\"#DejaVuSans-54\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"57.958984\"/>\\n-      <use xlink:href=\"#DejaVuSans-6d\" x=\"85.742188\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"183.154297\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"244.677734\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"276.464844\"/>\\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"315.673828\"/>\\n-      <use xlink:href=\"#DejaVuSans-6b\" x=\"376.953125\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.238281\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"492.761719\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"556.140625\"/>\\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"587.927734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"626.941406\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"654.724609\"/>\\n-      <use xlink:href=\"#DejaVuSans-77\" x=\"715.90625\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"797.693359\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"859.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"900.330078\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"932.117188\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"959.900391\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1012\"/>\\n-      <use xlink:href=\"#DejaVuSans-62\" x=\"1043.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1107.263672\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1168.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1207.996094\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1247.205078\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"1308.728516\"/>\\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"1349.841797\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"1388.855469\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1420.642578\"/>\\n-      <use xlink:href=\"#DejaVuSans-2a\" x=\"1452.429688\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1502.429688\"/>\\n-      <use xlink:href=\"#DejaVuSans-55\" x=\"1534.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"1607.410156\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"1670.789062\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"1722.888672\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"1750.671875\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1811.853516\"/>\\n-      <use xlink:href=\"#DejaVuSans-68\" x=\"1851.0625\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1914.441406\"/>\\n-      <use xlink:href=\"#DejaVuSans-4f\" x=\"1946.228516\"/>\\n-      <use xlink:href=\"#DejaVuSans-70\" x=\"2024.939453\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"2088.416016\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"2149.939453\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2213.318359\"/>\\n-      <use xlink:href=\"#DejaVuSans-75\" x=\"2245.105469\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"2308.484375\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"2360.583984\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"2422.107422\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2474.207031\"/>\\n-      <use xlink:href=\"#DejaVuSans-31\" x=\"2505.994141\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2569.617188\"/>\\n-      <use xlink:href=\"#DejaVuSans-47\" x=\"2601.404297\"/>\\n-      <use xlink:href=\"#DejaVuSans-50\" x=\"2678.894531\"/>\\n-      <use xlink:href=\"#DejaVuSans-55\" x=\"2739.197266\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"2812.390625\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"2844.177734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"2905.359375\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"2968.738281\"/>\\n-      <use xlink:href=\"#DejaVuSans-79\" x=\"2996.521484\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"3041.451172\"/>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"matplotlib.axis_2\">\\n-    <g id=\"ytick_1\">\\n-     <g id=\"line2d_10\">\\n-      <defs>\\n-       <path id=\"me3cc6245ad\" d=\"M 0 0 \\n-L -3.5 0 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"337.528421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_11\">\\n-      <!-- Huggingface -->\\n-      <g transform=\"translate(19.68125 341.32764) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2753 \\n-L 3553 2753 \\n-L 3553 4666 \\n-L 4184 4666 \\n-L 4184 0 \\n-L 3553 0 \\n-L 3553 2222 \\n-L 1259 2222 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \\n-Q 2906 2416 2648 2759 \\n-Q 2391 3103 1925 3103 \\n-Q 1463 3103 1205 2759 \\n-Q 947 2416 947 1791 \\n-Q 947 1169 1205 825 \\n-Q 1463 481 1925 481 \\n-Q 2391 481 2648 825 \\n-Q 2906 1169 2906 1791 \\n-z\\n-M 3481 434 \\n-Q 3481 -459 3084 -895 \\n-Q 2688 -1331 1869 -1331 \\n-Q 1566 -1331 1297 -1286 \\n-Q 1028 -1241 775 -1147 \\n-L 775 -588 \\n-Q 1028 -725 1275 -790 \\n-Q 1522 -856 1778 -856 \\n-Q 2344 -856 2625 -561 \\n-Q 2906 -266 2906 331 \\n-L 2906 616 \\n-Q 2728 306 2450 153 \\n-Q 2172 0 1784 0 \\n-Q 1141 0 747 490 \\n-Q 353 981 353 1791 \\n-Q 353 2603 747 3093 \\n-Q 1141 3584 1784 3584 \\n-Q 2172 3584 2450 3431 \\n-Q 2728 3278 2906 2969 \\n-L 2906 3500 \\n-L 3481 3500 \\n-L 3481 434 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \\n-L 2375 4384 \\n-L 1825 4384 \\n-Q 1516 4384 1395 4259 \\n-Q 1275 4134 1275 3809 \\n-L 1275 3500 \\n-L 2222 3500 \\n-L 2222 3053 \\n-L 1275 3053 \\n-L 1275 0 \\n-L 697 0 \\n-L 697 3053 \\n-L 147 3053 \\n-L 147 3500 \\n-L 697 3500 \\n-L 697 3744 \\n-Q 697 4328 969 4595 \\n-Q 1241 4863 1831 4863 \\n-L 2375 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \\n-L 3122 2828 \\n-Q 2878 2963 2633 3030 \\n-Q 2388 3097 2138 3097 \\n-Q 1578 3097 1268 2742 \\n-Q 959 2388 959 1747 \\n-Q 959 1106 1268 751 \\n-Q 1578 397 2138 397 \\n-Q 2388 397 2633 464 \\n-Q 2878 531 3122 666 \\n-L 3122 134 \\n-Q 2881 22 2623 -34 \\n-Q 2366 -91 2075 -91 \\n-Q 1284 -91 818 406 \\n-Q 353 903 353 1747 \\n-Q 353 2603 823 3093 \\n-Q 1294 3584 2113 3584 \\n-Q 2378 3584 2631 3529 \\n-Q 2884 3475 3122 3366 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-48\"/>\\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"138.574219\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"202.050781\"/>\\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"265.527344\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.310547\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"356.689453\"/>\\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"420.166016\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"455.371094\"/>\\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"516.650391\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"571.630859\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_2\">\\n-     <g id=\"line2d_11\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"257.949474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_12\">\\n-      <!-- Unsloth Open* -->\\n-      <g transform=\"translate(10.090625 261.748692) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4f\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"490.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"554.199219\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"615.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-2a\" x=\"679.101562\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_3\">\\n-     <g id=\"line2d_12\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"178.370526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_13\">\\n-      <!-- Unsloth Pro -->\\n-      <g transform=\"translate(25.942187 182.169745) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-50\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"470.564453\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"509.427734\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_4\">\\n-     <g id=\"line2d_13\">\\n-      <g>\\n-       <use xlink:href=\"#me3cc6245ad\" x=\"90\" y=\"98.791579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_14\">\\n-      <!-- Unsloth Max -->\\n-      <g transform=\"translate(21.126562 102.590798) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \\n-L 1569 4666 \\n-L 2759 1491 \\n-L 3956 4666 \\n-L 4897 4666 \\n-L 4897 0 \\n-L 4281 0 \\n-L 4281 4097 \\n-L 3078 897 \\n-L 2444 897 \\n-L 1241 4097 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \\n-L 2247 1797 \\n-L 3578 0 \\n-L 2900 0 \\n-L 1881 1375 \\n-L 863 0 \\n-L 184 0 \\n-L 1544 1831 \\n-L 300 3500 \\n-L 978 3500 \\n-L 1906 2253 \\n-L 2834 3500 \\n-L 3513 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4d\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"498.291016\"/>\\n-       <use xlink:href=\"#DejaVuSans-78\" x=\"559.570312\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"patch_7\">\\n-    <path d=\"M 90 384.48 \\n-L 90 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_8\">\\n-    <path d=\"M 648 384.48 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_9\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_10\">\\n-    <path d=\"M 90 51.84 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"text_15\">\\n-    <!-- 163.80 hrs (1.0X) -->\\n-    <g transform=\"translate(491.653585 340.839671) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \\n-Q 3050 2419 3304 2112 \\n-Q 3559 1806 3559 1356 \\n-Q 3559 666 3084 287 \\n-Q 2609 -91 1734 -91 \\n-Q 1441 -91 1130 -33 \\n-Q 819 25 488 141 \\n-L 488 750 \\n-Q 750 597 1062 519 \\n-Q 1375 441 1716 441 \\n-Q 2309 441 2620 675 \\n-Q 2931 909 2931 1356 \\n-Q 2931 1769 2642 2001 \\n-Q 2353 2234 1838 2234 \\n-L 1294 2234 \\n-L 1294 2753 \\n-L 1863 2753 \\n-Q 2328 2753 2575 2939 \\n-Q 2822 3125 2822 3475 \\n-Q 2822 3834 2567 4026 \\n-Q 2313 4219 1838 4219 \\n-Q 1578 4219 1281 4162 \\n-Q 984 4106 628 3988 \\n-L 628 4550 \\n-Q 988 4650 1302 4700 \\n-Q 1616 4750 1894 4750 \\n-Q 2613 4750 3031 4423 \\n-Q 3450 4097 3450 3541 \\n-Q 3450 3153 3228 2886 \\n-Q 3006 2619 2597 2516 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-58\" d=\"M 403 4666 \\n-L 1081 4666 \\n-L 2241 2931 \\n-L 3406 4666 \\n-L 4084 4666 \\n-L 2584 2425 \\n-L 4184 0 \\n-L 3506 0 \\n-L 2194 1984 \\n-L 872 0 \\n-L 191 0 \\n-L 1856 2491 \\n-L 403 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-31\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_16\">\\n-    <!-- 34.57 hrs (4.7X) -->\\n-    <g transform=\"translate(205.391593 261.260724) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-35\" d=\"M 691 4666 \\n-L 3169 4666 \\n-L 3169 4134 \\n-L 1269 4134 \\n-L 1269 2991 \\n-Q 1406 3038 1543 3061 \\n-Q 1681 3084 1819 3084 \\n-Q 2600 3084 3056 2656 \\n-Q 3513 2228 3513 1497 \\n-Q 3513 744 3044 326 \\n-Q 2575 -91 1722 -91 \\n-Q 1428 -91 1123 -41 \\n-Q 819 9 494 109 \\n-L 494 744 \\n-Q 775 591 1075 516 \\n-Q 1375 441 1709 441 \\n-Q 2250 441 2565 725 \\n-Q 2881 1009 2881 1497 \\n-Q 2881 1984 2565 2268 \\n-Q 2250 2553 1709 2553 \\n-Q 1456 2553 1204 2497 \\n-Q 953 2441 691 2322 \\n-L 691 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \\n-L 3525 4666 \\n-L 3525 4397 \\n-L 1831 0 \\n-L 1172 0 \\n-L 2766 4134 \\n-L 525 4134 \\n-L 525 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-33\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"318.066406\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"381.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"422.558594\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"474.658203\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"506.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_17\">\\n-    <!-- 5.72 hrs (28.7X) -->\\n-    <g transform=\"translate(111.791383 181.681776) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"254.443359\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"317.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"358.935547\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"411.035156\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"442.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"481.835938\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_18\">\\n-    <!-- 5.23 hrs (31.3X) -->\\n-    <g transform=\"translate(110.223269 102.102829) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"254.443359\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"317.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"358.935547\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"411.035156\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"442.822266\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"481.835938\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_19\">\\n-    <!-- LAION Chip2 210K (1 epoch on 2 T4 GPUs DDP) -->\\n-    <g transform=\"translate(178.88375 45.84) scale(0.16 -0.16)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 531 \\n-L 3531 531 \\n-L 3531 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \\n-L 1331 1722 \\n-L 3047 1722 \\n-L 2188 4044 \\n-z\\n-M 1831 4666 \\n-L 2547 4666 \\n-L 4325 0 \\n-L 3669 0 \\n-L 3244 1197 \\n-L 1141 1197 \\n-L 716 0 \\n-L 50 0 \\n-L 1831 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-49\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \\n-L 1478 4666 \\n-L 3547 763 \\n-L 3547 4666 \\n-L 4159 4666 \\n-L 4159 0 \\n-L 3309 0 \\n-L 1241 3903 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \\n-L 4122 3641 \\n-Q 3803 3938 3442 4084 \\n-Q 3081 4231 2675 4231 \\n-Q 1875 4231 1450 3742 \\n-Q 1025 3253 1025 2328 \\n-Q 1025 1406 1450 917 \\n-Q 1875 428 2675 428 \\n-Q 3081 428 3442 575 \\n-Q 3803 722 4122 1019 \\n-L 4122 359 \\n-Q 3791 134 3420 21 \\n-Q 3050 -91 2638 -91 \\n-Q 1578 -91 968 557 \\n-Q 359 1206 359 2328 \\n-Q 359 3453 968 4101 \\n-Q 1578 4750 2638 4750 \\n-Q 3056 4750 3426 4639 \\n-Q 3797 4528 4122 4306 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2694 \\n-L 3353 4666 \\n-L 4166 4666 \\n-L 1850 2491 \\n-L 4331 0 \\n-L 3500 0 \\n-L 1259 2247 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \\n-L 1259 519 \\n-L 2022 519 \\n-Q 2988 519 3436 956 \\n-Q 3884 1394 3884 2338 \\n-Q 3884 3275 3436 3711 \\n-Q 2988 4147 2022 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 1925 4666 \\n-Q 3281 4666 3915 4102 \\n-Q 4550 3538 4550 2338 \\n-Q 4550 1131 3912 565 \\n-Q 3275 0 1925 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-4c\"/>\\n-     <use xlink:href=\"#DejaVuSans-41\" x=\"57.962891\"/>\\n-     <use xlink:href=\"#DejaVuSans-49\" x=\"126.371094\"/>\\n-     <use xlink:href=\"#DejaVuSans-4f\" x=\"155.863281\"/>\\n-     <use xlink:href=\"#DejaVuSans-4e\" x=\"234.574219\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"309.378906\"/>\\n-     <use xlink:href=\"#DejaVuSans-43\" x=\"341.166016\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"410.990234\"/>\\n-     <use xlink:href=\"#DejaVuSans-69\" x=\"474.369141\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"502.152344\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"565.628906\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"629.251953\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"661.039062\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"724.662109\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"788.285156\"/>\\n-     <use xlink:href=\"#DejaVuSans-4b\" x=\"851.908203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"917.484375\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"949.271484\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"988.285156\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1051.908203\"/>\\n-     <use xlink:href=\"#DejaVuSans-65\" x=\"1083.695312\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"1145.21875\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1208.695312\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"1269.876953\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"1324.857422\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1388.236328\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1420.023438\"/>\\n-     <use xlink:href=\"#DejaVuSans-6e\" x=\"1481.205078\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1544.583984\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"1576.371094\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1639.994141\"/>\\n-     <use xlink:href=\"#DejaVuSans-54\" x=\"1671.78125\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"1732.865234\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1796.488281\"/>\\n-     <use xlink:href=\"#DejaVuSans-47\" x=\"1828.275391\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"1905.765625\"/>\\n-     <use xlink:href=\"#DejaVuSans-55\" x=\"1966.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"2039.261719\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"2091.361328\"/>\\n-     <use xlink:href=\"#DejaVuSans-44\" x=\"2123.148438\"/>\\n-     <use xlink:href=\"#DejaVuSans-44\" x=\"2200.150391\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"2277.152344\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"2337.455078\"/>\\n-    </g>\\n-   </g>\\n-  </g>\\n- </g>\\n- <defs>\\n-  <clipPath id=\"p42f2634aae\">\\n-   <rect x=\"90\" y=\"51.84\" width=\"558\" height=\"332.64\"/>\\n-  </clipPath>\\n- </defs>\\n-</svg>\\n',\n",
       " '@@ -1,1424 +0,0 @@\\n-<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\n-  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\\n- <metadata>\\n-  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n-   <cc:Work>\\n-    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\\n-    <dc:date>2023-11-30T13:15:28.212061</dc:date>\\n-    <dc:format>image/svg+xml</dc:format>\\n-    <dc:creator>\\n-     <cc:Agent>\\n-      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\\n-     </cc:Agent>\\n-    </dc:creator>\\n-   </cc:Work>\\n-  </rdf:RDF>\\n- </metadata>\\n- <defs>\\n-  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\\n- </defs>\\n- <g id=\"figure_1\">\\n-  <g id=\"patch_1\">\\n-   <path d=\"M 0 432 \\n-L 720 432 \\n-L 720 0 \\n-L 0 0 \\n-L 0 432 \\n-z\\n-\" style=\"fill: none\"/>\\n-  </g>\\n-  <g id=\"axes_1\">\\n-   <g id=\"patch_2\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-L 648 51.84 \\n-L 90 51.84 \\n-L 90 384.48 \\n-z\\n-\" style=\"fill: none\"/>\\n-   </g>\\n-   <g id=\"patch_3\">\\n-    <path d=\"M 90 369.36 \\n-L 621.428571 369.36 \\n-L 621.428571 305.696842 \\n-L 90 305.696842 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #fff4c6; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_4\">\\n-    <path d=\"M 90 289.781053 \\n-L 415.717933 289.781053 \\n-L 415.717933 226.117895 \\n-L 90 226.117895 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #f6f9f9; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_5\">\\n-    <path d=\"M 90 210.202105 \\n-L 287.004626 210.202105 \\n-L 287.004626 146.538947 \\n-L 90 146.538947 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #d9f4d7; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"patch_6\">\\n-    <path d=\"M 90 130.623158 \\n-L 159.87423 130.623158 \\n-L 159.87423 66.96 \\n-L 90 66.96 \\n-z\\n-\" clip-path=\"url(#p682a0e8bed)\" style=\"fill: #ede1ff; stroke: #000000; stroke-width: 0.5; stroke-linejoin: miter\"/>\\n-   </g>\\n-   <g id=\"matplotlib.axis_1\">\\n-    <g id=\"xtick_1\">\\n-     <g id=\"line2d_1\">\\n-      <defs>\\n-       <path id=\"m7e152711a6\" d=\"M 0 0 \\n-L 0 3.5 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"90\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_1\">\\n-      <!-- 0 -->\\n-      <g transform=\"translate(86.81875 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \\n-Q 1547 4250 1301 3770 \\n-Q 1056 3291 1056 2328 \\n-Q 1056 1369 1301 889 \\n-Q 1547 409 2034 409 \\n-Q 2525 409 2770 889 \\n-Q 3016 1369 3016 2328 \\n-Q 3016 3291 2770 3770 \\n-Q 2525 4250 2034 4250 \\n-z\\n-M 2034 4750 \\n-Q 2819 4750 3233 4129 \\n-Q 3647 3509 3647 2328 \\n-Q 3647 1150 3233 529 \\n-Q 2819 -91 2034 -91 \\n-Q 1250 -91 836 529 \\n-Q 422 1150 422 2328 \\n-Q 422 3509 836 4129 \\n-Q 1250 4750 2034 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-30\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_2\">\\n-     <g id=\"line2d_2\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"157.839059\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_2\">\\n-      <!-- 50 -->\\n-      <g transform=\"translate(151.476559 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \\n-L 3169 4666 \\n-L 3169 4134 \\n-L 1269 4134 \\n-L 1269 2991 \\n-Q 1406 3038 1543 3061 \\n-Q 1681 3084 1819 3084 \\n-Q 2600 3084 3056 2656 \\n-Q 3513 2228 3513 1497 \\n-Q 3513 744 3044 326 \\n-Q 2575 -91 1722 -91 \\n-Q 1428 -91 1123 -41 \\n-Q 819 9 494 109 \\n-L 494 744 \\n-Q 775 591 1075 516 \\n-Q 1375 441 1709 441 \\n-Q 2250 441 2565 725 \\n-Q 2881 1009 2881 1497 \\n-Q 2881 1984 2565 2268 \\n-Q 2250 2553 1709 2553 \\n-Q 1456 2553 1204 2497 \\n-Q 953 2441 691 2322 \\n-L 691 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-35\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_3\">\\n-     <g id=\"line2d_3\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"225.678117\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_3\">\\n-      <!-- 100 -->\\n-      <g transform=\"translate(216.134367 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-31\" d=\"M 794 531 \\n-L 1825 531 \\n-L 1825 4091 \\n-L 703 3866 \\n-L 703 4441 \\n-L 1819 4666 \\n-L 2450 4666 \\n-L 2450 531 \\n-L 3481 531 \\n-L 3481 0 \\n-L 794 0 \\n-L 794 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_4\">\\n-     <g id=\"line2d_4\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"293.517176\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_4\">\\n-      <!-- 150 -->\\n-      <g transform=\"translate(283.973426 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-31\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_5\">\\n-     <g id=\"line2d_5\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"361.356234\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_5\">\\n-      <!-- 200 -->\\n-      <g transform=\"translate(351.812484 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \\n-L 3431 531 \\n-L 3431 0 \\n-L 469 0 \\n-L 469 531 \\n-Q 828 903 1448 1529 \\n-Q 2069 2156 2228 2338 \\n-Q 2531 2678 2651 2914 \\n-Q 2772 3150 2772 3378 \\n-Q 2772 3750 2511 3984 \\n-Q 2250 4219 1831 4219 \\n-Q 1534 4219 1204 4116 \\n-Q 875 4013 500 3803 \\n-L 500 4441 \\n-Q 881 4594 1212 4672 \\n-Q 1544 4750 1819 4750 \\n-Q 2544 4750 2975 4387 \\n-Q 3406 4025 3406 3419 \\n-Q 3406 3131 3298 2873 \\n-Q 3191 2616 2906 2266 \\n-Q 2828 2175 2409 1742 \\n-Q 1991 1309 1228 531 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_6\">\\n-     <g id=\"line2d_6\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"429.195293\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_6\">\\n-      <!-- 250 -->\\n-      <g transform=\"translate(419.651543 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-32\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_7\">\\n-     <g id=\"line2d_7\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"497.034351\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_7\">\\n-      <!-- 300 -->\\n-      <g transform=\"translate(487.490601 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \\n-Q 3050 2419 3304 2112 \\n-Q 3559 1806 3559 1356 \\n-Q 3559 666 3084 287 \\n-Q 2609 -91 1734 -91 \\n-Q 1441 -91 1130 -33 \\n-Q 819 25 488 141 \\n-L 488 750 \\n-Q 750 597 1062 519 \\n-Q 1375 441 1716 441 \\n-Q 2309 441 2620 675 \\n-Q 2931 909 2931 1356 \\n-Q 2931 1769 2642 2001 \\n-Q 2353 2234 1838 2234 \\n-L 1294 2234 \\n-L 1294 2753 \\n-L 1863 2753 \\n-Q 2328 2753 2575 2939 \\n-Q 2822 3125 2822 3475 \\n-Q 2822 3834 2567 4026 \\n-Q 2313 4219 1838 4219 \\n-Q 1578 4219 1281 4162 \\n-Q 984 4106 628 3988 \\n-L 628 4550 \\n-Q 988 4650 1302 4700 \\n-Q 1616 4750 1894 4750 \\n-Q 2613 4750 3031 4423 \\n-Q 3450 4097 3450 3541 \\n-Q 3450 3153 3228 2886 \\n-Q 3006 2619 2597 2516 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-33\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_8\">\\n-     <g id=\"line2d_8\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"564.87341\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_8\">\\n-      <!-- 350 -->\\n-      <g transform=\"translate(555.32966 399.078438) scale(0.1 -0.1)\">\\n-       <use xlink:href=\"#DejaVuSans-33\"/>\\n-       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"xtick_9\">\\n-     <g id=\"line2d_9\">\\n-      <g>\\n-       <use xlink:href=\"#m7e152711a6\" x=\"632.712468\" y=\"384.48\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_9\">\\n-      <!-- 400 -->\\n-      <g transform=\"translate(623.168718 399.078438) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \\n-L 825 1625 \\n-L 2419 1625 \\n-L 2419 4116 \\n-z\\n-M 2253 4666 \\n-L 3047 4666 \\n-L 3047 1625 \\n-L 3713 1625 \\n-L 3713 1100 \\n-L 3047 1100 \\n-L 3047 0 \\n-L 2419 0 \\n-L 2419 1100 \\n-L 313 1100 \\n-L 313 1709 \\n-L 2253 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-34\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\\n-       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"text_10\">\\n-     <!-- Time taken (lower is better). -->\\n-     <g transform=\"translate(283.763438 414.27625) scale(0.12 -0.12)\">\\n-      <defs>\\n-       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \\n-L 3928 4666 \\n-L 3928 4134 \\n-L 2272 4134 \\n-L 2272 0 \\n-L 1638 0 \\n-L 1638 4134 \\n-L -19 4134 \\n-L -19 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \\n-L 1178 3500 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 3500 \\n-z\\n-M 603 4863 \\n-L 1178 4863 \\n-L 1178 4134 \\n-L 603 4134 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \\n-Q 3544 3216 3844 3400 \\n-Q 4144 3584 4550 3584 \\n-Q 5097 3584 5394 3201 \\n-Q 5691 2819 5691 2113 \\n-L 5691 0 \\n-L 5113 0 \\n-L 5113 2094 \\n-Q 5113 2597 4934 2840 \\n-Q 4756 3084 4391 3084 \\n-Q 3944 3084 3684 2787 \\n-Q 3425 2491 3425 1978 \\n-L 3425 0 \\n-L 2847 0 \\n-L 2847 2094 \\n-Q 2847 2600 2669 2842 \\n-Q 2491 3084 2119 3084 \\n-Q 1678 3084 1418 2786 \\n-Q 1159 2488 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1356 3278 1631 3431 \\n-Q 1906 3584 2284 3584 \\n-Q 2666 3584 2933 3390 \\n-Q 3200 3197 3328 2828 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \\n-L 3597 1613 \\n-L 953 1613 \\n-Q 991 1019 1311 708 \\n-Q 1631 397 2203 397 \\n-Q 2534 397 2845 478 \\n-Q 3156 559 3463 722 \\n-L 3463 178 \\n-Q 3153 47 2828 -22 \\n-Q 2503 -91 2169 -91 \\n-Q 1331 -91 842 396 \\n-Q 353 884 353 1716 \\n-Q 353 2575 817 3079 \\n-Q 1281 3584 2069 3584 \\n-Q 2775 3584 3186 3129 \\n-Q 3597 2675 3597 1894 \\n-z\\n-M 3022 2063 \\n-Q 3016 2534 2758 2815 \\n-Q 2500 3097 2075 3097 \\n-Q 1594 3097 1305 2825 \\n-Q 1016 2553 972 2059 \\n-L 3022 2063 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \\n-L 1172 3500 \\n-L 2356 3500 \\n-L 2356 3053 \\n-L 1172 3053 \\n-L 1172 1153 \\n-Q 1172 725 1289 603 \\n-Q 1406 481 1766 481 \\n-L 2356 481 \\n-L 2356 0 \\n-L 1766 0 \\n-Q 1100 0 847 248 \\n-Q 594 497 594 1153 \\n-L 594 3053 \\n-L 172 3053 \\n-L 172 3500 \\n-L 594 3500 \\n-L 594 4494 \\n-L 1172 4494 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \\n-Q 1497 1759 1228 1600 \\n-Q 959 1441 959 1056 \\n-Q 959 750 1161 570 \\n-Q 1363 391 1709 391 \\n-Q 2188 391 2477 730 \\n-Q 2766 1069 2766 1631 \\n-L 2766 1759 \\n-L 2194 1759 \\n-z\\n-M 3341 1997 \\n-L 3341 0 \\n-L 2766 0 \\n-L 2766 531 \\n-Q 2569 213 2275 61 \\n-Q 1981 -91 1556 -91 \\n-Q 1019 -91 701 211 \\n-Q 384 513 384 1019 \\n-Q 384 1609 779 1909 \\n-Q 1175 2209 1959 2209 \\n-L 2766 2209 \\n-L 2766 2266 \\n-Q 2766 2663 2505 2880 \\n-Q 2244 3097 1772 3097 \\n-Q 1472 3097 1187 3025 \\n-Q 903 2953 641 2809 \\n-L 641 3341 \\n-Q 956 3463 1253 3523 \\n-Q 1550 3584 1831 3584 \\n-Q 2591 3584 2966 3190 \\n-Q 3341 2797 3341 1997 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \\n-L 1159 4863 \\n-L 1159 1991 \\n-L 2875 3500 \\n-L 3609 3500 \\n-L 1753 1863 \\n-L 3688 0 \\n-L 2938 0 \\n-L 1159 1709 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \\n-Q 1566 4138 1362 3434 \\n-Q 1159 2731 1159 2009 \\n-Q 1159 1288 1364 580 \\n-Q 1569 -128 1984 -844 \\n-L 1484 -844 \\n-Q 1016 -109 783 600 \\n-Q 550 1309 550 2009 \\n-Q 550 2706 781 3412 \\n-Q 1013 4119 1484 4856 \\n-L 1984 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \\n-L 1178 4863 \\n-L 1178 0 \\n-L 603 0 \\n-L 603 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \\n-Q 1497 3097 1228 2736 \\n-Q 959 2375 959 1747 \\n-Q 959 1119 1226 758 \\n-Q 1494 397 1959 397 \\n-Q 2419 397 2687 759 \\n-Q 2956 1122 2956 1747 \\n-Q 2956 2369 2687 2733 \\n-Q 2419 3097 1959 3097 \\n-z\\n-M 1959 3584 \\n-Q 2709 3584 3137 3096 \\n-Q 3566 2609 3566 1747 \\n-Q 3566 888 3137 398 \\n-Q 2709 -91 1959 -91 \\n-Q 1206 -91 779 398 \\n-Q 353 888 353 1747 \\n-Q 353 2609 779 3096 \\n-Q 1206 3584 1959 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-77\" d=\"M 269 3500 \\n-L 844 3500 \\n-L 1563 769 \\n-L 2278 3500 \\n-L 2956 3500 \\n-L 3675 769 \\n-L 4391 3500 \\n-L 4966 3500 \\n-L 4050 0 \\n-L 3372 0 \\n-L 2619 2869 \\n-L 1863 0 \\n-L 1184 0 \\n-L 269 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \\n-Q 2534 3019 2420 3045 \\n-Q 2306 3072 2169 3072 \\n-Q 1681 3072 1420 2755 \\n-Q 1159 2438 1159 1844 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2956 \\n-Q 1341 3275 1631 3429 \\n-Q 1922 3584 2338 3584 \\n-Q 2397 3584 2469 3576 \\n-Q 2541 3569 2628 3553 \\n-L 2631 2963 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \\n-L 2834 2853 \\n-Q 2591 2978 2328 3040 \\n-Q 2066 3103 1784 3103 \\n-Q 1356 3103 1142 2972 \\n-Q 928 2841 928 2578 \\n-Q 928 2378 1081 2264 \\n-Q 1234 2150 1697 2047 \\n-L 1894 2003 \\n-Q 2506 1872 2764 1633 \\n-Q 3022 1394 3022 966 \\n-Q 3022 478 2636 193 \\n-Q 2250 -91 1575 -91 \\n-Q 1294 -91 989 -36 \\n-Q 684 19 347 128 \\n-L 347 722 \\n-Q 666 556 975 473 \\n-Q 1284 391 1588 391 \\n-Q 1994 391 2212 530 \\n-Q 2431 669 2431 922 \\n-Q 2431 1156 2273 1281 \\n-Q 2116 1406 1581 1522 \\n-L 1381 1569 \\n-Q 847 1681 609 1914 \\n-Q 372 2147 372 2553 \\n-Q 372 3047 722 3315 \\n-Q 1072 3584 1716 3584 \\n-Q 2034 3584 2315 3537 \\n-Q 2597 3491 2834 3397 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-M 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2969 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \\n-L 1013 4856 \\n-Q 1481 4119 1714 3412 \\n-Q 1947 2706 1947 2009 \\n-Q 1947 1309 1714 600 \\n-Q 1481 -109 1013 -844 \\n-L 513 -844 \\n-Q 928 -128 1133 580 \\n-Q 1338 1288 1338 2009 \\n-Q 1338 2731 1133 3434 \\n-Q 928 4138 513 4856 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       <path id=\"DejaVuSans-2e\" d=\"M 684 794 \\n-L 1344 794 \\n-L 1344 0 \\n-L 684 0 \\n-L 684 794 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      </defs>\\n-      <use xlink:href=\"#DejaVuSans-54\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"57.958984\"/>\\n-      <use xlink:href=\"#DejaVuSans-6d\" x=\"85.742188\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"183.154297\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"244.677734\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"276.464844\"/>\\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"315.673828\"/>\\n-      <use xlink:href=\"#DejaVuSans-6b\" x=\"376.953125\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.238281\"/>\\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"492.761719\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"556.140625\"/>\\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"587.927734\"/>\\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"626.941406\"/>\\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"654.724609\"/>\\n-      <use xlink:href=\"#DejaVuSans-77\" x=\"715.90625\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"797.693359\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"859.216797\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"900.330078\"/>\\n-      <use xlink:href=\"#DejaVuSans-69\" x=\"932.117188\"/>\\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"959.900391\"/>\\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"1012\"/>\\n-      <use xlink:href=\"#DejaVuSans-62\" x=\"1043.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1107.263672\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1168.787109\"/>\\n-      <use xlink:href=\"#DejaVuSans-74\" x=\"1207.996094\"/>\\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"1247.205078\"/>\\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"1308.728516\"/>\\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"1349.841797\"/>\\n-      <use xlink:href=\"#DejaVuSans-2e\" x=\"1388.855469\"/>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"matplotlib.axis_2\">\\n-    <g id=\"ytick_1\">\\n-     <g id=\"line2d_10\">\\n-      <defs>\\n-       <path id=\"m45234ecef3\" d=\"M 0 0 \\n-L -3.5 0 \\n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </defs>\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"337.528421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_11\">\\n-      <!-- Huggingface -->\\n-      <g transform=\"translate(19.68125 341.32764) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2753 \\n-L 3553 2753 \\n-L 3553 4666 \\n-L 4184 4666 \\n-L 4184 0 \\n-L 3553 0 \\n-L 3553 2222 \\n-L 1259 2222 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-75\" d=\"M 544 1381 \\n-L 544 3500 \\n-L 1119 3500 \\n-L 1119 1403 \\n-Q 1119 906 1312 657 \\n-Q 1506 409 1894 409 \\n-Q 2359 409 2629 706 \\n-Q 2900 1003 2900 1516 \\n-L 2900 3500 \\n-L 3475 3500 \\n-L 3475 0 \\n-L 2900 0 \\n-L 2900 538 \\n-Q 2691 219 2414 64 \\n-Q 2138 -91 1772 -91 \\n-Q 1169 -91 856 284 \\n-Q 544 659 544 1381 \\n-z\\n-M 1991 3584 \\n-L 1991 3584 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \\n-Q 2906 2416 2648 2759 \\n-Q 2391 3103 1925 3103 \\n-Q 1463 3103 1205 2759 \\n-Q 947 2416 947 1791 \\n-Q 947 1169 1205 825 \\n-Q 1463 481 1925 481 \\n-Q 2391 481 2648 825 \\n-Q 2906 1169 2906 1791 \\n-z\\n-M 3481 434 \\n-Q 3481 -459 3084 -895 \\n-Q 2688 -1331 1869 -1331 \\n-Q 1566 -1331 1297 -1286 \\n-Q 1028 -1241 775 -1147 \\n-L 775 -588 \\n-Q 1028 -725 1275 -790 \\n-Q 1522 -856 1778 -856 \\n-Q 2344 -856 2625 -561 \\n-Q 2906 -266 2906 331 \\n-L 2906 616 \\n-Q 2728 306 2450 153 \\n-Q 2172 0 1784 0 \\n-Q 1141 0 747 490 \\n-Q 353 981 353 1791 \\n-Q 353 2603 747 3093 \\n-Q 1141 3584 1784 3584 \\n-Q 2172 3584 2450 3431 \\n-Q 2728 3278 2906 2969 \\n-L 2906 3500 \\n-L 3481 3500 \\n-L 3481 434 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \\n-L 2375 4384 \\n-L 1825 4384 \\n-Q 1516 4384 1395 4259 \\n-Q 1275 4134 1275 3809 \\n-L 1275 3500 \\n-L 2222 3500 \\n-L 2222 3053 \\n-L 1275 3053 \\n-L 1275 0 \\n-L 697 0 \\n-L 697 3053 \\n-L 147 3053 \\n-L 147 3500 \\n-L 697 3500 \\n-L 697 3744 \\n-Q 697 4328 969 4595 \\n-Q 1241 4863 1831 4863 \\n-L 2375 4863 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \\n-L 3122 2828 \\n-Q 2878 2963 2633 3030 \\n-Q 2388 3097 2138 3097 \\n-Q 1578 3097 1268 2742 \\n-Q 959 2388 959 1747 \\n-Q 959 1106 1268 751 \\n-Q 1578 397 2138 397 \\n-Q 2388 397 2633 464 \\n-Q 2878 531 3122 666 \\n-L 3122 134 \\n-Q 2881 22 2623 -34 \\n-Q 2366 -91 2075 -91 \\n-Q 1284 -91 818 406 \\n-Q 353 903 353 1747 \\n-Q 353 2603 823 3093 \\n-Q 1294 3584 2113 3584 \\n-Q 2378 3584 2631 3529 \\n-Q 2884 3475 3122 3366 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-48\"/>\\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"138.574219\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"202.050781\"/>\\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"265.527344\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.310547\"/>\\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"356.689453\"/>\\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"420.166016\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"455.371094\"/>\\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"516.650391\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"571.630859\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_2\">\\n-     <g id=\"line2d_11\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"257.949474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_12\">\\n-      <!-- Unsloth Open -->\\n-      <g transform=\"translate(15.090625 261.748692) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-55\" d=\"M 556 4666 \\n-L 1191 4666 \\n-L 1191 1831 \\n-Q 1191 1081 1462 751 \\n-Q 1734 422 2344 422 \\n-Q 2950 422 3222 751 \\n-Q 3494 1081 3494 1831 \\n-L 3494 4666 \\n-L 4128 4666 \\n-L 4128 1753 \\n-Q 4128 841 3676 375 \\n-Q 3225 -91 2344 -91 \\n-Q 1459 -91 1007 375 \\n-Q 556 841 556 1753 \\n-L 556 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \\n-L 3513 0 \\n-L 2938 0 \\n-L 2938 2094 \\n-Q 2938 2591 2744 2837 \\n-Q 2550 3084 2163 3084 \\n-Q 1697 3084 1428 2787 \\n-Q 1159 2491 1159 1978 \\n-L 1159 0 \\n-L 581 0 \\n-L 581 4863 \\n-L 1159 4863 \\n-L 1159 2956 \\n-Q 1366 3272 1645 3428 \\n-Q 1925 3584 2291 3584 \\n-Q 2894 3584 3203 3211 \\n-Q 3513 2838 3513 2113 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-4f\" d=\"M 2522 4238 \\n-Q 1834 4238 1429 3725 \\n-Q 1025 3213 1025 2328 \\n-Q 1025 1447 1429 934 \\n-Q 1834 422 2522 422 \\n-Q 3209 422 3611 934 \\n-Q 4013 1447 4013 2328 \\n-Q 4013 3213 3611 3725 \\n-Q 3209 4238 2522 4238 \\n-z\\n-M 2522 4750 \\n-Q 3503 4750 4090 4092 \\n-Q 4678 3434 4678 2328 \\n-Q 4678 1225 4090 567 \\n-Q 3503 -91 2522 -91 \\n-Q 1538 -91 948 565 \\n-Q 359 1222 359 2328 \\n-Q 359 3434 948 4092 \\n-Q 1538 4750 2522 4750 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \\n-L 1159 -1331 \\n-L 581 -1331 \\n-L 581 3500 \\n-L 1159 3500 \\n-L 1159 2969 \\n-Q 1341 3281 1617 3432 \\n-Q 1894 3584 2278 3584 \\n-Q 2916 3584 3314 3078 \\n-Q 3713 2572 3713 1747 \\n-Q 3713 922 3314 415 \\n-Q 2916 -91 2278 -91 \\n-Q 1894 -91 1617 61 \\n-Q 1341 213 1159 525 \\n-z\\n-M 3116 1747 \\n-Q 3116 2381 2855 2742 \\n-Q 2594 3103 2138 3103 \\n-Q 1681 3103 1420 2742 \\n-Q 1159 2381 1159 1747 \\n-Q 1159 1113 1420 752 \\n-Q 1681 391 2138 391 \\n-Q 2594 391 2855 752 \\n-Q 3116 1113 3116 1747 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4f\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"490.722656\"/>\\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"554.199219\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"615.722656\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_3\">\\n-     <g id=\"line2d_12\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"178.370526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_13\">\\n-      <!-- Unsloth Pro -->\\n-      <g transform=\"translate(25.942187 182.169745) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \\n-L 1259 2394 \\n-L 2053 2394 \\n-Q 2494 2394 2734 2622 \\n-Q 2975 2850 2975 3272 \\n-Q 2975 3691 2734 3919 \\n-Q 2494 4147 2053 4147 \\n-L 1259 4147 \\n-z\\n-M 628 4666 \\n-L 2053 4666 \\n-Q 2838 4666 3239 4311 \\n-Q 3641 3956 3641 3272 \\n-Q 3641 2581 3239 2228 \\n-Q 2838 1875 2053 1875 \\n-L 1259 1875 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-50\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"470.564453\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"509.427734\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-    <g id=\"ytick_4\">\\n-     <g id=\"line2d_13\">\\n-      <g>\\n-       <use xlink:href=\"#m45234ecef3\" x=\"90\" y=\"98.791579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\\n-      </g>\\n-     </g>\\n-     <g id=\"text_14\">\\n-      <!-- Unsloth Max -->\\n-      <g transform=\"translate(21.126562 102.590798) scale(0.1 -0.1)\">\\n-       <defs>\\n-        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \\n-L 1569 4666 \\n-L 2759 1491 \\n-L 3956 4666 \\n-L 4897 4666 \\n-L 4897 0 \\n-L 4281 0 \\n-L 4281 4097 \\n-L 3078 897 \\n-L 2444 897 \\n-L 1241 4097 \\n-L 1241 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-        <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \\n-L 2247 1797 \\n-L 3578 0 \\n-L 2900 0 \\n-L 1881 1375 \\n-L 863 0 \\n-L 184 0 \\n-L 1544 1831 \\n-L 300 3500 \\n-L 978 3500 \\n-L 1906 2253 \\n-L 2834 3500 \\n-L 3513 3500 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-       </defs>\\n-       <use xlink:href=\"#DejaVuSans-55\"/>\\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"73.193359\"/>\\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"136.572266\"/>\\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"188.671875\"/>\\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"216.455078\"/>\\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"277.636719\"/>\\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"316.845703\"/>\\n-       <use xlink:href=\"#DejaVuSans-20\" x=\"380.224609\"/>\\n-       <use xlink:href=\"#DejaVuSans-4d\" x=\"412.011719\"/>\\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"498.291016\"/>\\n-       <use xlink:href=\"#DejaVuSans-78\" x=\"559.570312\"/>\\n-      </g>\\n-     </g>\\n-    </g>\\n-   </g>\\n-   <g id=\"patch_7\">\\n-    <path d=\"M 90 384.48 \\n-L 90 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_8\">\\n-    <path d=\"M 648 384.48 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_9\">\\n-    <path d=\"M 90 384.48 \\n-L 648 384.48 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"patch_10\">\\n-    <path d=\"M 90 51.84 \\n-L 648 51.84 \\n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\\n-   </g>\\n-   <g id=\"text_15\">\\n-    <!-- 391.68 hrs (1.0X) -->\\n-    <g transform=\"translate(512.886078 340.839671) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-39\" d=\"M 703 97 \\n-L 703 672 \\n-Q 941 559 1184 500 \\n-Q 1428 441 1663 441 \\n-Q 2288 441 2617 861 \\n-Q 2947 1281 2994 2138 \\n-Q 2813 1869 2534 1725 \\n-Q 2256 1581 1919 1581 \\n-Q 1219 1581 811 2004 \\n-Q 403 2428 403 3163 \\n-Q 403 3881 828 4315 \\n-Q 1253 4750 1959 4750 \\n-Q 2769 4750 3195 4129 \\n-Q 3622 3509 3622 2328 \\n-Q 3622 1225 3098 567 \\n-Q 2575 -91 1691 -91 \\n-Q 1453 -91 1209 -44 \\n-Q 966 3 703 97 \\n-z\\n-M 1959 2075 \\n-Q 2384 2075 2632 2365 \\n-Q 2881 2656 2881 3163 \\n-Q 2881 3666 2632 3958 \\n-Q 2384 4250 1959 4250 \\n-Q 1534 4250 1286 3958 \\n-Q 1038 3666 1038 3163 \\n-Q 1038 2656 1286 2365 \\n-Q 1534 2075 1959 2075 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \\n-Q 1688 2584 1439 2293 \\n-Q 1191 2003 1191 1497 \\n-Q 1191 994 1439 701 \\n-Q 1688 409 2113 409 \\n-Q 2538 409 2786 701 \\n-Q 3034 994 3034 1497 \\n-Q 3034 2003 2786 2293 \\n-Q 2538 2584 2113 2584 \\n-z\\n-M 3366 4563 \\n-L 3366 3988 \\n-Q 3128 4100 2886 4159 \\n-Q 2644 4219 2406 4219 \\n-Q 1781 4219 1451 3797 \\n-Q 1122 3375 1075 2522 \\n-Q 1259 2794 1537 2939 \\n-Q 1816 3084 2150 3084 \\n-Q 2853 3084 3261 2657 \\n-Q 3669 2231 3669 1497 \\n-Q 3669 778 3244 343 \\n-Q 2819 -91 2113 -91 \\n-Q 1303 -91 875 529 \\n-Q 447 1150 447 2328 \\n-Q 447 3434 972 4092 \\n-Q 1497 4750 2381 4750 \\n-Q 2619 4750 2861 4703 \\n-Q 3103 4656 3366 4563 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \\n-Q 1584 2216 1326 1975 \\n-Q 1069 1734 1069 1313 \\n-Q 1069 891 1326 650 \\n-Q 1584 409 2034 409 \\n-Q 2484 409 2743 651 \\n-Q 3003 894 3003 1313 \\n-Q 3003 1734 2745 1975 \\n-Q 2488 2216 2034 2216 \\n-z\\n-M 1403 2484 \\n-Q 997 2584 770 2862 \\n-Q 544 3141 544 3541 \\n-Q 544 4100 942 4425 \\n-Q 1341 4750 2034 4750 \\n-Q 2731 4750 3128 4425 \\n-Q 3525 4100 3525 3541 \\n-Q 3525 3141 3298 2862 \\n-Q 3072 2584 2669 2484 \\n-Q 3125 2378 3379 2068 \\n-Q 3634 1759 3634 1313 \\n-Q 3634 634 3220 271 \\n-Q 2806 -91 2034 -91 \\n-Q 1263 -91 848 271 \\n-Q 434 634 434 1313 \\n-Q 434 1759 690 2068 \\n-Q 947 2378 1403 2484 \\n-z\\n-M 1172 3481 \\n-Q 1172 3119 1398 2916 \\n-Q 1625 2713 2034 2713 \\n-Q 2441 2713 2670 2916 \\n-Q 2900 3119 2900 3481 \\n-Q 2900 3844 2670 4047 \\n-Q 2441 4250 2034 4250 \\n-Q 1625 4250 1398 4047 \\n-Q 1172 3844 1172 3481 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-58\" d=\"M 403 4666 \\n-L 1081 4666 \\n-L 2241 2931 \\n-L 3406 4666 \\n-L 4084 4666 \\n-L 2584 2425 \\n-L 4184 0 \\n-L 3506 0 \\n-L 2194 1984 \\n-L 872 0 \\n-L 191 0 \\n-L 1856 2491 \\n-L 403 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-33\"/>\\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_16\">\\n-    <!-- 240.07 hrs (1.6X) -->\\n-    <g transform=\"translate(417.074714 261.260724) scale(0.12 -0.12)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \\n-L 3525 4666 \\n-L 3525 4397 \\n-L 1831 0 \\n-L 1172 0 \\n-L 2766 4134 \\n-L 525 4134 \\n-L 525 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-32\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_17\">\\n-    <!-- 145.20 hrs (2.7X) -->\\n-    <g transform=\"translate(288.361407 181.681776) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-31\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"190.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"349.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"381.689453\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"445.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"486.181641\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"538.28125\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"570.068359\"/>\\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"672.705078\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"768.115234\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"836.621094\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_18\">\\n-    <!-- 51.50 hrs (7.6X) -->\\n-    <g transform=\"translate(161.231011 102.102829) scale(0.12 -0.12)\">\\n-     <use xlink:href=\"#DejaVuSans-35\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"286.279297\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"318.066406\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"381.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-73\" x=\"422.558594\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"474.658203\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"506.445312\"/>\\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"545.458984\"/>\\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"609.082031\"/>\\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"640.869141\"/>\\n-     <use xlink:href=\"#DejaVuSans-58\" x=\"704.492188\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"772.998047\"/>\\n-    </g>\\n-   </g>\\n-   <g id=\"text_19\">\\n-    <!-- SlimOrca 518K (1 epoch on 1 T4 GPU) -->\\n-    <g transform=\"translate(217.05625 45.84) scale(0.16 -0.16)\">\\n-     <defs>\\n-      <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \\n-L 3425 3897 \\n-Q 3066 4069 2747 4153 \\n-Q 2428 4238 2131 4238 \\n-Q 1616 4238 1336 4038 \\n-Q 1056 3838 1056 3469 \\n-Q 1056 3159 1242 3001 \\n-Q 1428 2844 1947 2747 \\n-L 2328 2669 \\n-Q 3034 2534 3370 2195 \\n-Q 3706 1856 3706 1288 \\n-Q 3706 609 3251 259 \\n-Q 2797 -91 1919 -91 \\n-Q 1588 -91 1214 -16 \\n-Q 841 59 441 206 \\n-L 441 856 \\n-Q 825 641 1194 531 \\n-Q 1563 422 1919 422 \\n-Q 2459 422 2753 634 \\n-Q 3047 847 3047 1241 \\n-Q 3047 1584 2836 1778 \\n-Q 2625 1972 2144 2069 \\n-L 1759 2144 \\n-Q 1053 2284 737 2584 \\n-Q 422 2884 422 3419 \\n-Q 422 4038 858 4394 \\n-Q 1294 4750 2059 4750 \\n-Q 2388 4750 2728 4690 \\n-Q 3069 4631 3425 4513 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \\n-L 1259 4666 \\n-L 1259 2694 \\n-L 3353 4666 \\n-L 4166 4666 \\n-L 1850 2491 \\n-L 4331 0 \\n-L 3500 0 \\n-L 1259 2247 \\n-L 1259 0 \\n-L 628 0 \\n-L 628 4666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-      <path id=\"DejaVuSans-47\" d=\"M 3809 666 \\n-L 3809 1919 \\n-L 2778 1919 \\n-L 2778 2438 \\n-L 4434 2438 \\n-L 4434 434 \\n-Q 4069 175 3628 42 \\n-Q 3188 -91 2688 -91 \\n-Q 1594 -91 976 548 \\n-Q 359 1188 359 2328 \\n-Q 359 3472 976 4111 \\n-Q 1594 4750 2688 4750 \\n-Q 3144 4750 3555 4637 \\n-Q 3966 4525 4313 4306 \\n-L 4313 3634 \\n-Q 3963 3931 3569 4081 \\n-Q 3175 4231 2741 4231 \\n-Q 1884 4231 1454 3753 \\n-Q 1025 3275 1025 2328 \\n-Q 1025 1384 1454 906 \\n-Q 1884 428 2741 428 \\n-Q 3075 428 3337 486 \\n-Q 3600 544 3809 666 \\n-z\\n-\" transform=\"scale(0.015625)\"/>\\n-     </defs>\\n-     <use xlink:href=\"#DejaVuSans-53\"/>\\n-     <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\\n-     <use xlink:href=\"#DejaVuSans-69\" x=\"91.259766\"/>\\n-     <use xlink:href=\"#DejaVuSans-6d\" x=\"119.042969\"/>\\n-     <use xlink:href=\"#DejaVuSans-4f\" x=\"216.455078\"/>\\n-     <use xlink:href=\"#DejaVuSans-72\" x=\"295.166016\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"334.029297\"/>\\n-     <use xlink:href=\"#DejaVuSans-61\" x=\"389.009766\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"450.289062\"/>\\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"482.076172\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"545.699219\"/>\\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"609.322266\"/>\\n-     <use xlink:href=\"#DejaVuSans-4b\" x=\"672.945312\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"738.521484\"/>\\n-     <use xlink:href=\"#DejaVuSans-28\" x=\"770.308594\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"809.322266\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"872.945312\"/>\\n-     <use xlink:href=\"#DejaVuSans-65\" x=\"904.732422\"/>\\n-     <use xlink:href=\"#DejaVuSans-70\" x=\"966.255859\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1029.732422\"/>\\n-     <use xlink:href=\"#DejaVuSans-63\" x=\"1090.914062\"/>\\n-     <use xlink:href=\"#DejaVuSans-68\" x=\"1145.894531\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1209.273438\"/>\\n-     <use xlink:href=\"#DejaVuSans-6f\" x=\"1241.060547\"/>\\n-     <use xlink:href=\"#DejaVuSans-6e\" x=\"1302.242188\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1365.621094\"/>\\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"1397.408203\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1461.03125\"/>\\n-     <use xlink:href=\"#DejaVuSans-54\" x=\"1492.818359\"/>\\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"1553.902344\"/>\\n-     <use xlink:href=\"#DejaVuSans-20\" x=\"1617.525391\"/>\\n-     <use xlink:href=\"#DejaVuSans-47\" x=\"1649.3125\"/>\\n-     <use xlink:href=\"#DejaVuSans-50\" x=\"1726.802734\"/>\\n-     <use xlink:href=\"#DejaVuSans-55\" x=\"1787.105469\"/>\\n-     <use xlink:href=\"#DejaVuSans-29\" x=\"1860.298828\"/>\\n-    </g>\\n-   </g>\\n-  </g>\\n- </g>\\n- <defs>\\n-  <clipPath id=\"p682a0e8bed\">\\n-   <rect x=\"90\" y=\"51.84\" width=\"558\" height=\"332.64\"/>\\n-  </clipPath>\\n- </defs>\\n-</svg>\\n',\n",
       " 'Binary files /dev/null and b/images/try live demo green.png differ\\n',\n",
       " '@@ -33,7 +33,7 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-\\t\"transformers\",\\n+    \"transformers\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n@@ -70,4 +70,4 @@ colab = [\\n [project.urls]\\n homepage = \"http://www.unsloth.ai\"\\n documentation = \"https://github.com/unslothai/unsloth\"\\n-repository = \"https://github.com/unslothai/unsloth\"\\n\\\\ No newline at end of file\\n+repository = \"https://github.com/unslothai/unsloth\"\\n',\n",
       " '@@ -11,7 +11,7 @@\\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n-__version__ = \"2023.11\"\\n+__version__ = \"2023.12\"\\n import os\\n import warnings\\n import importlib\\n@@ -35,7 +35,7 @@ if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n         )\\n         os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n else:\\n-    warnings.warn(\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is not set. We shall set it ourselves.\")\\n+    # warnings.warn(\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is not set. We shall set it ourselves.\")\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n',\n",
       " '@@ -43,7 +43,6 @@ def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n     mask = col_offsets < n_cols\\n \\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     label_idx = tl.load(labels_ptr).to(tl.int32)\\n     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(\"inf\")).to(tl.float32)\\n     max_logits = tl.max(logits, 0)\\n@@ -88,7 +87,6 @@ def _cross_entropy_backward(logits_ptr, logits_row_stride,\\n     col_offsets = tl.arange(0, BLOCK_SIZE)\\n     mask = col_offsets < n_cols\\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\\n \\n     if label_idx != -100:\\n',\n",
       " '@@ -35,7 +35,6 @@ def _rope_embedding(\\n     mask = col_offsets < half_head_dim\\n \\n     # TODO: Fixup int32 locations to int64\\n-    # https://github.com/Dao-AILab/flash-attention/commit/c79de85ffa0d19b80fa468f90c5086e837499d72\\n     rot_position = row_position % seqlen\\n \\n     Q   += row_position*  Q_row_stride + head_position*head_dim\\n@@ -48,8 +47,6 @@ def _rope_embedding(\\n \\n     Q2   = tl.load(Q   + half_head_dim*1 + col_offsets, mask = mask, other = 0)\\n     # RoPE repeats sin and cos so 128 = [64, 64].\\n-    # sin2 = tl.load(sin + half_head_dim*1, mask = mask, other = 0)\\n-    # cos2 = tl.load(cos + half_head_dim*1, mask = mask, other = 0)\\n \\n     if BACKWARD_PASS:\\n         \"\"\"\\n@@ -62,11 +59,8 @@ def _rope_embedding(\\n             where R.T is again the same  [ 0, -I]\\n             but the minus is transposed. [ I,  0]\\n         \"\"\"\\n-        # sin1, sin2 = -sin1, -sin2\\n         sin1 = -sin1\\n-\\n-    # tl.store(Q + half_head_dim*0, Q1*cos1 - Q2*sin1, mask = mask)\\n-    # tl.store(Q + half_head_dim*1, Q2*cos2 + Q1*sin2, mask = mask)\\n+    \\n     # RoPE repeats sin and cos so 128 = [64, 64].\\n     tl.store(Q + half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)\\n     tl.store(Q + half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)\\n',\n",
       " '@@ -13,12 +13,12 @@\\n # limitations under the License.\\n \\n import triton\\n-MAX_FUSED_SIZE = 65535 # 2**16 - 1\\n+MAX_FUSED_SIZE = 65536 # 2**16 Solves https://github.com/unslothai/unsloth/issues/7\\n next_power_of_2 = triton.next_power_of_2\\n \\n def calculate_settings(n):\\n     BLOCK_SIZE = next_power_of_2(n)\\n-    # CUDA only supports 65535 - 2^16-1 threads per block\\n+    # CUDA only supports 65536 - 2^16 threads per block\\n     if BLOCK_SIZE > MAX_FUSED_SIZE:\\n         raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\\\\n                            f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\\n',\n",
       " '@@ -16,14 +16,15 @@ import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n from transformers.models.llama.modeling_llama import (\\n-    # apply_rotary_pos_emb,\\n-    # repeat_kv,\\n-    # _prepare_4d_causal_attention_mask,\\n+    _prepare_4d_causal_attention_mask,\\n     logger,\\n     BaseModelOutputWithPast,\\n     CausalLMOutputWithPast,\\n )\\n from ..kernels import *\\n+from ._utils import (\\n+    prepare_model_for_kbit_training,\\n+)\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -37,7 +38,6 @@ else:\\n     # Tri Dao\\'s benchmark shows xformers is faster for now.\\n     HAS_FLASH_ATTENTION = False\\n pass\\n-\\n import xformers.ops.fmha as xformers\\n xformers_attention = xformers.memory_efficient_attention\\n \\n@@ -55,12 +55,9 @@ import bitsandbytes as bnb\\n import numpy as np\\n import types\\n \\n-from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n-from ._utils import (\\n-    prepare_model_for_kbit_training,\\n-)\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -92,10 +89,6 @@ def LlamaAttention_fast_forward(\\n ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\\n     \\n     bsz, q_len, _ = hidden_states.size()\\n-\\n-    # Q = self.q_proj(hidden_states)\\n-    # K = self.k_proj(hidden_states)\\n-    # V = self.v_proj(hidden_states)\\n     Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n     n_heads    = self.num_heads\\n@@ -112,8 +105,6 @@ def LlamaAttention_fast_forward(\\n     if past_key_value is not None:\\n         kv_seq_len += past_key_value[0].shape[-2]\\n \\n-    # cos, sin = self.rotary_emb(V, seq_len = kv_seq_len)\\n-    # Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)\\n     if position_ids is None:\\n         cos = self.rotary_emb.cos_cached\\n         sin = self.rotary_emb.sin_cached\\n@@ -130,10 +121,9 @@ def LlamaAttention_fast_forward(\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n-    # no_attention_mask = attention_mask is None\\n-    # Ignore attention_mask\\n-\\n-    if (not HAS_FLASH_ATTENTION): #and no_attention_mask:\\n+    # Xformers doesnt support backward pass for GQA (yet)\\n+    # TEMP fix\\n+    if (n_groups == 1) and (not HAS_FLASH_ATTENTION):\\n         # Xformers memory efficient attention\\n         # Also has Flash Attention v2 dispatching\\n         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)\\n@@ -143,18 +133,17 @@ def LlamaAttention_fast_forward(\\n \\n         # Grouped query attention\\n         if n_groups != 1:\\n-            Q = Q.reshape(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n-\\n-            K = K.reshape(bsz, q_len, n_groups,          1, head_dim)\\n-            V = V.reshape(bsz, q_len, n_groups,          1, head_dim)\\n-            K = K .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n-            V = V .expand(bsz, q_len, n_groups, n_kv_heads, head_dim)\\n+            Q = Q.reshape(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            K = K.reshape(bsz, q_len, n_kv_heads,        1, head_dim)\\n+            V = V.reshape(bsz, q_len, n_kv_heads,        1, head_dim)\\n+            K = K .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            V = V .expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n         A = A.view(bsz, q_len, n_heads, head_dim)\\n \\n-    elif HAS_FLASH_ATTENTION:# and no_attention_mask:\\n+    elif HAS_FLASH_ATTENTION:\\n         # Flash Attention\\n         # (batch_size, n_heads, seq_len, head_dim) -> (batch_size, seq_len, n_heads, head_dim)\\n         Q = Q.transpose(1, 2)\\n@@ -163,37 +152,22 @@ def LlamaAttention_fast_forward(\\n \\n         # Flash Attention v2 auto supports grouped query attention\\n         A = flash_attn_func(Q, K, V, causal = True)\\n-\\n     else:\\n-        # Uses Pytorch\\'s scaled dot product attention\\n-        if attention_mask is not None:\\n-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\\n-                raise ValueError(\\n-                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\\n-                )\\n-        pass\\n-\\n         # Grouped query attention\\n-        # K = repeat_kv(K, n_groups)\\n-        # V = repeat_kv(V, n_groups)\\n         if n_groups != 1:\\n             K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n             V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n             K = K.reshape(bsz, n_heads, q_len, head_dim)\\n             V = V.reshape(bsz, n_heads, q_len, head_dim)\\n         pass\\n-\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n-        A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = attention_mask is None)\\n+        A = scaled_dot_product_attention(Q, K, V, attn_mask = None, is_causal = True)\\n         # Go back to (batch_size, seq_len, n_heads, head_dim)\\n         A = A.transpose(1, 2)\\n     pass\\n     attn_output = A.reshape(bsz, q_len, self.hidden_size)\\n-\\n-    # attn_output = self.o_proj(attn_output)\\n     attn_output = self.apply_o(self, attn_output)\\n-\\n     attn_weights = None\\n     return attn_output, attn_weights, past_key_value\\n pass\\n@@ -227,7 +201,6 @@ def LlamaDecoderLayer_fast_forward(\\n     \"\"\"\\n     residual = hidden_states\\n \\n-    # hidden_states = self.input_layernorm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n \\n     # Self Attention\\n@@ -245,7 +218,6 @@ def LlamaDecoderLayer_fast_forward(\\n \\n     # Fully Connected\\n     residual = hidden_states\\n-    # hidden_states = self.post_attention_layernorm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n     hidden_states = self.mlp(hidden_states)\\n     hidden_states = residual + hidden_states\\n@@ -308,7 +280,7 @@ def LlamaModel_fast_forward(\\n     if (past_key_values_length != 0):\\n         position_ids = torch.arange(\\n             past_key_values_length, seq_length + past_key_values_length,\\n-            dtype  = torch.int32,#dtype=torch.long,\\n+            dtype  = torch.int32,\\n             device = \"cuda\",\\n         )\\n         position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\\n@@ -326,11 +298,7 @@ def LlamaModel_fast_forward(\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n     # Ignore attention_mask\\n-    if True:\\n-    # if attention_mask is None:\\n-        # attention_mask = torch.ones(\\n-        #     (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\\n-        # )\\n+    if attention_mask is None:\\n         padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n@@ -339,7 +307,7 @@ def LlamaModel_fast_forward(\\n             padding_mask = None\\n \\n         attention_mask = _prepare_4d_causal_attention_mask(\\n-            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\\n+            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length,\\n         )\\n     pass\\n \\n@@ -403,7 +371,6 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    # hidden_states = self.norm(hidden_states)\\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n \\n     # add hidden states from the last decoder layer\\n@@ -466,19 +433,13 @@ def LlamaForCausalLM_fast_forward(\\n \\n     loss = None\\n     if labels is not None:\\n-        # logits = logits.float()\\n-        # shift_logits = logits[..., :-1, :].contiguous()\\n-        # shift_labels = labels[..., 1:].contiguous()\\n-        # shift_labels = shift_labels.view(-1)\\n-        # shift_logits = shift_logits.view(-1, self.config.vocab_size)\\n         shift_logits = logits\\n+        if not hasattr(self, \"extra_ignored_labels\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/10\\n+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+        pass\\n+        \\n         shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-\\n-        # loss_fct = torch.nn.CrossEntropyLoss(\\n-        #     ignore_index = self.ignore_index,\\n-        #     label_smoothing = self.label_smoothing,\\n-        # )\\n-        # loss = loss_fct(shift_logits, shift_labels)\\n         loss = fast_cross_entropy_loss(\\n             logits = shift_logits,\\n             labels = shift_labels,\\n@@ -547,13 +508,14 @@ class FastLlamaModel:\\n         load_in_4bit = True,\\n         token = None,\\n         device_map = \"sequential\",\\n+        rope_scaling = None,\\n     ):\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n \\n         statistics = \\\\\\n-            \"==((====))==  Unsloth: Fast Llama patching release 23.11\\\\n\"\\\\\\n+            \"==((====))==  Unsloth: Fast Llama patching release 2023.12\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n            f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n            f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n@@ -570,9 +532,20 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # [TODO]: Determine RoPE scaling\\n-        # https://github.com/huggingface/transformers/pull/24653\\n-        assert(max_seq_length <= 4096)\\n+        # RoPE scaling\\n+        model_max_seq_length = \\\\\\n+            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+\\n+        if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+            rope_scaling = max_seq_length / model_max_seq_length\\n+            logger.warning_once(\\n+                f\"Unsloth: {model_name} can only handle sequence lengths of of most \"\\\\\\n+                f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n+                f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n+                f\"{max_seq_length}!\"\\n+            )\\n+            rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+        pass\\n \\n         bnb_config = None\\n         if load_in_4bit:\\n@@ -589,6 +562,7 @@ class FastLlamaModel:\\n             torch_dtype = dtype,\\n             quantization_config = bnb_config,\\n             token = token,\\n+            rope_scaling = rope_scaling,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n@@ -596,9 +570,22 @@ class FastLlamaModel:\\n             padding_side = \"right\",\\n             token = token,\\n         )\\n-        tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token});\\n-        tokenizer.pad_token = tokenizer.unk_token\\n-        config = model.config.update({\"pad_token_id\" : tokenizer.unk_token_id});\\n+\\n+        if not hasattr(tokenizer, \"pad_token\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/5\\n+            if hasattr(tokenizer, \"unk_token\"):\\n+                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n+                tokenizer.pad_token = tokenizer.unk_token\\n+            else:\\n+                logger.warning_one(\\n+                    f\"{model_name} does not have a padding or unknown token!\\\\n\"\\\\\\n+                    f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+                )\\n+                assert(hasattr(tokenizer, \"eos_token\"))\\n+                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n+                tokenizer.pad_token = tokenizer.eos_token\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+        pass\\n \\n         model = FastLlamaModel.post_patch(model)\\n \\n@@ -607,6 +594,8 @@ class FastLlamaModel:\\n             layer.self_attn.apply_qkv = original_apply_qkv\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n+\\n+        model.max_seq_length = max_seq_length\\n         return model, tokenizer\\n     pass\\n \\n@@ -668,6 +657,8 @@ class FastLlamaModel:\\n         random_state = 3407,\\n         max_seq_length = 2048,\\n     ):\\n+        assert(max_seq_length <= model.max_seq_length)\\n+\\n         if lora_dropout != 0:\\n             raise TypeError(\"Unsloth: Fast Llama patching only works with dropout = 0.\")\\n         if bias != \"none\":\\n@@ -727,8 +718,14 @@ class FastLlamaModel:\\n         pass\\n \\n         # Patch cross entropy loss labels\\n-        model.model.extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n-        \\n+        # Fixes https://github.com/unslothai/unsloth/issues/10\\n+        extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n+        model.model.extra_ignored_labels = extra_ignored_labels\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model.max_seq_length = max_seq_length\\n+            internal_model = internal_model.model\\n+        pass\\n         return model\\n     pass\\n pass\\n',\n",
       " '@@ -19,6 +19,14 @@ import warnings\\n import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n+__version__ = \"2023.12\"\\n+__all__ = [\\n+    \"prepare_model_for_kbit_training\",\\n+    \"patch_tokenizer\",\\n+    \"print_unsloth_message\",\\n+]\\n \\n \\n def prepare_model_for_kbit_training(\\n@@ -59,3 +67,38 @@ def prepare_model_for_kbit_training(\\n \\n     return model\\n pass\\n+\\n+\\n+def patch_tokenizer(model, tokenizer):\\n+    if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n+        # Fixes https://github.com/unslothai/unsloth/issues/5\\n+        if hasattr(tokenizer, \"unk_token\"):\\n+            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n+            tokenizer.pad_token = tokenizer.unk_token\\n+        else:\\n+            logger.warning_one(\\n+                f\"{model.config._name_or_path} does not have a padding or unknown token!\\\\n\"\\\\\\n+                f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+            )\\n+            assert(hasattr(tokenizer, \"eos_token\"))\\n+            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n+            tokenizer.pad_token = tokenizer.eos_token\\n+        config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+    pass\\n+    return model, tokenizer\\n+pass\\n+\\n+\\n+def print_unsloth_message(name):\\n+    SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n+    gpu_stats = torch.cuda.get_device_properties(0)\\n+    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+    statistics = \\\\\\n+       f\"==((====))==  Unsloth: Fast {name} patching release {__version__}\\\\n\"\\\\\\n+       f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+       f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n+       f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+       f\\' \"-____-\"     bfloat16 support = {str(SUPPORTS_BFLOAT16).upper()}\\\\n\\'\\n+    print(statistics)\\n+pass\\n',\n",
       " '@@ -22,9 +22,7 @@ from transformers.models.llama.modeling_llama import (\\n     CausalLMOutputWithPast,\\n )\\n from ..kernels import *\\n-from ._utils import (\\n-    prepare_model_for_kbit_training,\\n-)\\n+from ._utils import *\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -615,18 +613,8 @@ class FastLlamaModel:\\n         device_map = \"sequential\",\\n         rope_scaling = None,\\n     ):\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-\\n-        statistics = \\\\\\n-            \"==((====))==  Unsloth: Fast Llama patching release 2023.12\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n-           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n-           f\\' \"-____-\"     bfloat16 support = {str(SUPPORTS_BFLOAT16).upper()}\\\\n\\'\\n-        print(statistics)\\n-\\n+        print_unsloth_message(\"Mistral\")\\n         FastLlamaModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -676,22 +664,7 @@ class FastLlamaModel:\\n             token = token,\\n         )\\n \\n-        if not hasattr(tokenizer, \"pad_token\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/5\\n-            if hasattr(tokenizer, \"unk_token\"):\\n-                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n-                tokenizer.pad_token = tokenizer.unk_token\\n-            else:\\n-                logger.warning_one(\\n-                    f\"{model_name} does not have a padding or unknown token!\\\\n\"\\\\\\n-                    f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n-                )\\n-                assert(hasattr(tokenizer, \"eos_token\"))\\n-                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n-                tokenizer.pad_token = tokenizer.eos_token\\n-            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n-        pass\\n-\\n+        model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = FastLlamaModel.post_patch(model)\\n \\n         # Patch up QKV / O and MLP\\n',\n",
       " '@@ -231,18 +231,8 @@ class FastMistralModel(FastLlamaModel):\\n         device_map = \"sequential\",\\n         # rope_scaling = None, Mistral does not support RoPE scaling\\n     ):\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-\\n-        statistics = \\\\\\n-            \"==((====))==  Unsloth: Fast Mistral patching release 2023.12\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n-           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n-           f\\' \"-____-\"     bfloat16 support = {str(SUPPORTS_BFLOAT16).upper()}\\\\n\\'\\n-        print(statistics)\\n-\\n+        print_unsloth_message(\"Mistral\")\\n         FastMistralModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -277,22 +267,7 @@ class FastMistralModel(FastLlamaModel):\\n             token = token,\\n         )\\n \\n-        if not hasattr(tokenizer, \"pad_token\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/5\\n-            if hasattr(tokenizer, \"unk_token\"):\\n-                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n-                tokenizer.pad_token = tokenizer.unk_token\\n-            else:\\n-                logger.warning_one(\\n-                    f\"{model_name} does not have a padding or unknown token!\\\\n\"\\\\\\n-                    f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n-                )\\n-                assert(hasattr(tokenizer, \"eos_token\"))\\n-                tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n-                tokenizer.pad_token = tokenizer.eos_token\\n-            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n-        pass\\n-\\n+        model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = FastMistralModel.post_patch(model)\\n \\n         # Patch up QKV / O and MLP\\n',\n",
       " \"@@ -7,7 +7,7 @@\\n ## 2-5x faster 60% less memory local QLoRA finetuning\\n * Supports Llama 7b, 13b, 70b, CodeLlama 34b, Mistral 7b, TinyLlama and all Llama archs!\\n * Llama 7b [Colab T4 example](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) on 1 T4 2x faster, uses 43% less VRAM (8.4GB) LAION dataset. [Alpaca T4 example](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) 2x faster on 1 T4, using 6.4GB VRAM.\\n-* Mistral 7b [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) on 1 A100 2.2x faster, uses 62% less VRAM (12.4GB).\\n+* Mistral 7b [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) on 1 A100 2.2x faster, uses 62% less VRAM (12.4GB). [Colab T4 example](https://colab.research.google.com/drive/15pyLgRN97B_jA56HS0esx56knA9I5tuv?usp=sharing)\\n * CodeLlama 34b [Colab example](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) does not OOM is 1.9x faster, uses 32% less VRAM (27GB).\\n * Kaggle 2 Tesla T4s 5.28x faster on Alpaca. [Kaggle example](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp)\\n * All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language.\\n\",\n",
       " '@@ -37,6 +37,17 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n     Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n+    # Check for inference\\n+    if use_cache and past_key_value is not None and q_len == 1:\\n+        A, past_key_value = LlamaAttention_fast_forward_inference(\\n+            self,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+        )\\n+        return A, None, past_key_value\\n+    pass\\n+\\n     n_heads    = self.num_heads\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n@@ -152,8 +163,10 @@ def MistralForCausalLM_fast_forward(\\n         elif q_len <= sliding_window:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         else:\\n-            causal_mask = xformers.attn_bias.BlockDiagonalCausalLocalAttentionMask.\\\\\\n-                make_local_attention(window_size = sliding_window)\\n+            # Fix from https://github.com/Rypo\\n+            causal_mask = xformers.attn_bias.BlockDiagonalCausalMask\\\\\\n+                .from_seqlens([qlen]*bsz)\\\\\\n+                .make_local_attention(window_size = sliding_window)\\n     pass\\n \\n     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n',\n",
       " \"@@ -7,7 +7,7 @@\\n ## 2-5x faster 60% less memory local QLoRA finetuning\\n * Supports Llama 7b, 13b, 70b, CodeLlama 34b, Mistral 7b, TinyLlama and all Llama archs!\\n * Llama 7b [Colab T4 example](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) on 1 T4 2x faster, uses 43% less VRAM (8.4GB) LAION dataset. [Alpaca T4 example](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) 2x faster on 1 T4, using 6.4GB VRAM.\\n-* Mistral 7b [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) on 1 A100 2.2x faster, uses 62% less VRAM (12.4GB).\\n+* Mistral 7b [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) on 1 A100 2.2x faster, uses 62% less VRAM (12.4GB). [Colab T4 example](https://colab.research.google.com/drive/15pyLgRN97B_jA56HS0esx56knA9I5tuv?usp=sharing)\\n * CodeLlama 34b [Colab example](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) does not OOM is 1.9x faster, uses 32% less VRAM (27GB).\\n * Kaggle 2 Tesla T4s 5.28x faster on Alpaca. [Kaggle example](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp)\\n * All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language.\\n\",\n",
       " '@@ -37,6 +37,17 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n     Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n+    # Check for inference\\n+    if use_cache and past_key_value is not None and q_len == 1:\\n+        A, past_key_value = LlamaAttention_fast_forward_inference(\\n+            self,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+        )\\n+        return A, None, past_key_value\\n+    pass\\n+\\n     n_heads    = self.num_heads\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n@@ -152,8 +163,10 @@ def MistralForCausalLM_fast_forward(\\n         elif q_len <= sliding_window:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         else:\\n-            causal_mask = xformers.attn_bias.BlockDiagonalCausalLocalAttentionMask.\\\\\\n-                make_local_attention(window_size = sliding_window)\\n+            # Fix from https://github.com/Rypo\\n+            causal_mask = xformers.attn_bias.BlockDiagonalCausalMask\\\\\\n+                .from_seqlens([qlen]*bsz)\\\\\\n+                .make_local_attention(window_size = sliding_window)\\n     pass\\n \\n     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n',\n",
       " '@@ -33,7 +33,7 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n \\n # Installation Instructions - Conda\\n Unsloth currently only supports Linux distros and Pytorch == 2.1.\\n-```\\n+```bash\\n conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n   -c pytorch -c nvidia -c xformers -c conda-forge -y\\n pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\\n@@ -41,16 +41,16 @@ pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n # Installation Instructions - Pip\\n 1. Find your CUDA version via\\n-```\\n+```python\\n import torch; torch.version.cuda\\n ```\\n 2. We only support Pytorch 2.1 (2.1.1 bugs out for now): You can update Pytorch via Pip (interchange cu121 / cu118)\\n-```\\n+```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n ```\\n 2. Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the \"ampere\" path.\\n-```\\n+```bash\\n pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n@@ -59,13 +59,13 @@ pip install \"unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.gi\\n Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n \\n 4. If you get errors, try the below first, then go back to step 1:\\n-```\\n+```bash\\n pip install --upgrade pip\\n ```\\n \\n # Documentation\\n We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-```\\n+```python\\n from unsloth import FastLlamaModel, FastMistralModel\\n import torch\\n max_seq_length = 2048 # Can change to any number <= 4096\\n@@ -305,7 +305,7 @@ $$\\n \\n # Troubleshooting\\n 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```\\n+```bash\\n !ldconfig /usr/lib64-nvidia\\n ```\\n 2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n@@ -315,5 +315,5 @@ $$\\n # Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n-\\n+3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n <img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files a/images/unsloth made with love.png and b/images/unsloth made with love.png differ\\n',\n",
       " 'Binary files a/images/unsloth new logo.png and b/images/unsloth new logo.png differ\\n',\n",
       " '@@ -20,13 +20,36 @@ import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n-import platform\\n+from platform import system as platform_system\\n+platform_system = platform_system()\\n \\n __version__ = \"2023.12\"\\n+\\n+# Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n+major_version, minor_version = torch.cuda.get_device_capability()\\n+if major_version >= 8:\\n+    try:\\n+        from flash_attn import flash_attn_func\\n+        HAS_FLASH_ATTENTION = True\\n+    except:\\n+        HAS_FLASH_ATTENTION = False\\n+else:\\n+    # Tri Dao\\'s benchmark shows xformers is faster for now.\\n+    HAS_FLASH_ATTENTION = False\\n+pass\\n+import xformers.ops.fmha as xformers\\n+xformers_attention = xformers.memory_efficient_attention\\n+from xformers import __version__ as xformers_version\\n+\\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n     \"patch_tokenizer\",\\n-    \"print_unsloth_message\",\\n+    \"xformers\",\\n+    \"xformers_attention\",\\n+    \"xformers_version\",\\n+    \"__version__\",\\n+    \"HAS_FLASH_ATTENTION\",\\n+    \"platform_system\",\\n ]\\n \\n \\n@@ -71,6 +94,7 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    model.config.update({\"unsloth_version\" : __version__})\\n     if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n         # Fixes https://github.com/unslothai/unsloth/issues/5\\n         if hasattr(tokenizer, \"unk_token\"):\\n@@ -88,18 +112,3 @@ def patch_tokenizer(model, tokenizer):\\n     pass\\n     return model, tokenizer\\n pass\\n-\\n-\\n-def print_unsloth_message(name):\\n-    SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-    gpu_stats = torch.cuda.get_device_properties(0)\\n-    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-    statistics = \\\\\\n-       f\"==((====))==  Unsloth: Fast {name} patching release {__version__}\\\\n\"\\\\\\n-       f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n-       f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n-       f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n-       f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform.system()}\\\\n\\'\\n-    print(statistics)\\n-pass\\n',\n",
       " '@@ -23,21 +23,9 @@ from transformers.models.llama.modeling_llama import (\\n )\\n from ..kernels import *\\n from ._utils import *\\n-\\n-# Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n-major_version, minor_version = torch.cuda.get_device_capability()\\n-if major_version >= 8:\\n-    try:\\n-        from flash_attn import flash_attn_func\\n-        HAS_FLASH_ATTENTION = True\\n-    except:\\n-        HAS_FLASH_ATTENTION = False\\n-else:\\n-    # Tri Dao\\'s benchmark shows xformers is faster for now.\\n-    HAS_FLASH_ATTENTION = False\\n-pass\\n-import xformers.ops.fmha as xformers\\n-xformers_attention = xformers.memory_efficient_attention\\n+from ._utils import __version__\\n+if HAS_FLASH_ATTENTION:\\n+    from flash_attn import flash_attn_func\\n \\n # Final patching code\\n from transformers.models.llama.modeling_llama import (\\n@@ -139,19 +127,20 @@ def LlamaAttention_fast_forward_inference(\\n     # V = repeat_kv(V, n_groups)\\n     if n_groups != 1:\\n         _, _, cached_len, _ = Kn.shape\\n-        Kn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n-        Vn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n-        Kn = Kn.reshape(bsz, n_heads, cached_len, head_dim)\\n-        Vn = Vn.reshape(bsz, n_heads, cached_len, head_dim)\\n-    pass\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.view(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.view(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Kn.transpose(2, 3))\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n     A *= 1.0 / (self.head_dim**0.5)\\n     A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n-    A = torch.matmul(A, Vn)\\n+    A = torch.matmul(A, Vnn)\\n     A = A.transpose(1, 2)\\n-    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = A.view(bsz, 1, self.hidden_size)\\n     A = original_apply_o(self, A)\\n     return A, (Kn, Vn)\\n pass\\n@@ -359,13 +348,13 @@ def LlamaModel_fast_forward(\\n \\n     # retrieve input_ids and inputs_embeds\\n     if input_ids is not None and inputs_embeds is not None:\\n-        raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\\n+        raise ValueError(\"Unsloth: You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\\n     elif input_ids is not None:\\n         batch_size, seq_length = input_ids.shape\\n     elif inputs_embeds is not None:\\n         batch_size, seq_length, _ = inputs_embeds.shape\\n     else:\\n-        raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n+        raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n     past_key_values_length = 0\\n@@ -419,7 +408,7 @@ def LlamaModel_fast_forward(\\n     if self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\\n+                \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n             )\\n             use_cache = False\\n     pass\\n@@ -614,7 +603,16 @@ class FastLlamaModel:\\n         rope_scaling = None,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-        print_unsloth_message(\"Llama\")\\n+        gpu_stats = torch.cuda.get_device_properties(0)\\n+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+        statistics = \\\\\\n+           f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n+           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n+           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+           f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\\\\n\\'\\n+        logger.warning_once(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -632,7 +630,7 @@ class FastLlamaModel:\\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n             rope_scaling = max_seq_length / model_max_seq_length\\n             logger.warning_once(\\n-                f\"Unsloth: {model_name} can only handle sequence lengths of of most \"\\\\\\n+                f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n@@ -686,6 +684,7 @@ class FastLlamaModel:\\n         # Torch.compile fails on embedding matrix??\\n         # Workaround randomnly fixes it for torch versions < 2.2\\n         model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n \\n         # We also do this for the lm_head\\n         lm_head = torch.nn.Linear(1, 1, bias = None)\\n@@ -747,6 +746,7 @@ class FastLlamaModel:\\n \\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n         for module in target_modules:\\n             assert(module in accepted_modules)\\n         pass\\n@@ -771,6 +771,9 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         # Do patching\\n+        n_mlp = 0\\n+        n_qkv = 0\\n+        n_o   = 0\\n         for idx, layer in enumerate(model.model.model.layers):\\n \\n             # MLP patching\\n@@ -780,6 +783,7 @@ class FastLlamaModel:\\n \\n                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n+                n_mlp += 1\\n             pass\\n \\n             # QKV attention patching\\n@@ -788,15 +792,22 @@ class FastLlamaModel:\\n                 hasattr(layer.self_attn.v_proj, \"lora_A\"):\\n \\n                 layer.self_attn.apply_qkv = apply_lora_qkv\\n+                n_qkv += 1\\n             pass\\n \\n             # O attention patching\\n             if hasattr(layer.self_attn.o_proj, \"lora_A\"):\\n \\n                 layer.self_attn.apply_o = apply_lora_o\\n+                n_o += 1\\n             pass\\n         pass\\n \\n+        logger.warning_once(\\n+            f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n+            f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n+        )\\n+\\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n',\n",
       " '@@ -45,7 +45,7 @@ class FastLanguageModel:\\n             )\\n         elif model_type == \"mistral\":\\n             if rope_scaling is not None:\\n-                logger.warning_once(\"Mistral models do not support RoPE scaling.\")\\n+                logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n             return FastMistralModel.from_pretrained(\\n                 model_name = model_name,\\n                 max_seq_length = max_seq_length,\\n@@ -57,7 +57,8 @@ class FastLanguageModel:\\n             )\\n         else:\\n             raise NotImplementedError(\\n-                f\"{model_name} not supported yet! Make an issue to https://github.com/unslothai/unsloth!\",\\n+                f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n+                \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n     pass\\n pass\\n',\n",
       " '@@ -13,6 +13,7 @@\\n # limitations under the License.\\n \\n from .llama import *\\n+from ._utils import __version__\\n \\n from transformers.models.mistral.modeling_mistral import (\\n     MistralAttention,\\n@@ -245,7 +246,16 @@ class FastMistralModel(FastLlamaModel):\\n         # rope_scaling = None, Mistral does not support RoPE scaling\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-        print_unsloth_message(\"Mistral\")\\n+        gpu_stats = torch.cuda.get_device_properties(0)\\n+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+        statistics = \\\\\\n+           f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n+           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n+           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+           f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\\\\n\\'\\n+        logger.warning_once(statistics)\\n         FastMistralModel.pre_patch()\\n \\n         if dtype is None:\\n',\n",
       " '@@ -33,7 +33,7 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n \\n # Installation Instructions - Conda\\n Unsloth currently only supports Linux distros and Pytorch == 2.1.\\n-```\\n+```bash\\n conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n   -c pytorch -c nvidia -c xformers -c conda-forge -y\\n pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\\n@@ -41,16 +41,16 @@ pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n # Installation Instructions - Pip\\n 1. Find your CUDA version via\\n-```\\n+```python\\n import torch; torch.version.cuda\\n ```\\n 2. We only support Pytorch 2.1 (2.1.1 bugs out for now): You can update Pytorch via Pip (interchange cu121 / cu118)\\n-```\\n+```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n ```\\n 2. Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the \"ampere\" path.\\n-```\\n+```bash\\n pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n@@ -59,13 +59,13 @@ pip install \"unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.gi\\n Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n \\n 4. If you get errors, try the below first, then go back to step 1:\\n-```\\n+```bash\\n pip install --upgrade pip\\n ```\\n \\n # Documentation\\n We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-```\\n+```python\\n from unsloth import FastLlamaModel, FastMistralModel\\n import torch\\n max_seq_length = 2048 # Can change to any number <= 4096\\n@@ -305,7 +305,7 @@ $$\\n \\n # Troubleshooting\\n 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```\\n+```bash\\n !ldconfig /usr/lib64-nvidia\\n ```\\n 2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n@@ -315,5 +315,5 @@ $$\\n # Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n-\\n+3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n <img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files a/images/unsloth made with love.png and b/images/unsloth made with love.png differ\\n',\n",
       " 'Binary files a/images/unsloth new logo.png and b/images/unsloth new logo.png differ\\n',\n",
       " '@@ -20,13 +20,36 @@ import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n-import platform\\n+from platform import system as platform_system\\n+platform_system = platform_system()\\n \\n __version__ = \"2023.12\"\\n+\\n+# Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n+major_version, minor_version = torch.cuda.get_device_capability()\\n+if major_version >= 8:\\n+    try:\\n+        from flash_attn import flash_attn_func\\n+        HAS_FLASH_ATTENTION = True\\n+    except:\\n+        HAS_FLASH_ATTENTION = False\\n+else:\\n+    # Tri Dao\\'s benchmark shows xformers is faster for now.\\n+    HAS_FLASH_ATTENTION = False\\n+pass\\n+import xformers.ops.fmha as xformers\\n+xformers_attention = xformers.memory_efficient_attention\\n+from xformers import __version__ as xformers_version\\n+\\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n     \"patch_tokenizer\",\\n-    \"print_unsloth_message\",\\n+    \"xformers\",\\n+    \"xformers_attention\",\\n+    \"xformers_version\",\\n+    \"__version__\",\\n+    \"HAS_FLASH_ATTENTION\",\\n+    \"platform_system\",\\n ]\\n \\n \\n@@ -71,6 +94,7 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    model.config.update({\"unsloth_version\" : __version__})\\n     if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n         # Fixes https://github.com/unslothai/unsloth/issues/5\\n         if hasattr(tokenizer, \"unk_token\"):\\n@@ -88,18 +112,3 @@ def patch_tokenizer(model, tokenizer):\\n     pass\\n     return model, tokenizer\\n pass\\n-\\n-\\n-def print_unsloth_message(name):\\n-    SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-    gpu_stats = torch.cuda.get_device_properties(0)\\n-    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-    statistics = \\\\\\n-       f\"==((====))==  Unsloth: Fast {name} patching release {__version__}\\\\n\"\\\\\\n-       f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n-       f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n-       f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n-       f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform.system()}\\\\n\\'\\n-    print(statistics)\\n-pass\\n',\n",
       " '@@ -23,21 +23,9 @@ from transformers.models.llama.modeling_llama import (\\n )\\n from ..kernels import *\\n from ._utils import *\\n-\\n-# Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n-major_version, minor_version = torch.cuda.get_device_capability()\\n-if major_version >= 8:\\n-    try:\\n-        from flash_attn import flash_attn_func\\n-        HAS_FLASH_ATTENTION = True\\n-    except:\\n-        HAS_FLASH_ATTENTION = False\\n-else:\\n-    # Tri Dao\\'s benchmark shows xformers is faster for now.\\n-    HAS_FLASH_ATTENTION = False\\n-pass\\n-import xformers.ops.fmha as xformers\\n-xformers_attention = xformers.memory_efficient_attention\\n+from ._utils import __version__\\n+if HAS_FLASH_ATTENTION:\\n+    from flash_attn import flash_attn_func\\n \\n # Final patching code\\n from transformers.models.llama.modeling_llama import (\\n@@ -139,19 +127,20 @@ def LlamaAttention_fast_forward_inference(\\n     # V = repeat_kv(V, n_groups)\\n     if n_groups != 1:\\n         _, _, cached_len, _ = Kn.shape\\n-        Kn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n-        Vn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n-        Kn = Kn.reshape(bsz, n_heads, cached_len, head_dim)\\n-        Vn = Vn.reshape(bsz, n_heads, cached_len, head_dim)\\n-    pass\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.view(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.view(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Kn.transpose(2, 3))\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n     A *= 1.0 / (self.head_dim**0.5)\\n     A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n-    A = torch.matmul(A, Vn)\\n+    A = torch.matmul(A, Vnn)\\n     A = A.transpose(1, 2)\\n-    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = A.view(bsz, 1, self.hidden_size)\\n     A = original_apply_o(self, A)\\n     return A, (Kn, Vn)\\n pass\\n@@ -359,13 +348,13 @@ def LlamaModel_fast_forward(\\n \\n     # retrieve input_ids and inputs_embeds\\n     if input_ids is not None and inputs_embeds is not None:\\n-        raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\\n+        raise ValueError(\"Unsloth: You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\\n     elif input_ids is not None:\\n         batch_size, seq_length = input_ids.shape\\n     elif inputs_embeds is not None:\\n         batch_size, seq_length, _ = inputs_embeds.shape\\n     else:\\n-        raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n+        raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n     past_key_values_length = 0\\n@@ -419,7 +408,7 @@ def LlamaModel_fast_forward(\\n     if self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\\n+                \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n             )\\n             use_cache = False\\n     pass\\n@@ -614,7 +603,16 @@ class FastLlamaModel:\\n         rope_scaling = None,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-        print_unsloth_message(\"Llama\")\\n+        gpu_stats = torch.cuda.get_device_properties(0)\\n+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+        statistics = \\\\\\n+           f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n+           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n+           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+           f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\\\\n\\'\\n+        logger.warning_once(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -632,7 +630,7 @@ class FastLlamaModel:\\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n             rope_scaling = max_seq_length / model_max_seq_length\\n             logger.warning_once(\\n-                f\"Unsloth: {model_name} can only handle sequence lengths of of most \"\\\\\\n+                f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n@@ -686,6 +684,7 @@ class FastLlamaModel:\\n         # Torch.compile fails on embedding matrix??\\n         # Workaround randomnly fixes it for torch versions < 2.2\\n         model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n \\n         # We also do this for the lm_head\\n         lm_head = torch.nn.Linear(1, 1, bias = None)\\n@@ -747,6 +746,7 @@ class FastLlamaModel:\\n \\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n         for module in target_modules:\\n             assert(module in accepted_modules)\\n         pass\\n@@ -771,6 +771,9 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         # Do patching\\n+        n_mlp = 0\\n+        n_qkv = 0\\n+        n_o   = 0\\n         for idx, layer in enumerate(model.model.model.layers):\\n \\n             # MLP patching\\n@@ -780,6 +783,7 @@ class FastLlamaModel:\\n \\n                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n+                n_mlp += 1\\n             pass\\n \\n             # QKV attention patching\\n@@ -788,15 +792,22 @@ class FastLlamaModel:\\n                 hasattr(layer.self_attn.v_proj, \"lora_A\"):\\n \\n                 layer.self_attn.apply_qkv = apply_lora_qkv\\n+                n_qkv += 1\\n             pass\\n \\n             # O attention patching\\n             if hasattr(layer.self_attn.o_proj, \"lora_A\"):\\n \\n                 layer.self_attn.apply_o = apply_lora_o\\n+                n_o += 1\\n             pass\\n         pass\\n \\n+        logger.warning_once(\\n+            f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n+            f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n+        )\\n+\\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n',\n",
       " '@@ -45,7 +45,7 @@ class FastLanguageModel:\\n             )\\n         elif model_type == \"mistral\":\\n             if rope_scaling is not None:\\n-                logger.warning_once(\"Mistral models do not support RoPE scaling.\")\\n+                logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n             return FastMistralModel.from_pretrained(\\n                 model_name = model_name,\\n                 max_seq_length = max_seq_length,\\n@@ -57,7 +57,8 @@ class FastLanguageModel:\\n             )\\n         else:\\n             raise NotImplementedError(\\n-                f\"{model_name} not supported yet! Make an issue to https://github.com/unslothai/unsloth!\",\\n+                f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n+                \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n     pass\\n pass\\n',\n",
       " '@@ -13,6 +13,7 @@\\n # limitations under the License.\\n \\n from .llama import *\\n+from ._utils import __version__\\n \\n from transformers.models.mistral.modeling_mistral import (\\n     MistralAttention,\\n@@ -245,7 +246,16 @@ class FastMistralModel(FastLlamaModel):\\n         # rope_scaling = None, Mistral does not support RoPE scaling\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-        print_unsloth_message(\"Mistral\")\\n+        gpu_stats = torch.cuda.get_device_properties(0)\\n+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+        statistics = \\\\\\n+           f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n+           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n+           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+           f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\\\\n\\'\\n+        logger.warning_once(statistics)\\n         FastMistralModel.pre_patch()\\n \\n         if dtype is None:\\n',\n",
       " '@@ -33,7 +33,7 @@ If you trained a model with Unsloth, we made a cool sticker!!\\n \\n # Installation Instructions - Conda\\n Unsloth currently only supports Linux distros and Pytorch == 2.1.\\n-```\\n+```bash\\n conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n   -c pytorch -c nvidia -c xformers -c conda-forge -y\\n pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\\n@@ -41,16 +41,16 @@ pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n # Installation Instructions - Pip\\n 1. Find your CUDA version via\\n-```\\n+```python\\n import torch; torch.version.cuda\\n ```\\n 2. We only support Pytorch 2.1 (2.1.1 bugs out for now): You can update Pytorch via Pip (interchange cu121 / cu118)\\n-```\\n+```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n ```\\n 2. Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the \"ampere\" path.\\n-```\\n+```bash\\n pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n@@ -59,13 +59,13 @@ pip install \"unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.gi\\n Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n \\n 4. If you get errors, try the below first, then go back to step 1:\\n-```\\n+```bash\\n pip install --upgrade pip\\n ```\\n \\n # Documentation\\n We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-```\\n+```python\\n from unsloth import FastLlamaModel, FastMistralModel\\n import torch\\n max_seq_length = 2048 # Can change to any number <= 4096\\n@@ -305,7 +305,7 @@ $$\\n \\n # Troubleshooting\\n 1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```\\n+```bash\\n !ldconfig /usr/lib64-nvidia\\n ```\\n 2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n@@ -315,5 +315,5 @@ $$\\n # Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n-\\n+3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n <img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files a/images/unsloth made with love.png and b/images/unsloth made with love.png differ\\n',\n",
       " 'Binary files a/images/unsloth new logo.png and b/images/unsloth new logo.png differ\\n',\n",
       " '@@ -20,13 +20,36 @@ import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n-import platform\\n+from platform import system as platform_system\\n+platform_system = platform_system()\\n \\n __version__ = \"2023.12\"\\n+\\n+# Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n+major_version, minor_version = torch.cuda.get_device_capability()\\n+if major_version >= 8:\\n+    try:\\n+        from flash_attn import flash_attn_func\\n+        HAS_FLASH_ATTENTION = True\\n+    except:\\n+        HAS_FLASH_ATTENTION = False\\n+else:\\n+    # Tri Dao\\'s benchmark shows xformers is faster for now.\\n+    HAS_FLASH_ATTENTION = False\\n+pass\\n+import xformers.ops.fmha as xformers\\n+xformers_attention = xformers.memory_efficient_attention\\n+from xformers import __version__ as xformers_version\\n+\\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n     \"patch_tokenizer\",\\n-    \"print_unsloth_message\",\\n+    \"xformers\",\\n+    \"xformers_attention\",\\n+    \"xformers_version\",\\n+    \"__version__\",\\n+    \"HAS_FLASH_ATTENTION\",\\n+    \"platform_system\",\\n ]\\n \\n \\n@@ -71,6 +94,7 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    model.config.update({\"unsloth_version\" : __version__})\\n     if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n         # Fixes https://github.com/unslothai/unsloth/issues/5\\n         if hasattr(tokenizer, \"unk_token\"):\\n@@ -88,18 +112,3 @@ def patch_tokenizer(model, tokenizer):\\n     pass\\n     return model, tokenizer\\n pass\\n-\\n-\\n-def print_unsloth_message(name):\\n-    SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-    gpu_stats = torch.cuda.get_device_properties(0)\\n-    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-    statistics = \\\\\\n-       f\"==((====))==  Unsloth: Fast {name} patching release {__version__}\\\\n\"\\\\\\n-       f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n-       f\"O^O/ \\\\_/ \\\\\\\\    CUDA compute capability = {gpu_stats.major}.{gpu_stats.minor}\\\\n\"\\\\\\n-       f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n-       f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform.system()}\\\\n\\'\\n-    print(statistics)\\n-pass\\n',\n",
       " '@@ -23,21 +23,9 @@ from transformers.models.llama.modeling_llama import (\\n )\\n from ..kernels import *\\n from ._utils import *\\n-\\n-# Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n-major_version, minor_version = torch.cuda.get_device_capability()\\n-if major_version >= 8:\\n-    try:\\n-        from flash_attn import flash_attn_func\\n-        HAS_FLASH_ATTENTION = True\\n-    except:\\n-        HAS_FLASH_ATTENTION = False\\n-else:\\n-    # Tri Dao\\'s benchmark shows xformers is faster for now.\\n-    HAS_FLASH_ATTENTION = False\\n-pass\\n-import xformers.ops.fmha as xformers\\n-xformers_attention = xformers.memory_efficient_attention\\n+from ._utils import __version__\\n+if HAS_FLASH_ATTENTION:\\n+    from flash_attn import flash_attn_func\\n \\n # Final patching code\\n from transformers.models.llama.modeling_llama import (\\n@@ -139,19 +127,20 @@ def LlamaAttention_fast_forward_inference(\\n     # V = repeat_kv(V, n_groups)\\n     if n_groups != 1:\\n         _, _, cached_len, _ = Kn.shape\\n-        Kn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n-        Vn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n-        Kn = Kn.reshape(bsz, n_heads, cached_len, head_dim)\\n-        Vn = Vn.reshape(bsz, n_heads, cached_len, head_dim)\\n-    pass\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.view(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.view(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Kn.transpose(2, 3))\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n     A *= 1.0 / (self.head_dim**0.5)\\n     A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n-    A = torch.matmul(A, Vn)\\n+    A = torch.matmul(A, Vnn)\\n     A = A.transpose(1, 2)\\n-    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = A.view(bsz, 1, self.hidden_size)\\n     A = original_apply_o(self, A)\\n     return A, (Kn, Vn)\\n pass\\n@@ -359,13 +348,13 @@ def LlamaModel_fast_forward(\\n \\n     # retrieve input_ids and inputs_embeds\\n     if input_ids is not None and inputs_embeds is not None:\\n-        raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\\n+        raise ValueError(\"Unsloth: You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\\n     elif input_ids is not None:\\n         batch_size, seq_length = input_ids.shape\\n     elif inputs_embeds is not None:\\n         batch_size, seq_length, _ = inputs_embeds.shape\\n     else:\\n-        raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n+        raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n     past_key_values_length = 0\\n@@ -419,7 +408,7 @@ def LlamaModel_fast_forward(\\n     if self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\\n+                \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n             )\\n             use_cache = False\\n     pass\\n@@ -614,7 +603,16 @@ class FastLlamaModel:\\n         rope_scaling = None,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-        print_unsloth_message(\"Llama\")\\n+        gpu_stats = torch.cuda.get_device_properties(0)\\n+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+        statistics = \\\\\\n+           f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n+           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n+           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+           f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\\\\n\\'\\n+        logger.warning_once(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -632,7 +630,7 @@ class FastLlamaModel:\\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n             rope_scaling = max_seq_length / model_max_seq_length\\n             logger.warning_once(\\n-                f\"Unsloth: {model_name} can only handle sequence lengths of of most \"\\\\\\n+                f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n@@ -686,6 +684,7 @@ class FastLlamaModel:\\n         # Torch.compile fails on embedding matrix??\\n         # Workaround randomnly fixes it for torch versions < 2.2\\n         model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n \\n         # We also do this for the lm_head\\n         lm_head = torch.nn.Linear(1, 1, bias = None)\\n@@ -747,6 +746,7 @@ class FastLlamaModel:\\n \\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n         for module in target_modules:\\n             assert(module in accepted_modules)\\n         pass\\n@@ -771,6 +771,9 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         # Do patching\\n+        n_mlp = 0\\n+        n_qkv = 0\\n+        n_o   = 0\\n         for idx, layer in enumerate(model.model.model.layers):\\n \\n             # MLP patching\\n@@ -780,6 +783,7 @@ class FastLlamaModel:\\n \\n                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n+                n_mlp += 1\\n             pass\\n \\n             # QKV attention patching\\n@@ -788,15 +792,22 @@ class FastLlamaModel:\\n                 hasattr(layer.self_attn.v_proj, \"lora_A\"):\\n \\n                 layer.self_attn.apply_qkv = apply_lora_qkv\\n+                n_qkv += 1\\n             pass\\n \\n             # O attention patching\\n             if hasattr(layer.self_attn.o_proj, \"lora_A\"):\\n \\n                 layer.self_attn.apply_o = apply_lora_o\\n+                n_o += 1\\n             pass\\n         pass\\n \\n+        logger.warning_once(\\n+            f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n+            f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n+        )\\n+\\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n',\n",
       " '@@ -45,7 +45,7 @@ class FastLanguageModel:\\n             )\\n         elif model_type == \"mistral\":\\n             if rope_scaling is not None:\\n-                logger.warning_once(\"Mistral models do not support RoPE scaling.\")\\n+                logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n             return FastMistralModel.from_pretrained(\\n                 model_name = model_name,\\n                 max_seq_length = max_seq_length,\\n@@ -57,7 +57,8 @@ class FastLanguageModel:\\n             )\\n         else:\\n             raise NotImplementedError(\\n-                f\"{model_name} not supported yet! Make an issue to https://github.com/unslothai/unsloth!\",\\n+                f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n+                \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n     pass\\n pass\\n',\n",
       " '@@ -13,6 +13,7 @@\\n # limitations under the License.\\n \\n from .llama import *\\n+from ._utils import __version__\\n \\n from transformers.models.mistral.modeling_mistral import (\\n     MistralAttention,\\n@@ -245,7 +246,16 @@ class FastMistralModel(FastLlamaModel):\\n         # rope_scaling = None, Mistral does not support RoPE scaling\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n-        print_unsloth_message(\"Mistral\")\\n+        gpu_stats = torch.cuda.get_device_properties(0)\\n+        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n+\\n+        statistics = \\\\\\n+           f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n+           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    CUDA capability = {gpu_stats.major}.{gpu_stats.minor}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n+           f\"\\\\        /    Pytorch version: {torch.__version__}. CUDA Toolkit = {torch.version.cuda}\\\\n\"\\\\\\n+           f\\' \"-____-\"     bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Platform = {platform_system}\\\\n\\'\\n+        logger.warning_once(statistics)\\n         FastMistralModel.pre_patch()\\n \\n         if dtype is None:\\n',\n",
       " '@@ -369,6 +369,7 @@ def LlamaModel_fast_forward(\\n         raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n+    assert(seq_length <= self.max_seq_length)\\n     past_key_values_length = 0\\n \\n     if past_key_values is not None:\\n@@ -661,6 +662,9 @@ class FastLlamaModel:\\n                 bnb_4bit_compute_dtype    = dtype,\\n             )\\n \\n+        # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n+        # RoPE Scaling\\'s max_position_embeddings must be updated\\n+        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n             device_map = device_map,\\n@@ -668,6 +672,7 @@ class FastLlamaModel:\\n             quantization_config = bnb_config,\\n             token = token,\\n             rope_scaling = rope_scaling,\\n+            max_position_embeddings = max_position_embeddings,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n@@ -685,7 +690,7 @@ class FastLlamaModel:\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n-        model.max_seq_length = max_seq_length\\n+        model.max_seq_length = max_position_embeddings\\n         return model, tokenizer\\n     pass\\n \\n@@ -746,7 +751,7 @@ class FastLlamaModel:\\n         layers_to_transform = None,\\n         use_gradient_checkpointing = True,\\n         random_state = 3407,\\n-        max_seq_length = 2048,\\n+        max_seq_length = 2048, # not used anymore\\n         **kwargs,\\n     ):\\n         assert(max_seq_length <= model.max_seq_length)\\n@@ -824,6 +829,7 @@ class FastLlamaModel:\\n \\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n+        max_seq_length = model.max_seq_length\\n         extra_ignored_labels = torch.full((max_seq_length, 1), -100, device = \"cuda\")\\n         model.model.extra_ignored_labels = extra_ignored_labels\\n         internal_model = model\\n',\n",
       " '@@ -125,7 +125,7 @@ def MistralAttention_fast_forward(\\n         V = V.transpose(1, 2)\\n \\n         # Flash Attention v2 auto supports grouped query attention\\n-        sliding_window = self.config.sliding_window\\n+        sliding_window = getattr(self.config, \"sliding_window\")\\n         sliding_window = q_len if sliding_window is None else sliding_window\\n         window = (-1, -1) if (q_len <= sliding_window) else (sliding_window, sliding_window)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n@@ -169,7 +169,7 @@ def MistralForCausalLM_fast_forward(\\n \\n     if causal_mask is None:\\n         bsz, q_len = input_ids.shape\\n-        sliding_window = self.config.sliding_window\\n+        sliding_window = getattr(self.config, \"sliding_window\")\\n         if sliding_window is None or sliding_window <= 0:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         elif q_len <= sliding_window:\\n@@ -312,7 +312,7 @@ class FastMistralModel(FastLlamaModel):\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n-        model.max_seq_length = max_seq_length\\n+        model.max_seq_length = max(max_seq_length, model.config.max_position_embeddings)\\n         return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -1,26 +1,27 @@\\n <div class=\"align-center\">\\n   <img src=\"./images/unsloth new logo.png\" width=\"350\" />\\n   <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord.png\" width=\"160\"></a>\\n-  <a href=\"https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing\"><img src=\"./images/try live demo green.png\" width=\"130\"></a>\\n+  <a href=\"https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\"><img src=\"./images/try live demo green.png\" width=\"130\"></a>\\n </div>\\n \\n ## 2-5x faster 60% less memory local QLoRA finetuning\\n | Llama 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n |-----------------------------|-----------------------------|-------------------------|------------------------|\\n | **2.2x faster, -43%  VRAM**     | **2.2x faster, -62%  VRAM**     | **1.9x faster, -27% VRAM**  | **5.5x faster, -44% VRAM** |\\n-| [Colab Alpaca example + inference, saving](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing) | [Colab T4 example + inference, saving](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [A100 example](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Kaggle Alpaca example](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) |\\n-| [Colab A100 example](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | (59 more examples if you scroll down) | [Kaggle Slim Orca](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n+| [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing) | [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [Kaggle Alpaca example](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) |\\n+| [Colab A100 example](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | (59 more examples if you scroll down) | [Kaggle Slim Orca example](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n \\n-* Supports Llama (7, 13, 70b), Yi (6, 34b), Mistral (7b), Tinyllama, CodeLlama (7, 13, 34b), and all Llama / Mistral derived architectures!\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language.\\n+* Supports Llama, Yi, Mistral, CodeLlama, and their derived models (Open Hermes etc).\\n+* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backpropagation engine**.\\n * **0% loss in accuracy** - no approximation methods - all exact.\\n * No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)\\n * **NEW!** Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Experimental support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290)!\\n+* **NEW!** Support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290), PPO and Reward Modelling via [TRL](https://huggingface.co/docs/trl/dpo_trainer).\\n+* **NEW!** Download 4 bit models 4x faster directly from Huggingface!\\n * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* Open source version trains 5x faster or you can check out [Unsloth Pro and Max](https://unsloth.ai/) codepaths for **30x faster training**!\\n+* Open source version trains 5x faster - check out [Unsloth Max](https://unsloth.ai/) for **30x faster training**!\\n \\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+| 1 A100 40GB | Huggingface | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n | LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n@@ -28,11 +29,11 @@\\n | Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n \\n Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n-If you trained a model with Unsloth, we made a cool sticker!!\\n+If you trained a model with Unsloth, we made a cool sticker if you want to use it!\\n <img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n \\n # Installation Instructions - Conda\\n-Unsloth currently only supports Linux distros and Pytorch == 2.1.\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n ```bash\\n conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n   -c pytorch -c nvidia -c xformers -c conda-forge -y\\n@@ -40,25 +41,36 @@ pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n # Installation Instructions - Pip\\n+Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n+\\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. We only support Pytorch 2.1 (2.1.1 bugs out for now): You can update Pytorch via Pip (interchange cu121 / cu118)\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n ```\\n-2. Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the \"ampere\" path.\\n ```bash\\n pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-Change `cu121` to `cu118` for CUDA version 11.8 or 12.1. Go to https://pytorch.org/ to learn more.\\n-\\n-4. If you get errors, try the below first, then go back to step 1:\\n+3. For Pytorch 2.1.1: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n+4. We\\'re working on Pytorch 2.1.2 support.\\n+5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n@@ -120,10 +132,8 @@ trainer = SFTTrainer(\\n trainer.train()\\n ```\\n \\n-# DPO (Direct Preference Optimization) Experimental support\\n-[152334H](https://github.com/152334H) hacked Unsloth to work with DPO via TRL!\\n-1. Hack the model\\'s `config.json` to be llama model. [Example gist](https://gist.github.com/152334H/d8a68b51b83bac008a02e69ecc81d5c1).\\n-2. Use Unsloth for DPO for both base and reference models. [Example gist](https://gist.github.com/152334H/4847f3a8cca12894877e6b30698b0b64).\\n+# DPO (Direct Preference Optimization) Support\\n+DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory).\\n \\n # Future Milestones and limitations\\n 1. Support Mixtral.\\n@@ -173,6 +183,40 @@ Two Tesla T4s on Kaggle\\n \\n * Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n \\n+# Llama-Factory 3rd party benchmarking\\n+\\n+| Method | Bits | TGS | GRAM | Speed |\\n+| --- | --- | --- | --- | --- |\\n+| HF | 16 | 2392 | 18GB | 100% |\\n+| HF+FA2 | 16 | 2954 | 17GB | 123% |\\n+| Unsloth+FA2 | 16 | 4007 | 16GB | **168%** |\\n+| HF | 4 | 2415 | 9GB | 101% |\\n+| Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n+\\n+[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n+\\n+# How did we make it faster?\\n+Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n+\\n+$$\\n+\\\\begin{align}\\n+y &= \\\\frac{x_i}{\\\\sqrt{\\\\frac{1}{n}\\\\sum{x_i^2}+\\\\epsilon}} \\\\cdot w \\\\\\\\\\n+r &= \\\\frac{1}{\\\\sqrt{\\\\frac{1}{n}\\\\sum{x_i^2}+\\\\epsilon}} \\\\\\\\\\n+\\\\frac{dC}{dX} &= \\\\frac{1}{n} r \\\\bigg( n (dY \\\\cdot w) - \\\\bigg( x_i \\\\cdot r \\\\cdot \\\\sum{dY \\\\cdot y_i }  \\\\bigg) \\\\bigg)\\n+\\\\end{align}\\n+$$\\n+\\n+\\n+# Troubleshooting\\n+1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n+```bash\\n+!ldconfig /usr/lib64-nvidia\\n+```\\n+2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n+\\n+3. If it doesn\\'t install - maybe try updating `pip`.\\n+\\n+\\n # Full benchmarking tables\\n Click  \"Code\" for a fully reproducible example.\\n \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n@@ -312,27 +356,6 @@ Click  \"Code\" for a fully reproducible example.\\n | memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n | % saved | OOM  | OOM  |       |       |  | |\\n \\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-$$\\n-\\\\begin{align}\\n-y &= \\\\frac{x_i}{\\\\sqrt{\\\\frac{1}{n}\\\\sum{x_i^2}+\\\\epsilon}} \\\\cdot w \\\\\\\\\\n-r &= \\\\frac{1}{\\\\sqrt{\\\\frac{1}{n}\\\\sum{x_i^2}+\\\\epsilon}} \\\\\\\\\\n-\\\\frac{dC}{dX} &= \\\\frac{1}{n} r \\\\bigg( n (dY \\\\cdot w) - \\\\bigg( x_i \\\\cdot r \\\\cdot \\\\sum{dY \\\\cdot y_i }  \\\\bigg) \\\\bigg)\\n-\\\\end{align}\\n-$$\\n-\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n # Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n',\n",
       " '@@ -5,21 +5,21 @@ build-backend = \"setuptools.build_meta\"\\n [project]\\n name = \"unsloth\"\\n dynamic = [\"version\"]\\n-description = \"2X faster LLM finetuning\"\\n+description = \"2-5X faster LLM finetuning\"\\n readme = \"README.md\"\\n requires-python = \">=3.9\"\\n license = {file = \"LICENSE\"}\\n keywords = [\"ai\", \"llm\",]\\n authors = [\\n-\\t{email = \"info@unsloth.ai\"},\\n-\\t{name = \"Unsloth AI team\"},\\n+    {email = \"info@unsloth.ai\"},\\n+    {name = \"Unsloth AI team\"},\\n ]\\n maintainers = [\\n- \\t{name = \"Daniel Han\", email = \"danielhanchen@gmail.com\"},\\n- \\t{name = \"Michael Han\", email = \"info@unsloth.ai\"},\\n+    {name = \"Daniel Han\", email = \"danielhanchen@gmail.com\"},\\n+    {name = \"Michael Han\", email = \"info@unsloth.ai\"},\\n ]\\n classifiers = [\\n- \\t\"Programming Language :: Python\",\\n+    \"Programming Language :: Python\",\\n ]\\n \\n [tool.setuptools.dynamic]\\n@@ -40,54 +40,84 @@ huggingface = [\\n     \"trl\",\\n     \"peft\",\\n     \"packaging\",\\n+    \"ninja\",\\n ]\\n cu118only = [\\n-\\t\"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n-\\t\"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n-\\t\"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n ]\\n cu121only = [\\n-\\t\"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n-\\t\"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n-\\t\"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+cu118only_torch211 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+cu121only_torch211 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n ]\\n cu118 = [\\n-\\t\"unsloth[huggingface]\",\\n-\\t\"bitsandbytes\",\\n-\\t\"unsloth[cu118only]\",\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118only]\",\\n ]\\n cu121 = [\\n-\\t\"unsloth[huggingface]\",\\n-\\t\"bitsandbytes\",\\n-\\t\"unsloth[cu121only]\",\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121only]\",\\n+]\\n+cu118_torch211 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118only_torch211]\",\\n+]\\n+cu121_torch211 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121only_torch211]\",\\n ]\\n kaggle = [\\n-\\t\"unsloth[huggingface]\",\\n+    \"unsloth[huggingface]\",\\n ]\\n conda = [\\n-\\t\"unsloth[huggingface]\",\\n+    \"unsloth[huggingface]\",\\n ]\\n colab = [\\n-\\t\"unsloth[cu121]\",\\n+    \"unsloth[cu121]\",\\n+]\\n+colab_ampere = [\\n+    \"unsloth[cu121]\",\\n+    \"flash-attn\",\\n ]\\n cu118_ampere = [\\n-\\t\"unsloth[huggingface]\",\\n-\\t\"bitsandbytes\",\\n-\\t\"unsloth[cu118only]\",\\n-\\t\"ninja\",\\n-\\t\"flash-attn\",\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118only]\",\\n+    \"flash-attn\",\\n ]\\n cu121_ampere = [\\n-\\t\"unsloth[huggingface]\",\\n-\\t\"bitsandbytes\",\\n-\\t\"unsloth[cu121only]\",\\n-\\t\"ninja\",\\n-\\t\"flash-attn\",\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121only]\",\\n+    \"flash-attn\",\\n ]\\n-colab_ampere = [\\n-\\t\"unsloth[cu121]\",\\n-\\t\"ninja\",\\n-\\t\"flash-attn\",\\n+cu118_ampere_torch211 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118only_torch211]\",\\n+    \"flash-attn\",\\n+]\\n+cu121_ampere_torch211 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121only_torch211]\",\\n+    \"flash-attn\",\\n ]\\n \\n [project.urls]\\n',\n",
       " '@@ -11,7 +11,7 @@\\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n-__version__ = \"2023.12\"\\n+__version__ = \"2024.1\"\\n import os\\n import warnings\\n import importlib\\n',\n",
       " '@@ -369,7 +369,8 @@ def LlamaModel_fast_forward(\\n         raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n-    assert(seq_length <= self.max_seq_length)\\n+    if hasattr(self, \"max_seq_length\"):\\n+        assert(seq_length <= self.max_seq_length)\\n     past_key_values_length = 0\\n \\n     if past_key_values is not None:\\n@@ -690,7 +691,14 @@ class FastLlamaModel:\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n+        # Save max_seq_length\\n         model.max_seq_length = max_position_embeddings\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model.max_seq_length = max_position_embeddings\\n+            internal_model = internal_model.model\\n+        pass\\n+        internal_model.max_seq_length = max_position_embeddings\\n         return model, tokenizer\\n     pass\\n \\n@@ -757,9 +765,9 @@ class FastLlamaModel:\\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n-            raise TypeError(\"Unsloth: Fast Llama patching only works with dropout = 0.\")\\n+            raise TypeError(\"Unsloth: Fast model patching only works with dropout = 0.\")\\n         if bias != \"none\":\\n-            raise TypeError(\"Unsloth: Fast Llama patching only works with bias = \\'none\\'.\")\\n+            raise TypeError(\"Unsloth: Fast model patching only works with bias = \\'none\\'.\")\\n \\n         transformers_set_seed(random_state)\\n \\n',\n",
       " '@@ -15,6 +15,21 @@\\n from .llama import FastLlamaModel, logger\\n from .mistral import FastMistralModel\\n from transformers import AutoConfig\\n+from transformers import __version__ as transformers_version\\n+\\n+FOURBIT_MAPPER = \\\\\\n+{\\n+    \"unsloth/mistral-7b-bnb-4bit\"    : \"unsloth/mistral-7b\",\\n+    \"unsloth/llama-2-7b-bnb-4bit\"    : \"unsloth/llama-2-7b\",\\n+    \"unsloth/llama-2-13b-bnb-4bit\"   : \"unsloth/llama-13-7b\",\\n+    \"unsloth/codellama-34b-bnb-4bit\" : \"codellama/CodeLlama-34b-hf\",\\n+}\\n+\\n+# https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n+major, minor = transformers_version.split(\".\")[:2]\\n+major, minor = int(major), int(minor)\\n+SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)\\n+del major, minor\\n \\n \\n class FastLanguageModel(FastLlamaModel):\\n@@ -29,36 +44,37 @@ class FastLanguageModel(FastLlamaModel):\\n         rope_scaling = None,\\n         *args, **kwargs,\\n     ):\\n+        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:\\n+            model_name = FOURBIT_MAPPER[model_name]\\n+            logger.warning_once(\\n+                f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n+                f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n+                f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+                f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n+                f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n+            )\\n+        pass\\n+\\n         model_config = AutoConfig.from_pretrained(model_name)\\n         model_type = model_config.model_type\\n \\n-        if model_type == \"llama\":\\n-            return FastLlamaModel.from_pretrained(\\n-                model_name = model_name,\\n-                max_seq_length = max_seq_length,\\n-                dtype = dtype,\\n-                load_in_4bit = load_in_4bit,\\n-                token = token,\\n-                device_map = device_map,\\n-                rope_scaling = rope_scaling,\\n-                *args, **kwargs,\\n-            )\\n-        elif model_type == \"mistral\":\\n-            if rope_scaling is not None:\\n-                logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-            return FastMistralModel.from_pretrained(\\n-                model_name = model_name,\\n-                max_seq_length = max_seq_length,\\n-                dtype = dtype,\\n-                load_in_4bit = load_in_4bit,\\n-                token = token,\\n-                device_map = device_map,\\n-                *args, **kwargs,\\n-            )\\n+        if   model_type == \"llama\":   dispatch_model = FastLlamaModel\\n+        elif model_type == \"mistral\": dispatch_model = FastMistralModel\\n         else:\\n             raise NotImplementedError(\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n                 \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n+\\n+        return dispatch_model.from_pretrained(\\n+            model_name = model_name,\\n+            max_seq_length = max_seq_length,\\n+            dtype = dtype,\\n+            load_in_4bit = load_in_4bit,\\n+            token = token,\\n+            device_map = device_map,\\n+            rope_scaling = rope_scaling,\\n+            *args, **kwargs,\\n+        )\\n     pass\\n pass\\n',\n",
       " '@@ -243,7 +243,7 @@ class FastMistralModel(FastLlamaModel):\\n         MistralDecoderLayer   .forward = LlamaDecoderLayer_fast_forward\\n         MistralModel          .forward = LlamaModel_fast_forward\\n         MistralForCausalLM    .forward = MistralForCausalLM_fast_forward\\n-        PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n+        PeftModelForCausalLM  .forward = PeftModelForCausalLM_fast_forward\\n         return\\n     pass\\n \\n@@ -256,8 +256,11 @@ class FastMistralModel(FastLlamaModel):\\n         load_in_4bit = True,\\n         token = None,\\n         device_map = \"sequential\",\\n-        # rope_scaling = None, Mistral does not support RoPE scaling\\n-    ):\\n+        rope_scaling = None, # Mistral does not support RoPE scaling\\n+    ): \\n+        if rope_scaling is not None:\\n+            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n+\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n@@ -312,7 +315,15 @@ class FastMistralModel(FastLlamaModel):\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n-        model.max_seq_length = max(max_seq_length, model.config.max_position_embeddings)\\n+        # Save max_seq_length\\n+        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n+        model.max_seq_length = max_position_embeddings\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model.max_seq_length = max_position_embeddings\\n+            internal_model = internal_model.model\\n+        pass\\n+        internal_model.max_seq_length = max_position_embeddings\\n         return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -17,10 +17,16 @@ from .utils import fast_dequantize, QUANT_STATE\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n def get_lora_parameters(proj):\\n-    active_adapter = proj.active_adapters[0] if \\\\\\n-        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n+    # For DPO or disabled adapters\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n+\\n+    if proj.disable_adapters or proj.merged:\\n+        return W, QUANT_STATE(W), None, None, None\\n+    pass\\n+\\n+    active_adapter = proj.active_adapters[0] if \\\\\\n+        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n     A = proj.lora_A [active_adapter].weight\\n     B = proj.lora_B [active_adapter].weight\\n     s = proj.scaling[active_adapter]\\n@@ -31,7 +37,6 @@ pass\\n def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n     dtype = X.dtype\\n     W = fast_dequantize(W.t(), W_quant)\\n-    A, B = A.t(), B.t()\\n \\n     if X.dim() == 3:\\n         batch, seq_len, d = X.shape\\n@@ -43,7 +48,13 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n \\n     out = torch.matmul(X, W, out = out)\\n     if W_quant is not None: del W\\n-    out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n     return out.view(batch, seq_len, -1) if reshape else out\\n pass\\n \\n',\n",
       " '@@ -369,8 +369,21 @@ def LlamaModel_fast_forward(\\n         raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n+\\n+    # Fix out of bounds tokenization\\n     if hasattr(self, \"max_seq_length\"):\\n-        assert(seq_length <= self.max_seq_length)\\n+        if seq_length > self.max_seq_length:\\n+            logger.warning_once(\\n+                f\"Unsloth: Input IDs of length {seq_length} > the model\\'s max sequence length of {self.max_seq_length}.\\\\n\"\\\\\\n+                \"We shall truncate it ourselves. It\\'s imperative if you correct this issue first.\"\\n+            )\\n+        if input_ids is not None:\\n+            input_ids = input_ids[:,:self.max_seq_length]\\n+        elif inputs_embeds is not None:\\n+            inputs_embeds = inputs_embeds[:,:self.max_seq_length,:]\\n+        pass\\n+    pass\\n+    \\n     past_key_values_length = 0\\n \\n     if past_key_values is not None:\\n',\n",
       " '@@ -97,21 +97,36 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n+        M = bsz * q_len\\n+\\n+        has_sliding_window = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            if hidden_states.requires_grad:\\n-                # Xformers does not support backward, so we have to convert\\n-                # GQA to MQA by cloning K and V\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-            else:\\n-                # Xformers does support the forward pass though\\n-                Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        # if n_groups != 1:\\n+        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        if hidden_states.requires_grad:\\n+            # Xformers does not support backward, so we have to convert\\n+            # GQA to MQA by cloning K and V\\n+            K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+            V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_heads, head_dim)\\n+                K = K.view(1, M, n_heads, head_dim)\\n+                V = V.view(1, M, n_heads, head_dim)\\n+            pass\\n+        else:\\n+            # Xformers does support the forward pass though\\n+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+            pass\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n@@ -131,12 +146,12 @@ def MistralAttention_fast_forward(\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n-        if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n-        pass\\n+        # if n_groups != 1:\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n         A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)\\n',\n",
       " '@@ -17,10 +17,16 @@ from .utils import fast_dequantize, QUANT_STATE\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n def get_lora_parameters(proj):\\n-    active_adapter = proj.active_adapters[0] if \\\\\\n-        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n+    # For DPO or disabled adapters\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n+\\n+    if proj.disable_adapters or proj.merged:\\n+        return W, QUANT_STATE(W), None, None, None\\n+    pass\\n+\\n+    active_adapter = proj.active_adapters[0] if \\\\\\n+        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n     A = proj.lora_A [active_adapter].weight\\n     B = proj.lora_B [active_adapter].weight\\n     s = proj.scaling[active_adapter]\\n@@ -31,7 +37,6 @@ pass\\n def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n     dtype = X.dtype\\n     W = fast_dequantize(W.t(), W_quant)\\n-    A, B = A.t(), B.t()\\n \\n     if X.dim() == 3:\\n         batch, seq_len, d = X.shape\\n@@ -43,7 +48,13 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n \\n     out = torch.matmul(X, W, out = out)\\n     if W_quant is not None: del W\\n-    out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n     return out.view(batch, seq_len, -1) if reshape else out\\n pass\\n \\n',\n",
       " '@@ -369,8 +369,21 @@ def LlamaModel_fast_forward(\\n         raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n+\\n+    # Fix out of bounds tokenization\\n     if hasattr(self, \"max_seq_length\"):\\n-        assert(seq_length <= self.max_seq_length)\\n+        if seq_length > self.max_seq_length:\\n+            logger.warning_once(\\n+                f\"Unsloth: Input IDs of length {seq_length} > the model\\'s max sequence length of {self.max_seq_length}.\\\\n\"\\\\\\n+                \"We shall truncate it ourselves. It\\'s imperative if you correct this issue first.\"\\n+            )\\n+        if input_ids is not None:\\n+            input_ids = input_ids[:,:self.max_seq_length]\\n+        elif inputs_embeds is not None:\\n+            inputs_embeds = inputs_embeds[:,:self.max_seq_length,:]\\n+        pass\\n+    pass\\n+    \\n     past_key_values_length = 0\\n \\n     if past_key_values is not None:\\n',\n",
       " '@@ -97,21 +97,36 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n+        M = bsz * q_len\\n+\\n+        has_sliding_window = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            if hidden_states.requires_grad:\\n-                # Xformers does not support backward, so we have to convert\\n-                # GQA to MQA by cloning K and V\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-            else:\\n-                # Xformers does support the forward pass though\\n-                Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        # if n_groups != 1:\\n+        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        if hidden_states.requires_grad:\\n+            # Xformers does not support backward, so we have to convert\\n+            # GQA to MQA by cloning K and V\\n+            K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+            V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_heads, head_dim)\\n+                K = K.view(1, M, n_heads, head_dim)\\n+                V = V.view(1, M, n_heads, head_dim)\\n+            pass\\n+        else:\\n+            # Xformers does support the forward pass though\\n+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+            pass\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n@@ -131,12 +146,12 @@ def MistralAttention_fast_forward(\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n-        if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n-        pass\\n+        # if n_groups != 1:\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n         A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)\\n',\n",
       " '@@ -17,10 +17,16 @@ from .utils import fast_dequantize, QUANT_STATE\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n def get_lora_parameters(proj):\\n-    active_adapter = proj.active_adapters[0] if \\\\\\n-        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n+    # For DPO or disabled adapters\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n+\\n+    if proj.disable_adapters or proj.merged:\\n+        return W, QUANT_STATE(W), None, None, None\\n+    pass\\n+\\n+    active_adapter = proj.active_adapters[0] if \\\\\\n+        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n     A = proj.lora_A [active_adapter].weight\\n     B = proj.lora_B [active_adapter].weight\\n     s = proj.scaling[active_adapter]\\n@@ -31,7 +37,6 @@ pass\\n def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n     dtype = X.dtype\\n     W = fast_dequantize(W.t(), W_quant)\\n-    A, B = A.t(), B.t()\\n \\n     if X.dim() == 3:\\n         batch, seq_len, d = X.shape\\n@@ -43,7 +48,13 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n \\n     out = torch.matmul(X, W, out = out)\\n     if W_quant is not None: del W\\n-    out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n     return out.view(batch, seq_len, -1) if reshape else out\\n pass\\n \\n',\n",
       " '@@ -369,8 +369,21 @@ def LlamaModel_fast_forward(\\n         raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n+\\n+    # Fix out of bounds tokenization\\n     if hasattr(self, \"max_seq_length\"):\\n-        assert(seq_length <= self.max_seq_length)\\n+        if seq_length > self.max_seq_length:\\n+            logger.warning_once(\\n+                f\"Unsloth: Input IDs of length {seq_length} > the model\\'s max sequence length of {self.max_seq_length}.\\\\n\"\\\\\\n+                \"We shall truncate it ourselves. It\\'s imperative if you correct this issue first.\"\\n+            )\\n+        if input_ids is not None:\\n+            input_ids = input_ids[:,:self.max_seq_length]\\n+        elif inputs_embeds is not None:\\n+            inputs_embeds = inputs_embeds[:,:self.max_seq_length,:]\\n+        pass\\n+    pass\\n+    \\n     past_key_values_length = 0\\n \\n     if past_key_values is not None:\\n',\n",
       " '@@ -97,21 +97,36 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n+        M = bsz * q_len\\n+\\n+        has_sliding_window = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            if hidden_states.requires_grad:\\n-                # Xformers does not support backward, so we have to convert\\n-                # GQA to MQA by cloning K and V\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-            else:\\n-                # Xformers does support the forward pass though\\n-                Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        # if n_groups != 1:\\n+        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        if hidden_states.requires_grad:\\n+            # Xformers does not support backward, so we have to convert\\n+            # GQA to MQA by cloning K and V\\n+            K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+            V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_heads, head_dim)\\n+                K = K.view(1, M, n_heads, head_dim)\\n+                V = V.view(1, M, n_heads, head_dim)\\n+            pass\\n+        else:\\n+            # Xformers does support the forward pass though\\n+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+            pass\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n@@ -131,12 +146,12 @@ def MistralAttention_fast_forward(\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n-        if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n-        pass\\n+        # if n_groups != 1:\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n         A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)\\n',\n",
       " '@@ -17,10 +17,16 @@ from .utils import fast_dequantize, QUANT_STATE\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n def get_lora_parameters(proj):\\n-    active_adapter = proj.active_adapters[0] if \\\\\\n-        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n+    # For DPO or disabled adapters\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n+\\n+    if proj.disable_adapters or proj.merged:\\n+        return W, QUANT_STATE(W), None, None, None\\n+    pass\\n+\\n+    active_adapter = proj.active_adapters[0] if \\\\\\n+        hasattr(proj, \"active_adapters\") else proj.active_adapter\\n     A = proj.lora_A [active_adapter].weight\\n     B = proj.lora_B [active_adapter].weight\\n     s = proj.scaling[active_adapter]\\n@@ -31,7 +37,6 @@ pass\\n def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n     dtype = X.dtype\\n     W = fast_dequantize(W.t(), W_quant)\\n-    A, B = A.t(), B.t()\\n \\n     if X.dim() == 3:\\n         batch, seq_len, d = X.shape\\n@@ -43,7 +48,13 @@ def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n \\n     out = torch.matmul(X, W, out = out)\\n     if W_quant is not None: del W\\n-    out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n     return out.view(batch, seq_len, -1) if reshape else out\\n pass\\n \\n',\n",
       " '@@ -369,8 +369,21 @@ def LlamaModel_fast_forward(\\n         raise ValueError(\"Unsloth: You have to specify either decoder_input_ids or decoder_inputs_embeds\")\\n \\n     seq_length_with_past = seq_length\\n+\\n+    # Fix out of bounds tokenization\\n     if hasattr(self, \"max_seq_length\"):\\n-        assert(seq_length <= self.max_seq_length)\\n+        if seq_length > self.max_seq_length:\\n+            logger.warning_once(\\n+                f\"Unsloth: Input IDs of length {seq_length} > the model\\'s max sequence length of {self.max_seq_length}.\\\\n\"\\\\\\n+                \"We shall truncate it ourselves. It\\'s imperative if you correct this issue first.\"\\n+            )\\n+        if input_ids is not None:\\n+            input_ids = input_ids[:,:self.max_seq_length]\\n+        elif inputs_embeds is not None:\\n+            inputs_embeds = inputs_embeds[:,:self.max_seq_length,:]\\n+        pass\\n+    pass\\n+    \\n     past_key_values_length = 0\\n \\n     if past_key_values is not None:\\n',\n",
       " '@@ -97,21 +97,36 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n+        M = bsz * q_len\\n+\\n+        has_sliding_window = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            if hidden_states.requires_grad:\\n-                # Xformers does not support backward, so we have to convert\\n-                # GQA to MQA by cloning K and V\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n-            else:\\n-                # Xformers does support the forward pass though\\n-                Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        # if n_groups != 1:\\n+        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        if hidden_states.requires_grad:\\n+            # Xformers does not support backward, so we have to convert\\n+            # GQA to MQA by cloning K and V\\n+            K = K.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+            V = V.reshape(bsz, q_len, n_heads, head_dim) # A copy will be made\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_heads, head_dim)\\n+                K = K.view(1, M, n_heads, head_dim)\\n+                V = V.view(1, M, n_heads, head_dim)\\n+            pass\\n+        else:\\n+            # Xformers does support the forward pass though\\n+            Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+\\n+            if has_sliding_window:\\n+                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+            pass\\n         pass\\n \\n         A = xformers_attention(Q, K, V, attn_bias = causal_mask)\\n@@ -131,12 +146,12 @@ def MistralAttention_fast_forward(\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n-        if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n-        pass\\n+        # if n_groups != 1:\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n         A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)\\n',\n",
       " '@@ -15,7 +15,9 @@\\n import triton\\n import triton.language as tl\\n import torch\\n-from .utils import calculate_settings\\n+from .utils import calculate_settings, MAX_FUSED_SIZE\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n \\n @triton.jit\\n def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n@@ -145,6 +147,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):\\n pass\\n \\n \\n+slow_cross_entropy_loss = torch.nn.functional.cross_entropy\\n def fast_cross_entropy_loss(logits, labels):\\n     \"\"\"\\n     Arguments:\\n@@ -156,10 +159,25 @@ def fast_cross_entropy_loss(logits, labels):\\n     batch, seq_len, d = logits.shape\\n     assert(labels.shape == (batch, seq_len))\\n \\n-    loss = Fast_CrossEntropyLoss.apply(\\n-        logits.view(batch*seq_len, d),\\n-        labels.view(-1),\\n-    )\\n-    n_items = torch.count_nonzero(labels != -100)\\n-    return loss.sum() / n_items\\n+    # Prelim support Qwen, Deepseek other large vocab sizes > 2^16\\n+    if d > MAX_FUSED_SIZE:\\n+        logger.warning_once(\\n+            f\"Unsloth: Vocab size of {d} exceeds the max CUDA blocksize of {MAX_FUSED_SIZE}.\\\\n\"\\\\\\n+            \"For now, Unsloth will use Pytorch\\'s CrossEntropyLoss, which will entail a\\\\n\"\\\\\\n+            \"25% increase in memory usage and be slower. Make an issue on \\\\n\"\\\\\\n+            \"Unsloth\\'s Github page if you want a faster and more memory efficient kernel!\"\\n+        )\\n+        loss = slow_cross_entropy_loss(\\n+            logits.float().view(batch*seq_len, d), # Must cast to float32 for numerical stability\\n+            labels.view(-1),\\n+        )\\n+        return loss\\n+    else:\\n+        loss = Fast_CrossEntropyLoss.apply(\\n+            logits.view(batch*seq_len, d),\\n+            labels.view(-1),\\n+        )\\n+        n_items = torch.count_nonzero(labels != -100)\\n+        return loss.sum() / n_items\\n+    pass\\n pass\\n',\n",
       " '@@ -16,6 +16,7 @@ import torch\\n from .utils import fast_dequantize, QUANT_STATE\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n+\\n def get_lora_parameters(proj):\\n     # For DPO or disabled adapters\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n@@ -104,7 +105,7 @@ class LoRA_MLP(torch.autograd.Function):\\n         dtype = X.dtype\\n \\n         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)\\n-        g = matmul_lora(X,   upW,   upW_quant,  upA,   upB,   upS)\\n+        g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\\n         h = swiglu_fg_kernel(e, g)\\n         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)\\n \\n@@ -123,10 +124,10 @@ class LoRA_MLP(torch.autograd.Function):\\n     def backward(ctx, dY : torch.Tensor):\\n         gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, = \\\\\\n             ctx.custom_saved_tensors\\n-        gateA, gateB, upA,upB, downA, downB, \\\\\\n+        gateA, gateB, upA, upB, downA, downB, \\\\\\n             X, e, g = ctx.saved_tensors\\n \\n-        gateA, gateB, upA,upB, downA, downB = \\\\\\n+        gateA, gateB, upA, upB, downA, downB = \\\\\\n             gateA.t(), gateB.t(), upA.t(), upB.t(), downA.t(), downB.t()\\n \\n         batch, seq_len, hd = X.shape\\n',\n",
       " '@@ -23,7 +23,7 @@ from transformers.models.llama.modeling_llama import logger\\n from platform import system as platform_system\\n platform_system = platform_system()\\n \\n-__version__ = \"2023.12\"\\n+__version__ = \"2024.1\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n',\n",
       " '@@ -817,29 +817,59 @@ class FastLlamaModel:\\n         for idx, layer in enumerate(model.model.model.layers):\\n \\n             # MLP patching\\n-            if  hasattr(layer.mlp.gate_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.mlp.  up_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.mlp.down_proj, \"lora_A\"):\\n+            gate_proj = layer.mlp.gate_proj\\n+            up_proj   = layer.mlp.  up_proj\\n+            down_proj = layer.mlp.down_proj\\n+\\n+            if  hasattr(gate_proj, \"lora_A\") and \\\\\\n+                hasattr(  up_proj, \"lora_A\") and \\\\\\n+                hasattr(down_proj, \"lora_A\") and \\\\\\n+                (gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None and \\\\\\n+                (  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None and \\\\\\n+                (down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None:\\n \\n                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n                 n_mlp += 1\\n+            else:\\n+                logger.warning_once(\\n+                    \"Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\\\\n\"\\\\\\n+                    \"are not enabled or a bias term (like in Qwen) is used.\"\\n+                )\\n             pass\\n \\n             # QKV attention patching\\n-            if  hasattr(layer.self_attn.q_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.self_attn.k_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.self_attn.v_proj, \"lora_A\"):\\n+            q_proj = layer.self_attn.q_proj\\n+            k_proj = layer.self_attn.k_proj\\n+            v_proj = layer.self_attn.v_proj\\n+            if  hasattr(q_proj, \"lora_A\") and \\\\\\n+                hasattr(k_proj, \"lora_A\") and \\\\\\n+                hasattr(v_proj, \"lora_A\") and \\\\\\n+                (q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None and \\\\\\n+                (k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None and \\\\\\n+                (v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None:\\n \\n                 layer.self_attn.apply_qkv = apply_lora_qkv\\n                 n_qkv += 1\\n+            else:\\n+                logger.warning_once(\\n+                    \"Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\\\\n\"\\\\\\n+                    \"are not enabled or a bias term (like in Qwen) is used.\"\\n+                )\\n             pass\\n \\n             # O attention patching\\n-            if hasattr(layer.self_attn.o_proj, \"lora_A\"):\\n+            o_proj = layer.self_attn.o_proj\\n+            if hasattr(o_proj, \"lora_A\") and \\\\\\n+                (o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None:\\n \\n                 layer.self_attn.apply_o = apply_lora_o\\n                 n_o += 1\\n+            else:\\n+                logger.warning_once(\\n+                    \"Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\\\\n\"\\\\\\n+                    \"are not enabled or a bias term (like in Qwen) is used.\"\\n+                )\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -15,7 +15,9 @@\\n import triton\\n import triton.language as tl\\n import torch\\n-from .utils import calculate_settings\\n+from .utils import calculate_settings, MAX_FUSED_SIZE\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n \\n @triton.jit\\n def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n@@ -145,6 +147,7 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):\\n pass\\n \\n \\n+slow_cross_entropy_loss = torch.nn.functional.cross_entropy\\n def fast_cross_entropy_loss(logits, labels):\\n     \"\"\"\\n     Arguments:\\n@@ -156,10 +159,25 @@ def fast_cross_entropy_loss(logits, labels):\\n     batch, seq_len, d = logits.shape\\n     assert(labels.shape == (batch, seq_len))\\n \\n-    loss = Fast_CrossEntropyLoss.apply(\\n-        logits.view(batch*seq_len, d),\\n-        labels.view(-1),\\n-    )\\n-    n_items = torch.count_nonzero(labels != -100)\\n-    return loss.sum() / n_items\\n+    # Prelim support Qwen, Deepseek other large vocab sizes > 2^16\\n+    if d > MAX_FUSED_SIZE:\\n+        logger.warning_once(\\n+            f\"Unsloth: Vocab size of {d} exceeds the max CUDA blocksize of {MAX_FUSED_SIZE}.\\\\n\"\\\\\\n+            \"For now, Unsloth will use Pytorch\\'s CrossEntropyLoss, which will entail a\\\\n\"\\\\\\n+            \"25% increase in memory usage and be slower. Make an issue on \\\\n\"\\\\\\n+            \"Unsloth\\'s Github page if you want a faster and more memory efficient kernel!\"\\n+        )\\n+        loss = slow_cross_entropy_loss(\\n+            logits.float().view(batch*seq_len, d), # Must cast to float32 for numerical stability\\n+            labels.view(-1),\\n+        )\\n+        return loss\\n+    else:\\n+        loss = Fast_CrossEntropyLoss.apply(\\n+            logits.view(batch*seq_len, d),\\n+            labels.view(-1),\\n+        )\\n+        n_items = torch.count_nonzero(labels != -100)\\n+        return loss.sum() / n_items\\n+    pass\\n pass\\n',\n",
       " '@@ -16,6 +16,7 @@ import torch\\n from .utils import fast_dequantize, QUANT_STATE\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n+\\n def get_lora_parameters(proj):\\n     # For DPO or disabled adapters\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n@@ -104,7 +105,7 @@ class LoRA_MLP(torch.autograd.Function):\\n         dtype = X.dtype\\n \\n         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)\\n-        g = matmul_lora(X,   upW,   upW_quant,  upA,   upB,   upS)\\n+        g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\\n         h = swiglu_fg_kernel(e, g)\\n         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)\\n \\n@@ -123,10 +124,10 @@ class LoRA_MLP(torch.autograd.Function):\\n     def backward(ctx, dY : torch.Tensor):\\n         gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, = \\\\\\n             ctx.custom_saved_tensors\\n-        gateA, gateB, upA,upB, downA, downB, \\\\\\n+        gateA, gateB, upA, upB, downA, downB, \\\\\\n             X, e, g = ctx.saved_tensors\\n \\n-        gateA, gateB, upA,upB, downA, downB = \\\\\\n+        gateA, gateB, upA, upB, downA, downB = \\\\\\n             gateA.t(), gateB.t(), upA.t(), upB.t(), downA.t(), downB.t()\\n \\n         batch, seq_len, hd = X.shape\\n',\n",
       " '@@ -23,7 +23,7 @@ from transformers.models.llama.modeling_llama import logger\\n from platform import system as platform_system\\n platform_system = platform_system()\\n \\n-__version__ = \"2023.12\"\\n+__version__ = \"2024.1\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n',\n",
       " '@@ -817,29 +817,59 @@ class FastLlamaModel:\\n         for idx, layer in enumerate(model.model.model.layers):\\n \\n             # MLP patching\\n-            if  hasattr(layer.mlp.gate_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.mlp.  up_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.mlp.down_proj, \"lora_A\"):\\n+            gate_proj = layer.mlp.gate_proj\\n+            up_proj   = layer.mlp.  up_proj\\n+            down_proj = layer.mlp.down_proj\\n+\\n+            if  hasattr(gate_proj, \"lora_A\") and \\\\\\n+                hasattr(  up_proj, \"lora_A\") and \\\\\\n+                hasattr(down_proj, \"lora_A\") and \\\\\\n+                (gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None and \\\\\\n+                (  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None and \\\\\\n+                (down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None:\\n \\n                 # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                 layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n                 n_mlp += 1\\n+            else:\\n+                logger.warning_once(\\n+                    \"Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\\\\n\"\\\\\\n+                    \"are not enabled or a bias term (like in Qwen) is used.\"\\n+                )\\n             pass\\n \\n             # QKV attention patching\\n-            if  hasattr(layer.self_attn.q_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.self_attn.k_proj, \"lora_A\") and \\\\\\n-                hasattr(layer.self_attn.v_proj, \"lora_A\"):\\n+            q_proj = layer.self_attn.q_proj\\n+            k_proj = layer.self_attn.k_proj\\n+            v_proj = layer.self_attn.v_proj\\n+            if  hasattr(q_proj, \"lora_A\") and \\\\\\n+                hasattr(k_proj, \"lora_A\") and \\\\\\n+                hasattr(v_proj, \"lora_A\") and \\\\\\n+                (q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None and \\\\\\n+                (k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None and \\\\\\n+                (v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None:\\n \\n                 layer.self_attn.apply_qkv = apply_lora_qkv\\n                 n_qkv += 1\\n+            else:\\n+                logger.warning_once(\\n+                    \"Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\\\\n\"\\\\\\n+                    \"are not enabled or a bias term (like in Qwen) is used.\"\\n+                )\\n             pass\\n \\n             # O attention patching\\n-            if hasattr(layer.self_attn.o_proj, \"lora_A\"):\\n+            o_proj = layer.self_attn.o_proj\\n+            if hasattr(o_proj, \"lora_A\") and \\\\\\n+                (o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None:\\n \\n                 layer.self_attn.apply_o = apply_lora_o\\n                 n_o += 1\\n+            else:\\n+                logger.warning_once(\\n+                    \"Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\\\\n\"\\\\\\n+                    \"are not enabled or a bias term (like in Qwen) is used.\"\\n+                )\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -11,17 +11,17 @@\\n | [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing) | [Free Colab Alpaca dataset example](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [Kaggle Alpaca example](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) |\\n | [Colab A100 example](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Colab A100 example](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | (59 more examples if you scroll down) | [Kaggle Slim Orca example](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n \\n-* Supports Llama, Yi, Mistral, CodeLlama, and their derived models (Open Hermes etc).\\n+* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n * All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backpropagation engine**.\\n * **0% loss in accuracy** - no approximation methods - all exact.\\n * No change of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU](https://developer.nvidia.com/cuda-gpus)\\n * **NEW!** Works on **Linux** and **Windows** via WSL.\\n * **NEW!** Support for [DPO (Direct Preference Optimization)](https://arxiv.org/abs/2305.18290), PPO and Reward Modelling via [TRL](https://huggingface.co/docs/trl/dpo_trainer).\\n-* **NEW!** Download 4 bit models 4x faster directly from Huggingface!\\n+* **NEW!** Download 4 bit models 4x faster from Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`.\\n * Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n * Open source version trains 5x faster - check out [Unsloth Max](https://unsloth.ai/) for **30x faster training**!\\n \\n-| 1 A100 40GB | Huggingface | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+| 1 A100 40GB | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n | LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n@@ -88,9 +88,16 @@ max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\\n url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\\n dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\\n \\n+# 4bit pre quantized models we support - 4x faster downloading!\\n+fourbit_models = [\\n+    \"unsloth/mistral-7b-bnb-4bit\",\\n+    \"unsloth/llama-2-7b-bnb-4bit\",\\n+    \"unsloth/llama-2-13b-bnb-4bit\",\\n+    \"unsloth/codellama-34b-bnb-4bit\",\\n+]\\n # Load Llama model\\n model, tokenizer = FastLanguageModel.from_pretrained(\\n-    model_name = \"unsloth/llama-2-7b\", # Supports Llama, Mistral - replace this!\\n+    model_name = \"unsloth/mistral-7b\", # Supports Llama, Mistral - replace this!\\n     max_seq_length = max_seq_length,\\n     dtype = None,\\n     load_in_4bit = True,\\n@@ -133,7 +140,7 @@ trainer.train()\\n ```\\n \\n # DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory).\\n+DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on 1x A100 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n # Future Milestones and limitations\\n 1. Support Mixtral.\\n',\n",
       " '@@ -46,12 +46,12 @@ except:\\n     raise ImportError(\"Pytorch is not installed. Go to https://pytorch.org/.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n-# We only support torch 2.1\\n+# We support torch 2.1 and 2.1.1\\n # Fixes https://github.com/unslothai/unsloth/issues/38\\n torch_version = torch.__version__.split(\".\")\\n major_torch, minor_torch = torch_version[0], torch_version[1]\\n major_torch, minor_torch = int(major_torch), int(minor_torch)\\n-if (major_torch != 2) or (major_torch == 2 and minor_torch < 1):\\n+if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n     raise ImportError(\"Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n',\n",
       " '@@ -37,7 +37,7 @@ huggingface = [\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n-    \"trl\",\\n+    \"trl==0.7.7\",\\n     \"peft\",\\n     \"packaging\",\\n     \"ninja\",\\n',\n",
       " '@@ -83,3 +83,4 @@ except:\\n pass\\n \\n from .models import *\\n+from .save import *\\n',\n",
       " '@@ -17,6 +17,7 @@ from .rms_layernorm import fast_rms_layernorm\\n from .rope_embedding import fast_rope_embedding, inplace_rope_embedding\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n from .fast_lora import (\\n+\\tget_lora_parameters,\\n \\tapply_lora_mlp,\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n',\n",
       " '@@ -0,0 +1,207 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from peft import PeftModelForCausalLM\\n+from collections import OrderedDict\\n+import bitsandbytes as bnb\\n+import peft\\n+import gc\\n+import os\\n+from tqdm import tqdm as ProgressBar\\n+import shutil\\n+from typing import Optional, Callable, Union\\n+import torch\\n+from transformers.models.llama.modeling_llama import logger\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+\\n+__all__ = [\\n+    \"unsloth_save_model\",\\n+    #\"colab_quantize_to_gguf\",\\n+]\\n+\\n+LLAMA_WEIGHTS = (\\n+    \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n+    \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n+)\\n+LLAMA_LAYERNORMS = (\\n+    \"input_layernorm\", \"post_attention_layernorm\",\\n+)\\n+\\n+# From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n+ALLOWED_QUANTS = \\\\\\n+{\\n+    \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+    \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_s\" : \"Uses Q3_K for all tensors\",\\n+    \"q4_0\"   : \"Original quant method, 4-bit.\",\\n+    \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+    \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+    \"q4_k_s\" : \"Uses Q4_K for all tensors\",\\n+    \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\\n+    \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\\n+    \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+    \"q5_k_s\" : \"Uses Q5_K for all tensors\",\\n+    \"q6_k\"   : \"Uses Q8_K for all tensors\",\\n+    \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\\n+}\\n+\\n+\\n+def _merge_lora(layer, name):\\n+    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+        # Is LoRA so we need to merge!\\n+        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n+        W = fast_dequantize(W, quant_state).to(torch.float32).t()\\n+        sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n+        W += sAB\\n+        if not torch.isfinite(W).all():\\n+            raise ValueError(f\"Unsloth: Merge failed.\\\\n{name} has some elements = infinity.\")\\n+        W = W.t().to(dtype)\\n+    else:\\n+        W = layer.weight\\n+    return W\\n+pass\\n+\\n+\\n+@torch.inference_mode\\n+def unsloth_save_model(\\n+    model,\\n+    tokenizer,\\n+    save_directory: Union[str, os.PathLike],\\n+    is_main_process: bool = True,\\n+    state_dict: Optional[dict] = None,\\n+    save_function: Callable = torch.save,\\n+    push_to_hub: bool = False,\\n+    max_shard_size: Union[int, str] = \"7GB\",\\n+    safe_serialization: bool = True,\\n+    variant: Optional[str] = None,\\n+    token: Optional[Union[str, bool]] = None,\\n+    save_peft_format: bool = True,\\n+    temporary_location = \"_unsloth_temporary_saved_buffers\",\\n+    **kwargs,\\n+):\\n+    logger.warning_once(\\n+        \"Unsloth: `unsloth_save_model` is still in development mode.\\\\n\"\\\\\\n+        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n+        \"Also, if you used this successfully, please tell us on Discord!\"\\n+    )\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    assert(hasattr(model, \"model\"))\\n+    assert(hasattr(model.model, \"model\"))\\n+    assert(hasattr(model.model.model, \"layers\"))\\n+\\n+    # HF also uses a OrderedDict\\n+    state_dict = OrderedDict()\\n+    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight\\n+\\n+    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+        for item in LLAMA_WEIGHTS:\\n+            proj = eval(f\"layer.{item}\")\\n+            name = f\"model.layers.{j}.{item}.weight\"\\n+            W = _merge_lora(proj, name)\\n+            filename = os.path.join(temporary_location, f\"{name}.pt\")\\n+            torch.save(W, filename)\\n+            state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n+        pass\\n+        for item in LLAMA_LAYERNORMS:\\n+            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight\")\\n+        pass\\n+    pass\\n+\\n+    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight\\n+    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight\\n+\\n+    print(\"Unsloth: Saving tokenizer...\")\\n+    tokenizer.save_pretrained(\\n+        save_directory = save_directory,\\n+        is_main_process = is_main_process,\\n+        state_dict = state_dict,\\n+        save_function = save_function,\\n+        push_to_hub = push_to_hub,\\n+        max_shard_size = max_shard_size,\\n+        safe_serialization = safe_serialization,\\n+        variant = variant,\\n+        token = token,\\n+        save_peft_format = save_peft_format,\\n+    )\\n+\\n+    print(\"Unsloth: Saving model. This will take 5 minutes for Llama-7b...\")\\n+    model.model.save_pretrained(\\n+        save_directory = save_directory,\\n+        is_main_process = is_main_process,\\n+        state_dict = state_dict,\\n+        save_function = save_function,\\n+        push_to_hub = push_to_hub,\\n+        max_shard_size = max_shard_size,\\n+        safe_serialization = safe_serialization,\\n+        variant = variant,\\n+        token = token,\\n+        save_peft_format = save_peft_format,\\n+    )\\n+\\n+    # Remove temporary location\\n+    shutil.rmtree(temporary_location)\\n+pass\\n+\\n+\\n+\"\"\"\\n+def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n+\\n+    logger.warning_once(\\n+        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\\\n\"\\\\\\n+        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n+        \"Also, if you used this successfully, please tell us on Discord!\"\\n+    )\\n+\\n+    if quantization_method not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+        for key, value in ALLOWED_QUANTS.items():\\n+            error += f\"[{key}] => {value}\\\\n\"\\n+        raise RuntimeError(error)\\n+    pass\\n+\\n+    print_info = \\\\\\n+        f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n+        f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n+        f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\\\n\"\\\\\\n+        f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n+    print(print_info)\\n+\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n+        !git clone https://github.com/ggerganov/llama.cpp\\n+        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\\n+        !pip install gguf protobuf\\n+    pass\\n+\\n+    print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\\n+    !python llama.cpp/convert.py {save_directory} \\\\\\n+        --outfile {save_directory}-unsloth.gguf \\\\\\n+        --outtype f16\\n+\\n+    print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\\n+    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\\n+    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\\\\n+        {final_location} {quantization_method}\\n+\\n+    print(f\"Unsloth: Output location: {final_location}\")\\n+pass\\n+\"\"\"\\n',\n",
       " '@@ -37,7 +37,7 @@ huggingface = [\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n-    \"trl\",\\n+    \"trl==0.7.7\",\\n     \"peft\",\\n     \"packaging\",\\n     \"ninja\",\\n',\n",
       " '@@ -83,3 +83,4 @@ except:\\n pass\\n \\n from .models import *\\n+from .save import *\\n',\n",
       " '@@ -17,6 +17,7 @@ from .rms_layernorm import fast_rms_layernorm\\n from .rope_embedding import fast_rope_embedding, inplace_rope_embedding\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n from .fast_lora import (\\n+\\tget_lora_parameters,\\n \\tapply_lora_mlp,\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n',\n",
       " '@@ -0,0 +1,207 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from peft import PeftModelForCausalLM\\n+from collections import OrderedDict\\n+import bitsandbytes as bnb\\n+import peft\\n+import gc\\n+import os\\n+from tqdm import tqdm as ProgressBar\\n+import shutil\\n+from typing import Optional, Callable, Union\\n+import torch\\n+from transformers.models.llama.modeling_llama import logger\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+\\n+__all__ = [\\n+    \"unsloth_save_model\",\\n+    #\"colab_quantize_to_gguf\",\\n+]\\n+\\n+LLAMA_WEIGHTS = (\\n+    \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n+    \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n+)\\n+LLAMA_LAYERNORMS = (\\n+    \"input_layernorm\", \"post_attention_layernorm\",\\n+)\\n+\\n+# From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n+ALLOWED_QUANTS = \\\\\\n+{\\n+    \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+    \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_s\" : \"Uses Q3_K for all tensors\",\\n+    \"q4_0\"   : \"Original quant method, 4-bit.\",\\n+    \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+    \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+    \"q4_k_s\" : \"Uses Q4_K for all tensors\",\\n+    \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\\n+    \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\\n+    \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+    \"q5_k_s\" : \"Uses Q5_K for all tensors\",\\n+    \"q6_k\"   : \"Uses Q8_K for all tensors\",\\n+    \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\\n+}\\n+\\n+\\n+def _merge_lora(layer, name):\\n+    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+        # Is LoRA so we need to merge!\\n+        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n+        W = fast_dequantize(W, quant_state).to(torch.float32).t()\\n+        sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n+        W += sAB\\n+        if not torch.isfinite(W).all():\\n+            raise ValueError(f\"Unsloth: Merge failed.\\\\n{name} has some elements = infinity.\")\\n+        W = W.t().to(dtype)\\n+    else:\\n+        W = layer.weight\\n+    return W\\n+pass\\n+\\n+\\n+@torch.inference_mode\\n+def unsloth_save_model(\\n+    model,\\n+    tokenizer,\\n+    save_directory: Union[str, os.PathLike],\\n+    is_main_process: bool = True,\\n+    state_dict: Optional[dict] = None,\\n+    save_function: Callable = torch.save,\\n+    push_to_hub: bool = False,\\n+    max_shard_size: Union[int, str] = \"7GB\",\\n+    safe_serialization: bool = True,\\n+    variant: Optional[str] = None,\\n+    token: Optional[Union[str, bool]] = None,\\n+    save_peft_format: bool = True,\\n+    temporary_location = \"_unsloth_temporary_saved_buffers\",\\n+    **kwargs,\\n+):\\n+    logger.warning_once(\\n+        \"Unsloth: `unsloth_save_model` is still in development mode.\\\\n\"\\\\\\n+        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n+        \"Also, if you used this successfully, please tell us on Discord!\"\\n+    )\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    assert(hasattr(model, \"model\"))\\n+    assert(hasattr(model.model, \"model\"))\\n+    assert(hasattr(model.model.model, \"layers\"))\\n+\\n+    # HF also uses a OrderedDict\\n+    state_dict = OrderedDict()\\n+    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight\\n+\\n+    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+        for item in LLAMA_WEIGHTS:\\n+            proj = eval(f\"layer.{item}\")\\n+            name = f\"model.layers.{j}.{item}.weight\"\\n+            W = _merge_lora(proj, name)\\n+            filename = os.path.join(temporary_location, f\"{name}.pt\")\\n+            torch.save(W, filename)\\n+            state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n+        pass\\n+        for item in LLAMA_LAYERNORMS:\\n+            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight\")\\n+        pass\\n+    pass\\n+\\n+    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight\\n+    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight\\n+\\n+    print(\"Unsloth: Saving tokenizer...\")\\n+    tokenizer.save_pretrained(\\n+        save_directory = save_directory,\\n+        is_main_process = is_main_process,\\n+        state_dict = state_dict,\\n+        save_function = save_function,\\n+        push_to_hub = push_to_hub,\\n+        max_shard_size = max_shard_size,\\n+        safe_serialization = safe_serialization,\\n+        variant = variant,\\n+        token = token,\\n+        save_peft_format = save_peft_format,\\n+    )\\n+\\n+    print(\"Unsloth: Saving model. This will take 5 minutes for Llama-7b...\")\\n+    model.model.save_pretrained(\\n+        save_directory = save_directory,\\n+        is_main_process = is_main_process,\\n+        state_dict = state_dict,\\n+        save_function = save_function,\\n+        push_to_hub = push_to_hub,\\n+        max_shard_size = max_shard_size,\\n+        safe_serialization = safe_serialization,\\n+        variant = variant,\\n+        token = token,\\n+        save_peft_format = save_peft_format,\\n+    )\\n+\\n+    # Remove temporary location\\n+    shutil.rmtree(temporary_location)\\n+pass\\n+\\n+\\n+\"\"\"\\n+def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n+\\n+    logger.warning_once(\\n+        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\\\n\"\\\\\\n+        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n+        \"Also, if you used this successfully, please tell us on Discord!\"\\n+    )\\n+\\n+    if quantization_method not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+        for key, value in ALLOWED_QUANTS.items():\\n+            error += f\"[{key}] => {value}\\\\n\"\\n+        raise RuntimeError(error)\\n+    pass\\n+\\n+    print_info = \\\\\\n+        f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n+        f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n+        f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\\\n\"\\\\\\n+        f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n+    print(print_info)\\n+\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n+        !git clone https://github.com/ggerganov/llama.cpp\\n+        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\\n+        !pip install gguf protobuf\\n+    pass\\n+\\n+    print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\\n+    !python llama.cpp/convert.py {save_directory} \\\\\\n+        --outfile {save_directory}-unsloth.gguf \\\\\\n+        --outtype f16\\n+\\n+    print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\\n+    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\\n+    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\\\\n+        {final_location} {quantization_method}\\n+\\n+    print(f\"Unsloth: Output location: {final_location}\")\\n+pass\\n+\"\"\"\\n',\n",
       " '@@ -37,7 +37,7 @@ huggingface = [\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n-    \"trl==0.7.7\",\\n+    \"trl\",\\n     \"peft\",\\n     \"packaging\",\\n     \"ninja\",\\n',\n",
       " '@@ -22,7 +22,7 @@ def get_lora_parameters(proj):\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n \\n-    if proj.disable_adapters or proj.merged:\\n+    if not hasattr(proj, \"disable_adapters\") or proj.disable_adapters or proj.merged:\\n         return W, QUANT_STATE(W), None, None, None\\n     pass\\n \\n',\n",
       " '@@ -116,6 +116,11 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -131,6 +136,11 @@ def check_tokenizer(\\n     # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n     # Seems like the Fast tokenizer in Rust breaks things!\\n \\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n     max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n     added_tokens_fast = tokenizer.added_tokens_decoder\\n     added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n',\n",
       " '@@ -714,6 +714,16 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+\\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.config._name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.config.update({\"_name_or_path\" : name})\\n+        pass\\n+        # Log Unsloth version for future fastpaths for inference\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+\\n         return model, tokenizer\\n     pass\\n \\n',\n",
       " '@@ -343,6 +343,16 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+\\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.config._name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.config.update({\"_name_or_path\" : name})\\n+        pass\\n+        # Log Unsloth version for future fastpaths for inference\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+        \\n         return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -37,7 +37,7 @@ huggingface = [\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n-    \"trl==0.7.7\",\\n+    \"trl\",\\n     \"peft\",\\n     \"packaging\",\\n     \"ninja\",\\n',\n",
       " '@@ -22,7 +22,7 @@ def get_lora_parameters(proj):\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n \\n-    if proj.disable_adapters or proj.merged:\\n+    if not hasattr(proj, \"disable_adapters\") or proj.disable_adapters or proj.merged:\\n         return W, QUANT_STATE(W), None, None, None\\n     pass\\n \\n',\n",
       " '@@ -116,6 +116,11 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -131,6 +136,11 @@ def check_tokenizer(\\n     # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n     # Seems like the Fast tokenizer in Rust breaks things!\\n \\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n     max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n     added_tokens_fast = tokenizer.added_tokens_decoder\\n     added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n',\n",
       " '@@ -714,6 +714,16 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+\\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.config._name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.config.update({\"_name_or_path\" : name})\\n+        pass\\n+        # Log Unsloth version for future fastpaths for inference\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+\\n         return model, tokenizer\\n     pass\\n \\n',\n",
       " '@@ -343,6 +343,16 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+\\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.config._name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.config.update({\"_name_or_path\" : name})\\n+        pass\\n+        # Log Unsloth version for future fastpaths for inference\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+        \\n         return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -37,7 +37,7 @@ huggingface = [\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n-    \"trl==0.7.7\",\\n+    \"trl\",\\n     \"peft\",\\n     \"packaging\",\\n     \"ninja\",\\n',\n",
       " '@@ -22,7 +22,7 @@ def get_lora_parameters(proj):\\n     base_layer = (proj.base_layer if hasattr(proj, \"base_layer\") else proj)\\n     W = base_layer.weight\\n \\n-    if proj.disable_adapters or proj.merged:\\n+    if not hasattr(proj, \"disable_adapters\") or proj.disable_adapters or proj.merged:\\n         return W, QUANT_STATE(W), None, None, None\\n     pass\\n \\n',\n",
       " '@@ -116,6 +116,11 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -131,6 +136,11 @@ def check_tokenizer(\\n     # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n     # Seems like the Fast tokenizer in Rust breaks things!\\n \\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n     max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n     added_tokens_fast = tokenizer.added_tokens_decoder\\n     added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n',\n",
       " '@@ -714,6 +714,16 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+\\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.config._name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.config.update({\"_name_or_path\" : name})\\n+        pass\\n+        # Log Unsloth version for future fastpaths for inference\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+\\n         return model, tokenizer\\n     pass\\n \\n',\n",
       " '@@ -343,6 +343,16 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+\\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.config._name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.config.update({\"_name_or_path\" : name})\\n+        pass\\n+        # Log Unsloth version for future fastpaths for inference\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+        \\n         return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -65,8 +65,7 @@ try:\\n     libcuda_dirs()\\n except:\\n     warnings.warn(\\n-        \"CUDA is not linked properly.\\\\n\"\\\\\\n-        \"We shall run `ldconfig /usr/lib64-nvidia` to try to fix it.\"\\n+        \"Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n     )\\n     os.system(\"ldconfig /usr/lib64-nvidia\")\\n     importlib.reload(bnb)\\n',\n",
       " '@@ -41,12 +41,13 @@ def _rms_layernorm_forward(\\n     r += row_idx * r_row_stride\\n \\n     X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n-    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n-    inv_var = 1 / tl.sqrt(row_var + eps)\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps)\\n     tl.store(r, inv_var)\\n     normed = X_row * inv_var\\n+    normed = normed.to(W_row.dtype) # Exact copy from HF\\n     output = normed * W_row\\n     tl.store(Y + col_offsets, output, mask = mask)\\n pass\\n',\n",
       " '@@ -25,10 +25,11 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     mask = offsets < n_elements\\n \\n     e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row = tl.load(g + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n     f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n \\n@@ -53,12 +54,13 @@ def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n-    DW_row = tl.load(DW + offsets, mask = mask, other = 0).to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row  = tl.load(g  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row))\\n+    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n+    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n     # f = e * se\\n     f_row = e_row * se_row\\n     # h = f * g\\n',\n",
       " '@@ -14,9 +14,7 @@\\n \\n import torch\\n from typing import Union, Optional, List, Any, Callable\\n-import numpy as np\\n import warnings\\n-import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n',\n",
       " '@@ -15,7 +15,6 @@\\n import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n-from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n from transformers.models.llama.modeling_llama import (\\n     logger,\\n     BaseModelOutputWithPast,\\n@@ -46,16 +45,13 @@ except:\\n     LlamaFlashAttention2 = LlamaAttention\\n pass\\n \\n-from peft import PeftModelForCausalLM\\n-import gc\\n-import peft\\n-import bitsandbytes as bnb\\n-import numpy as np\\n-import types\\n-\\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n+from peft import PeftModelForCausalLM\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from ..save import patch_saving_functions\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -110,18 +106,15 @@ def LlamaAttention_fast_forward_inference(\\n     bsz, _, _ = hidden_states.size()\\n     K1, V1 = past_key_value\\n \\n-    Wq = self.q_proj.weight\\n-    Wk = self.k_proj.weight\\n-    Wv = self.v_proj.weight\\n-    Wo = self.o_proj.weight\\n-\\n     n_heads    = self.num_heads\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n     head_dim   = self.head_dim\\n     assert(n_kv_heads * n_groups == n_heads)\\n \\n-    Qn, Kn, Vn = original_apply_qkv(self, Xn)\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n     Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n     Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n     Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n@@ -156,6 +149,28 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_silu = torch.nn.functional.silu\\n+def fast_mlp_inference(self, X):\\n+    gate = self.gate_proj(X)\\n+    up   = self.up_proj(X)\\n+    gate = torch_silu(gate, inplace = True)\\n+    gate *= up\\n+    X = self.down_proj(gate)\\n+    return X\\n+pass\\n+\\n+\\n+def fast_rms_layernorm_inference(self, X):\\n+    X = X.to(torch.float32)\\n+    variance = X.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    X *= variance.rsqrt_()\\n+    X = X.to(residual.dtype)\\n+    X *= self.weight\\n+    return X\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -287,28 +302,51 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    residual = hidden_states\\n-\\n-    hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+    bsz, q_len, hd = hidden_states.size()\\n+\\n+    if (self.training):\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states = residual + hidden_states\\n \\n-    # Self Attention\\n-    hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-        hidden_states=hidden_states,\\n-        causal_mask=causal_mask,\\n-        attention_mask=attention_mask,\\n-        position_ids=position_ids,\\n-        past_key_value=past_key_value,\\n-        output_attentions=output_attentions,\\n-        use_cache=use_cache,\\n-        padding_mask=padding_mask,\\n-    )\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = self.mlp(hidden_states)\\n+        hidden_states = residual + hidden_states\\n+    else:\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states += residual\\n \\n-    # Fully Connected\\n-    residual = hidden_states\\n-    hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n-    hidden_states = self.mlp(hidden_states)\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n+        hidden_states += residual\\n+    pass\\n \\n     outputs = (hidden_states,)\\n \\n@@ -378,6 +416,7 @@ def LlamaModel_fast_forward(\\n     if past_key_values is not None:\\n         past_key_values_length = past_key_values[0][0].shape[2]\\n         seq_length_with_past = seq_length_with_past + past_key_values_length\\n+    pass\\n \\n     # We already handle KV cache position_ids ourselves.\\n     if (past_key_values_length != 0):\\n@@ -391,10 +430,12 @@ def LlamaModel_fast_forward(\\n         position_ids = position_ids.view(-1, seq_length).to(torch.int32)#.long()\\n     else:\\n         position_ids = None\\n+    pass\\n \\n     if position_ids is not None:\\n         if position_ids.shape[0] != batch_size:\\n             position_ids = position_ids.repeat((batch_size, 1))\\n+    pass\\n \\n     # embed positions\\n     if inputs_embeds is None:\\n@@ -403,19 +444,22 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n+    elif self.training:\\n+        attention_mask = None\\n+        padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n             padding_mask = attention_mask\\n         else:\\n             padding_mask = None\\n \\n+        from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n         attention_mask = _prepare_4d_causal_attention_mask(\\n             attention_mask,\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = None if not hasattr(self.config, \"sliding_window\") else \\\\\\n-                self.config.sliding_window,\\n+            sliding_window = getattr(self.config, \"sliding_window\"),\\n         )\\n     pass\\n \\n@@ -479,7 +523,11 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    if (self.training):\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    pass\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -665,6 +713,7 @@ class FastLlamaModel:\\n                 bnb_4bit_quant_type       = \"nf4\",\\n                 bnb_4bit_compute_dtype    = dtype,\\n             )\\n+        pass\\n \\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n@@ -714,6 +763,7 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -721,6 +771,7 @@ class FastLlamaModel:\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+\\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n@@ -751,7 +802,7 @@ class FastLlamaModel:\\n         correct_dtype = lm_head.weight.dtype\\n \\n         for name, module in model.named_modules():\\n-            if isinstance(module, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):\\n                 weight = module.weight\\n                 quant_state = weight.quant_state\\n \\n@@ -766,8 +817,10 @@ class FastLlamaModel:\\n         pass\\n \\n         # Clear deleted GPU items\\n-        gc.collect()\\n-        torch.cuda.empty_cache()\\n+        import gc\\n+        for _ in range(3):\\n+            gc.collect()\\n+            torch.cuda.empty_cache()\\n         return model\\n     pass\\n \\n@@ -782,11 +835,26 @@ class FastLlamaModel:\\n         lora_dropout = 0,\\n         bias = \"none\",\\n         layers_to_transform = None,\\n+        layers_pattern = None,\\n         use_gradient_checkpointing = True,\\n         random_state = 3407,\\n         max_seq_length = 2048, # not used anymore\\n+        use_rslora = False,\\n+        init_lora_weights = True,\\n+        loftq_config = None,\\n         **kwargs,\\n     ):\\n+        if isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n+            )\\n+        pass\\n+\\n+        import inspect\\n+        signature = str(inspect.signature(LoraConfig))\\n+        SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n+        SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n+\\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n@@ -794,11 +862,61 @@ class FastLlamaModel:\\n                 f\"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = {lora_dropout}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n         if bias != \"none\":\\n             logger.warning_once(\\n                 f\"Unsloth: bias = `none` is supported for fast patching. You are using bias = {bias}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n+        if not (type(init_lora_weights) is bool or \\\\\\n+            init_lora_weights == \"gaussian\" or init_lora_weights == \"loftq\"):\\n+            raise ValueError(\\n+                \\'Unsloth: `init_lora_weights` must be either [True, False, \"gaussian\", \"loftq\"].\\'\\n+            )\\n+        pass\\n+\\n+        if init_lora_weights == \"loftq\":\\n+\\n+            if not SUPPORTS_LOFTQ:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+\\n+            if loftq_config is None:\\n+                from peft import LoftQConfig\\n+                logger.warning_once(\\n+                    f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n+                    f\"We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.\"\\n+                )\\n+                loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)\\n+            pass\\n+            \\n+            if hasattr(model.config, \"quantization_config\"):\\n+                raise ValueError(\\n+                    \"Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\\\\n\"\\\\\\n+                    \"Reload your model without any quantization by setting `load_in_4bit = False`.\"\\n+                )\\n+            pass\\n+        pass\\n+\\n+        assert(type(use_rslora) is bool)\\n+        if use_rslora:\\n+            if not SUPPORTS_RSLORA:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+        pass\\n \\n         transformers_set_seed(random_state)\\n \\n@@ -810,16 +928,23 @@ class FastLlamaModel:\\n         pass\\n \\n         # Get LoRA\\n-        lora_config = LoraConfig(\\n-            r              = r,\\n-            lora_alpha     = lora_alpha,\\n-            target_modules = target_modules,\\n-            lora_dropout   = lora_dropout,\\n-            bias           = bias,\\n-            task_type      = TaskType.CAUSAL_LM,\\n+        arguments = dict(\\n+            r                   = r,\\n+            lora_alpha          = lora_alpha,\\n+            target_modules      = target_modules,\\n+            lora_dropout        = lora_dropout,\\n+            bias                = bias,\\n+            task_type           = TaskType.CAUSAL_LM,\\n             layers_to_transform = layers_to_transform,\\n+            init_lora_weights   = init_lora_weights,\\n+            loftq_config        = loftq_config,\\n+            use_rslora          = use_rslora,\\n             **kwargs,\\n         )\\n+        if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n+        if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n+\\n+        lora_config = LoraConfig(**arguments)\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n@@ -828,10 +953,21 @@ class FastLlamaModel:\\n         )\\n         model = _get_peft_model(model, lora_config)\\n \\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.peft_config[\"default\"].base_model_name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        pass\\n+        # Add revision to enable future fast inference paths\\n+        model.peft_config[\"default\"].revision = f\"unsloth\"\\n+\\n         # Do patching\\n         n_mlp = 0\\n         n_qkv = 0\\n         n_o   = 0\\n+        import types\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n@@ -897,6 +1033,7 @@ class FastLlamaModel:\\n             f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n             f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n         )\\n+        patch_saving_functions(model)\\n \\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n',\n",
       " '@@ -16,16 +16,9 @@ from .llama import FastLlamaModel, logger\\n from .mistral import FastMistralModel\\n from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n+from peft import PeftConfig, PeftModel\\n+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n \\n-FOURBIT_MAPPER = \\\\\\n-{\\n-    \"unsloth/mistral-7b-bnb-4bit\"    : \"unsloth/mistral-7b\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\"    : \"unsloth/llama-2-7b\",\\n-    \"unsloth/llama-2-13b-bnb-4bit\"   : \"unsloth/llama-13-7b\",\\n-    \"unsloth/codellama-34b-bnb-4bit\" : \"codellama/CodeLlama-34b-hf\",\\n-    \"unsloth/zephyr-sft-bnb-4bit\"    : \"unsloth/zephyr-sft\",\\n-    \"unsloth/tinyllama-bnb-4bit\"     : \"unsloth/tinyllama\",\\n-}\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -34,6 +27,39 @@ SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)\\n del major, minor\\n \\n \\n+def _get_model_name(model_name, load_in_4bit = True):\\n+\\n+    if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n+            f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n+            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n+            f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n+        )\\n+    \\n+    elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n+            f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n+        )\\n+        model_name = new_model_name\\n+\\n+    elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n+            f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n+        )\\n+        model_name = new_model_name\\n+    pass\\n+\\n+    return model_name\\n+pass\\n+\\n+\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n@@ -47,25 +73,27 @@ class FastLanguageModel(FastLlamaModel):\\n         fix_tokenizer = True,\\n         *args, **kwargs,\\n     ):\\n-        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:\\n-            model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n-                f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-                f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n-                f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n-                f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n-            )\\n-        elif not load_in_4bit and model_name in FOURBIT_MAPPER:\\n-            new_model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n-                f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n-            )\\n-            model_name = new_model_name\\n+        old_model_name = model_name\\n+        model_name = _get_model_name(model_name, load_in_4bit)\\n+\\n+        # First check if it\\'s a normal model via AutoConfig\\n+        is_peft = False\\n+        try:\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = False\\n+        except:\\n+            try:\\n+                # Most likely a PEFT model\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+            except:\\n+                raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n+            \\n+            # Check base model again for PEFT\\n+            model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = True\\n         pass\\n \\n-        model_config = AutoConfig.from_pretrained(model_name)\\n         model_type = model_config.model_type\\n \\n         if   model_type == \"llama\":   dispatch_model = FastLlamaModel\\n@@ -75,8 +103,9 @@ class FastLanguageModel(FastLlamaModel):\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n                 \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n+        pass\\n \\n-        return dispatch_model.from_pretrained(\\n+        model, tokenizer = dispatch_model.from_pretrained(\\n             model_name = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype = dtype,\\n@@ -87,5 +116,30 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n+\\n+        if load_in_4bit:\\n+            # Fix up bitsandbytes config\\n+            quantization_config = \\\\\\n+            {\\n+                # Sometimes torch_dtype is not a string!!\\n+                \"bnb_4bit_compute_dtype\"           : model.config.to_dict()[\"torch_dtype\"],\\n+                \"bnb_4bit_quant_type\"              : \"nf4\",\\n+                \"bnb_4bit_use_double_quant\"        : True,\\n+                \"llm_int8_enable_fp32_cpu_offload\" : False,\\n+                \"llm_int8_has_fp16_weight\"         : False,\\n+                \"llm_int8_skip_modules\"            : \"null\",\\n+                \"llm_int8_threshold\"               : 6.0,\\n+                \"load_in_4bit\"                     : True,\\n+                \"load_in_8bit\"                     : False,\\n+                \"quant_method\"                     : \"bitsandbytes\",\\n+            }\\n+            model.config.update({\"quantization_config\" : quantization_config})\\n+        pass\\n+\\n+        if is_peft:\\n+            # Now add PEFT adapters\\n+            model = PeftModel.from_pretrained(model, old_model_name)\\n+        pass\\n+        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -0,0 +1,56 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+__all__ = [\\n+    \"INT_TO_FLOAT_MAPPER\",\\n+    \"FLOAT_TO_INT_MAPPER\",\\n+]\\n+\\n+__INT_TO_FLOAT_MAPPER = \\\\\\n+{\\n+    \"unsloth/mistral-7b-bnb-4bit\"    : (\\n+        \"unsloth/mistral-7b\",\\n+        \"mistralai/Mistral-7B-v0.1\",\\n+    ),\\n+    \"unsloth/llama-2-7b-bnb-4bit\"    : (\\n+        \"unsloth/llama-2-7b\",\\n+        \"meta-llama/Llama-2-7b-hf\",\\n+    ),\\n+    \"unsloth/llama-2-13b-bnb-4bit\"   : (\\n+        \"unsloth/llama-13-7b\",\\n+        \"meta-llama/Llama-2-13b-hf\",\\n+    ),\\n+    \"unsloth/codellama-34b-bnb-4bit\" : (\\n+        \"codellama/CodeLlama-34b-hf\",\\n+    ),\\n+    \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n+        \"unsloth/zephyr-sft\",\\n+        \"alignment-handbook/zephyr-7b-sft-full\",\\n+    ),\\n+    \"unsloth/tinyllama-bnb-4bit\"     : (\\n+        \"unsloth/tinyllama\",\\n+        \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\\n+    ),\\n+}\\n+\\n+INT_TO_FLOAT_MAPPER = {}\\n+FLOAT_TO_INT_MAPPER = {}\\n+\\n+for key, values in __INT_TO_FLOAT_MAPPER.items():\\n+    INT_TO_FLOAT_MAPPER[key] = values[0]\\n+\\n+    for value in values:\\n+        FLOAT_TO_INT_MAPPER[value] = key\\n+    pass\\n+pass\\n',\n",
       " '@@ -343,6 +343,7 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -350,6 +351,7 @@ class FastMistralModel(FastLlamaModel):\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+        \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n         \\n',\n",
       " '@@ -12,24 +12,26 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from peft import PeftModelForCausalLM\\n-from collections import OrderedDict\\n-import bitsandbytes as bnb\\n-import peft\\n-import gc\\n-import os\\n-from tqdm import tqdm as ProgressBar\\n-import shutil\\n-from typing import Optional, Callable, Union\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from typing import Optional, Callable, Union, List\\n import torch\\n+import os\\n+import pickle\\n+import gc\\n from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+import subprocess\\n+import psutil\\n \\n __all__ = [\\n+    \"print_quantization_methods\",\\n     \"unsloth_save_model\",\\n-    #\"colab_quantize_to_gguf\",\\n+    \"save_to_gguf\",\\n+    \"patch_saving_functions\",\\n ]\\n \\n+\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -41,25 +43,36 @@ LLAMA_LAYERNORMS = (\\n # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n ALLOWED_QUANTS = \\\\\\n {\\n-    \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n-    \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_s\" : \"Uses Q3_K for all tensors\",\\n-    \"q4_0\"   : \"Original quant method, 4-bit.\",\\n-    \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n-    \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n-    \"q4_k_s\" : \"Uses Q4_K for all tensors\",\\n-    \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\\n-    \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\\n-    \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n-    \"q5_k_s\" : \"Uses Q5_K for all tensors\",\\n-    \"q6_k\"   : \"Uses Q8_K for all tensors\",\\n-    \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\\n+    \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+    \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+    \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+    \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+    \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+    \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+    \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+    \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+    \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+    \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+    \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+    \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+    \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+    \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+    \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+    \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+    \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n }\\n \\n+def print_quantization_methods():\\n+    for key, value in ALLOWED_QUANTS.items():\\n+        print(f\\'\"{key}\"  ==> {value}\\')\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n         dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n@@ -75,100 +88,362 @@ def _merge_lora(layer, name):\\n pass\\n \\n \\n+def fast_save_pickle(shard, name):\\n+    # Use this if # CPUs is <= 2\\n+    print(f\"Unsloth: Saving {name}...\")\\n+    torch.save(\\n+        shard,\\n+        name,\\n+        pickle_module = pickle,\\n+        pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n+    )\\n+    return\\n+pass\\n+\\n+\\n @torch.inference_mode\\n def unsloth_save_model(\\n     model,\\n     tokenizer,\\n-    save_directory: Union[str, os.PathLike],\\n-    is_main_process: bool = True,\\n-    state_dict: Optional[dict] = None,\\n-    save_function: Callable = torch.save,\\n-    push_to_hub: bool = False,\\n-    max_shard_size: Union[int, str] = \"7GB\",\\n-    safe_serialization: bool = True,\\n-    variant: Optional[str] = None,\\n-    token: Optional[Union[str, bool]] = None,\\n-    save_peft_format: bool = True,\\n-    temporary_location = \"_unsloth_temporary_saved_buffers\",\\n-    **kwargs,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+\\n+    # Push to hub\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    create_pr            : bool = False,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : List[str] = None,\\n+\\n+    # Our functions\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.9,\\n ):\\n-    logger.warning_once(\\n-        \"Unsloth: `unsloth_save_model` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+    save_pretrained_settings = dict(locals())\\n+    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    import re\\n+\\n+    assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n+\\n+    # Clean memory up first\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n+    pass\\n+\\n+    save_method = save_method.lower().replace(\" \", \"_\")\\n+    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n+            \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n+            \\'\"merged_16bit\" ==> This merges LoRA weights and saves to float16. Needed for llama.cpp / GGUF.\\\\n\\'\\\\\\n+            \\'\"merged_4bit\"  ==> This merges LoRA weights and saves to 4bit. Useful for DPO / inference.\\'\\n+        )\\n+    pass\\n+\\n+    if save_method == \"merged_4bit\":\\n+        print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n+        print(\"This might take 5 minutes...\")\\n+        model = model.merge_and_unload()\\n+        print(\"Done.\")\\n+    pass\\n+\\n+    if tags is not None:\\n+        assert(isinstance(tags, (list, tuple)))\\n+        tags = list(tags) + [\"unsloth\",]\\n+    else:\\n+        tags = [\"unsloth\",]\\n+    pass\\n+    save_pretrained_settings[\"tags\"] = tags\\n+\\n+    if (save_method == \"lora\") and push_to_hub:\\n+        if token is None:\\n+            raise RuntimeError(\\n+                \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens.\"\\n+            )\\n+        pass\\n+\\n+        model.push_to_hub(\\n+            repo_id            = save_directory,\\n+            use_temp_dir       = use_temp_dir,\\n+            commit_message     = commit_message,\\n+            private            = private,\\n+            token              = token,\\n+            max_shard_size     = max_shard_size,\\n+            create_pr          = create_pr,\\n+            safe_serialization = safe_serialization,\\n+            revision           = revision,\\n+            commit_description = commit_description,\\n+            tags               = tags,\\n+        )\\n+        if tokenizer is not None:\\n+            tokenizer.push_to_hub(\\n+                repo_id            = save_directory,\\n+                use_temp_dir       = use_temp_dir,\\n+                commit_message     = commit_message,\\n+                private            = private,\\n+                token              = token,\\n+                max_shard_size     = max_shard_size,\\n+                create_pr          = create_pr,\\n+                safe_serialization = safe_serialization,\\n+                revision           = revision,\\n+                commit_description = commit_description,\\n+                tags               = tags,\\n+            )\\n+        pass\\n+        return save_directory\\n+    pass\\n+\\n+    # If push_to_hub, we must remove the .../ part of a repo\\n+    if push_to_hub and \"/\" in save_directory:\\n+\\n+        new_save_directory = save_directory[save_directory.find(\"/\"):]\\n+\\n+        logger.warning_once(\\n+            f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n+            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n+        )\\n+\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_directory = new_save_directory\\n+    pass\\n+    \\n+    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+        not hasattr(model, \"model\") or \\\\\\n+        not hasattr(model.model, \"model\") or \\\\\\n+        not hasattr(model.model.model, \"layers\")\\n+    ):\\n+        # Do general saving\\n+        \\n+        # Edit save_pretrained_settings\\n+        # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+        for deletion in \\\\\\n+            (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+            del save_pretrained_settings[deletion]\\n+        pass\\n+        if hasattr(model, \"add_model_tags\"):\\n+            model.add_model_tags([\"unsloth\",])\\n+\\n+        if tokenizer is not None:\\n+            print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            print(\" Done.\")\\n+        else:\\n+            print()\\n \\n+        print(\"Unsloth: Saving model...\", end = \"\")\\n+        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+\\n+        model.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+        return save_directory\\n+    pass\\n+\\n+    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+\\n+    # Determine max RAM usage minus sharding\\n+    max_ram = psutil.virtual_memory().available\\n+    sharded_ram_usage = 5 * 1024 * 1024 * 1024\\n+    if type(max_shard_size) is str:\\n+        gb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}GB\", max_shard_size, flags = re.IGNORECASE)\\n+        mb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}MB\", max_shard_size, flags = re.IGNORECASE)\\n+        if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024\\n+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 \\n+    elif type(max_shard_size) is int:\\n+        sharded_ram_usage = sharded_ram_usage\\n+    pass\\n+\\n+    # Switch to our fast saving modules if it\\'s a slow PC!\\n+    n_cpus = psutil.cpu_count(logical = False)\\n+\\n+    if safe_serialization is None:\\n+        safe_serialization = True\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+\\n+    elif safe_serialization and (n_cpus <= 2):\\n+        logger.warning_once(\\n+            f\"Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\\\\n\"\\\\\\n+            f\"We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\\\\n\"\\\\\\n+            f\"To force `safe_serialization`, set it to None instead.\",\\n+        )\\n+        safe_serialization = False\\n+        save_function = fast_save_pickle\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+        save_pretrained_settings[\"save_function\"]      = save_function\\n+    pass\\n+\\n+    # Only safe_serialization uses more RAM\\n+    if safe_serialization:\\n+        max_ram -= sharded_ram_usage\\n+    else:\\n+        max_ram -= sharded_ram_usage*0.25 # Uses much less\\n+    pass\\n+\\n+    max_ram = int(max(0, max_ram) * maximum_memory_usage)\\n+    print(f\"Unsloth: Will use up to \"\\\\\\n+          f\"{round(max_ram/1024/1024/1024, 2)} out of \"\\\\\\n+          f\"{round(psutil.virtual_memory().total/1024/1024/1024, 2)} RAM for saving.\")\\n+\\n+    # Max directory for disk saving\\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    assert(hasattr(model, \"model\"))\\n-    assert(hasattr(model.model, \"model\"))\\n-    assert(hasattr(model.model.model, \"layers\"))\\n-\\n     # HF also uses a OrderedDict\\n+    from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight\\n+    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n \\n-    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+    max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n+\\n+    from tqdm import tqdm as ProgressBar\\n     for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n             W = _merge_lora(proj, name)\\n-            filename = os.path.join(temporary_location, f\"{name}.pt\")\\n-            torch.save(W, filename)\\n-            state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n+\\n+            if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n+                # Save to GPU memory\\n+                state_dict[name] = W\\n+            # elif (max_ram - W.nbytes) > 0:\\n+            #     # Save to CPU memory\\n+            #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n+            #     state_dict[name] = W.to(\"cpu\", non_blocking = True)\\n+            #     max_ram = max(max_ram - W.nbytes, 0)\\n+            else:\\n+                # Save to Disk\\n+                logger.warning_once(f\"We will save to Disk and not RAM now.\")\\n+                filename = os.path.join(temporary_location, f\"{name}.pt\")\\n+                torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)\\n+                state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n         pass\\n         for item in LLAMA_LAYERNORMS:\\n-            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight\")\\n+            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight.data\")\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight\\n-\\n-    print(\"Unsloth: Saving tokenizer...\")\\n-    tokenizer.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n \\n-    print(\"Unsloth: Saving model. This will take 5 minutes for Llama-7b...\")\\n-    model.model.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n+    for key, value in state_dict.items():\\n+        if hasattr(value, \"data\"): state_dict[key] = value = value.data\\n+        if type(value) is not torch.Tensor:\\n+            logger.warning_once(f\"Unsloth: {key} is not a Tensor but a {type(value)}.\")\\n+        pass\\n+    pass\\n+\\n+    # Edit save_pretrained_settings\\n+    # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+    save_pretrained_settings[\"state_dict\"] = state_dict\\n+    for deletion in \\\\\\n+        (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if tokenizer is not None:\\n+        print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+    else:\\n+        print()\\n+\\n+    print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+    model.model.save_pretrained(**save_pretrained_settings)\\n+    print(\"Done.\")\\n+\\n+    save_pretrained_settings[\"state_dict\"] = None\\n+\\n+    # for j, (key, value) in enumerate(state_dict.items()):\\n+    #     state_dict[key] = None\\n+    #     if j % 10 == 0:\\n+    #         torch.cuda.empty_cache()\\n+    #         gc.collect()\\n+    #     pass\\n+    # pass\\n+    # state_dict = None\\n+    # del state_dict\\n+    # torch.cuda.empty_cache()\\n+    # gc.collect()\\n \\n     # Remove temporary location\\n+    import shutil\\n     shutil.rmtree(temporary_location)\\n+\\n+    # for _ in range(3):\\n+    #     torch.cuda.empty_cache()\\n+    #     gc.collect()\\n+    return save_directory\\n pass\\n \\n \\n-\"\"\"\\n-def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n+def install_llama_cpp_clone_non_blocking():\\n+    full_command = [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n \\n-    logger.warning_once(\\n-        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+\\n+def install_llama_cpp_make_non_blocking():\\n+    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_python_non_blocking(packages = []):\\n+    full_command = [\"pip\", \"install\"] + packages\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_llama_cpp_blocking():\\n+    commands = [\\n+        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}\",\\n+        \"pip install gguf protobuf\",\\n+    ]\\n+    if os.path.exists(\"llama.cpp\"): return\\n+    for command in commands:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n+def save_to_gguf(\\n+    model_directory     : str = \"unsloth_finetuned_model\",\\n+    quantization_method : str = \"fast_quantized\",\\n+    _run_installer = None, # Non blocking install of llama.cpp\\n+):\\n+    from transformers.models.llama.modeling_llama import logger\\n+\\n+    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n+    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n+    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n+    elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -181,27 +456,409 @@ def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n-    if not os.path.exists(\"llama.cpp\"):\\n-        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n-        !git clone https://github.com/ggerganov/llama.cpp\\n-        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\\n-        !pip install gguf protobuf\\n+    print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n+    if _run_installer is not None:\\n+        _run_installer.wait()\\n+    else:\\n+        install_llama_cpp_blocking()\\n+    pass\\n+\\n+    print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n+    first_conversion = \"f16\"\\n+    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+\\n+    n_cpus = psutil.cpu_count()*2\\n+    # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n+    \\n+    final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n+\\n+    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+        f\"--outfile {final_location} \"\\\\\\n+        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+\\n+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+        for line in sp.stdout:\\n+            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+    pass\\n+\\n+    print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+\\n+    if quantization_method != first_conversion:\\n+        old_location = final_location\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+\\n+        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n+        \\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+        print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+    pass\\n+\\n+    return final_location\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_merged(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]     = self\\n+    arguments[\"tokenizer\"] = None\\n+    del arguments[\"self\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_merged(\\n+    self,\\n+    repo_id              : str,\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = None\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = True\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_gguf(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n+\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]       = self\\n+    arguments[\"tokenizer\"]   = tokenizer\\n+    arguments[\"push_to_hub\"] = False # We save ourselves\\n+    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"quantization_method\"]\\n+\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n+    python_install.wait()\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # And save to HF\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+        from huggingface_hub import create_repo\\n+        create_repo(\\n+            repo_id   = save_directory,\\n+            token     = token,\\n+            repo_type = \"model\",\\n+            exist_ok  = True,\\n+        )\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n+\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n+\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n     pass\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_gguf(\\n+    self,\\n+    repo_id              : str,\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n-    print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\\n-    !python llama.cpp/convert.py {save_directory} \\\\\\n-        --outfile {save_directory}-unsloth.gguf \\\\\\n-        --outtype f16\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = tokenizer\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = False # We save ourselves\\n+    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    del arguments[\"quantization_method\"]\\n \\n-    print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\\n-    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\\n-    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\\\\n-        {final_location} {quantization_method}\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n \\n-    print(f\"Unsloth: Output location: {final_location}\")\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    python_install.wait()\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # Save to hub\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        private   = private,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n+def patch_saving_functions(model):\\n+    import inspect\\n+    import re\\n+    import types\\n+    from typing import Callable, Optional, Union, List\\n+\\n+    if hasattr(model, \"_original_push_to_hub\"): return\\n+\\n+    original_push_to_hub = model.push_to_hub\\n+    signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n+    signature = signature[1:]\\n+    signature = re.sub(\"<function save at .+?>\", \"torch.save\", signature)\\n+    docs = original_push_to_hub.__doc__.encode(\"utf-8\").decode(\"utf-8\")\\n+    model._original_push_to_hub = original_push_to_hub\\n+\\n+    push_to_hub_text = f\\'\\'\\'def unsloth_push_to_hub(self, {signature}:\\n+    \"\"\"\\n+    {docs}\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    del arguments[\"self\"]\\n+    if \"tags\" in arguments and arguments[\"tags\"] is not None:\\n+        assert(isinstance(arguments[\"tags\"], (list, tuple)))\\n+        arguments[\"tags\"] = list(arguments[\"tags\"]) + [\"unsloth\",]\\n+    elif \"tags\" in arguments:\\n+        arguments[\"tags\"] = [\"unsloth\",]\\n+    elif hasattr(self, \"add_model_tags\"):\\n+        self.add_model_tags([\"unsloth\",])\\n+    try:\\n+        return self._original_push_to_hub(**arguments)\\n+    except:\\n+        del arguments[\"tags\"]\\n+        return self._original_push_to_hub(**arguments)\\n+    pass\\n+    \\'\\'\\'\\n+    exec(push_to_hub_text, globals())\\n+    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)\\n+\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if hasattr(model, \"config\"):\\n+        # Counteract tokenizers\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+    else:\\n+        model.push_to_hub_merged     = model.push_to_hub\\n+        model.save_pretrained_merged = model.save_pretrained\\n+        model.push_to_hub_gguf       = model.push_to_hub\\n+        model.save_pretrained_gguf   = model.save_pretrained\\n+    pass\\n+\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        if hasattr(original_model, \"_original_push_to_hub\"): continue\\n+        \\n+        original_model._original_push_to_hub = original_model.push_to_hub\\n+        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n+\\n+        if hasattr(original_model, \"add_model_tags\"):\\n+            original_model.add_model_tags([\"unsloth\",])\\n+\\n+        if hasattr(original_model, \"config\"):\\n+            # Counteract tokenizers\\n+            original_model.push_to_hub_merged     = \\\\\\n+                types.MethodType(unsloth_push_to_hub_merged,     original_model)\\n+\\n+            original_model.save_pretrained_merged = \\\\\\n+                types.MethodType(unsloth_save_pretrained_merged, original_model)\\n+\\n+            original_model.push_to_hub_gguf       = \\\\\\n+                types.MethodType(unsloth_push_to_hub_gguf,       original_model)\\n+\\n+            original_model.save_pretrained_gguf   = \\\\\\n+                types.MethodType(unsloth_save_pretrained_gguf,   original_model)\\n+        pass\\n+    pass\\n+    return\\n pass\\n-\"\"\"\\n',\n",
       " '@@ -65,8 +65,7 @@ try:\\n     libcuda_dirs()\\n except:\\n     warnings.warn(\\n-        \"CUDA is not linked properly.\\\\n\"\\\\\\n-        \"We shall run `ldconfig /usr/lib64-nvidia` to try to fix it.\"\\n+        \"Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n     )\\n     os.system(\"ldconfig /usr/lib64-nvidia\")\\n     importlib.reload(bnb)\\n',\n",
       " '@@ -41,12 +41,13 @@ def _rms_layernorm_forward(\\n     r += row_idx * r_row_stride\\n \\n     X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n-    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n-    inv_var = 1 / tl.sqrt(row_var + eps)\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps)\\n     tl.store(r, inv_var)\\n     normed = X_row * inv_var\\n+    normed = normed.to(W_row.dtype) # Exact copy from HF\\n     output = normed * W_row\\n     tl.store(Y + col_offsets, output, mask = mask)\\n pass\\n',\n",
       " '@@ -25,10 +25,11 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     mask = offsets < n_elements\\n \\n     e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row = tl.load(g + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n     f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n \\n@@ -53,12 +54,13 @@ def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n-    DW_row = tl.load(DW + offsets, mask = mask, other = 0).to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row  = tl.load(g  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row))\\n+    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n+    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n     # f = e * se\\n     f_row = e_row * se_row\\n     # h = f * g\\n',\n",
       " '@@ -14,9 +14,7 @@\\n \\n import torch\\n from typing import Union, Optional, List, Any, Callable\\n-import numpy as np\\n import warnings\\n-import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n',\n",
       " '@@ -15,7 +15,6 @@\\n import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n-from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n from transformers.models.llama.modeling_llama import (\\n     logger,\\n     BaseModelOutputWithPast,\\n@@ -46,16 +45,13 @@ except:\\n     LlamaFlashAttention2 = LlamaAttention\\n pass\\n \\n-from peft import PeftModelForCausalLM\\n-import gc\\n-import peft\\n-import bitsandbytes as bnb\\n-import numpy as np\\n-import types\\n-\\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n+from peft import PeftModelForCausalLM\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from ..save import patch_saving_functions\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -110,18 +106,15 @@ def LlamaAttention_fast_forward_inference(\\n     bsz, _, _ = hidden_states.size()\\n     K1, V1 = past_key_value\\n \\n-    Wq = self.q_proj.weight\\n-    Wk = self.k_proj.weight\\n-    Wv = self.v_proj.weight\\n-    Wo = self.o_proj.weight\\n-\\n     n_heads    = self.num_heads\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n     head_dim   = self.head_dim\\n     assert(n_kv_heads * n_groups == n_heads)\\n \\n-    Qn, Kn, Vn = original_apply_qkv(self, Xn)\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n     Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n     Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n     Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n@@ -156,6 +149,28 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_silu = torch.nn.functional.silu\\n+def fast_mlp_inference(self, X):\\n+    gate = self.gate_proj(X)\\n+    up   = self.up_proj(X)\\n+    gate = torch_silu(gate, inplace = True)\\n+    gate *= up\\n+    X = self.down_proj(gate)\\n+    return X\\n+pass\\n+\\n+\\n+def fast_rms_layernorm_inference(self, X):\\n+    X = X.to(torch.float32)\\n+    variance = X.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    X *= variance.rsqrt_()\\n+    X = X.to(residual.dtype)\\n+    X *= self.weight\\n+    return X\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -287,28 +302,51 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    residual = hidden_states\\n-\\n-    hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+    bsz, q_len, hd = hidden_states.size()\\n+\\n+    if (self.training):\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states = residual + hidden_states\\n \\n-    # Self Attention\\n-    hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-        hidden_states=hidden_states,\\n-        causal_mask=causal_mask,\\n-        attention_mask=attention_mask,\\n-        position_ids=position_ids,\\n-        past_key_value=past_key_value,\\n-        output_attentions=output_attentions,\\n-        use_cache=use_cache,\\n-        padding_mask=padding_mask,\\n-    )\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = self.mlp(hidden_states)\\n+        hidden_states = residual + hidden_states\\n+    else:\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states += residual\\n \\n-    # Fully Connected\\n-    residual = hidden_states\\n-    hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n-    hidden_states = self.mlp(hidden_states)\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n+        hidden_states += residual\\n+    pass\\n \\n     outputs = (hidden_states,)\\n \\n@@ -378,6 +416,7 @@ def LlamaModel_fast_forward(\\n     if past_key_values is not None:\\n         past_key_values_length = past_key_values[0][0].shape[2]\\n         seq_length_with_past = seq_length_with_past + past_key_values_length\\n+    pass\\n \\n     # We already handle KV cache position_ids ourselves.\\n     if (past_key_values_length != 0):\\n@@ -391,10 +430,12 @@ def LlamaModel_fast_forward(\\n         position_ids = position_ids.view(-1, seq_length).to(torch.int32)#.long()\\n     else:\\n         position_ids = None\\n+    pass\\n \\n     if position_ids is not None:\\n         if position_ids.shape[0] != batch_size:\\n             position_ids = position_ids.repeat((batch_size, 1))\\n+    pass\\n \\n     # embed positions\\n     if inputs_embeds is None:\\n@@ -403,19 +444,22 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n+    elif self.training:\\n+        attention_mask = None\\n+        padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n             padding_mask = attention_mask\\n         else:\\n             padding_mask = None\\n \\n+        from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n         attention_mask = _prepare_4d_causal_attention_mask(\\n             attention_mask,\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = None if not hasattr(self.config, \"sliding_window\") else \\\\\\n-                self.config.sliding_window,\\n+            sliding_window = getattr(self.config, \"sliding_window\"),\\n         )\\n     pass\\n \\n@@ -479,7 +523,11 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    if (self.training):\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    pass\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -665,6 +713,7 @@ class FastLlamaModel:\\n                 bnb_4bit_quant_type       = \"nf4\",\\n                 bnb_4bit_compute_dtype    = dtype,\\n             )\\n+        pass\\n \\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n@@ -714,6 +763,7 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -721,6 +771,7 @@ class FastLlamaModel:\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+\\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n@@ -751,7 +802,7 @@ class FastLlamaModel:\\n         correct_dtype = lm_head.weight.dtype\\n \\n         for name, module in model.named_modules():\\n-            if isinstance(module, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):\\n                 weight = module.weight\\n                 quant_state = weight.quant_state\\n \\n@@ -766,8 +817,10 @@ class FastLlamaModel:\\n         pass\\n \\n         # Clear deleted GPU items\\n-        gc.collect()\\n-        torch.cuda.empty_cache()\\n+        import gc\\n+        for _ in range(3):\\n+            gc.collect()\\n+            torch.cuda.empty_cache()\\n         return model\\n     pass\\n \\n@@ -782,11 +835,26 @@ class FastLlamaModel:\\n         lora_dropout = 0,\\n         bias = \"none\",\\n         layers_to_transform = None,\\n+        layers_pattern = None,\\n         use_gradient_checkpointing = True,\\n         random_state = 3407,\\n         max_seq_length = 2048, # not used anymore\\n+        use_rslora = False,\\n+        init_lora_weights = True,\\n+        loftq_config = None,\\n         **kwargs,\\n     ):\\n+        if isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n+            )\\n+        pass\\n+\\n+        import inspect\\n+        signature = str(inspect.signature(LoraConfig))\\n+        SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n+        SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n+\\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n@@ -794,11 +862,61 @@ class FastLlamaModel:\\n                 f\"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = {lora_dropout}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n         if bias != \"none\":\\n             logger.warning_once(\\n                 f\"Unsloth: bias = `none` is supported for fast patching. You are using bias = {bias}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n+        if not (type(init_lora_weights) is bool or \\\\\\n+            init_lora_weights == \"gaussian\" or init_lora_weights == \"loftq\"):\\n+            raise ValueError(\\n+                \\'Unsloth: `init_lora_weights` must be either [True, False, \"gaussian\", \"loftq\"].\\'\\n+            )\\n+        pass\\n+\\n+        if init_lora_weights == \"loftq\":\\n+\\n+            if not SUPPORTS_LOFTQ:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+\\n+            if loftq_config is None:\\n+                from peft import LoftQConfig\\n+                logger.warning_once(\\n+                    f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n+                    f\"We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.\"\\n+                )\\n+                loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)\\n+            pass\\n+            \\n+            if hasattr(model.config, \"quantization_config\"):\\n+                raise ValueError(\\n+                    \"Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\\\\n\"\\\\\\n+                    \"Reload your model without any quantization by setting `load_in_4bit = False`.\"\\n+                )\\n+            pass\\n+        pass\\n+\\n+        assert(type(use_rslora) is bool)\\n+        if use_rslora:\\n+            if not SUPPORTS_RSLORA:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+        pass\\n \\n         transformers_set_seed(random_state)\\n \\n@@ -810,16 +928,23 @@ class FastLlamaModel:\\n         pass\\n \\n         # Get LoRA\\n-        lora_config = LoraConfig(\\n-            r              = r,\\n-            lora_alpha     = lora_alpha,\\n-            target_modules = target_modules,\\n-            lora_dropout   = lora_dropout,\\n-            bias           = bias,\\n-            task_type      = TaskType.CAUSAL_LM,\\n+        arguments = dict(\\n+            r                   = r,\\n+            lora_alpha          = lora_alpha,\\n+            target_modules      = target_modules,\\n+            lora_dropout        = lora_dropout,\\n+            bias                = bias,\\n+            task_type           = TaskType.CAUSAL_LM,\\n             layers_to_transform = layers_to_transform,\\n+            init_lora_weights   = init_lora_weights,\\n+            loftq_config        = loftq_config,\\n+            use_rslora          = use_rslora,\\n             **kwargs,\\n         )\\n+        if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n+        if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n+\\n+        lora_config = LoraConfig(**arguments)\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n@@ -828,10 +953,21 @@ class FastLlamaModel:\\n         )\\n         model = _get_peft_model(model, lora_config)\\n \\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.peft_config[\"default\"].base_model_name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        pass\\n+        # Add revision to enable future fast inference paths\\n+        model.peft_config[\"default\"].revision = f\"unsloth\"\\n+\\n         # Do patching\\n         n_mlp = 0\\n         n_qkv = 0\\n         n_o   = 0\\n+        import types\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n@@ -897,6 +1033,7 @@ class FastLlamaModel:\\n             f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n             f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n         )\\n+        patch_saving_functions(model)\\n \\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n',\n",
       " '@@ -16,16 +16,9 @@ from .llama import FastLlamaModel, logger\\n from .mistral import FastMistralModel\\n from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n+from peft import PeftConfig, PeftModel\\n+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n \\n-FOURBIT_MAPPER = \\\\\\n-{\\n-    \"unsloth/mistral-7b-bnb-4bit\"    : \"unsloth/mistral-7b\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\"    : \"unsloth/llama-2-7b\",\\n-    \"unsloth/llama-2-13b-bnb-4bit\"   : \"unsloth/llama-13-7b\",\\n-    \"unsloth/codellama-34b-bnb-4bit\" : \"codellama/CodeLlama-34b-hf\",\\n-    \"unsloth/zephyr-sft-bnb-4bit\"    : \"unsloth/zephyr-sft\",\\n-    \"unsloth/tinyllama-bnb-4bit\"     : \"unsloth/tinyllama\",\\n-}\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -34,6 +27,39 @@ SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)\\n del major, minor\\n \\n \\n+def _get_model_name(model_name, load_in_4bit = True):\\n+\\n+    if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n+            f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n+            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n+            f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n+        )\\n+    \\n+    elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n+            f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n+        )\\n+        model_name = new_model_name\\n+\\n+    elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n+            f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n+        )\\n+        model_name = new_model_name\\n+    pass\\n+\\n+    return model_name\\n+pass\\n+\\n+\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n@@ -47,25 +73,27 @@ class FastLanguageModel(FastLlamaModel):\\n         fix_tokenizer = True,\\n         *args, **kwargs,\\n     ):\\n-        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:\\n-            model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n-                f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-                f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n-                f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n-                f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n-            )\\n-        elif not load_in_4bit and model_name in FOURBIT_MAPPER:\\n-            new_model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n-                f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n-            )\\n-            model_name = new_model_name\\n+        old_model_name = model_name\\n+        model_name = _get_model_name(model_name, load_in_4bit)\\n+\\n+        # First check if it\\'s a normal model via AutoConfig\\n+        is_peft = False\\n+        try:\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = False\\n+        except:\\n+            try:\\n+                # Most likely a PEFT model\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+            except:\\n+                raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n+            \\n+            # Check base model again for PEFT\\n+            model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = True\\n         pass\\n \\n-        model_config = AutoConfig.from_pretrained(model_name)\\n         model_type = model_config.model_type\\n \\n         if   model_type == \"llama\":   dispatch_model = FastLlamaModel\\n@@ -75,8 +103,9 @@ class FastLanguageModel(FastLlamaModel):\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n                 \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n+        pass\\n \\n-        return dispatch_model.from_pretrained(\\n+        model, tokenizer = dispatch_model.from_pretrained(\\n             model_name = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype = dtype,\\n@@ -87,5 +116,30 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n+\\n+        if load_in_4bit:\\n+            # Fix up bitsandbytes config\\n+            quantization_config = \\\\\\n+            {\\n+                # Sometimes torch_dtype is not a string!!\\n+                \"bnb_4bit_compute_dtype\"           : model.config.to_dict()[\"torch_dtype\"],\\n+                \"bnb_4bit_quant_type\"              : \"nf4\",\\n+                \"bnb_4bit_use_double_quant\"        : True,\\n+                \"llm_int8_enable_fp32_cpu_offload\" : False,\\n+                \"llm_int8_has_fp16_weight\"         : False,\\n+                \"llm_int8_skip_modules\"            : \"null\",\\n+                \"llm_int8_threshold\"               : 6.0,\\n+                \"load_in_4bit\"                     : True,\\n+                \"load_in_8bit\"                     : False,\\n+                \"quant_method\"                     : \"bitsandbytes\",\\n+            }\\n+            model.config.update({\"quantization_config\" : quantization_config})\\n+        pass\\n+\\n+        if is_peft:\\n+            # Now add PEFT adapters\\n+            model = PeftModel.from_pretrained(model, old_model_name)\\n+        pass\\n+        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -0,0 +1,56 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+__all__ = [\\n+    \"INT_TO_FLOAT_MAPPER\",\\n+    \"FLOAT_TO_INT_MAPPER\",\\n+]\\n+\\n+__INT_TO_FLOAT_MAPPER = \\\\\\n+{\\n+    \"unsloth/mistral-7b-bnb-4bit\"    : (\\n+        \"unsloth/mistral-7b\",\\n+        \"mistralai/Mistral-7B-v0.1\",\\n+    ),\\n+    \"unsloth/llama-2-7b-bnb-4bit\"    : (\\n+        \"unsloth/llama-2-7b\",\\n+        \"meta-llama/Llama-2-7b-hf\",\\n+    ),\\n+    \"unsloth/llama-2-13b-bnb-4bit\"   : (\\n+        \"unsloth/llama-13-7b\",\\n+        \"meta-llama/Llama-2-13b-hf\",\\n+    ),\\n+    \"unsloth/codellama-34b-bnb-4bit\" : (\\n+        \"codellama/CodeLlama-34b-hf\",\\n+    ),\\n+    \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n+        \"unsloth/zephyr-sft\",\\n+        \"alignment-handbook/zephyr-7b-sft-full\",\\n+    ),\\n+    \"unsloth/tinyllama-bnb-4bit\"     : (\\n+        \"unsloth/tinyllama\",\\n+        \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\\n+    ),\\n+}\\n+\\n+INT_TO_FLOAT_MAPPER = {}\\n+FLOAT_TO_INT_MAPPER = {}\\n+\\n+for key, values in __INT_TO_FLOAT_MAPPER.items():\\n+    INT_TO_FLOAT_MAPPER[key] = values[0]\\n+\\n+    for value in values:\\n+        FLOAT_TO_INT_MAPPER[value] = key\\n+    pass\\n+pass\\n',\n",
       " '@@ -343,6 +343,7 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -350,6 +351,7 @@ class FastMistralModel(FastLlamaModel):\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+        \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n         \\n',\n",
       " '@@ -12,24 +12,26 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from peft import PeftModelForCausalLM\\n-from collections import OrderedDict\\n-import bitsandbytes as bnb\\n-import peft\\n-import gc\\n-import os\\n-from tqdm import tqdm as ProgressBar\\n-import shutil\\n-from typing import Optional, Callable, Union\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from typing import Optional, Callable, Union, List\\n import torch\\n+import os\\n+import pickle\\n+import gc\\n from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+import subprocess\\n+import psutil\\n \\n __all__ = [\\n+    \"print_quantization_methods\",\\n     \"unsloth_save_model\",\\n-    #\"colab_quantize_to_gguf\",\\n+    \"save_to_gguf\",\\n+    \"patch_saving_functions\",\\n ]\\n \\n+\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -41,25 +43,36 @@ LLAMA_LAYERNORMS = (\\n # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n ALLOWED_QUANTS = \\\\\\n {\\n-    \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n-    \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_s\" : \"Uses Q3_K for all tensors\",\\n-    \"q4_0\"   : \"Original quant method, 4-bit.\",\\n-    \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n-    \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n-    \"q4_k_s\" : \"Uses Q4_K for all tensors\",\\n-    \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\\n-    \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\\n-    \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n-    \"q5_k_s\" : \"Uses Q5_K for all tensors\",\\n-    \"q6_k\"   : \"Uses Q8_K for all tensors\",\\n-    \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\\n+    \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+    \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+    \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+    \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+    \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+    \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+    \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+    \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+    \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+    \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+    \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+    \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+    \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+    \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+    \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+    \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+    \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n }\\n \\n+def print_quantization_methods():\\n+    for key, value in ALLOWED_QUANTS.items():\\n+        print(f\\'\"{key}\"  ==> {value}\\')\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n         dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n@@ -75,100 +88,362 @@ def _merge_lora(layer, name):\\n pass\\n \\n \\n+def fast_save_pickle(shard, name):\\n+    # Use this if # CPUs is <= 2\\n+    print(f\"Unsloth: Saving {name}...\")\\n+    torch.save(\\n+        shard,\\n+        name,\\n+        pickle_module = pickle,\\n+        pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n+    )\\n+    return\\n+pass\\n+\\n+\\n @torch.inference_mode\\n def unsloth_save_model(\\n     model,\\n     tokenizer,\\n-    save_directory: Union[str, os.PathLike],\\n-    is_main_process: bool = True,\\n-    state_dict: Optional[dict] = None,\\n-    save_function: Callable = torch.save,\\n-    push_to_hub: bool = False,\\n-    max_shard_size: Union[int, str] = \"7GB\",\\n-    safe_serialization: bool = True,\\n-    variant: Optional[str] = None,\\n-    token: Optional[Union[str, bool]] = None,\\n-    save_peft_format: bool = True,\\n-    temporary_location = \"_unsloth_temporary_saved_buffers\",\\n-    **kwargs,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+\\n+    # Push to hub\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    create_pr            : bool = False,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : List[str] = None,\\n+\\n+    # Our functions\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.9,\\n ):\\n-    logger.warning_once(\\n-        \"Unsloth: `unsloth_save_model` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+    save_pretrained_settings = dict(locals())\\n+    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    import re\\n+\\n+    assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n+\\n+    # Clean memory up first\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n+    pass\\n+\\n+    save_method = save_method.lower().replace(\" \", \"_\")\\n+    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n+            \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n+            \\'\"merged_16bit\" ==> This merges LoRA weights and saves to float16. Needed for llama.cpp / GGUF.\\\\n\\'\\\\\\n+            \\'\"merged_4bit\"  ==> This merges LoRA weights and saves to 4bit. Useful for DPO / inference.\\'\\n+        )\\n+    pass\\n+\\n+    if save_method == \"merged_4bit\":\\n+        print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n+        print(\"This might take 5 minutes...\")\\n+        model = model.merge_and_unload()\\n+        print(\"Done.\")\\n+    pass\\n+\\n+    if tags is not None:\\n+        assert(isinstance(tags, (list, tuple)))\\n+        tags = list(tags) + [\"unsloth\",]\\n+    else:\\n+        tags = [\"unsloth\",]\\n+    pass\\n+    save_pretrained_settings[\"tags\"] = tags\\n+\\n+    if (save_method == \"lora\") and push_to_hub:\\n+        if token is None:\\n+            raise RuntimeError(\\n+                \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens.\"\\n+            )\\n+        pass\\n+\\n+        model.push_to_hub(\\n+            repo_id            = save_directory,\\n+            use_temp_dir       = use_temp_dir,\\n+            commit_message     = commit_message,\\n+            private            = private,\\n+            token              = token,\\n+            max_shard_size     = max_shard_size,\\n+            create_pr          = create_pr,\\n+            safe_serialization = safe_serialization,\\n+            revision           = revision,\\n+            commit_description = commit_description,\\n+            tags               = tags,\\n+        )\\n+        if tokenizer is not None:\\n+            tokenizer.push_to_hub(\\n+                repo_id            = save_directory,\\n+                use_temp_dir       = use_temp_dir,\\n+                commit_message     = commit_message,\\n+                private            = private,\\n+                token              = token,\\n+                max_shard_size     = max_shard_size,\\n+                create_pr          = create_pr,\\n+                safe_serialization = safe_serialization,\\n+                revision           = revision,\\n+                commit_description = commit_description,\\n+                tags               = tags,\\n+            )\\n+        pass\\n+        return save_directory\\n+    pass\\n+\\n+    # If push_to_hub, we must remove the .../ part of a repo\\n+    if push_to_hub and \"/\" in save_directory:\\n+\\n+        new_save_directory = save_directory[save_directory.find(\"/\"):]\\n+\\n+        logger.warning_once(\\n+            f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n+            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n+        )\\n+\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_directory = new_save_directory\\n+    pass\\n+    \\n+    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+        not hasattr(model, \"model\") or \\\\\\n+        not hasattr(model.model, \"model\") or \\\\\\n+        not hasattr(model.model.model, \"layers\")\\n+    ):\\n+        # Do general saving\\n+        \\n+        # Edit save_pretrained_settings\\n+        # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+        for deletion in \\\\\\n+            (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+            del save_pretrained_settings[deletion]\\n+        pass\\n+        if hasattr(model, \"add_model_tags\"):\\n+            model.add_model_tags([\"unsloth\",])\\n+\\n+        if tokenizer is not None:\\n+            print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            print(\" Done.\")\\n+        else:\\n+            print()\\n \\n+        print(\"Unsloth: Saving model...\", end = \"\")\\n+        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+\\n+        model.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+        return save_directory\\n+    pass\\n+\\n+    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+\\n+    # Determine max RAM usage minus sharding\\n+    max_ram = psutil.virtual_memory().available\\n+    sharded_ram_usage = 5 * 1024 * 1024 * 1024\\n+    if type(max_shard_size) is str:\\n+        gb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}GB\", max_shard_size, flags = re.IGNORECASE)\\n+        mb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}MB\", max_shard_size, flags = re.IGNORECASE)\\n+        if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024\\n+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 \\n+    elif type(max_shard_size) is int:\\n+        sharded_ram_usage = sharded_ram_usage\\n+    pass\\n+\\n+    # Switch to our fast saving modules if it\\'s a slow PC!\\n+    n_cpus = psutil.cpu_count(logical = False)\\n+\\n+    if safe_serialization is None:\\n+        safe_serialization = True\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+\\n+    elif safe_serialization and (n_cpus <= 2):\\n+        logger.warning_once(\\n+            f\"Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\\\\n\"\\\\\\n+            f\"We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\\\\n\"\\\\\\n+            f\"To force `safe_serialization`, set it to None instead.\",\\n+        )\\n+        safe_serialization = False\\n+        save_function = fast_save_pickle\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+        save_pretrained_settings[\"save_function\"]      = save_function\\n+    pass\\n+\\n+    # Only safe_serialization uses more RAM\\n+    if safe_serialization:\\n+        max_ram -= sharded_ram_usage\\n+    else:\\n+        max_ram -= sharded_ram_usage*0.25 # Uses much less\\n+    pass\\n+\\n+    max_ram = int(max(0, max_ram) * maximum_memory_usage)\\n+    print(f\"Unsloth: Will use up to \"\\\\\\n+          f\"{round(max_ram/1024/1024/1024, 2)} out of \"\\\\\\n+          f\"{round(psutil.virtual_memory().total/1024/1024/1024, 2)} RAM for saving.\")\\n+\\n+    # Max directory for disk saving\\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    assert(hasattr(model, \"model\"))\\n-    assert(hasattr(model.model, \"model\"))\\n-    assert(hasattr(model.model.model, \"layers\"))\\n-\\n     # HF also uses a OrderedDict\\n+    from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight\\n+    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n \\n-    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+    max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n+\\n+    from tqdm import tqdm as ProgressBar\\n     for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n             W = _merge_lora(proj, name)\\n-            filename = os.path.join(temporary_location, f\"{name}.pt\")\\n-            torch.save(W, filename)\\n-            state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n+\\n+            if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n+                # Save to GPU memory\\n+                state_dict[name] = W\\n+            # elif (max_ram - W.nbytes) > 0:\\n+            #     # Save to CPU memory\\n+            #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n+            #     state_dict[name] = W.to(\"cpu\", non_blocking = True)\\n+            #     max_ram = max(max_ram - W.nbytes, 0)\\n+            else:\\n+                # Save to Disk\\n+                logger.warning_once(f\"We will save to Disk and not RAM now.\")\\n+                filename = os.path.join(temporary_location, f\"{name}.pt\")\\n+                torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)\\n+                state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n         pass\\n         for item in LLAMA_LAYERNORMS:\\n-            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight\")\\n+            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight.data\")\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight\\n-\\n-    print(\"Unsloth: Saving tokenizer...\")\\n-    tokenizer.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n \\n-    print(\"Unsloth: Saving model. This will take 5 minutes for Llama-7b...\")\\n-    model.model.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n+    for key, value in state_dict.items():\\n+        if hasattr(value, \"data\"): state_dict[key] = value = value.data\\n+        if type(value) is not torch.Tensor:\\n+            logger.warning_once(f\"Unsloth: {key} is not a Tensor but a {type(value)}.\")\\n+        pass\\n+    pass\\n+\\n+    # Edit save_pretrained_settings\\n+    # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+    save_pretrained_settings[\"state_dict\"] = state_dict\\n+    for deletion in \\\\\\n+        (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if tokenizer is not None:\\n+        print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+    else:\\n+        print()\\n+\\n+    print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+    model.model.save_pretrained(**save_pretrained_settings)\\n+    print(\"Done.\")\\n+\\n+    save_pretrained_settings[\"state_dict\"] = None\\n+\\n+    # for j, (key, value) in enumerate(state_dict.items()):\\n+    #     state_dict[key] = None\\n+    #     if j % 10 == 0:\\n+    #         torch.cuda.empty_cache()\\n+    #         gc.collect()\\n+    #     pass\\n+    # pass\\n+    # state_dict = None\\n+    # del state_dict\\n+    # torch.cuda.empty_cache()\\n+    # gc.collect()\\n \\n     # Remove temporary location\\n+    import shutil\\n     shutil.rmtree(temporary_location)\\n+\\n+    # for _ in range(3):\\n+    #     torch.cuda.empty_cache()\\n+    #     gc.collect()\\n+    return save_directory\\n pass\\n \\n \\n-\"\"\"\\n-def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n+def install_llama_cpp_clone_non_blocking():\\n+    full_command = [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n \\n-    logger.warning_once(\\n-        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+\\n+def install_llama_cpp_make_non_blocking():\\n+    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_python_non_blocking(packages = []):\\n+    full_command = [\"pip\", \"install\"] + packages\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_llama_cpp_blocking():\\n+    commands = [\\n+        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}\",\\n+        \"pip install gguf protobuf\",\\n+    ]\\n+    if os.path.exists(\"llama.cpp\"): return\\n+    for command in commands:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n+def save_to_gguf(\\n+    model_directory     : str = \"unsloth_finetuned_model\",\\n+    quantization_method : str = \"fast_quantized\",\\n+    _run_installer = None, # Non blocking install of llama.cpp\\n+):\\n+    from transformers.models.llama.modeling_llama import logger\\n+\\n+    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n+    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n+    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n+    elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -181,27 +456,409 @@ def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n-    if not os.path.exists(\"llama.cpp\"):\\n-        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n-        !git clone https://github.com/ggerganov/llama.cpp\\n-        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\\n-        !pip install gguf protobuf\\n+    print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n+    if _run_installer is not None:\\n+        _run_installer.wait()\\n+    else:\\n+        install_llama_cpp_blocking()\\n+    pass\\n+\\n+    print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n+    first_conversion = \"f16\"\\n+    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+\\n+    n_cpus = psutil.cpu_count()*2\\n+    # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n+    \\n+    final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n+\\n+    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+        f\"--outfile {final_location} \"\\\\\\n+        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+\\n+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+        for line in sp.stdout:\\n+            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+    pass\\n+\\n+    print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+\\n+    if quantization_method != first_conversion:\\n+        old_location = final_location\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+\\n+        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n+        \\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+        print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+    pass\\n+\\n+    return final_location\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_merged(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]     = self\\n+    arguments[\"tokenizer\"] = None\\n+    del arguments[\"self\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_merged(\\n+    self,\\n+    repo_id              : str,\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = None\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = True\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_gguf(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n+\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]       = self\\n+    arguments[\"tokenizer\"]   = tokenizer\\n+    arguments[\"push_to_hub\"] = False # We save ourselves\\n+    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"quantization_method\"]\\n+\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n+    python_install.wait()\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # And save to HF\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+        from huggingface_hub import create_repo\\n+        create_repo(\\n+            repo_id   = save_directory,\\n+            token     = token,\\n+            repo_type = \"model\",\\n+            exist_ok  = True,\\n+        )\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n+\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n+\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n     pass\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_gguf(\\n+    self,\\n+    repo_id              : str,\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n-    print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\\n-    !python llama.cpp/convert.py {save_directory} \\\\\\n-        --outfile {save_directory}-unsloth.gguf \\\\\\n-        --outtype f16\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = tokenizer\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = False # We save ourselves\\n+    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    del arguments[\"quantization_method\"]\\n \\n-    print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\\n-    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\\n-    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\\\\n-        {final_location} {quantization_method}\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n \\n-    print(f\"Unsloth: Output location: {final_location}\")\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    python_install.wait()\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # Save to hub\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        private   = private,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n+def patch_saving_functions(model):\\n+    import inspect\\n+    import re\\n+    import types\\n+    from typing import Callable, Optional, Union, List\\n+\\n+    if hasattr(model, \"_original_push_to_hub\"): return\\n+\\n+    original_push_to_hub = model.push_to_hub\\n+    signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n+    signature = signature[1:]\\n+    signature = re.sub(\"<function save at .+?>\", \"torch.save\", signature)\\n+    docs = original_push_to_hub.__doc__.encode(\"utf-8\").decode(\"utf-8\")\\n+    model._original_push_to_hub = original_push_to_hub\\n+\\n+    push_to_hub_text = f\\'\\'\\'def unsloth_push_to_hub(self, {signature}:\\n+    \"\"\"\\n+    {docs}\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    del arguments[\"self\"]\\n+    if \"tags\" in arguments and arguments[\"tags\"] is not None:\\n+        assert(isinstance(arguments[\"tags\"], (list, tuple)))\\n+        arguments[\"tags\"] = list(arguments[\"tags\"]) + [\"unsloth\",]\\n+    elif \"tags\" in arguments:\\n+        arguments[\"tags\"] = [\"unsloth\",]\\n+    elif hasattr(self, \"add_model_tags\"):\\n+        self.add_model_tags([\"unsloth\",])\\n+    try:\\n+        return self._original_push_to_hub(**arguments)\\n+    except:\\n+        del arguments[\"tags\"]\\n+        return self._original_push_to_hub(**arguments)\\n+    pass\\n+    \\'\\'\\'\\n+    exec(push_to_hub_text, globals())\\n+    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)\\n+\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if hasattr(model, \"config\"):\\n+        # Counteract tokenizers\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+    else:\\n+        model.push_to_hub_merged     = model.push_to_hub\\n+        model.save_pretrained_merged = model.save_pretrained\\n+        model.push_to_hub_gguf       = model.push_to_hub\\n+        model.save_pretrained_gguf   = model.save_pretrained\\n+    pass\\n+\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        if hasattr(original_model, \"_original_push_to_hub\"): continue\\n+        \\n+        original_model._original_push_to_hub = original_model.push_to_hub\\n+        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n+\\n+        if hasattr(original_model, \"add_model_tags\"):\\n+            original_model.add_model_tags([\"unsloth\",])\\n+\\n+        if hasattr(original_model, \"config\"):\\n+            # Counteract tokenizers\\n+            original_model.push_to_hub_merged     = \\\\\\n+                types.MethodType(unsloth_push_to_hub_merged,     original_model)\\n+\\n+            original_model.save_pretrained_merged = \\\\\\n+                types.MethodType(unsloth_save_pretrained_merged, original_model)\\n+\\n+            original_model.push_to_hub_gguf       = \\\\\\n+                types.MethodType(unsloth_push_to_hub_gguf,       original_model)\\n+\\n+            original_model.save_pretrained_gguf   = \\\\\\n+                types.MethodType(unsloth_save_pretrained_gguf,   original_model)\\n+        pass\\n+    pass\\n+    return\\n pass\\n-\"\"\"\\n',\n",
       " '@@ -65,8 +65,7 @@ try:\\n     libcuda_dirs()\\n except:\\n     warnings.warn(\\n-        \"CUDA is not linked properly.\\\\n\"\\\\\\n-        \"We shall run `ldconfig /usr/lib64-nvidia` to try to fix it.\"\\n+        \"Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n     )\\n     os.system(\"ldconfig /usr/lib64-nvidia\")\\n     importlib.reload(bnb)\\n',\n",
       " '@@ -41,12 +41,13 @@ def _rms_layernorm_forward(\\n     r += row_idx * r_row_stride\\n \\n     X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n-    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n-    inv_var = 1 / tl.sqrt(row_var + eps)\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps)\\n     tl.store(r, inv_var)\\n     normed = X_row * inv_var\\n+    normed = normed.to(W_row.dtype) # Exact copy from HF\\n     output = normed * W_row\\n     tl.store(Y + col_offsets, output, mask = mask)\\n pass\\n',\n",
       " '@@ -25,10 +25,11 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     mask = offsets < n_elements\\n \\n     e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row = tl.load(g + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n     f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n \\n@@ -53,12 +54,13 @@ def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n-    DW_row = tl.load(DW + offsets, mask = mask, other = 0).to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row  = tl.load(g  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row))\\n+    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n+    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n     # f = e * se\\n     f_row = e_row * se_row\\n     # h = f * g\\n',\n",
       " '@@ -14,9 +14,7 @@\\n \\n import torch\\n from typing import Union, Optional, List, Any, Callable\\n-import numpy as np\\n import warnings\\n-import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n',\n",
       " '@@ -15,7 +15,6 @@\\n import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n-from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n from transformers.models.llama.modeling_llama import (\\n     logger,\\n     BaseModelOutputWithPast,\\n@@ -46,16 +45,13 @@ except:\\n     LlamaFlashAttention2 = LlamaAttention\\n pass\\n \\n-from peft import PeftModelForCausalLM\\n-import gc\\n-import peft\\n-import bitsandbytes as bnb\\n-import numpy as np\\n-import types\\n-\\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n+from peft import PeftModelForCausalLM\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from ..save import patch_saving_functions\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -110,18 +106,15 @@ def LlamaAttention_fast_forward_inference(\\n     bsz, _, _ = hidden_states.size()\\n     K1, V1 = past_key_value\\n \\n-    Wq = self.q_proj.weight\\n-    Wk = self.k_proj.weight\\n-    Wv = self.v_proj.weight\\n-    Wo = self.o_proj.weight\\n-\\n     n_heads    = self.num_heads\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n     head_dim   = self.head_dim\\n     assert(n_kv_heads * n_groups == n_heads)\\n \\n-    Qn, Kn, Vn = original_apply_qkv(self, Xn)\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n     Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n     Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n     Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n@@ -156,6 +149,28 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_silu = torch.nn.functional.silu\\n+def fast_mlp_inference(self, X):\\n+    gate = self.gate_proj(X)\\n+    up   = self.up_proj(X)\\n+    gate = torch_silu(gate, inplace = True)\\n+    gate *= up\\n+    X = self.down_proj(gate)\\n+    return X\\n+pass\\n+\\n+\\n+def fast_rms_layernorm_inference(self, X):\\n+    X = X.to(torch.float32)\\n+    variance = X.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    X *= variance.rsqrt_()\\n+    X = X.to(residual.dtype)\\n+    X *= self.weight\\n+    return X\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -287,28 +302,51 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    residual = hidden_states\\n-\\n-    hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+    bsz, q_len, hd = hidden_states.size()\\n+\\n+    if (self.training):\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states = residual + hidden_states\\n \\n-    # Self Attention\\n-    hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-        hidden_states=hidden_states,\\n-        causal_mask=causal_mask,\\n-        attention_mask=attention_mask,\\n-        position_ids=position_ids,\\n-        past_key_value=past_key_value,\\n-        output_attentions=output_attentions,\\n-        use_cache=use_cache,\\n-        padding_mask=padding_mask,\\n-    )\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = self.mlp(hidden_states)\\n+        hidden_states = residual + hidden_states\\n+    else:\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states += residual\\n \\n-    # Fully Connected\\n-    residual = hidden_states\\n-    hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n-    hidden_states = self.mlp(hidden_states)\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n+        hidden_states += residual\\n+    pass\\n \\n     outputs = (hidden_states,)\\n \\n@@ -378,6 +416,7 @@ def LlamaModel_fast_forward(\\n     if past_key_values is not None:\\n         past_key_values_length = past_key_values[0][0].shape[2]\\n         seq_length_with_past = seq_length_with_past + past_key_values_length\\n+    pass\\n \\n     # We already handle KV cache position_ids ourselves.\\n     if (past_key_values_length != 0):\\n@@ -391,10 +430,12 @@ def LlamaModel_fast_forward(\\n         position_ids = position_ids.view(-1, seq_length).to(torch.int32)#.long()\\n     else:\\n         position_ids = None\\n+    pass\\n \\n     if position_ids is not None:\\n         if position_ids.shape[0] != batch_size:\\n             position_ids = position_ids.repeat((batch_size, 1))\\n+    pass\\n \\n     # embed positions\\n     if inputs_embeds is None:\\n@@ -403,19 +444,22 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n+    elif self.training:\\n+        attention_mask = None\\n+        padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n             padding_mask = attention_mask\\n         else:\\n             padding_mask = None\\n \\n+        from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n         attention_mask = _prepare_4d_causal_attention_mask(\\n             attention_mask,\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = None if not hasattr(self.config, \"sliding_window\") else \\\\\\n-                self.config.sliding_window,\\n+            sliding_window = getattr(self.config, \"sliding_window\"),\\n         )\\n     pass\\n \\n@@ -479,7 +523,11 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    if (self.training):\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    pass\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -665,6 +713,7 @@ class FastLlamaModel:\\n                 bnb_4bit_quant_type       = \"nf4\",\\n                 bnb_4bit_compute_dtype    = dtype,\\n             )\\n+        pass\\n \\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n@@ -714,6 +763,7 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -721,6 +771,7 @@ class FastLlamaModel:\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+\\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n@@ -751,7 +802,7 @@ class FastLlamaModel:\\n         correct_dtype = lm_head.weight.dtype\\n \\n         for name, module in model.named_modules():\\n-            if isinstance(module, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):\\n                 weight = module.weight\\n                 quant_state = weight.quant_state\\n \\n@@ -766,8 +817,10 @@ class FastLlamaModel:\\n         pass\\n \\n         # Clear deleted GPU items\\n-        gc.collect()\\n-        torch.cuda.empty_cache()\\n+        import gc\\n+        for _ in range(3):\\n+            gc.collect()\\n+            torch.cuda.empty_cache()\\n         return model\\n     pass\\n \\n@@ -782,11 +835,26 @@ class FastLlamaModel:\\n         lora_dropout = 0,\\n         bias = \"none\",\\n         layers_to_transform = None,\\n+        layers_pattern = None,\\n         use_gradient_checkpointing = True,\\n         random_state = 3407,\\n         max_seq_length = 2048, # not used anymore\\n+        use_rslora = False,\\n+        init_lora_weights = True,\\n+        loftq_config = None,\\n         **kwargs,\\n     ):\\n+        if isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n+            )\\n+        pass\\n+\\n+        import inspect\\n+        signature = str(inspect.signature(LoraConfig))\\n+        SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n+        SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n+\\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n@@ -794,11 +862,61 @@ class FastLlamaModel:\\n                 f\"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = {lora_dropout}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n         if bias != \"none\":\\n             logger.warning_once(\\n                 f\"Unsloth: bias = `none` is supported for fast patching. You are using bias = {bias}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n+        if not (type(init_lora_weights) is bool or \\\\\\n+            init_lora_weights == \"gaussian\" or init_lora_weights == \"loftq\"):\\n+            raise ValueError(\\n+                \\'Unsloth: `init_lora_weights` must be either [True, False, \"gaussian\", \"loftq\"].\\'\\n+            )\\n+        pass\\n+\\n+        if init_lora_weights == \"loftq\":\\n+\\n+            if not SUPPORTS_LOFTQ:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+\\n+            if loftq_config is None:\\n+                from peft import LoftQConfig\\n+                logger.warning_once(\\n+                    f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n+                    f\"We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.\"\\n+                )\\n+                loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)\\n+            pass\\n+            \\n+            if hasattr(model.config, \"quantization_config\"):\\n+                raise ValueError(\\n+                    \"Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\\\\n\"\\\\\\n+                    \"Reload your model without any quantization by setting `load_in_4bit = False`.\"\\n+                )\\n+            pass\\n+        pass\\n+\\n+        assert(type(use_rslora) is bool)\\n+        if use_rslora:\\n+            if not SUPPORTS_RSLORA:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+        pass\\n \\n         transformers_set_seed(random_state)\\n \\n@@ -810,16 +928,23 @@ class FastLlamaModel:\\n         pass\\n \\n         # Get LoRA\\n-        lora_config = LoraConfig(\\n-            r              = r,\\n-            lora_alpha     = lora_alpha,\\n-            target_modules = target_modules,\\n-            lora_dropout   = lora_dropout,\\n-            bias           = bias,\\n-            task_type      = TaskType.CAUSAL_LM,\\n+        arguments = dict(\\n+            r                   = r,\\n+            lora_alpha          = lora_alpha,\\n+            target_modules      = target_modules,\\n+            lora_dropout        = lora_dropout,\\n+            bias                = bias,\\n+            task_type           = TaskType.CAUSAL_LM,\\n             layers_to_transform = layers_to_transform,\\n+            init_lora_weights   = init_lora_weights,\\n+            loftq_config        = loftq_config,\\n+            use_rslora          = use_rslora,\\n             **kwargs,\\n         )\\n+        if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n+        if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n+\\n+        lora_config = LoraConfig(**arguments)\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n@@ -828,10 +953,21 @@ class FastLlamaModel:\\n         )\\n         model = _get_peft_model(model, lora_config)\\n \\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.peft_config[\"default\"].base_model_name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        pass\\n+        # Add revision to enable future fast inference paths\\n+        model.peft_config[\"default\"].revision = f\"unsloth\"\\n+\\n         # Do patching\\n         n_mlp = 0\\n         n_qkv = 0\\n         n_o   = 0\\n+        import types\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n@@ -897,6 +1033,7 @@ class FastLlamaModel:\\n             f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n             f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n         )\\n+        patch_saving_functions(model)\\n \\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n',\n",
       " '@@ -16,16 +16,9 @@ from .llama import FastLlamaModel, logger\\n from .mistral import FastMistralModel\\n from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n+from peft import PeftConfig, PeftModel\\n+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n \\n-FOURBIT_MAPPER = \\\\\\n-{\\n-    \"unsloth/mistral-7b-bnb-4bit\"    : \"unsloth/mistral-7b\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\"    : \"unsloth/llama-2-7b\",\\n-    \"unsloth/llama-2-13b-bnb-4bit\"   : \"unsloth/llama-13-7b\",\\n-    \"unsloth/codellama-34b-bnb-4bit\" : \"codellama/CodeLlama-34b-hf\",\\n-    \"unsloth/zephyr-sft-bnb-4bit\"    : \"unsloth/zephyr-sft\",\\n-    \"unsloth/tinyllama-bnb-4bit\"     : \"unsloth/tinyllama\",\\n-}\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -34,6 +27,39 @@ SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)\\n del major, minor\\n \\n \\n+def _get_model_name(model_name, load_in_4bit = True):\\n+\\n+    if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n+            f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n+            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n+            f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n+        )\\n+    \\n+    elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n+            f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n+        )\\n+        model_name = new_model_name\\n+\\n+    elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n+            f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n+        )\\n+        model_name = new_model_name\\n+    pass\\n+\\n+    return model_name\\n+pass\\n+\\n+\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n@@ -47,25 +73,27 @@ class FastLanguageModel(FastLlamaModel):\\n         fix_tokenizer = True,\\n         *args, **kwargs,\\n     ):\\n-        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:\\n-            model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n-                f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-                f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n-                f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n-                f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n-            )\\n-        elif not load_in_4bit and model_name in FOURBIT_MAPPER:\\n-            new_model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n-                f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n-            )\\n-            model_name = new_model_name\\n+        old_model_name = model_name\\n+        model_name = _get_model_name(model_name, load_in_4bit)\\n+\\n+        # First check if it\\'s a normal model via AutoConfig\\n+        is_peft = False\\n+        try:\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = False\\n+        except:\\n+            try:\\n+                # Most likely a PEFT model\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+            except:\\n+                raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n+            \\n+            # Check base model again for PEFT\\n+            model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = True\\n         pass\\n \\n-        model_config = AutoConfig.from_pretrained(model_name)\\n         model_type = model_config.model_type\\n \\n         if   model_type == \"llama\":   dispatch_model = FastLlamaModel\\n@@ -75,8 +103,9 @@ class FastLanguageModel(FastLlamaModel):\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n                 \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n+        pass\\n \\n-        return dispatch_model.from_pretrained(\\n+        model, tokenizer = dispatch_model.from_pretrained(\\n             model_name = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype = dtype,\\n@@ -87,5 +116,30 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n+\\n+        if load_in_4bit:\\n+            # Fix up bitsandbytes config\\n+            quantization_config = \\\\\\n+            {\\n+                # Sometimes torch_dtype is not a string!!\\n+                \"bnb_4bit_compute_dtype\"           : model.config.to_dict()[\"torch_dtype\"],\\n+                \"bnb_4bit_quant_type\"              : \"nf4\",\\n+                \"bnb_4bit_use_double_quant\"        : True,\\n+                \"llm_int8_enable_fp32_cpu_offload\" : False,\\n+                \"llm_int8_has_fp16_weight\"         : False,\\n+                \"llm_int8_skip_modules\"            : \"null\",\\n+                \"llm_int8_threshold\"               : 6.0,\\n+                \"load_in_4bit\"                     : True,\\n+                \"load_in_8bit\"                     : False,\\n+                \"quant_method\"                     : \"bitsandbytes\",\\n+            }\\n+            model.config.update({\"quantization_config\" : quantization_config})\\n+        pass\\n+\\n+        if is_peft:\\n+            # Now add PEFT adapters\\n+            model = PeftModel.from_pretrained(model, old_model_name)\\n+        pass\\n+        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -0,0 +1,56 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+__all__ = [\\n+    \"INT_TO_FLOAT_MAPPER\",\\n+    \"FLOAT_TO_INT_MAPPER\",\\n+]\\n+\\n+__INT_TO_FLOAT_MAPPER = \\\\\\n+{\\n+    \"unsloth/mistral-7b-bnb-4bit\"    : (\\n+        \"unsloth/mistral-7b\",\\n+        \"mistralai/Mistral-7B-v0.1\",\\n+    ),\\n+    \"unsloth/llama-2-7b-bnb-4bit\"    : (\\n+        \"unsloth/llama-2-7b\",\\n+        \"meta-llama/Llama-2-7b-hf\",\\n+    ),\\n+    \"unsloth/llama-2-13b-bnb-4bit\"   : (\\n+        \"unsloth/llama-13-7b\",\\n+        \"meta-llama/Llama-2-13b-hf\",\\n+    ),\\n+    \"unsloth/codellama-34b-bnb-4bit\" : (\\n+        \"codellama/CodeLlama-34b-hf\",\\n+    ),\\n+    \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n+        \"unsloth/zephyr-sft\",\\n+        \"alignment-handbook/zephyr-7b-sft-full\",\\n+    ),\\n+    \"unsloth/tinyllama-bnb-4bit\"     : (\\n+        \"unsloth/tinyllama\",\\n+        \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\\n+    ),\\n+}\\n+\\n+INT_TO_FLOAT_MAPPER = {}\\n+FLOAT_TO_INT_MAPPER = {}\\n+\\n+for key, values in __INT_TO_FLOAT_MAPPER.items():\\n+    INT_TO_FLOAT_MAPPER[key] = values[0]\\n+\\n+    for value in values:\\n+        FLOAT_TO_INT_MAPPER[value] = key\\n+    pass\\n+pass\\n',\n",
       " '@@ -343,6 +343,7 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -350,6 +351,7 @@ class FastMistralModel(FastLlamaModel):\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+        \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n         \\n',\n",
       " '@@ -12,24 +12,26 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from peft import PeftModelForCausalLM\\n-from collections import OrderedDict\\n-import bitsandbytes as bnb\\n-import peft\\n-import gc\\n-import os\\n-from tqdm import tqdm as ProgressBar\\n-import shutil\\n-from typing import Optional, Callable, Union\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from typing import Optional, Callable, Union, List\\n import torch\\n+import os\\n+import pickle\\n+import gc\\n from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+import subprocess\\n+import psutil\\n \\n __all__ = [\\n+    \"print_quantization_methods\",\\n     \"unsloth_save_model\",\\n-    #\"colab_quantize_to_gguf\",\\n+    \"save_to_gguf\",\\n+    \"patch_saving_functions\",\\n ]\\n \\n+\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -41,25 +43,36 @@ LLAMA_LAYERNORMS = (\\n # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n ALLOWED_QUANTS = \\\\\\n {\\n-    \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n-    \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_s\" : \"Uses Q3_K for all tensors\",\\n-    \"q4_0\"   : \"Original quant method, 4-bit.\",\\n-    \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n-    \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n-    \"q4_k_s\" : \"Uses Q4_K for all tensors\",\\n-    \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\\n-    \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\\n-    \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n-    \"q5_k_s\" : \"Uses Q5_K for all tensors\",\\n-    \"q6_k\"   : \"Uses Q8_K for all tensors\",\\n-    \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\\n+    \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+    \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+    \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+    \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+    \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+    \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+    \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+    \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+    \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+    \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+    \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+    \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+    \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+    \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+    \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+    \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+    \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n }\\n \\n+def print_quantization_methods():\\n+    for key, value in ALLOWED_QUANTS.items():\\n+        print(f\\'\"{key}\"  ==> {value}\\')\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n         dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n@@ -75,100 +88,362 @@ def _merge_lora(layer, name):\\n pass\\n \\n \\n+def fast_save_pickle(shard, name):\\n+    # Use this if # CPUs is <= 2\\n+    print(f\"Unsloth: Saving {name}...\")\\n+    torch.save(\\n+        shard,\\n+        name,\\n+        pickle_module = pickle,\\n+        pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n+    )\\n+    return\\n+pass\\n+\\n+\\n @torch.inference_mode\\n def unsloth_save_model(\\n     model,\\n     tokenizer,\\n-    save_directory: Union[str, os.PathLike],\\n-    is_main_process: bool = True,\\n-    state_dict: Optional[dict] = None,\\n-    save_function: Callable = torch.save,\\n-    push_to_hub: bool = False,\\n-    max_shard_size: Union[int, str] = \"7GB\",\\n-    safe_serialization: bool = True,\\n-    variant: Optional[str] = None,\\n-    token: Optional[Union[str, bool]] = None,\\n-    save_peft_format: bool = True,\\n-    temporary_location = \"_unsloth_temporary_saved_buffers\",\\n-    **kwargs,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+\\n+    # Push to hub\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    create_pr            : bool = False,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : List[str] = None,\\n+\\n+    # Our functions\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.9,\\n ):\\n-    logger.warning_once(\\n-        \"Unsloth: `unsloth_save_model` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+    save_pretrained_settings = dict(locals())\\n+    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    import re\\n+\\n+    assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n+\\n+    # Clean memory up first\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n+    pass\\n+\\n+    save_method = save_method.lower().replace(\" \", \"_\")\\n+    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n+            \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n+            \\'\"merged_16bit\" ==> This merges LoRA weights and saves to float16. Needed for llama.cpp / GGUF.\\\\n\\'\\\\\\n+            \\'\"merged_4bit\"  ==> This merges LoRA weights and saves to 4bit. Useful for DPO / inference.\\'\\n+        )\\n+    pass\\n+\\n+    if save_method == \"merged_4bit\":\\n+        print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n+        print(\"This might take 5 minutes...\")\\n+        model = model.merge_and_unload()\\n+        print(\"Done.\")\\n+    pass\\n+\\n+    if tags is not None:\\n+        assert(isinstance(tags, (list, tuple)))\\n+        tags = list(tags) + [\"unsloth\",]\\n+    else:\\n+        tags = [\"unsloth\",]\\n+    pass\\n+    save_pretrained_settings[\"tags\"] = tags\\n+\\n+    if (save_method == \"lora\") and push_to_hub:\\n+        if token is None:\\n+            raise RuntimeError(\\n+                \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens.\"\\n+            )\\n+        pass\\n+\\n+        model.push_to_hub(\\n+            repo_id            = save_directory,\\n+            use_temp_dir       = use_temp_dir,\\n+            commit_message     = commit_message,\\n+            private            = private,\\n+            token              = token,\\n+            max_shard_size     = max_shard_size,\\n+            create_pr          = create_pr,\\n+            safe_serialization = safe_serialization,\\n+            revision           = revision,\\n+            commit_description = commit_description,\\n+            tags               = tags,\\n+        )\\n+        if tokenizer is not None:\\n+            tokenizer.push_to_hub(\\n+                repo_id            = save_directory,\\n+                use_temp_dir       = use_temp_dir,\\n+                commit_message     = commit_message,\\n+                private            = private,\\n+                token              = token,\\n+                max_shard_size     = max_shard_size,\\n+                create_pr          = create_pr,\\n+                safe_serialization = safe_serialization,\\n+                revision           = revision,\\n+                commit_description = commit_description,\\n+                tags               = tags,\\n+            )\\n+        pass\\n+        return save_directory\\n+    pass\\n+\\n+    # If push_to_hub, we must remove the .../ part of a repo\\n+    if push_to_hub and \"/\" in save_directory:\\n+\\n+        new_save_directory = save_directory[save_directory.find(\"/\"):]\\n+\\n+        logger.warning_once(\\n+            f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n+            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n+        )\\n+\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_directory = new_save_directory\\n+    pass\\n+    \\n+    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+        not hasattr(model, \"model\") or \\\\\\n+        not hasattr(model.model, \"model\") or \\\\\\n+        not hasattr(model.model.model, \"layers\")\\n+    ):\\n+        # Do general saving\\n+        \\n+        # Edit save_pretrained_settings\\n+        # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+        for deletion in \\\\\\n+            (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+            del save_pretrained_settings[deletion]\\n+        pass\\n+        if hasattr(model, \"add_model_tags\"):\\n+            model.add_model_tags([\"unsloth\",])\\n+\\n+        if tokenizer is not None:\\n+            print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            print(\" Done.\")\\n+        else:\\n+            print()\\n \\n+        print(\"Unsloth: Saving model...\", end = \"\")\\n+        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+\\n+        model.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+        return save_directory\\n+    pass\\n+\\n+    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+\\n+    # Determine max RAM usage minus sharding\\n+    max_ram = psutil.virtual_memory().available\\n+    sharded_ram_usage = 5 * 1024 * 1024 * 1024\\n+    if type(max_shard_size) is str:\\n+        gb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}GB\", max_shard_size, flags = re.IGNORECASE)\\n+        mb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}MB\", max_shard_size, flags = re.IGNORECASE)\\n+        if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024\\n+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 \\n+    elif type(max_shard_size) is int:\\n+        sharded_ram_usage = sharded_ram_usage\\n+    pass\\n+\\n+    # Switch to our fast saving modules if it\\'s a slow PC!\\n+    n_cpus = psutil.cpu_count(logical = False)\\n+\\n+    if safe_serialization is None:\\n+        safe_serialization = True\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+\\n+    elif safe_serialization and (n_cpus <= 2):\\n+        logger.warning_once(\\n+            f\"Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\\\\n\"\\\\\\n+            f\"We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\\\\n\"\\\\\\n+            f\"To force `safe_serialization`, set it to None instead.\",\\n+        )\\n+        safe_serialization = False\\n+        save_function = fast_save_pickle\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+        save_pretrained_settings[\"save_function\"]      = save_function\\n+    pass\\n+\\n+    # Only safe_serialization uses more RAM\\n+    if safe_serialization:\\n+        max_ram -= sharded_ram_usage\\n+    else:\\n+        max_ram -= sharded_ram_usage*0.25 # Uses much less\\n+    pass\\n+\\n+    max_ram = int(max(0, max_ram) * maximum_memory_usage)\\n+    print(f\"Unsloth: Will use up to \"\\\\\\n+          f\"{round(max_ram/1024/1024/1024, 2)} out of \"\\\\\\n+          f\"{round(psutil.virtual_memory().total/1024/1024/1024, 2)} RAM for saving.\")\\n+\\n+    # Max directory for disk saving\\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    assert(hasattr(model, \"model\"))\\n-    assert(hasattr(model.model, \"model\"))\\n-    assert(hasattr(model.model.model, \"layers\"))\\n-\\n     # HF also uses a OrderedDict\\n+    from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight\\n+    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n \\n-    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+    max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n+\\n+    from tqdm import tqdm as ProgressBar\\n     for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n             W = _merge_lora(proj, name)\\n-            filename = os.path.join(temporary_location, f\"{name}.pt\")\\n-            torch.save(W, filename)\\n-            state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n+\\n+            if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n+                # Save to GPU memory\\n+                state_dict[name] = W\\n+            # elif (max_ram - W.nbytes) > 0:\\n+            #     # Save to CPU memory\\n+            #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n+            #     state_dict[name] = W.to(\"cpu\", non_blocking = True)\\n+            #     max_ram = max(max_ram - W.nbytes, 0)\\n+            else:\\n+                # Save to Disk\\n+                logger.warning_once(f\"We will save to Disk and not RAM now.\")\\n+                filename = os.path.join(temporary_location, f\"{name}.pt\")\\n+                torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)\\n+                state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n         pass\\n         for item in LLAMA_LAYERNORMS:\\n-            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight\")\\n+            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight.data\")\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight\\n-\\n-    print(\"Unsloth: Saving tokenizer...\")\\n-    tokenizer.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n \\n-    print(\"Unsloth: Saving model. This will take 5 minutes for Llama-7b...\")\\n-    model.model.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n+    for key, value in state_dict.items():\\n+        if hasattr(value, \"data\"): state_dict[key] = value = value.data\\n+        if type(value) is not torch.Tensor:\\n+            logger.warning_once(f\"Unsloth: {key} is not a Tensor but a {type(value)}.\")\\n+        pass\\n+    pass\\n+\\n+    # Edit save_pretrained_settings\\n+    # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+    save_pretrained_settings[\"state_dict\"] = state_dict\\n+    for deletion in \\\\\\n+        (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if tokenizer is not None:\\n+        print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+    else:\\n+        print()\\n+\\n+    print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+    model.model.save_pretrained(**save_pretrained_settings)\\n+    print(\"Done.\")\\n+\\n+    save_pretrained_settings[\"state_dict\"] = None\\n+\\n+    # for j, (key, value) in enumerate(state_dict.items()):\\n+    #     state_dict[key] = None\\n+    #     if j % 10 == 0:\\n+    #         torch.cuda.empty_cache()\\n+    #         gc.collect()\\n+    #     pass\\n+    # pass\\n+    # state_dict = None\\n+    # del state_dict\\n+    # torch.cuda.empty_cache()\\n+    # gc.collect()\\n \\n     # Remove temporary location\\n+    import shutil\\n     shutil.rmtree(temporary_location)\\n+\\n+    # for _ in range(3):\\n+    #     torch.cuda.empty_cache()\\n+    #     gc.collect()\\n+    return save_directory\\n pass\\n \\n \\n-\"\"\"\\n-def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n+def install_llama_cpp_clone_non_blocking():\\n+    full_command = [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n \\n-    logger.warning_once(\\n-        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+\\n+def install_llama_cpp_make_non_blocking():\\n+    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_python_non_blocking(packages = []):\\n+    full_command = [\"pip\", \"install\"] + packages\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_llama_cpp_blocking():\\n+    commands = [\\n+        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}\",\\n+        \"pip install gguf protobuf\",\\n+    ]\\n+    if os.path.exists(\"llama.cpp\"): return\\n+    for command in commands:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n+def save_to_gguf(\\n+    model_directory     : str = \"unsloth_finetuned_model\",\\n+    quantization_method : str = \"fast_quantized\",\\n+    _run_installer = None, # Non blocking install of llama.cpp\\n+):\\n+    from transformers.models.llama.modeling_llama import logger\\n+\\n+    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n+    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n+    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n+    elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -181,27 +456,409 @@ def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n-    if not os.path.exists(\"llama.cpp\"):\\n-        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n-        !git clone https://github.com/ggerganov/llama.cpp\\n-        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\\n-        !pip install gguf protobuf\\n+    print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n+    if _run_installer is not None:\\n+        _run_installer.wait()\\n+    else:\\n+        install_llama_cpp_blocking()\\n+    pass\\n+\\n+    print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n+    first_conversion = \"f16\"\\n+    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+\\n+    n_cpus = psutil.cpu_count()*2\\n+    # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n+    \\n+    final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n+\\n+    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+        f\"--outfile {final_location} \"\\\\\\n+        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+\\n+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+        for line in sp.stdout:\\n+            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+    pass\\n+\\n+    print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+\\n+    if quantization_method != first_conversion:\\n+        old_location = final_location\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+\\n+        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n+        \\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+        print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+    pass\\n+\\n+    return final_location\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_merged(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]     = self\\n+    arguments[\"tokenizer\"] = None\\n+    del arguments[\"self\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_merged(\\n+    self,\\n+    repo_id              : str,\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = None\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = True\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_gguf(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n+\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]       = self\\n+    arguments[\"tokenizer\"]   = tokenizer\\n+    arguments[\"push_to_hub\"] = False # We save ourselves\\n+    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"quantization_method\"]\\n+\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n+    python_install.wait()\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # And save to HF\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+        from huggingface_hub import create_repo\\n+        create_repo(\\n+            repo_id   = save_directory,\\n+            token     = token,\\n+            repo_type = \"model\",\\n+            exist_ok  = True,\\n+        )\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n+\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n+\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n     pass\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_gguf(\\n+    self,\\n+    repo_id              : str,\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n-    print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\\n-    !python llama.cpp/convert.py {save_directory} \\\\\\n-        --outfile {save_directory}-unsloth.gguf \\\\\\n-        --outtype f16\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = tokenizer\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = False # We save ourselves\\n+    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    del arguments[\"quantization_method\"]\\n \\n-    print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\\n-    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\\n-    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\\\\n-        {final_location} {quantization_method}\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n \\n-    print(f\"Unsloth: Output location: {final_location}\")\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    python_install.wait()\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # Save to hub\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        private   = private,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n+def patch_saving_functions(model):\\n+    import inspect\\n+    import re\\n+    import types\\n+    from typing import Callable, Optional, Union, List\\n+\\n+    if hasattr(model, \"_original_push_to_hub\"): return\\n+\\n+    original_push_to_hub = model.push_to_hub\\n+    signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n+    signature = signature[1:]\\n+    signature = re.sub(\"<function save at .+?>\", \"torch.save\", signature)\\n+    docs = original_push_to_hub.__doc__.encode(\"utf-8\").decode(\"utf-8\")\\n+    model._original_push_to_hub = original_push_to_hub\\n+\\n+    push_to_hub_text = f\\'\\'\\'def unsloth_push_to_hub(self, {signature}:\\n+    \"\"\"\\n+    {docs}\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    del arguments[\"self\"]\\n+    if \"tags\" in arguments and arguments[\"tags\"] is not None:\\n+        assert(isinstance(arguments[\"tags\"], (list, tuple)))\\n+        arguments[\"tags\"] = list(arguments[\"tags\"]) + [\"unsloth\",]\\n+    elif \"tags\" in arguments:\\n+        arguments[\"tags\"] = [\"unsloth\",]\\n+    elif hasattr(self, \"add_model_tags\"):\\n+        self.add_model_tags([\"unsloth\",])\\n+    try:\\n+        return self._original_push_to_hub(**arguments)\\n+    except:\\n+        del arguments[\"tags\"]\\n+        return self._original_push_to_hub(**arguments)\\n+    pass\\n+    \\'\\'\\'\\n+    exec(push_to_hub_text, globals())\\n+    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)\\n+\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if hasattr(model, \"config\"):\\n+        # Counteract tokenizers\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+    else:\\n+        model.push_to_hub_merged     = model.push_to_hub\\n+        model.save_pretrained_merged = model.save_pretrained\\n+        model.push_to_hub_gguf       = model.push_to_hub\\n+        model.save_pretrained_gguf   = model.save_pretrained\\n+    pass\\n+\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        if hasattr(original_model, \"_original_push_to_hub\"): continue\\n+        \\n+        original_model._original_push_to_hub = original_model.push_to_hub\\n+        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n+\\n+        if hasattr(original_model, \"add_model_tags\"):\\n+            original_model.add_model_tags([\"unsloth\",])\\n+\\n+        if hasattr(original_model, \"config\"):\\n+            # Counteract tokenizers\\n+            original_model.push_to_hub_merged     = \\\\\\n+                types.MethodType(unsloth_push_to_hub_merged,     original_model)\\n+\\n+            original_model.save_pretrained_merged = \\\\\\n+                types.MethodType(unsloth_save_pretrained_merged, original_model)\\n+\\n+            original_model.push_to_hub_gguf       = \\\\\\n+                types.MethodType(unsloth_push_to_hub_gguf,       original_model)\\n+\\n+            original_model.save_pretrained_gguf   = \\\\\\n+                types.MethodType(unsloth_save_pretrained_gguf,   original_model)\\n+        pass\\n+    pass\\n+    return\\n pass\\n-\"\"\"\\n',\n",
       " '@@ -65,8 +65,7 @@ try:\\n     libcuda_dirs()\\n except:\\n     warnings.warn(\\n-        \"CUDA is not linked properly.\\\\n\"\\\\\\n-        \"We shall run `ldconfig /usr/lib64-nvidia` to try to fix it.\"\\n+        \"Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n     )\\n     os.system(\"ldconfig /usr/lib64-nvidia\")\\n     importlib.reload(bnb)\\n',\n",
       " '@@ -41,12 +41,13 @@ def _rms_layernorm_forward(\\n     r += row_idx * r_row_stride\\n \\n     X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n-    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n-    inv_var = 1 / tl.sqrt(row_var + eps)\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps)\\n     tl.store(r, inv_var)\\n     normed = X_row * inv_var\\n+    normed = normed.to(W_row.dtype) # Exact copy from HF\\n     output = normed * W_row\\n     tl.store(Y + col_offsets, output, mask = mask)\\n pass\\n',\n",
       " '@@ -25,10 +25,11 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     mask = offsets < n_elements\\n \\n     e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row = tl.load(g + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n     f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n \\n@@ -53,12 +54,13 @@ def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n-    DW_row = tl.load(DW + offsets, mask = mask, other = 0).to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n-    g_row  = tl.load(g  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row))\\n+    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n+    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n     # f = e * se\\n     f_row = e_row * se_row\\n     # h = f * g\\n',\n",
       " '@@ -14,9 +14,7 @@\\n \\n import torch\\n from typing import Union, Optional, List, Any, Callable\\n-import numpy as np\\n import warnings\\n-import gc\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n',\n",
       " '@@ -15,7 +15,6 @@\\n import torch\\n from typing import Optional, Tuple, List, Union\\n from torch.nn.functional import scaled_dot_product_attention\\n-from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n from transformers.models.llama.modeling_llama import (\\n     logger,\\n     BaseModelOutputWithPast,\\n@@ -46,16 +45,13 @@ except:\\n     LlamaFlashAttention2 = LlamaAttention\\n pass\\n \\n-from peft import PeftModelForCausalLM\\n-import gc\\n-import peft\\n-import bitsandbytes as bnb\\n-import numpy as np\\n-import types\\n-\\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n+from peft import PeftModelForCausalLM\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from ..save import patch_saving_functions\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -110,18 +106,15 @@ def LlamaAttention_fast_forward_inference(\\n     bsz, _, _ = hidden_states.size()\\n     K1, V1 = past_key_value\\n \\n-    Wq = self.q_proj.weight\\n-    Wk = self.k_proj.weight\\n-    Wv = self.v_proj.weight\\n-    Wo = self.o_proj.weight\\n-\\n     n_heads    = self.num_heads\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n     head_dim   = self.head_dim\\n     assert(n_kv_heads * n_groups == n_heads)\\n \\n-    Qn, Kn, Vn = original_apply_qkv(self, Xn)\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n     Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n     Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n     Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n@@ -156,6 +149,28 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_silu = torch.nn.functional.silu\\n+def fast_mlp_inference(self, X):\\n+    gate = self.gate_proj(X)\\n+    up   = self.up_proj(X)\\n+    gate = torch_silu(gate, inplace = True)\\n+    gate *= up\\n+    X = self.down_proj(gate)\\n+    return X\\n+pass\\n+\\n+\\n+def fast_rms_layernorm_inference(self, X):\\n+    X = X.to(torch.float32)\\n+    variance = X.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    X *= variance.rsqrt_()\\n+    X = X.to(residual.dtype)\\n+    X *= self.weight\\n+    return X\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -287,28 +302,51 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    residual = hidden_states\\n-\\n-    hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+    bsz, q_len, hd = hidden_states.size()\\n+\\n+    if (self.training):\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states = residual + hidden_states\\n \\n-    # Self Attention\\n-    hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-        hidden_states=hidden_states,\\n-        causal_mask=causal_mask,\\n-        attention_mask=attention_mask,\\n-        position_ids=position_ids,\\n-        past_key_value=past_key_value,\\n-        output_attentions=output_attentions,\\n-        use_cache=use_cache,\\n-        padding_mask=padding_mask,\\n-    )\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = self.mlp(hidden_states)\\n+        hidden_states = residual + hidden_states\\n+    else:\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states += residual\\n \\n-    # Fully Connected\\n-    residual = hidden_states\\n-    hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n-    hidden_states = self.mlp(hidden_states)\\n-    hidden_states = residual + hidden_states\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n+        hidden_states += residual\\n+    pass\\n \\n     outputs = (hidden_states,)\\n \\n@@ -378,6 +416,7 @@ def LlamaModel_fast_forward(\\n     if past_key_values is not None:\\n         past_key_values_length = past_key_values[0][0].shape[2]\\n         seq_length_with_past = seq_length_with_past + past_key_values_length\\n+    pass\\n \\n     # We already handle KV cache position_ids ourselves.\\n     if (past_key_values_length != 0):\\n@@ -391,10 +430,12 @@ def LlamaModel_fast_forward(\\n         position_ids = position_ids.view(-1, seq_length).to(torch.int32)#.long()\\n     else:\\n         position_ids = None\\n+    pass\\n \\n     if position_ids is not None:\\n         if position_ids.shape[0] != batch_size:\\n             position_ids = position_ids.repeat((batch_size, 1))\\n+    pass\\n \\n     # embed positions\\n     if inputs_embeds is None:\\n@@ -403,19 +444,22 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n+    elif self.training:\\n+        attention_mask = None\\n+        padding_mask = None\\n     else:\\n         if 0 in attention_mask:\\n             padding_mask = attention_mask\\n         else:\\n             padding_mask = None\\n \\n+        from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\\n         attention_mask = _prepare_4d_causal_attention_mask(\\n             attention_mask,\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = None if not hasattr(self.config, \"sliding_window\") else \\\\\\n-                self.config.sliding_window,\\n+            sliding_window = getattr(self.config, \"sliding_window\"),\\n         )\\n     pass\\n \\n@@ -479,7 +523,11 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    if (self.training):\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    pass\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -665,6 +713,7 @@ class FastLlamaModel:\\n                 bnb_4bit_quant_type       = \"nf4\",\\n                 bnb_4bit_compute_dtype    = dtype,\\n             )\\n+        pass\\n \\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n@@ -714,6 +763,7 @@ class FastLlamaModel:\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -721,6 +771,7 @@ class FastLlamaModel:\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+\\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n@@ -751,7 +802,7 @@ class FastLlamaModel:\\n         correct_dtype = lm_head.weight.dtype\\n \\n         for name, module in model.named_modules():\\n-            if isinstance(module, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):\\n                 weight = module.weight\\n                 quant_state = weight.quant_state\\n \\n@@ -766,8 +817,10 @@ class FastLlamaModel:\\n         pass\\n \\n         # Clear deleted GPU items\\n-        gc.collect()\\n-        torch.cuda.empty_cache()\\n+        import gc\\n+        for _ in range(3):\\n+            gc.collect()\\n+            torch.cuda.empty_cache()\\n         return model\\n     pass\\n \\n@@ -782,11 +835,26 @@ class FastLlamaModel:\\n         lora_dropout = 0,\\n         bias = \"none\",\\n         layers_to_transform = None,\\n+        layers_pattern = None,\\n         use_gradient_checkpointing = True,\\n         random_state = 3407,\\n         max_seq_length = 2048, # not used anymore\\n+        use_rslora = False,\\n+        init_lora_weights = True,\\n+        loftq_config = None,\\n         **kwargs,\\n     ):\\n+        if isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n+            )\\n+        pass\\n+\\n+        import inspect\\n+        signature = str(inspect.signature(LoraConfig))\\n+        SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n+        SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n+\\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n@@ -794,11 +862,61 @@ class FastLlamaModel:\\n                 f\"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = {lora_dropout}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n         if bias != \"none\":\\n             logger.warning_once(\\n                 f\"Unsloth: bias = `none` is supported for fast patching. You are using bias = {bias}.\\\\n\"\\\\\\n                 f\"Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\"\\n             )\\n+        pass\\n+\\n+        if not (type(init_lora_weights) is bool or \\\\\\n+            init_lora_weights == \"gaussian\" or init_lora_weights == \"loftq\"):\\n+            raise ValueError(\\n+                \\'Unsloth: `init_lora_weights` must be either [True, False, \"gaussian\", \"loftq\"].\\'\\n+            )\\n+        pass\\n+\\n+        if init_lora_weights == \"loftq\":\\n+\\n+            if not SUPPORTS_LOFTQ:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+\\n+            if loftq_config is None:\\n+                from peft import LoftQConfig\\n+                logger.warning_once(\\n+                    f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n+                    f\"We shall use `loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)`.\"\\n+                )\\n+                loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1)\\n+            pass\\n+            \\n+            if hasattr(model.config, \"quantization_config\"):\\n+                raise ValueError(\\n+                    \"Unsloth: You are using `loftq` init, yet `load_in_4bit = True` was set.\\\\n\"\\\\\\n+                    \"Reload your model without any quantization by setting `load_in_4bit = False`.\"\\n+                )\\n+            pass\\n+        pass\\n+\\n+        assert(type(use_rslora) is bool)\\n+        if use_rslora:\\n+            if not SUPPORTS_RSLORA:\\n+                import peft\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                )\\n+            pass\\n+        pass\\n \\n         transformers_set_seed(random_state)\\n \\n@@ -810,16 +928,23 @@ class FastLlamaModel:\\n         pass\\n \\n         # Get LoRA\\n-        lora_config = LoraConfig(\\n-            r              = r,\\n-            lora_alpha     = lora_alpha,\\n-            target_modules = target_modules,\\n-            lora_dropout   = lora_dropout,\\n-            bias           = bias,\\n-            task_type      = TaskType.CAUSAL_LM,\\n+        arguments = dict(\\n+            r                   = r,\\n+            lora_alpha          = lora_alpha,\\n+            target_modules      = target_modules,\\n+            lora_dropout        = lora_dropout,\\n+            bias                = bias,\\n+            task_type           = TaskType.CAUSAL_LM,\\n             layers_to_transform = layers_to_transform,\\n+            init_lora_weights   = init_lora_weights,\\n+            loftq_config        = loftq_config,\\n+            use_rslora          = use_rslora,\\n             **kwargs,\\n         )\\n+        if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n+        if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n+\\n+        lora_config = LoraConfig(**arguments)\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n@@ -828,10 +953,21 @@ class FastLlamaModel:\\n         )\\n         model = _get_peft_model(model, lora_config)\\n \\n+        # Fix up config for transformers uploading PEFT\\n+        name = model.peft_config[\"default\"].base_model_name_or_path\\n+        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+            name = name[:len(name) - len(\"-bnb-4bit\")]\\n+            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        pass\\n+        # Add revision to enable future fast inference paths\\n+        model.peft_config[\"default\"].revision = f\"unsloth\"\\n+\\n         # Do patching\\n         n_mlp = 0\\n         n_qkv = 0\\n         n_o   = 0\\n+        import types\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n@@ -897,6 +1033,7 @@ class FastLlamaModel:\\n             f\"Unsloth {__version__} patched {len(model.model.model.layers)} layers with \"\\\\\\n             f\"{n_qkv} QKV layers, {n_o} O layers and {n_mlp} MLP layers.\",\\n         )\\n+        patch_saving_functions(model)\\n \\n         # Patch cross entropy loss labels\\n         # Fixes https://github.com/unslothai/unsloth/issues/10\\n',\n",
       " '@@ -16,16 +16,9 @@ from .llama import FastLlamaModel, logger\\n from .mistral import FastMistralModel\\n from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n+from peft import PeftConfig, PeftModel\\n+from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n \\n-FOURBIT_MAPPER = \\\\\\n-{\\n-    \"unsloth/mistral-7b-bnb-4bit\"    : \"unsloth/mistral-7b\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\"    : \"unsloth/llama-2-7b\",\\n-    \"unsloth/llama-2-13b-bnb-4bit\"   : \"unsloth/llama-13-7b\",\\n-    \"unsloth/codellama-34b-bnb-4bit\" : \"codellama/CodeLlama-34b-hf\",\\n-    \"unsloth/zephyr-sft-bnb-4bit\"    : \"unsloth/zephyr-sft\",\\n-    \"unsloth/tinyllama-bnb-4bit\"     : \"unsloth/tinyllama\",\\n-}\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -34,6 +27,39 @@ SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)\\n del major, minor\\n \\n \\n+def _get_model_name(model_name, load_in_4bit = True):\\n+\\n+    if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n+            f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n+            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n+            f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n+        )\\n+    \\n+    elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n+            f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n+        )\\n+        model_name = new_model_name\\n+\\n+    elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        logger.warning_once(\\n+            f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n+            f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n+        )\\n+        model_name = new_model_name\\n+    pass\\n+\\n+    return model_name\\n+pass\\n+\\n+\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n@@ -47,25 +73,27 @@ class FastLanguageModel(FastLlamaModel):\\n         fix_tokenizer = True,\\n         *args, **kwargs,\\n     ):\\n-        if not SUPPORTS_FOURBIT and model_name in FOURBIT_MAPPER:\\n-            model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n-                f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-                f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n-                f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n-                f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n-            )\\n-        elif not load_in_4bit and model_name in FOURBIT_MAPPER:\\n-            new_model_name = FOURBIT_MAPPER[model_name]\\n-            logger.warning_once(\\n-                f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n-                f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n-            )\\n-            model_name = new_model_name\\n+        old_model_name = model_name\\n+        model_name = _get_model_name(model_name, load_in_4bit)\\n+\\n+        # First check if it\\'s a normal model via AutoConfig\\n+        is_peft = False\\n+        try:\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = False\\n+        except:\\n+            try:\\n+                # Most likely a PEFT model\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+            except:\\n+                raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n+            \\n+            # Check base model again for PEFT\\n+            model_name = _get_model_name(peft_config.base_model_name_or_path, load_in_4bit)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            is_peft = True\\n         pass\\n \\n-        model_config = AutoConfig.from_pretrained(model_name)\\n         model_type = model_config.model_type\\n \\n         if   model_type == \"llama\":   dispatch_model = FastLlamaModel\\n@@ -75,8 +103,9 @@ class FastLanguageModel(FastLlamaModel):\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n                 \"Make an issue to https://github.com/unslothai/unsloth!\",\\n             )\\n+        pass\\n \\n-        return dispatch_model.from_pretrained(\\n+        model, tokenizer = dispatch_model.from_pretrained(\\n             model_name = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype = dtype,\\n@@ -87,5 +116,30 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n+\\n+        if load_in_4bit:\\n+            # Fix up bitsandbytes config\\n+            quantization_config = \\\\\\n+            {\\n+                # Sometimes torch_dtype is not a string!!\\n+                \"bnb_4bit_compute_dtype\"           : model.config.to_dict()[\"torch_dtype\"],\\n+                \"bnb_4bit_quant_type\"              : \"nf4\",\\n+                \"bnb_4bit_use_double_quant\"        : True,\\n+                \"llm_int8_enable_fp32_cpu_offload\" : False,\\n+                \"llm_int8_has_fp16_weight\"         : False,\\n+                \"llm_int8_skip_modules\"            : \"null\",\\n+                \"llm_int8_threshold\"               : 6.0,\\n+                \"load_in_4bit\"                     : True,\\n+                \"load_in_8bit\"                     : False,\\n+                \"quant_method\"                     : \"bitsandbytes\",\\n+            }\\n+            model.config.update({\"quantization_config\" : quantization_config})\\n+        pass\\n+\\n+        if is_peft:\\n+            # Now add PEFT adapters\\n+            model = PeftModel.from_pretrained(model, old_model_name)\\n+        pass\\n+        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -0,0 +1,56 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+__all__ = [\\n+    \"INT_TO_FLOAT_MAPPER\",\\n+    \"FLOAT_TO_INT_MAPPER\",\\n+]\\n+\\n+__INT_TO_FLOAT_MAPPER = \\\\\\n+{\\n+    \"unsloth/mistral-7b-bnb-4bit\"    : (\\n+        \"unsloth/mistral-7b\",\\n+        \"mistralai/Mistral-7B-v0.1\",\\n+    ),\\n+    \"unsloth/llama-2-7b-bnb-4bit\"    : (\\n+        \"unsloth/llama-2-7b\",\\n+        \"meta-llama/Llama-2-7b-hf\",\\n+    ),\\n+    \"unsloth/llama-2-13b-bnb-4bit\"   : (\\n+        \"unsloth/llama-13-7b\",\\n+        \"meta-llama/Llama-2-13b-hf\",\\n+    ),\\n+    \"unsloth/codellama-34b-bnb-4bit\" : (\\n+        \"codellama/CodeLlama-34b-hf\",\\n+    ),\\n+    \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n+        \"unsloth/zephyr-sft\",\\n+        \"alignment-handbook/zephyr-7b-sft-full\",\\n+    ),\\n+    \"unsloth/tinyllama-bnb-4bit\"     : (\\n+        \"unsloth/tinyllama\",\\n+        \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\\n+    ),\\n+}\\n+\\n+INT_TO_FLOAT_MAPPER = {}\\n+FLOAT_TO_INT_MAPPER = {}\\n+\\n+for key, values in __INT_TO_FLOAT_MAPPER.items():\\n+    INT_TO_FLOAT_MAPPER[key] = values[0]\\n+\\n+    for value in values:\\n+        FLOAT_TO_INT_MAPPER[value] = key\\n+    pass\\n+pass\\n',\n",
       " '@@ -343,6 +343,7 @@ class FastMistralModel(FastLlamaModel):\\n                 token = token,\\n             )\\n         pass\\n+        patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n         name = model.config._name_or_path\\n@@ -350,6 +351,7 @@ class FastMistralModel(FastLlamaModel):\\n             name = name[:len(name) - len(\"-bnb-4bit\")]\\n             model.config.update({\"_name_or_path\" : name})\\n         pass\\n+        \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n         \\n',\n",
       " '@@ -12,24 +12,26 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from peft import PeftModelForCausalLM\\n-from collections import OrderedDict\\n-import bitsandbytes as bnb\\n-import peft\\n-import gc\\n-import os\\n-from tqdm import tqdm as ProgressBar\\n-import shutil\\n-from typing import Optional, Callable, Union\\n+from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n+from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from typing import Optional, Callable, Union, List\\n import torch\\n+import os\\n+import pickle\\n+import gc\\n from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+import subprocess\\n+import psutil\\n \\n __all__ = [\\n+    \"print_quantization_methods\",\\n     \"unsloth_save_model\",\\n-    #\"colab_quantize_to_gguf\",\\n+    \"save_to_gguf\",\\n+    \"patch_saving_functions\",\\n ]\\n \\n+\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -41,25 +43,36 @@ LLAMA_LAYERNORMS = (\\n # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n ALLOWED_QUANTS = \\\\\\n {\\n-    \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n-    \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n-    \"q3_k_s\" : \"Uses Q3_K for all tensors\",\\n-    \"q4_0\"   : \"Original quant method, 4-bit.\",\\n-    \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n-    \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n-    \"q4_k_s\" : \"Uses Q4_K for all tensors\",\\n-    \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\\n-    \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\\n-    \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n-    \"q5_k_s\" : \"Uses Q5_K for all tensors\",\\n-    \"q6_k\"   : \"Uses Q8_K for all tensors\",\\n-    \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\\n+    \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+    \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+    \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+    \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+    \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+    \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+    \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+    \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+    \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+    \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+    \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+    \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+    \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+    \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+    \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+    \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+    \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+    \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n }\\n \\n+def print_quantization_methods():\\n+    for key, value in ALLOWED_QUANTS.items():\\n+        print(f\\'\"{key}\"  ==> {value}\\')\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (bnb.nn.Linear4bit, peft.tuners.lora.Linear4bit)):\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n         dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n@@ -75,100 +88,362 @@ def _merge_lora(layer, name):\\n pass\\n \\n \\n+def fast_save_pickle(shard, name):\\n+    # Use this if # CPUs is <= 2\\n+    print(f\"Unsloth: Saving {name}...\")\\n+    torch.save(\\n+        shard,\\n+        name,\\n+        pickle_module = pickle,\\n+        pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n+    )\\n+    return\\n+pass\\n+\\n+\\n @torch.inference_mode\\n def unsloth_save_model(\\n     model,\\n     tokenizer,\\n-    save_directory: Union[str, os.PathLike],\\n-    is_main_process: bool = True,\\n-    state_dict: Optional[dict] = None,\\n-    save_function: Callable = torch.save,\\n-    push_to_hub: bool = False,\\n-    max_shard_size: Union[int, str] = \"7GB\",\\n-    safe_serialization: bool = True,\\n-    variant: Optional[str] = None,\\n-    token: Optional[Union[str, bool]] = None,\\n-    save_peft_format: bool = True,\\n-    temporary_location = \"_unsloth_temporary_saved_buffers\",\\n-    **kwargs,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+\\n+    # Push to hub\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    create_pr            : bool = False,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : List[str] = None,\\n+\\n+    # Our functions\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.9,\\n ):\\n-    logger.warning_once(\\n-        \"Unsloth: `unsloth_save_model` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+    save_pretrained_settings = dict(locals())\\n+    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    import re\\n+\\n+    assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n+\\n+    # Clean memory up first\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n+    pass\\n+\\n+    save_method = save_method.lower().replace(\" \", \"_\")\\n+    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n+            \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n+            \\'\"merged_16bit\" ==> This merges LoRA weights and saves to float16. Needed for llama.cpp / GGUF.\\\\n\\'\\\\\\n+            \\'\"merged_4bit\"  ==> This merges LoRA weights and saves to 4bit. Useful for DPO / inference.\\'\\n+        )\\n+    pass\\n+\\n+    if save_method == \"merged_4bit\":\\n+        print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n+        print(\"This might take 5 minutes...\")\\n+        model = model.merge_and_unload()\\n+        print(\"Done.\")\\n+    pass\\n+\\n+    if tags is not None:\\n+        assert(isinstance(tags, (list, tuple)))\\n+        tags = list(tags) + [\"unsloth\",]\\n+    else:\\n+        tags = [\"unsloth\",]\\n+    pass\\n+    save_pretrained_settings[\"tags\"] = tags\\n+\\n+    if (save_method == \"lora\") and push_to_hub:\\n+        if token is None:\\n+            raise RuntimeError(\\n+                \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens.\"\\n+            )\\n+        pass\\n+\\n+        model.push_to_hub(\\n+            repo_id            = save_directory,\\n+            use_temp_dir       = use_temp_dir,\\n+            commit_message     = commit_message,\\n+            private            = private,\\n+            token              = token,\\n+            max_shard_size     = max_shard_size,\\n+            create_pr          = create_pr,\\n+            safe_serialization = safe_serialization,\\n+            revision           = revision,\\n+            commit_description = commit_description,\\n+            tags               = tags,\\n+        )\\n+        if tokenizer is not None:\\n+            tokenizer.push_to_hub(\\n+                repo_id            = save_directory,\\n+                use_temp_dir       = use_temp_dir,\\n+                commit_message     = commit_message,\\n+                private            = private,\\n+                token              = token,\\n+                max_shard_size     = max_shard_size,\\n+                create_pr          = create_pr,\\n+                safe_serialization = safe_serialization,\\n+                revision           = revision,\\n+                commit_description = commit_description,\\n+                tags               = tags,\\n+            )\\n+        pass\\n+        return save_directory\\n+    pass\\n+\\n+    # If push_to_hub, we must remove the .../ part of a repo\\n+    if push_to_hub and \"/\" in save_directory:\\n+\\n+        new_save_directory = save_directory[save_directory.find(\"/\"):]\\n+\\n+        logger.warning_once(\\n+            f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n+            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n+        )\\n+\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_directory = new_save_directory\\n+    pass\\n+    \\n+    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+        not hasattr(model, \"model\") or \\\\\\n+        not hasattr(model.model, \"model\") or \\\\\\n+        not hasattr(model.model.model, \"layers\")\\n+    ):\\n+        # Do general saving\\n+        \\n+        # Edit save_pretrained_settings\\n+        # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+        for deletion in \\\\\\n+            (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+            del save_pretrained_settings[deletion]\\n+        pass\\n+        if hasattr(model, \"add_model_tags\"):\\n+            model.add_model_tags([\"unsloth\",])\\n+\\n+        if tokenizer is not None:\\n+            print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            print(\" Done.\")\\n+        else:\\n+            print()\\n \\n+        print(\"Unsloth: Saving model...\", end = \"\")\\n+        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+\\n+        model.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+        return save_directory\\n+    pass\\n+\\n+    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+\\n+    # Determine max RAM usage minus sharding\\n+    max_ram = psutil.virtual_memory().available\\n+    sharded_ram_usage = 5 * 1024 * 1024 * 1024\\n+    if type(max_shard_size) is str:\\n+        gb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}GB\", max_shard_size, flags = re.IGNORECASE)\\n+        mb_found = re.match(\"([0-9]{1,})[\\\\s]{0,}MB\", max_shard_size, flags = re.IGNORECASE)\\n+        if   gb_found: sharded_ram_usage = int(gb_found.group(1)) * 1024 * 1024 * 1024\\n+        elif mb_found: sharded_ram_usage = int(mb_found.group(1)) * 1024 * 1024 \\n+    elif type(max_shard_size) is int:\\n+        sharded_ram_usage = sharded_ram_usage\\n+    pass\\n+\\n+    # Switch to our fast saving modules if it\\'s a slow PC!\\n+    n_cpus = psutil.cpu_count(logical = False)\\n+\\n+    if safe_serialization is None:\\n+        safe_serialization = True\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+\\n+    elif safe_serialization and (n_cpus <= 2):\\n+        logger.warning_once(\\n+            f\"Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\\\\n\"\\\\\\n+            f\"We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\\\\n\"\\\\\\n+            f\"To force `safe_serialization`, set it to None instead.\",\\n+        )\\n+        safe_serialization = False\\n+        save_function = fast_save_pickle\\n+        save_pretrained_settings[\"safe_serialization\"] = safe_serialization\\n+        save_pretrained_settings[\"save_function\"]      = save_function\\n+    pass\\n+\\n+    # Only safe_serialization uses more RAM\\n+    if safe_serialization:\\n+        max_ram -= sharded_ram_usage\\n+    else:\\n+        max_ram -= sharded_ram_usage*0.25 # Uses much less\\n+    pass\\n+\\n+    max_ram = int(max(0, max_ram) * maximum_memory_usage)\\n+    print(f\"Unsloth: Will use up to \"\\\\\\n+          f\"{round(max_ram/1024/1024/1024, 2)} out of \"\\\\\\n+          f\"{round(psutil.virtual_memory().total/1024/1024/1024, 2)} RAM for saving.\")\\n+\\n+    # Max directory for disk saving\\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    assert(hasattr(model, \"model\"))\\n-    assert(hasattr(model.model, \"model\"))\\n-    assert(hasattr(model.model.model, \"layers\"))\\n-\\n     # HF also uses a OrderedDict\\n+    from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight\\n+    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n \\n-    print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n+    max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n+\\n+    from tqdm import tqdm as ProgressBar\\n     for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n             W = _merge_lora(proj, name)\\n-            filename = os.path.join(temporary_location, f\"{name}.pt\")\\n-            torch.save(W, filename)\\n-            state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n+\\n+            if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n+                # Save to GPU memory\\n+                state_dict[name] = W\\n+            # elif (max_ram - W.nbytes) > 0:\\n+            #     # Save to CPU memory\\n+            #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n+            #     state_dict[name] = W.to(\"cpu\", non_blocking = True)\\n+            #     max_ram = max(max_ram - W.nbytes, 0)\\n+            else:\\n+                # Save to Disk\\n+                logger.warning_once(f\"We will save to Disk and not RAM now.\")\\n+                filename = os.path.join(temporary_location, f\"{name}.pt\")\\n+                torch.save(W, filename, pickle_module = pickle, pickle_protocol = pickle.HIGHEST_PROTOCOL,)\\n+                state_dict[name] = torch.load(filename, map_location = \"cpu\", mmap = True)\\n         pass\\n         for item in LLAMA_LAYERNORMS:\\n-            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight\")\\n+            state_dict[f\"model.layers.{j}.{item}.weight\"] = eval(f\"layer.{item}.weight.data\")\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight\\n-\\n-    print(\"Unsloth: Saving tokenizer...\")\\n-    tokenizer.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n \\n-    print(\"Unsloth: Saving model. This will take 5 minutes for Llama-7b...\")\\n-    model.model.save_pretrained(\\n-        save_directory = save_directory,\\n-        is_main_process = is_main_process,\\n-        state_dict = state_dict,\\n-        save_function = save_function,\\n-        push_to_hub = push_to_hub,\\n-        max_shard_size = max_shard_size,\\n-        safe_serialization = safe_serialization,\\n-        variant = variant,\\n-        token = token,\\n-        save_peft_format = save_peft_format,\\n-    )\\n+    # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n+    for key, value in state_dict.items():\\n+        if hasattr(value, \"data\"): state_dict[key] = value = value.data\\n+        if type(value) is not torch.Tensor:\\n+            logger.warning_once(f\"Unsloth: {key} is not a Tensor but a {type(value)}.\")\\n+        pass\\n+    pass\\n+\\n+    # Edit save_pretrained_settings\\n+    # [TODO] _create_repo has errors due to **kwargs getting accepted\\n+    save_pretrained_settings[\"state_dict\"] = state_dict\\n+    for deletion in \\\\\\n+        (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+        del save_pretrained_settings[deletion]\\n+    pass\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if tokenizer is not None:\\n+        print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        print(\" Done.\")\\n+    else:\\n+        print()\\n+\\n+    print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+    model.model.save_pretrained(**save_pretrained_settings)\\n+    print(\"Done.\")\\n+\\n+    save_pretrained_settings[\"state_dict\"] = None\\n+\\n+    # for j, (key, value) in enumerate(state_dict.items()):\\n+    #     state_dict[key] = None\\n+    #     if j % 10 == 0:\\n+    #         torch.cuda.empty_cache()\\n+    #         gc.collect()\\n+    #     pass\\n+    # pass\\n+    # state_dict = None\\n+    # del state_dict\\n+    # torch.cuda.empty_cache()\\n+    # gc.collect()\\n \\n     # Remove temporary location\\n+    import shutil\\n     shutil.rmtree(temporary_location)\\n+\\n+    # for _ in range(3):\\n+    #     torch.cuda.empty_cache()\\n+    #     gc.collect()\\n+    return save_directory\\n pass\\n \\n \\n-\"\"\"\\n-def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n+def install_llama_cpp_clone_non_blocking():\\n+    full_command = [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n \\n-    logger.warning_once(\\n-        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\\\n\"\\\\\\n-        \"If anything errors or breaks, please file a ticket on Github.\\\\n\"\\\\\\n-        \"Also, if you used this successfully, please tell us on Discord!\"\\n-    )\\n+\\n+def install_llama_cpp_make_non_blocking():\\n+    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_python_non_blocking(packages = []):\\n+    full_command = [\"pip\", \"install\"] + packages\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    return run_installer\\n+pass\\n+\\n+\\n+def install_llama_cpp_blocking():\\n+    commands = [\\n+        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}\",\\n+        \"pip install gguf protobuf\",\\n+    ]\\n+    if os.path.exists(\"llama.cpp\"): return\\n+    for command in commands:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n+def save_to_gguf(\\n+    model_directory     : str = \"unsloth_finetuned_model\",\\n+    quantization_method : str = \"fast_quantized\",\\n+    _run_installer = None, # Non blocking install of llama.cpp\\n+):\\n+    from transformers.models.llama.modeling_llama import logger\\n+\\n+    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n+    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n+    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n+    elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -181,27 +456,409 @@ def _colab_quantize_to_gguf(save_directory, quantization_method = \"q4_k_m\"):\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n-    if not os.path.exists(\"llama.cpp\"):\\n-        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n-        !git clone https://github.com/ggerganov/llama.cpp\\n-        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\\n-        !pip install gguf protobuf\\n+    print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n+    if _run_installer is not None:\\n+        _run_installer.wait()\\n+    else:\\n+        install_llama_cpp_blocking()\\n+    pass\\n+\\n+    print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n+    first_conversion = \"f16\"\\n+    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+\\n+    n_cpus = psutil.cpu_count()*2\\n+    # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n+    \\n+    final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n+\\n+    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+        f\"--outfile {final_location} \"\\\\\\n+        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+\\n+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+        for line in sp.stdout:\\n+            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+    pass\\n+\\n+    print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+\\n+    if quantization_method != first_conversion:\\n+        old_location = final_location\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+\\n+        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n+        \\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+        print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n+    pass\\n+\\n+    return final_location\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_merged(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]     = self\\n+    arguments[\"tokenizer\"] = None\\n+    del arguments[\"self\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_merged(\\n+    self,\\n+    repo_id              : str,\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 with as few overhead as possible.\\n+\\n+        Choose for `save_method` to be either:\\n+        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = None\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = True\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    unsloth_save_model(**arguments)\\n+    for _ in range(3):\\n+        gc.collect()\\n+pass\\n+\\n+\\n+def unsloth_save_pretrained_gguf(\\n+    self,\\n+    save_directory       : Union[str, os.PathLike],\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    push_to_hub          : bool = False,\\n+    token                : Optional[Union[str, bool]] = None,\\n+    is_main_process      : bool = True,\\n+    state_dict           : Optional[dict] = None,\\n+    save_function        : Callable = torch.save,\\n+    max_shard_size       : Union[int, str] = \"5GB\",\\n+    safe_serialization   : bool = True,\\n+    variant              : Optional[str] = None,\\n+    save_peft_format     : bool = True,\\n+    tags                 : List[str] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,   \\n+):\\n+    \"\"\"\\n+        Same as .save_pretrained(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n+\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]       = self\\n+    arguments[\"tokenizer\"]   = tokenizer\\n+    arguments[\"push_to_hub\"] = False # We save ourselves\\n+    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"quantization_method\"]\\n+\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n+    python_install.wait()\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # And save to HF\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+        from huggingface_hub import create_repo\\n+        create_repo(\\n+            repo_id   = save_directory,\\n+            token     = token,\\n+            repo_type = \"model\",\\n+            exist_ok  = True,\\n+        )\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n+\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n+\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n     pass\\n+pass\\n+\\n+\\n+def unsloth_push_to_hub_gguf(\\n+    self,\\n+    repo_id              : str,\\n+    tokenizer            = None,\\n+    quantization_method  : str = \"fast_quantized\",\\n+    use_temp_dir         : Optional[bool] = None,\\n+    commit_message       : Optional[str] = None,\\n+    private              : Optional[bool] = None,\\n+    token                : Union[bool, str, None] = None,\\n+    max_shard_size       : Union[int, str, None] = \"5GB\",\\n+    create_pr            : bool = False,\\n+    safe_serialization   : bool = True,\\n+    revision             : str = None,\\n+    commit_description   : str = None,\\n+    tags                 : Optional[List[str]] = None,\\n+    temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage : float = 0.85,\\n+):\\n+    \"\"\"\\n+        Same as .push_to_hub(...) except 4bit weights are auto\\n+        converted to float16 then converted to GGUF / llama.cpp format.\\n+\\n+        Choose for `quantization_method` to be:\\n+        \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n+        \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n+        \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n+        \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\\n+        \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\\n+        \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\\n+        \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\\n+        \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\\n+        \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\\n+        \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\\n+        \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\\n+        \"q4_0\"    : \"Original quant method, 4-bit.\",\\n+        \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n+        \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n+        \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n+        \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n+        \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"\"\"\\n+    if tokenizer is None:\\n+        raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n-    print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\\n-    !python llama.cpp/convert.py {save_directory} \\\\\\n-        --outfile {save_directory}-unsloth.gguf \\\\\\n-        --outtype f16\\n+    arguments = dict(locals())\\n+    arguments[\"model\"]          = self\\n+    arguments[\"tokenizer\"]      = tokenizer\\n+    arguments[\"save_directory\"] = repo_id\\n+    arguments[\"push_to_hub\"]    = False # We save ourselves\\n+    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    del arguments[\"self\"]\\n+    del arguments[\"repo_id\"]\\n+    del arguments[\"quantization_method\"]\\n \\n-    print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\\n-    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\\n-    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\\\\n-        {final_location} {quantization_method}\\n+    # Non blocking install GGUF first\\n+    git_clone = install_llama_cpp_clone_non_blocking()\\n+    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+    git_clone.wait()\\n+    makefile  = install_llama_cpp_make_non_blocking()\\n+    new_save_directory = unsloth_save_model(**arguments)\\n \\n-    print(f\"Unsloth: Output location: {final_location}\")\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    python_install.wait()\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+\\n+    # Save to hub\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        private   = private,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n+def patch_saving_functions(model):\\n+    import inspect\\n+    import re\\n+    import types\\n+    from typing import Callable, Optional, Union, List\\n+\\n+    if hasattr(model, \"_original_push_to_hub\"): return\\n+\\n+    original_push_to_hub = model.push_to_hub\\n+    signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n+    signature = signature[1:]\\n+    signature = re.sub(\"<function save at .+?>\", \"torch.save\", signature)\\n+    docs = original_push_to_hub.__doc__.encode(\"utf-8\").decode(\"utf-8\")\\n+    model._original_push_to_hub = original_push_to_hub\\n+\\n+    push_to_hub_text = f\\'\\'\\'def unsloth_push_to_hub(self, {signature}:\\n+    \"\"\"\\n+    {docs}\\n+    \"\"\"\\n+    arguments = dict(locals())\\n+    del arguments[\"self\"]\\n+    if \"tags\" in arguments and arguments[\"tags\"] is not None:\\n+        assert(isinstance(arguments[\"tags\"], (list, tuple)))\\n+        arguments[\"tags\"] = list(arguments[\"tags\"]) + [\"unsloth\",]\\n+    elif \"tags\" in arguments:\\n+        arguments[\"tags\"] = [\"unsloth\",]\\n+    elif hasattr(self, \"add_model_tags\"):\\n+        self.add_model_tags([\"unsloth\",])\\n+    try:\\n+        return self._original_push_to_hub(**arguments)\\n+    except:\\n+        del arguments[\"tags\"]\\n+        return self._original_push_to_hub(**arguments)\\n+    pass\\n+    \\'\\'\\'\\n+    exec(push_to_hub_text, globals())\\n+    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)\\n+\\n+    if hasattr(model, \"add_model_tags\"):\\n+        model.add_model_tags([\"unsloth\",])\\n+\\n+    if hasattr(model, \"config\"):\\n+        # Counteract tokenizers\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+    else:\\n+        model.push_to_hub_merged     = model.push_to_hub\\n+        model.save_pretrained_merged = model.save_pretrained\\n+        model.push_to_hub_gguf       = model.push_to_hub\\n+        model.save_pretrained_gguf   = model.save_pretrained\\n+    pass\\n+\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        if hasattr(original_model, \"_original_push_to_hub\"): continue\\n+        \\n+        original_model._original_push_to_hub = original_model.push_to_hub\\n+        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n+\\n+        if hasattr(original_model, \"add_model_tags\"):\\n+            original_model.add_model_tags([\"unsloth\",])\\n+\\n+        if hasattr(original_model, \"config\"):\\n+            # Counteract tokenizers\\n+            original_model.push_to_hub_merged     = \\\\\\n+                types.MethodType(unsloth_push_to_hub_merged,     original_model)\\n+\\n+            original_model.save_pretrained_merged = \\\\\\n+                types.MethodType(unsloth_save_pretrained_merged, original_model)\\n+\\n+            original_model.push_to_hub_gguf       = \\\\\\n+                types.MethodType(unsloth_push_to_hub_gguf,       original_model)\\n+\\n+            original_model.save_pretrained_gguf   = \\\\\\n+                types.MethodType(unsloth_save_pretrained_gguf,   original_model)\\n+        pass\\n+    pass\\n+    return\\n pass\\n-\"\"\"\\n',\n",
       " '@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n pass\\n \\n \\n+def NotebookTrainingTracker_write_line(self, values):\\n+    \"\"\"\\n+    Write the values in the inner table.\\n+\\n+    Args:\\n+        values (`Dict[str, float]`): The values to display.\\n+    \"\"\"\\n+    if self.inner_table is None:\\n+        self.inner_table = [list(values.keys()), list(values.values())]\\n+    else:\\n+        columns = self.inner_table[0]\\n+        print(columns)\\n+        for key in values.keys():\\n+            if key not in columns:\\n+                columns.append(key)\\n+        self.inner_table[0] = columns\\n+        if len(self.inner_table) > 1:\\n+            last_values = self.inner_table[-1]\\n+            first_column = self.inner_table[0][0]\\n+            if last_values[0] != values[first_column]:\\n+                # write new line\\n+                self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\\n+            else:\\n+                # update last line\\n+                new_values = values\\n+                for c in columns:\\n+                    if c not in new_values.keys():\\n+                        new_values[c] = last_values[columns.index(c)]\\n+                self.inner_table[-1] = [new_values[c] for c in columns]\\n+        else:\\n+            # Edit for evaluation purposes\\n+            self.inner_table.append([values[c] if c in values else 0 for c in columns])\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -161,11 +161,12 @@ pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n+    old_dtype = X.dtype\\n     X = X.to(torch.float32)\\n     variance = X.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n     X *= variance.rsqrt_()\\n-    X = X.to(residual.dtype)\\n+    X = X.to(old_dtype)\\n     X *= self.weight\\n     return X\\n pass\\n@@ -660,14 +661,15 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -720,18 +722,19 @@ class FastLlamaModel:\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n-            device_map = device_map,\\n-            torch_dtype = dtype,\\n-            quantization_config = bnb_config,\\n-            token = token,\\n-            rope_scaling = rope_scaling,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n             model_max_length = max_seq_length,\\n-            padding_side = \"right\",\\n-            token = token,\\n+            padding_side     = \"right\",\\n+            token            = token,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -755,12 +758,12 @@ class FastLlamaModel:\\n         # We check the tokenizer first for errors\\n         if fix_tokenizer:\\n             tokenizer = check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n+                model            = model,\\n+                tokenizer        = tokenizer,\\n+                model_name       = model_name,\\n                 model_max_length = max_seq_length,\\n-                padding_side = \"right\",\\n-                token = token,\\n+                padding_side     = \"right\",\\n+                token            = token,\\n             )\\n         pass\\n         patch_saving_functions(tokenizer)\\n@@ -828,20 +831,20 @@ class FastLlamaModel:\\n     @staticmethod\\n     def get_peft_model(\\n         model,\\n-        r = 16,\\n-        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n-                          \"gate_proj\", \"up_proj\", \"down_proj\"],\\n-        lora_alpha = 16,\\n-        lora_dropout = 0,\\n-        bias = \"none\",\\n+        r                   = 16,\\n+        target_modules      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n+                               \"gate_proj\", \"up_proj\", \"down_proj\"],\\n+        lora_alpha          = 16,\\n+        lora_dropout        = 0,\\n+        bias                = \"none\",\\n         layers_to_transform = None,\\n-        layers_pattern = None,\\n+        layers_pattern      = None,\\n         use_gradient_checkpointing = True,\\n-        random_state = 3407,\\n-        max_seq_length = 2048, # not used anymore\\n-        use_rslora = False,\\n-        init_lora_weights = True,\\n-        loftq_config = None,\\n+        random_state        = 3407,\\n+        max_seq_length      = 2048, # not used anymore\\n+        use_rslora          = False,\\n+        init_lora_weights   = True,\\n+        loftq_config        = None,\\n         **kwargs,\\n     ):\\n         if isinstance(model, PeftModelForCausalLM):\\n@@ -909,12 +912,14 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n+                # We do it ourselves!\\n+                new_alpha = lora_alpha / (r**0.5)\\n                 import peft\\n-                raise RuntimeError(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n-                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                logger.warning_once(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n+                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n                 )\\n+                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -63,14 +63,14 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name = model_name,\\n+            model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n-            dtype = dtype,\\n-            load_in_4bit = load_in_4bit,\\n-            token = token,\\n-            device_map = device_map,\\n-            rope_scaling = rope_scaling,\\n-            fix_tokenizer = fix_tokenizer,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ): \\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token = token,\\n             # rope_scaling = rope_scaling,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n',\n",
       " '@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module = pickle,\\n+        pickle_module   = pickle,\\n         pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n@@ -106,7 +106,7 @@ def unsloth_save_model(\\n     model,\\n     tokenizer,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    merge_method         : str = \"lora\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -131,7 +131,7 @@ def unsloth_save_model(\\n     maximum_memory_usage : float = 0.9,\\n ):\\n     save_pretrained_settings = dict(locals())\\n-    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+    for deletion in (\"model\", \"tokenizer\", \"merge_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n     import re\\n@@ -144,8 +144,8 @@ def unsloth_save_model(\\n         gc.collect()\\n     pass\\n \\n-    save_method = save_method.lower().replace(\" \", \"_\")\\n-    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+    merge_method = merge_method.lower().replace(\" \", \"_\")\\n+    if merge_method != \"lora\" and merge_method != \"16bit\" and merge_method != \"4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n             \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n@@ -154,7 +154,7 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n-    if save_method == \"merged_4bit\":\\n+    if merge_method == \"4bit\":\\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n         model = model.merge_and_unload()\\n@@ -169,7 +169,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if (merge_method == \"lora\") and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -222,7 +222,7 @@ def unsloth_save_model(\\n         save_directory = new_save_directory\\n     pass\\n     \\n-    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+    if (merge_method == \"4bit\") or (merge_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n         not hasattr(model.model, \"model\") or \\\\\\n         not hasattr(model.model.model, \"layers\")\\n@@ -246,7 +246,7 @@ def unsloth_save_model(\\n             print()\\n \\n         print(\"Unsloth: Saving model...\", end = \"\")\\n-        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+        if merge_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n         print(\" Done.\")\\n@@ -434,19 +434,19 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory     : str = \"unsloth_finetuned_model\",\\n-    quantization_method : str = \"fast_quantized\",\\n+    model_directory : str = \"unsloth_finetuned_model\",\\n+    quantization    : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n-    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n-    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n-    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n-    elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    if   quantization == \"not_quantized\":  quantization = \"f16\"\\n+    elif quantization == \"fast_quantized\": quantization = \"q8_0\"\\n+    elif quantization == \"quantized\":      quantization = \"q4_k_m\"\\n+    elif quantization is None:             quantization = \"q8_0\"\\n \\n-    if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+    if quantization not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +456,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -469,9 +469,9 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n     first_conversion = \"f16\"\\n-    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n-    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n-    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    if   quantization == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization == \"q8_0\": first_conversion = \"q8_0\"\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -489,13 +489,13 @@ def save_to_gguf(\\n \\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n-    if quantization_method != first_conversion:\\n+    if quantization != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization_method} {n_cpus}\"\\n+            f\"{final_location} {quantization} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -511,7 +511,8 @@ pass\\n def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.save_pretrained(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n-    arguments[\"model\"]     = self\\n-    arguments[\"tokenizer\"] = None\\n+    arguments[\"model\"] = self\\n     del arguments[\"self\"]\\n     unsloth_save_model(**arguments)\\n     for _ in range(3):\\n@@ -547,7 +554,8 @@ pass\\n def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.push_to_hub(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n     arguments[\"model\"]          = self\\n-    arguments[\"tokenizer\"]      = None\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = True\\n     del arguments[\"self\"]\\n@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n     arguments = dict(locals())\\n-    arguments[\"model\"]       = self\\n-    arguments[\"tokenizer\"]   = tokenizer\\n-    arguments[\"push_to_hub\"] = False # We save ourselves\\n-    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"model\"]        = self\\n+    arguments[\"tokenizer\"]    = tokenizer\\n+    arguments[\"push_to_hub\"]  = False # We save ourselves\\n+    arguments[\"merge_method\"] = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # And save to HF\\n     if push_to_hub:\\n@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"tokenizer\"]      = tokenizer\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = False # We save ourselves\\n-    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"merge_method\"]   = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # Save to hub\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n',\n",
       " '@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n pass\\n \\n \\n+def NotebookTrainingTracker_write_line(self, values):\\n+    \"\"\"\\n+    Write the values in the inner table.\\n+\\n+    Args:\\n+        values (`Dict[str, float]`): The values to display.\\n+    \"\"\"\\n+    if self.inner_table is None:\\n+        self.inner_table = [list(values.keys()), list(values.values())]\\n+    else:\\n+        columns = self.inner_table[0]\\n+        print(columns)\\n+        for key in values.keys():\\n+            if key not in columns:\\n+                columns.append(key)\\n+        self.inner_table[0] = columns\\n+        if len(self.inner_table) > 1:\\n+            last_values = self.inner_table[-1]\\n+            first_column = self.inner_table[0][0]\\n+            if last_values[0] != values[first_column]:\\n+                # write new line\\n+                self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\\n+            else:\\n+                # update last line\\n+                new_values = values\\n+                for c in columns:\\n+                    if c not in new_values.keys():\\n+                        new_values[c] = last_values[columns.index(c)]\\n+                self.inner_table[-1] = [new_values[c] for c in columns]\\n+        else:\\n+            # Edit for evaluation purposes\\n+            self.inner_table.append([values[c] if c in values else 0 for c in columns])\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -161,11 +161,12 @@ pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n+    old_dtype = X.dtype\\n     X = X.to(torch.float32)\\n     variance = X.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n     X *= variance.rsqrt_()\\n-    X = X.to(residual.dtype)\\n+    X = X.to(old_dtype)\\n     X *= self.weight\\n     return X\\n pass\\n@@ -660,14 +661,15 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -720,18 +722,19 @@ class FastLlamaModel:\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n-            device_map = device_map,\\n-            torch_dtype = dtype,\\n-            quantization_config = bnb_config,\\n-            token = token,\\n-            rope_scaling = rope_scaling,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n             model_max_length = max_seq_length,\\n-            padding_side = \"right\",\\n-            token = token,\\n+            padding_side     = \"right\",\\n+            token            = token,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -755,12 +758,12 @@ class FastLlamaModel:\\n         # We check the tokenizer first for errors\\n         if fix_tokenizer:\\n             tokenizer = check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n+                model            = model,\\n+                tokenizer        = tokenizer,\\n+                model_name       = model_name,\\n                 model_max_length = max_seq_length,\\n-                padding_side = \"right\",\\n-                token = token,\\n+                padding_side     = \"right\",\\n+                token            = token,\\n             )\\n         pass\\n         patch_saving_functions(tokenizer)\\n@@ -828,20 +831,20 @@ class FastLlamaModel:\\n     @staticmethod\\n     def get_peft_model(\\n         model,\\n-        r = 16,\\n-        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n-                          \"gate_proj\", \"up_proj\", \"down_proj\"],\\n-        lora_alpha = 16,\\n-        lora_dropout = 0,\\n-        bias = \"none\",\\n+        r                   = 16,\\n+        target_modules      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n+                               \"gate_proj\", \"up_proj\", \"down_proj\"],\\n+        lora_alpha          = 16,\\n+        lora_dropout        = 0,\\n+        bias                = \"none\",\\n         layers_to_transform = None,\\n-        layers_pattern = None,\\n+        layers_pattern      = None,\\n         use_gradient_checkpointing = True,\\n-        random_state = 3407,\\n-        max_seq_length = 2048, # not used anymore\\n-        use_rslora = False,\\n-        init_lora_weights = True,\\n-        loftq_config = None,\\n+        random_state        = 3407,\\n+        max_seq_length      = 2048, # not used anymore\\n+        use_rslora          = False,\\n+        init_lora_weights   = True,\\n+        loftq_config        = None,\\n         **kwargs,\\n     ):\\n         if isinstance(model, PeftModelForCausalLM):\\n@@ -909,12 +912,14 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n+                # We do it ourselves!\\n+                new_alpha = lora_alpha / (r**0.5)\\n                 import peft\\n-                raise RuntimeError(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n-                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                logger.warning_once(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n+                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n                 )\\n+                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -63,14 +63,14 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name = model_name,\\n+            model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n-            dtype = dtype,\\n-            load_in_4bit = load_in_4bit,\\n-            token = token,\\n-            device_map = device_map,\\n-            rope_scaling = rope_scaling,\\n-            fix_tokenizer = fix_tokenizer,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ): \\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token = token,\\n             # rope_scaling = rope_scaling,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n',\n",
       " '@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module = pickle,\\n+        pickle_module   = pickle,\\n         pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n@@ -106,7 +106,7 @@ def unsloth_save_model(\\n     model,\\n     tokenizer,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    merge_method         : str = \"lora\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -131,7 +131,7 @@ def unsloth_save_model(\\n     maximum_memory_usage : float = 0.9,\\n ):\\n     save_pretrained_settings = dict(locals())\\n-    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+    for deletion in (\"model\", \"tokenizer\", \"merge_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n     import re\\n@@ -144,8 +144,8 @@ def unsloth_save_model(\\n         gc.collect()\\n     pass\\n \\n-    save_method = save_method.lower().replace(\" \", \"_\")\\n-    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+    merge_method = merge_method.lower().replace(\" \", \"_\")\\n+    if merge_method != \"lora\" and merge_method != \"16bit\" and merge_method != \"4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n             \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n@@ -154,7 +154,7 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n-    if save_method == \"merged_4bit\":\\n+    if merge_method == \"4bit\":\\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n         model = model.merge_and_unload()\\n@@ -169,7 +169,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if (merge_method == \"lora\") and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -222,7 +222,7 @@ def unsloth_save_model(\\n         save_directory = new_save_directory\\n     pass\\n     \\n-    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+    if (merge_method == \"4bit\") or (merge_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n         not hasattr(model.model, \"model\") or \\\\\\n         not hasattr(model.model.model, \"layers\")\\n@@ -246,7 +246,7 @@ def unsloth_save_model(\\n             print()\\n \\n         print(\"Unsloth: Saving model...\", end = \"\")\\n-        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+        if merge_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n         print(\" Done.\")\\n@@ -434,19 +434,19 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory     : str = \"unsloth_finetuned_model\",\\n-    quantization_method : str = \"fast_quantized\",\\n+    model_directory : str = \"unsloth_finetuned_model\",\\n+    quantization    : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n-    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n-    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n-    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n-    elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    if   quantization == \"not_quantized\":  quantization = \"f16\"\\n+    elif quantization == \"fast_quantized\": quantization = \"q8_0\"\\n+    elif quantization == \"quantized\":      quantization = \"q4_k_m\"\\n+    elif quantization is None:             quantization = \"q8_0\"\\n \\n-    if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+    if quantization not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +456,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -469,9 +469,9 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n     first_conversion = \"f16\"\\n-    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n-    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n-    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    if   quantization == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization == \"q8_0\": first_conversion = \"q8_0\"\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -489,13 +489,13 @@ def save_to_gguf(\\n \\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n-    if quantization_method != first_conversion:\\n+    if quantization != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization_method} {n_cpus}\"\\n+            f\"{final_location} {quantization} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -511,7 +511,8 @@ pass\\n def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.save_pretrained(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n-    arguments[\"model\"]     = self\\n-    arguments[\"tokenizer\"] = None\\n+    arguments[\"model\"] = self\\n     del arguments[\"self\"]\\n     unsloth_save_model(**arguments)\\n     for _ in range(3):\\n@@ -547,7 +554,8 @@ pass\\n def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.push_to_hub(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n     arguments[\"model\"]          = self\\n-    arguments[\"tokenizer\"]      = None\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = True\\n     del arguments[\"self\"]\\n@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n     arguments = dict(locals())\\n-    arguments[\"model\"]       = self\\n-    arguments[\"tokenizer\"]   = tokenizer\\n-    arguments[\"push_to_hub\"] = False # We save ourselves\\n-    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"model\"]        = self\\n+    arguments[\"tokenizer\"]    = tokenizer\\n+    arguments[\"push_to_hub\"]  = False # We save ourselves\\n+    arguments[\"merge_method\"] = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # And save to HF\\n     if push_to_hub:\\n@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"tokenizer\"]      = tokenizer\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = False # We save ourselves\\n-    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"merge_method\"]   = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # Save to hub\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n',\n",
       " '@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n pass\\n \\n \\n+def NotebookTrainingTracker_write_line(self, values):\\n+    \"\"\"\\n+    Write the values in the inner table.\\n+\\n+    Args:\\n+        values (`Dict[str, float]`): The values to display.\\n+    \"\"\"\\n+    if self.inner_table is None:\\n+        self.inner_table = [list(values.keys()), list(values.values())]\\n+    else:\\n+        columns = self.inner_table[0]\\n+        print(columns)\\n+        for key in values.keys():\\n+            if key not in columns:\\n+                columns.append(key)\\n+        self.inner_table[0] = columns\\n+        if len(self.inner_table) > 1:\\n+            last_values = self.inner_table[-1]\\n+            first_column = self.inner_table[0][0]\\n+            if last_values[0] != values[first_column]:\\n+                # write new line\\n+                self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\\n+            else:\\n+                # update last line\\n+                new_values = values\\n+                for c in columns:\\n+                    if c not in new_values.keys():\\n+                        new_values[c] = last_values[columns.index(c)]\\n+                self.inner_table[-1] = [new_values[c] for c in columns]\\n+        else:\\n+            # Edit for evaluation purposes\\n+            self.inner_table.append([values[c] if c in values else 0 for c in columns])\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -161,11 +161,12 @@ pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n+    old_dtype = X.dtype\\n     X = X.to(torch.float32)\\n     variance = X.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n     X *= variance.rsqrt_()\\n-    X = X.to(residual.dtype)\\n+    X = X.to(old_dtype)\\n     X *= self.weight\\n     return X\\n pass\\n@@ -660,14 +661,15 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -720,18 +722,19 @@ class FastLlamaModel:\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n-            device_map = device_map,\\n-            torch_dtype = dtype,\\n-            quantization_config = bnb_config,\\n-            token = token,\\n-            rope_scaling = rope_scaling,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n             model_max_length = max_seq_length,\\n-            padding_side = \"right\",\\n-            token = token,\\n+            padding_side     = \"right\",\\n+            token            = token,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -755,12 +758,12 @@ class FastLlamaModel:\\n         # We check the tokenizer first for errors\\n         if fix_tokenizer:\\n             tokenizer = check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n+                model            = model,\\n+                tokenizer        = tokenizer,\\n+                model_name       = model_name,\\n                 model_max_length = max_seq_length,\\n-                padding_side = \"right\",\\n-                token = token,\\n+                padding_side     = \"right\",\\n+                token            = token,\\n             )\\n         pass\\n         patch_saving_functions(tokenizer)\\n@@ -828,20 +831,20 @@ class FastLlamaModel:\\n     @staticmethod\\n     def get_peft_model(\\n         model,\\n-        r = 16,\\n-        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n-                          \"gate_proj\", \"up_proj\", \"down_proj\"],\\n-        lora_alpha = 16,\\n-        lora_dropout = 0,\\n-        bias = \"none\",\\n+        r                   = 16,\\n+        target_modules      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n+                               \"gate_proj\", \"up_proj\", \"down_proj\"],\\n+        lora_alpha          = 16,\\n+        lora_dropout        = 0,\\n+        bias                = \"none\",\\n         layers_to_transform = None,\\n-        layers_pattern = None,\\n+        layers_pattern      = None,\\n         use_gradient_checkpointing = True,\\n-        random_state = 3407,\\n-        max_seq_length = 2048, # not used anymore\\n-        use_rslora = False,\\n-        init_lora_weights = True,\\n-        loftq_config = None,\\n+        random_state        = 3407,\\n+        max_seq_length      = 2048, # not used anymore\\n+        use_rslora          = False,\\n+        init_lora_weights   = True,\\n+        loftq_config        = None,\\n         **kwargs,\\n     ):\\n         if isinstance(model, PeftModelForCausalLM):\\n@@ -909,12 +912,14 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n+                # We do it ourselves!\\n+                new_alpha = lora_alpha / (r**0.5)\\n                 import peft\\n-                raise RuntimeError(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n-                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                logger.warning_once(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n+                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n                 )\\n+                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -63,14 +63,14 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name = model_name,\\n+            model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n-            dtype = dtype,\\n-            load_in_4bit = load_in_4bit,\\n-            token = token,\\n-            device_map = device_map,\\n-            rope_scaling = rope_scaling,\\n-            fix_tokenizer = fix_tokenizer,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ): \\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token = token,\\n             # rope_scaling = rope_scaling,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n',\n",
       " '@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module = pickle,\\n+        pickle_module   = pickle,\\n         pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n@@ -106,7 +106,7 @@ def unsloth_save_model(\\n     model,\\n     tokenizer,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    merge_method         : str = \"lora\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -131,7 +131,7 @@ def unsloth_save_model(\\n     maximum_memory_usage : float = 0.9,\\n ):\\n     save_pretrained_settings = dict(locals())\\n-    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+    for deletion in (\"model\", \"tokenizer\", \"merge_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n     import re\\n@@ -144,8 +144,8 @@ def unsloth_save_model(\\n         gc.collect()\\n     pass\\n \\n-    save_method = save_method.lower().replace(\" \", \"_\")\\n-    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+    merge_method = merge_method.lower().replace(\" \", \"_\")\\n+    if merge_method != \"lora\" and merge_method != \"16bit\" and merge_method != \"4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n             \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n@@ -154,7 +154,7 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n-    if save_method == \"merged_4bit\":\\n+    if merge_method == \"4bit\":\\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n         model = model.merge_and_unload()\\n@@ -169,7 +169,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if (merge_method == \"lora\") and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -222,7 +222,7 @@ def unsloth_save_model(\\n         save_directory = new_save_directory\\n     pass\\n     \\n-    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+    if (merge_method == \"4bit\") or (merge_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n         not hasattr(model.model, \"model\") or \\\\\\n         not hasattr(model.model.model, \"layers\")\\n@@ -246,7 +246,7 @@ def unsloth_save_model(\\n             print()\\n \\n         print(\"Unsloth: Saving model...\", end = \"\")\\n-        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+        if merge_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n         print(\" Done.\")\\n@@ -434,19 +434,19 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory     : str = \"unsloth_finetuned_model\",\\n-    quantization_method : str = \"fast_quantized\",\\n+    model_directory : str = \"unsloth_finetuned_model\",\\n+    quantization    : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n-    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n-    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n-    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n-    elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    if   quantization == \"not_quantized\":  quantization = \"f16\"\\n+    elif quantization == \"fast_quantized\": quantization = \"q8_0\"\\n+    elif quantization == \"quantized\":      quantization = \"q4_k_m\"\\n+    elif quantization is None:             quantization = \"q8_0\"\\n \\n-    if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+    if quantization not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +456,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -469,9 +469,9 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n     first_conversion = \"f16\"\\n-    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n-    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n-    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    if   quantization == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization == \"q8_0\": first_conversion = \"q8_0\"\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -489,13 +489,13 @@ def save_to_gguf(\\n \\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n-    if quantization_method != first_conversion:\\n+    if quantization != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization_method} {n_cpus}\"\\n+            f\"{final_location} {quantization} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -511,7 +511,8 @@ pass\\n def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.save_pretrained(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n-    arguments[\"model\"]     = self\\n-    arguments[\"tokenizer\"] = None\\n+    arguments[\"model\"] = self\\n     del arguments[\"self\"]\\n     unsloth_save_model(**arguments)\\n     for _ in range(3):\\n@@ -547,7 +554,8 @@ pass\\n def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.push_to_hub(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n     arguments[\"model\"]          = self\\n-    arguments[\"tokenizer\"]      = None\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = True\\n     del arguments[\"self\"]\\n@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n     arguments = dict(locals())\\n-    arguments[\"model\"]       = self\\n-    arguments[\"tokenizer\"]   = tokenizer\\n-    arguments[\"push_to_hub\"] = False # We save ourselves\\n-    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"model\"]        = self\\n+    arguments[\"tokenizer\"]    = tokenizer\\n+    arguments[\"push_to_hub\"]  = False # We save ourselves\\n+    arguments[\"merge_method\"] = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # And save to HF\\n     if push_to_hub:\\n@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"tokenizer\"]      = tokenizer\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = False # We save ourselves\\n-    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"merge_method\"]   = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # Save to hub\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n',\n",
       " '@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n pass\\n \\n \\n+def NotebookTrainingTracker_write_line(self, values):\\n+    \"\"\"\\n+    Write the values in the inner table.\\n+\\n+    Args:\\n+        values (`Dict[str, float]`): The values to display.\\n+    \"\"\"\\n+    if self.inner_table is None:\\n+        self.inner_table = [list(values.keys()), list(values.values())]\\n+    else:\\n+        columns = self.inner_table[0]\\n+        print(columns)\\n+        for key in values.keys():\\n+            if key not in columns:\\n+                columns.append(key)\\n+        self.inner_table[0] = columns\\n+        if len(self.inner_table) > 1:\\n+            last_values = self.inner_table[-1]\\n+            first_column = self.inner_table[0][0]\\n+            if last_values[0] != values[first_column]:\\n+                # write new line\\n+                self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\\n+            else:\\n+                # update last line\\n+                new_values = values\\n+                for c in columns:\\n+                    if c not in new_values.keys():\\n+                        new_values[c] = last_values[columns.index(c)]\\n+                self.inner_table[-1] = [new_values[c] for c in columns]\\n+        else:\\n+            # Edit for evaluation purposes\\n+            self.inner_table.append([values[c] if c in values else 0 for c in columns])\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -161,11 +161,12 @@ pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n+    old_dtype = X.dtype\\n     X = X.to(torch.float32)\\n     variance = X.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n     X *= variance.rsqrt_()\\n-    X = X.to(residual.dtype)\\n+    X = X.to(old_dtype)\\n     X *= self.weight\\n     return X\\n pass\\n@@ -660,14 +661,15 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -720,18 +722,19 @@ class FastLlamaModel:\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n-            device_map = device_map,\\n-            torch_dtype = dtype,\\n-            quantization_config = bnb_config,\\n-            token = token,\\n-            rope_scaling = rope_scaling,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n             model_max_length = max_seq_length,\\n-            padding_side = \"right\",\\n-            token = token,\\n+            padding_side     = \"right\",\\n+            token            = token,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -755,12 +758,12 @@ class FastLlamaModel:\\n         # We check the tokenizer first for errors\\n         if fix_tokenizer:\\n             tokenizer = check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n+                model            = model,\\n+                tokenizer        = tokenizer,\\n+                model_name       = model_name,\\n                 model_max_length = max_seq_length,\\n-                padding_side = \"right\",\\n-                token = token,\\n+                padding_side     = \"right\",\\n+                token            = token,\\n             )\\n         pass\\n         patch_saving_functions(tokenizer)\\n@@ -828,20 +831,20 @@ class FastLlamaModel:\\n     @staticmethod\\n     def get_peft_model(\\n         model,\\n-        r = 16,\\n-        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n-                          \"gate_proj\", \"up_proj\", \"down_proj\"],\\n-        lora_alpha = 16,\\n-        lora_dropout = 0,\\n-        bias = \"none\",\\n+        r                   = 16,\\n+        target_modules      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n+                               \"gate_proj\", \"up_proj\", \"down_proj\"],\\n+        lora_alpha          = 16,\\n+        lora_dropout        = 0,\\n+        bias                = \"none\",\\n         layers_to_transform = None,\\n-        layers_pattern = None,\\n+        layers_pattern      = None,\\n         use_gradient_checkpointing = True,\\n-        random_state = 3407,\\n-        max_seq_length = 2048, # not used anymore\\n-        use_rslora = False,\\n-        init_lora_weights = True,\\n-        loftq_config = None,\\n+        random_state        = 3407,\\n+        max_seq_length      = 2048, # not used anymore\\n+        use_rslora          = False,\\n+        init_lora_weights   = True,\\n+        loftq_config        = None,\\n         **kwargs,\\n     ):\\n         if isinstance(model, PeftModelForCausalLM):\\n@@ -909,12 +912,14 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n+                # We do it ourselves!\\n+                new_alpha = lora_alpha / (r**0.5)\\n                 import peft\\n-                raise RuntimeError(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n-                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                logger.warning_once(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n+                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n                 )\\n+                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -63,14 +63,14 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name = model_name,\\n+            model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n-            dtype = dtype,\\n-            load_in_4bit = load_in_4bit,\\n-            token = token,\\n-            device_map = device_map,\\n-            rope_scaling = rope_scaling,\\n-            fix_tokenizer = fix_tokenizer,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ): \\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token = token,\\n             # rope_scaling = rope_scaling,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n',\n",
       " '@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module = pickle,\\n+        pickle_module   = pickle,\\n         pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n@@ -106,7 +106,7 @@ def unsloth_save_model(\\n     model,\\n     tokenizer,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    merge_method         : str = \"lora\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -131,7 +131,7 @@ def unsloth_save_model(\\n     maximum_memory_usage : float = 0.9,\\n ):\\n     save_pretrained_settings = dict(locals())\\n-    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+    for deletion in (\"model\", \"tokenizer\", \"merge_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n     import re\\n@@ -144,8 +144,8 @@ def unsloth_save_model(\\n         gc.collect()\\n     pass\\n \\n-    save_method = save_method.lower().replace(\" \", \"_\")\\n-    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+    merge_method = merge_method.lower().replace(\" \", \"_\")\\n+    if merge_method != \"lora\" and merge_method != \"16bit\" and merge_method != \"4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n             \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n@@ -154,7 +154,7 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n-    if save_method == \"merged_4bit\":\\n+    if merge_method == \"4bit\":\\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n         model = model.merge_and_unload()\\n@@ -169,7 +169,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if (merge_method == \"lora\") and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -222,7 +222,7 @@ def unsloth_save_model(\\n         save_directory = new_save_directory\\n     pass\\n     \\n-    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+    if (merge_method == \"4bit\") or (merge_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n         not hasattr(model.model, \"model\") or \\\\\\n         not hasattr(model.model.model, \"layers\")\\n@@ -246,7 +246,7 @@ def unsloth_save_model(\\n             print()\\n \\n         print(\"Unsloth: Saving model...\", end = \"\")\\n-        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+        if merge_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n         print(\" Done.\")\\n@@ -434,19 +434,19 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory     : str = \"unsloth_finetuned_model\",\\n-    quantization_method : str = \"fast_quantized\",\\n+    model_directory : str = \"unsloth_finetuned_model\",\\n+    quantization    : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n-    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n-    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n-    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n-    elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    if   quantization == \"not_quantized\":  quantization = \"f16\"\\n+    elif quantization == \"fast_quantized\": quantization = \"q8_0\"\\n+    elif quantization == \"quantized\":      quantization = \"q4_k_m\"\\n+    elif quantization is None:             quantization = \"q8_0\"\\n \\n-    if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+    if quantization not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +456,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -469,9 +469,9 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n     first_conversion = \"f16\"\\n-    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n-    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n-    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    if   quantization == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization == \"q8_0\": first_conversion = \"q8_0\"\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -489,13 +489,13 @@ def save_to_gguf(\\n \\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n-    if quantization_method != first_conversion:\\n+    if quantization != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization_method} {n_cpus}\"\\n+            f\"{final_location} {quantization} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -511,7 +511,8 @@ pass\\n def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.save_pretrained(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n-    arguments[\"model\"]     = self\\n-    arguments[\"tokenizer\"] = None\\n+    arguments[\"model\"] = self\\n     del arguments[\"self\"]\\n     unsloth_save_model(**arguments)\\n     for _ in range(3):\\n@@ -547,7 +554,8 @@ pass\\n def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.push_to_hub(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n     arguments[\"model\"]          = self\\n-    arguments[\"tokenizer\"]      = None\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = True\\n     del arguments[\"self\"]\\n@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n     arguments = dict(locals())\\n-    arguments[\"model\"]       = self\\n-    arguments[\"tokenizer\"]   = tokenizer\\n-    arguments[\"push_to_hub\"] = False # We save ourselves\\n-    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"model\"]        = self\\n+    arguments[\"tokenizer\"]    = tokenizer\\n+    arguments[\"push_to_hub\"]  = False # We save ourselves\\n+    arguments[\"merge_method\"] = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # And save to HF\\n     if push_to_hub:\\n@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"tokenizer\"]      = tokenizer\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = False # We save ourselves\\n-    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"merge_method\"]   = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # Save to hub\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n',\n",
       " '@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n pass\\n \\n \\n+def NotebookTrainingTracker_write_line(self, values):\\n+    \"\"\"\\n+    Write the values in the inner table.\\n+\\n+    Args:\\n+        values (`Dict[str, float]`): The values to display.\\n+    \"\"\"\\n+    if self.inner_table is None:\\n+        self.inner_table = [list(values.keys()), list(values.values())]\\n+    else:\\n+        columns = self.inner_table[0]\\n+        print(columns)\\n+        for key in values.keys():\\n+            if key not in columns:\\n+                columns.append(key)\\n+        self.inner_table[0] = columns\\n+        if len(self.inner_table) > 1:\\n+            last_values = self.inner_table[-1]\\n+            first_column = self.inner_table[0][0]\\n+            if last_values[0] != values[first_column]:\\n+                # write new line\\n+                self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\\n+            else:\\n+                # update last line\\n+                new_values = values\\n+                for c in columns:\\n+                    if c not in new_values.keys():\\n+                        new_values[c] = last_values[columns.index(c)]\\n+                self.inner_table[-1] = [new_values[c] for c in columns]\\n+        else:\\n+            # Edit for evaluation purposes\\n+            self.inner_table.append([values[c] if c in values else 0 for c in columns])\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -161,11 +161,12 @@ pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n+    old_dtype = X.dtype\\n     X = X.to(torch.float32)\\n     variance = X.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n     X *= variance.rsqrt_()\\n-    X = X.to(residual.dtype)\\n+    X = X.to(old_dtype)\\n     X *= self.weight\\n     return X\\n pass\\n@@ -660,14 +661,15 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -720,18 +722,19 @@ class FastLlamaModel:\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n-            device_map = device_map,\\n-            torch_dtype = dtype,\\n-            quantization_config = bnb_config,\\n-            token = token,\\n-            rope_scaling = rope_scaling,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n             model_max_length = max_seq_length,\\n-            padding_side = \"right\",\\n-            token = token,\\n+            padding_side     = \"right\",\\n+            token            = token,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -755,12 +758,12 @@ class FastLlamaModel:\\n         # We check the tokenizer first for errors\\n         if fix_tokenizer:\\n             tokenizer = check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n+                model            = model,\\n+                tokenizer        = tokenizer,\\n+                model_name       = model_name,\\n                 model_max_length = max_seq_length,\\n-                padding_side = \"right\",\\n-                token = token,\\n+                padding_side     = \"right\",\\n+                token            = token,\\n             )\\n         pass\\n         patch_saving_functions(tokenizer)\\n@@ -828,20 +831,20 @@ class FastLlamaModel:\\n     @staticmethod\\n     def get_peft_model(\\n         model,\\n-        r = 16,\\n-        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n-                          \"gate_proj\", \"up_proj\", \"down_proj\"],\\n-        lora_alpha = 16,\\n-        lora_dropout = 0,\\n-        bias = \"none\",\\n+        r                   = 16,\\n+        target_modules      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n+                               \"gate_proj\", \"up_proj\", \"down_proj\"],\\n+        lora_alpha          = 16,\\n+        lora_dropout        = 0,\\n+        bias                = \"none\",\\n         layers_to_transform = None,\\n-        layers_pattern = None,\\n+        layers_pattern      = None,\\n         use_gradient_checkpointing = True,\\n-        random_state = 3407,\\n-        max_seq_length = 2048, # not used anymore\\n-        use_rslora = False,\\n-        init_lora_weights = True,\\n-        loftq_config = None,\\n+        random_state        = 3407,\\n+        max_seq_length      = 2048, # not used anymore\\n+        use_rslora          = False,\\n+        init_lora_weights   = True,\\n+        loftq_config        = None,\\n         **kwargs,\\n     ):\\n         if isinstance(model, PeftModelForCausalLM):\\n@@ -909,12 +912,14 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n+                # We do it ourselves!\\n+                new_alpha = lora_alpha / (r**0.5)\\n                 import peft\\n-                raise RuntimeError(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n-                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                logger.warning_once(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n+                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n                 )\\n+                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -63,14 +63,14 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name = model_name,\\n+            model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n-            dtype = dtype,\\n-            load_in_4bit = load_in_4bit,\\n-            token = token,\\n-            device_map = device_map,\\n-            rope_scaling = rope_scaling,\\n-            fix_tokenizer = fix_tokenizer,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ): \\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token = token,\\n             # rope_scaling = rope_scaling,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n',\n",
       " '@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module = pickle,\\n+        pickle_module   = pickle,\\n         pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n@@ -106,7 +106,7 @@ def unsloth_save_model(\\n     model,\\n     tokenizer,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    merge_method         : str = \"lora\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -131,7 +131,7 @@ def unsloth_save_model(\\n     maximum_memory_usage : float = 0.9,\\n ):\\n     save_pretrained_settings = dict(locals())\\n-    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+    for deletion in (\"model\", \"tokenizer\", \"merge_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n     import re\\n@@ -144,8 +144,8 @@ def unsloth_save_model(\\n         gc.collect()\\n     pass\\n \\n-    save_method = save_method.lower().replace(\" \", \"_\")\\n-    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+    merge_method = merge_method.lower().replace(\" \", \"_\")\\n+    if merge_method != \"lora\" and merge_method != \"16bit\" and merge_method != \"4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n             \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n@@ -154,7 +154,7 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n-    if save_method == \"merged_4bit\":\\n+    if merge_method == \"4bit\":\\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n         model = model.merge_and_unload()\\n@@ -169,7 +169,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if (merge_method == \"lora\") and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -222,7 +222,7 @@ def unsloth_save_model(\\n         save_directory = new_save_directory\\n     pass\\n     \\n-    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+    if (merge_method == \"4bit\") or (merge_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n         not hasattr(model.model, \"model\") or \\\\\\n         not hasattr(model.model.model, \"layers\")\\n@@ -246,7 +246,7 @@ def unsloth_save_model(\\n             print()\\n \\n         print(\"Unsloth: Saving model...\", end = \"\")\\n-        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+        if merge_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n         print(\" Done.\")\\n@@ -434,19 +434,19 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory     : str = \"unsloth_finetuned_model\",\\n-    quantization_method : str = \"fast_quantized\",\\n+    model_directory : str = \"unsloth_finetuned_model\",\\n+    quantization    : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n-    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n-    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n-    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n-    elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    if   quantization == \"not_quantized\":  quantization = \"f16\"\\n+    elif quantization == \"fast_quantized\": quantization = \"q8_0\"\\n+    elif quantization == \"quantized\":      quantization = \"q4_k_m\"\\n+    elif quantization is None:             quantization = \"q8_0\"\\n \\n-    if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+    if quantization not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +456,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -469,9 +469,9 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n     first_conversion = \"f16\"\\n-    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n-    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n-    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    if   quantization == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization == \"q8_0\": first_conversion = \"q8_0\"\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -489,13 +489,13 @@ def save_to_gguf(\\n \\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n-    if quantization_method != first_conversion:\\n+    if quantization != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization_method} {n_cpus}\"\\n+            f\"{final_location} {quantization} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -511,7 +511,8 @@ pass\\n def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.save_pretrained(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n-    arguments[\"model\"]     = self\\n-    arguments[\"tokenizer\"] = None\\n+    arguments[\"model\"] = self\\n     del arguments[\"self\"]\\n     unsloth_save_model(**arguments)\\n     for _ in range(3):\\n@@ -547,7 +554,8 @@ pass\\n def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.push_to_hub(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n     arguments[\"model\"]          = self\\n-    arguments[\"tokenizer\"]      = None\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = True\\n     del arguments[\"self\"]\\n@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n     arguments = dict(locals())\\n-    arguments[\"model\"]       = self\\n-    arguments[\"tokenizer\"]   = tokenizer\\n-    arguments[\"push_to_hub\"] = False # We save ourselves\\n-    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"model\"]        = self\\n+    arguments[\"tokenizer\"]    = tokenizer\\n+    arguments[\"push_to_hub\"]  = False # We save ourselves\\n+    arguments[\"merge_method\"] = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # And save to HF\\n     if push_to_hub:\\n@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"tokenizer\"]      = tokenizer\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = False # We save ourselves\\n-    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"merge_method\"]   = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # Save to hub\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n',\n",
       " '@@ -65,8 +65,46 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n pass\\n \\n \\n+def NotebookTrainingTracker_write_line(self, values):\\n+    \"\"\"\\n+    Write the values in the inner table.\\n+\\n+    Args:\\n+        values (`Dict[str, float]`): The values to display.\\n+    \"\"\"\\n+    if self.inner_table is None:\\n+        self.inner_table = [list(values.keys()), list(values.values())]\\n+    else:\\n+        columns = self.inner_table[0]\\n+        print(columns)\\n+        for key in values.keys():\\n+            if key not in columns:\\n+                columns.append(key)\\n+        self.inner_table[0] = columns\\n+        if len(self.inner_table) > 1:\\n+            last_values = self.inner_table[-1]\\n+            first_column = self.inner_table[0][0]\\n+            if last_values[0] != values[first_column]:\\n+                # write new line\\n+                self.inner_table.append([values[c] if c in values else \"No Log\" for c in columns])\\n+            else:\\n+                # update last line\\n+                new_values = values\\n+                for c in columns:\\n+                    if c not in new_values.keys():\\n+                        new_values[c] = last_values[columns.index(c)]\\n+                self.inner_table[-1] = [new_values[c] for c in columns]\\n+        else:\\n+            # Edit for evaluation purposes\\n+            self.inner_table.append([values[c] if c in values else 0 for c in columns])\\n+        pass\\n+    pass\\n+pass\\n+\\n+\\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n+    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -161,11 +161,12 @@ pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n+    old_dtype = X.dtype\\n     X = X.to(torch.float32)\\n     variance = X.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n     X *= variance.rsqrt_()\\n-    X = X.to(residual.dtype)\\n+    X = X.to(old_dtype)\\n     X *= self.weight\\n     return X\\n pass\\n@@ -660,14 +661,15 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ):\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -720,18 +722,19 @@ class FastLlamaModel:\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n         model = AutoModelForCausalLM.from_pretrained(\\n             model_name,\\n-            device_map = device_map,\\n-            torch_dtype = dtype,\\n-            quantization_config = bnb_config,\\n-            token = token,\\n-            rope_scaling = rope_scaling,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n             model_max_length = max_seq_length,\\n-            padding_side = \"right\",\\n-            token = token,\\n+            padding_side     = \"right\",\\n+            token            = token,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -755,12 +758,12 @@ class FastLlamaModel:\\n         # We check the tokenizer first for errors\\n         if fix_tokenizer:\\n             tokenizer = check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n+                model            = model,\\n+                tokenizer        = tokenizer,\\n+                model_name       = model_name,\\n                 model_max_length = max_seq_length,\\n-                padding_side = \"right\",\\n-                token = token,\\n+                padding_side     = \"right\",\\n+                token            = token,\\n             )\\n         pass\\n         patch_saving_functions(tokenizer)\\n@@ -828,20 +831,20 @@ class FastLlamaModel:\\n     @staticmethod\\n     def get_peft_model(\\n         model,\\n-        r = 16,\\n-        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n-                          \"gate_proj\", \"up_proj\", \"down_proj\"],\\n-        lora_alpha = 16,\\n-        lora_dropout = 0,\\n-        bias = \"none\",\\n+        r                   = 16,\\n+        target_modules      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n+                               \"gate_proj\", \"up_proj\", \"down_proj\"],\\n+        lora_alpha          = 16,\\n+        lora_dropout        = 0,\\n+        bias                = \"none\",\\n         layers_to_transform = None,\\n-        layers_pattern = None,\\n+        layers_pattern      = None,\\n         use_gradient_checkpointing = True,\\n-        random_state = 3407,\\n-        max_seq_length = 2048, # not used anymore\\n-        use_rslora = False,\\n-        init_lora_weights = True,\\n-        loftq_config = None,\\n+        random_state        = 3407,\\n+        max_seq_length      = 2048, # not used anymore\\n+        use_rslora          = False,\\n+        init_lora_weights   = True,\\n+        loftq_config        = None,\\n         **kwargs,\\n     ):\\n         if isinstance(model, PeftModelForCausalLM):\\n@@ -909,12 +912,14 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n+                # We do it ourselves!\\n+                new_alpha = lora_alpha / (r**0.5)\\n                 import peft\\n-                raise RuntimeError(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support use_rslora.\\\\n\"\\\\\\n-                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+                logger.warning_once(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n+                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n                 )\\n+                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -63,14 +63,14 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None,\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None,\\n+        fix_tokenizer  = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -106,14 +106,14 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name = model_name,\\n+            model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n-            dtype = dtype,\\n-            load_in_4bit = load_in_4bit,\\n-            token = token,\\n-            device_map = device_map,\\n-            rope_scaling = rope_scaling,\\n-            fix_tokenizer = fix_tokenizer,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -256,14 +256,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name = \"unsloth/mistral-7b-bnb-4bit\",\\n+        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n         max_seq_length = 4096,\\n-        dtype = None,\\n-        load_in_4bit = True,\\n-        token = None,\\n-        device_map = \"sequential\",\\n-        rope_scaling = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer = True,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        **kwargs,\\n     ): \\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,6 +306,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token = token,\\n             # rope_scaling = rope_scaling,\\n+            **kwargs,\\n         )\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             model_name,\\n',\n",
       " '@@ -94,7 +94,7 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module = pickle,\\n+        pickle_module   = pickle,\\n         pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n@@ -106,7 +106,7 @@ def unsloth_save_model(\\n     model,\\n     tokenizer,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"lora\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    merge_method         : str = \"lora\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -131,7 +131,7 @@ def unsloth_save_model(\\n     maximum_memory_usage : float = 0.9,\\n ):\\n     save_pretrained_settings = dict(locals())\\n-    for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n+    for deletion in (\"model\", \"tokenizer\", \"merge_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n     import re\\n@@ -144,8 +144,8 @@ def unsloth_save_model(\\n         gc.collect()\\n     pass\\n \\n-    save_method = save_method.lower().replace(\" \", \"_\")\\n-    if save_method != \"lora\" and save_method != \"merged_16bit\" and save_method != \"merged_4bit\":\\n+    merge_method = merge_method.lower().replace(\" \", \"_\")\\n+    if merge_method != \"lora\" and merge_method != \"16bit\" and merge_method != \"4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: You must select one of 3 options when saving models:\\\\n\"\\\\\\n             \\'\"lora\"         ==> This is the fastest and easiet. Just saves LoRA modules.\\\\n\\'\\\\\\n@@ -154,7 +154,7 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n-    if save_method == \"merged_4bit\":\\n+    if merge_method == \"4bit\":\\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n         model = model.merge_and_unload()\\n@@ -169,7 +169,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if (merge_method == \"lora\") and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -222,7 +222,7 @@ def unsloth_save_model(\\n         save_directory = new_save_directory\\n     pass\\n     \\n-    if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n+    if (merge_method == \"4bit\") or (merge_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n         not hasattr(model.model, \"model\") or \\\\\\n         not hasattr(model.model.model, \"layers\")\\n@@ -246,7 +246,7 @@ def unsloth_save_model(\\n             print()\\n \\n         print(\"Unsloth: Saving model...\", end = \"\")\\n-        if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n+        if merge_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n         print(\" Done.\")\\n@@ -434,19 +434,19 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory     : str = \"unsloth_finetuned_model\",\\n-    quantization_method : str = \"fast_quantized\",\\n+    model_directory : str = \"unsloth_finetuned_model\",\\n+    quantization    : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n-    if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n-    elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n-    elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n-    elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    if   quantization == \"not_quantized\":  quantization = \"f16\"\\n+    elif quantization == \"fast_quantized\": quantization = \"q8_0\"\\n+    elif quantization == \"quantized\":      quantization = \"q4_k_m\"\\n+    elif quantization is None:             quantization = \"q8_0\"\\n \\n-    if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n+    if quantization not in ALLOWED_QUANTS.keys():\\n+        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +456,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -469,9 +469,9 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n     first_conversion = \"f16\"\\n-    if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n-    elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n-    elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    if   quantization == \"f32\":  first_conversion = \"f32\"\\n+    elif quantization == \"f16\":  first_conversion = \"f16\"\\n+    elif quantization == \"q8_0\": first_conversion = \"q8_0\"\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -489,13 +489,13 @@ def save_to_gguf(\\n \\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n-    if quantization_method != first_conversion:\\n+    if quantization != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization_method} {n_cpus}\"\\n+            f\"{final_location} {quantization} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -511,7 +511,8 @@ pass\\n def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -529,14 +530,20 @@ def unsloth_save_pretrained_merged(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.save_pretrained(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n-    arguments[\"model\"]     = self\\n-    arguments[\"tokenizer\"] = None\\n+    arguments[\"model\"] = self\\n     del arguments[\"self\"]\\n     unsloth_save_model(**arguments)\\n     for _ in range(3):\\n@@ -547,7 +554,8 @@ pass\\n def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n-    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    tokenizer            = None,\\n+    merge_method         : str = \"16bit\", # [\"lora\", \"16bit\", \"4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -565,14 +573,20 @@ def unsloth_push_to_hub_merged(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 with as few overhead as possible.\\n \\n-        Choose for `save_method` to be either:\\n-        1. `merged_16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n-        2.  `merged_4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n-        3.         `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n+        Choose for `merge_method` to be either:\\n+        1. `16bit`: Merge LoRA into float16 weights. Useful for GGUF / llama.cpp.\\n+        2.  `4bit`: Merge LoRA into int4 weights. Useful for DPO / HF inference.\\n+        3.  `lora`: Save LoRA adapters with no merging. Useful for HF inference.\\n     \"\"\"\\n+    if tokenizer is None:\\n+        logger.warning_once(\\n+            \"Unsloth: You\\'re not saving a tokenizer as well?\\\\n\"\\\\\\n+            \"You can do it separately via `tokenizer.push_to_hub(...)`\"\\n+        )\\n+    pass\\n+\\n     arguments = dict(locals())\\n     arguments[\"model\"]          = self\\n-    arguments[\"tokenizer\"]      = None\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = True\\n     del arguments[\"self\"]\\n@@ -587,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -605,7 +619,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -630,12 +644,12 @@ def unsloth_save_pretrained_gguf(\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n \\n     arguments = dict(locals())\\n-    arguments[\"model\"]       = self\\n-    arguments[\"tokenizer\"]   = tokenizer\\n-    arguments[\"push_to_hub\"] = False # We save ourselves\\n-    arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"model\"]        = self\\n+    arguments[\"tokenizer\"]    = tokenizer\\n+    arguments[\"push_to_hub\"]  = False # We save ourselves\\n+    arguments[\"merge_method\"] = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -648,7 +662,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # And save to HF\\n     if push_to_hub:\\n@@ -685,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method  : str = \"fast_quantized\",\\n+    quantization         : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -703,7 +717,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization_method` to be:\\n+        Choose for `quantization` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -732,10 +746,10 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"tokenizer\"]      = tokenizer\\n     arguments[\"save_directory\"] = repo_id\\n     arguments[\"push_to_hub\"]    = False # We save ourselves\\n-    arguments[\"save_method\"]    = \"merged_16bit\" # Must be 16bit\\n+    arguments[\"merge_method\"]   = \"16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization_method\"]\\n+    del arguments[\"quantization\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -748,7 +762,7 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n \\n     # Save to hub\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n',\n",
       " '@@ -460,7 +460,7 @@ def LlamaModel_fast_forward(\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = getattr(self.config, \"sliding_window\"),\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n         )\\n     pass\\n \\n',\n",
       " '@@ -131,7 +131,7 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        sw = getattr(self.config, \"sliding_window\")\\n+        sw = getattr(self.config, \"sliding_window\", None)\\n         sw = q_len if sw is None else sw\\n         window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n@@ -175,7 +175,7 @@ def MistralForCausalLM_fast_forward(\\n \\n     if causal_mask is None:\\n         bsz, q_len = input_ids.shape\\n-        sliding_window = getattr(self.config, \"sliding_window\")\\n+        sliding_window = getattr(self.config, \"sliding_window\", None)\\n         if sliding_window is None or sliding_window <= 0:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         elif q_len <= sliding_window:\\n',\n",
       " '@@ -460,7 +460,7 @@ def LlamaModel_fast_forward(\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = getattr(self.config, \"sliding_window\"),\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n         )\\n     pass\\n \\n',\n",
       " '@@ -131,7 +131,7 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        sw = getattr(self.config, \"sliding_window\")\\n+        sw = getattr(self.config, \"sliding_window\", None)\\n         sw = q_len if sw is None else sw\\n         window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n@@ -175,7 +175,7 @@ def MistralForCausalLM_fast_forward(\\n \\n     if causal_mask is None:\\n         bsz, q_len = input_ids.shape\\n-        sliding_window = getattr(self.config, \"sliding_window\")\\n+        sliding_window = getattr(self.config, \"sliding_window\", None)\\n         if sliding_window is None or sliding_window <= 0:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         elif q_len <= sliding_window:\\n',\n",
       " '@@ -460,7 +460,7 @@ def LlamaModel_fast_forward(\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = getattr(self.config, \"sliding_window\"),\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n         )\\n     pass\\n \\n',\n",
       " '@@ -131,7 +131,7 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        sw = getattr(self.config, \"sliding_window\")\\n+        sw = getattr(self.config, \"sliding_window\", None)\\n         sw = q_len if sw is None else sw\\n         window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n@@ -175,7 +175,7 @@ def MistralForCausalLM_fast_forward(\\n \\n     if causal_mask is None:\\n         bsz, q_len = input_ids.shape\\n-        sliding_window = getattr(self.config, \"sliding_window\")\\n+        sliding_window = getattr(self.config, \"sliding_window\", None)\\n         if sliding_window is None or sliding_window <= 0:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         elif q_len <= sliding_window:\\n',\n",
       " '@@ -460,7 +460,7 @@ def LlamaModel_fast_forward(\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = getattr(self.config, \"sliding_window\"),\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n         )\\n     pass\\n \\n',\n",
       " '@@ -131,7 +131,7 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        sw = getattr(self.config, \"sliding_window\")\\n+        sw = getattr(self.config, \"sliding_window\", None)\\n         sw = q_len if sw is None else sw\\n         window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n@@ -175,7 +175,7 @@ def MistralForCausalLM_fast_forward(\\n \\n     if causal_mask is None:\\n         bsz, q_len = input_ids.shape\\n-        sliding_window = getattr(self.config, \"sliding_window\")\\n+        sliding_window = getattr(self.config, \"sliding_window\", None)\\n         if sliding_window is None or sliding_window <= 0:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         elif q_len <= sliding_window:\\n',\n",
       " '@@ -460,7 +460,7 @@ def LlamaModel_fast_forward(\\n             (batch_size, seq_length),\\n             inputs_embeds,\\n             past_key_values_length,\\n-            sliding_window = getattr(self.config, \"sliding_window\"),\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n         )\\n     pass\\n \\n',\n",
       " '@@ -131,7 +131,7 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        sw = getattr(self.config, \"sliding_window\")\\n+        sw = getattr(self.config, \"sliding_window\", None)\\n         sw = q_len if sw is None else sw\\n         window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n@@ -175,7 +175,7 @@ def MistralForCausalLM_fast_forward(\\n \\n     if causal_mask is None:\\n         bsz, q_len = input_ids.shape\\n-        sliding_window = getattr(self.config, \"sliding_window\")\\n+        sliding_window = getattr(self.config, \"sliding_window\", None)\\n         if sliding_window is None or sliding_window <= 0:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n         elif q_len <= sliding_window:\\n',\n",
       " '@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n         self.scaling[adapter_name] = lora_alpha / r\\n \\n     if init_lora_weights == \"loftq\":\\n+        # We manually check for PEFT\\n+        if not hasattr(self, \"loftq_init\"):\\n+            import peft\\n+            raise RuntimeError(\\n+                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+            )\\n+        pass\\n         self.loftq_init(adapter_name)\\n+        \\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -28,6 +28,7 @@ DPOTrainer_metrics = [\\n     \"logits/rejected\",\\n     \"logits/chosen\",\\n ]\\n+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)\\n \\n \\n def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):\\n@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n     if args.evaluation_strategy == IntervalStrategy.NO and \"loss\" in logs:\\n         values = {\"Training Loss\": logs[\"loss\"]}\\n         for metric in DPOTrainer_metrics:\\n-            if metric in logs:\\n-                values[metric.replace(\"/\", \" / \")] = logs[metric]\\n-            else:\\n-                # Maybe not a DPO Trainer anymore? Redo the tracker\\n-                column_names = [self.first_column] + [\"Training Loss\"]\\n-                if args.evaluation_strategy != IntervalStrategy.NO:\\n-                    column_names.append(\"Validation Loss\")\\n-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)\\n-                break\\n-            pass\\n+            values[metric.replace(\"/\", \" / \")] = logs[metric]\\n         pass\\n         # First column is necessarily Step since we\\'re not in epoch eval strategy\\n         values[\"Step\"] = state.global_step\\n@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):\\n         self.inner_table = [list(values.keys()), list(values.values())]\\n     else:\\n         columns = self.inner_table[0]\\n-        print(columns)\\n-        for key in values.keys():\\n-            if key not in columns:\\n-                columns.append(key)\\n+        new_values = {}\\n+        for key, value in values.items():\\n+            lowered = key.lower()\\n+            if lowered in set_DPOTrainer_metrics:\\n+                new_values[lowered.replace(\"/\", \" / \")] = value\\n+            else:\\n+                new_values[key] = value\\n+        pass\\n+        values = new_values\\n+\\n         self.inner_table[0] = columns\\n         if len(self.inner_table) > 1:\\n             last_values = self.inner_table[-1]\\n@@ -104,7 +102,7 @@ pass\\n \\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -912,14 +912,13 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n-                # We do it ourselves!\\n-                new_alpha = lora_alpha / (r**0.5)\\n+                # We manually check for PEFT\\n                 import peft\\n-                logger.warning_once(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n-                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n                 )\\n-                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"push_to_hub\"]  = False # We save ourselves\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"save_method\"]   = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n',\n",
       " '@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n         self.scaling[adapter_name] = lora_alpha / r\\n \\n     if init_lora_weights == \"loftq\":\\n+        # We manually check for PEFT\\n+        if not hasattr(self, \"loftq_init\"):\\n+            import peft\\n+            raise RuntimeError(\\n+                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+            )\\n+        pass\\n         self.loftq_init(adapter_name)\\n+        \\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -28,6 +28,7 @@ DPOTrainer_metrics = [\\n     \"logits/rejected\",\\n     \"logits/chosen\",\\n ]\\n+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)\\n \\n \\n def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):\\n@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n     if args.evaluation_strategy == IntervalStrategy.NO and \"loss\" in logs:\\n         values = {\"Training Loss\": logs[\"loss\"]}\\n         for metric in DPOTrainer_metrics:\\n-            if metric in logs:\\n-                values[metric.replace(\"/\", \" / \")] = logs[metric]\\n-            else:\\n-                # Maybe not a DPO Trainer anymore? Redo the tracker\\n-                column_names = [self.first_column] + [\"Training Loss\"]\\n-                if args.evaluation_strategy != IntervalStrategy.NO:\\n-                    column_names.append(\"Validation Loss\")\\n-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)\\n-                break\\n-            pass\\n+            values[metric.replace(\"/\", \" / \")] = logs[metric]\\n         pass\\n         # First column is necessarily Step since we\\'re not in epoch eval strategy\\n         values[\"Step\"] = state.global_step\\n@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):\\n         self.inner_table = [list(values.keys()), list(values.values())]\\n     else:\\n         columns = self.inner_table[0]\\n-        print(columns)\\n-        for key in values.keys():\\n-            if key not in columns:\\n-                columns.append(key)\\n+        new_values = {}\\n+        for key, value in values.items():\\n+            lowered = key.lower()\\n+            if lowered in set_DPOTrainer_metrics:\\n+                new_values[lowered.replace(\"/\", \" / \")] = value\\n+            else:\\n+                new_values[key] = value\\n+        pass\\n+        values = new_values\\n+\\n         self.inner_table[0] = columns\\n         if len(self.inner_table) > 1:\\n             last_values = self.inner_table[-1]\\n@@ -104,7 +102,7 @@ pass\\n \\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -912,14 +912,13 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n-                # We do it ourselves!\\n-                new_alpha = lora_alpha / (r**0.5)\\n+                # We manually check for PEFT\\n                 import peft\\n-                logger.warning_once(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n-                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n                 )\\n-                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"push_to_hub\"]  = False # We save ourselves\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"save_method\"]   = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n',\n",
       " '@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n         self.scaling[adapter_name] = lora_alpha / r\\n \\n     if init_lora_weights == \"loftq\":\\n+        # We manually check for PEFT\\n+        if not hasattr(self, \"loftq_init\"):\\n+            import peft\\n+            raise RuntimeError(\\n+                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+            )\\n+        pass\\n         self.loftq_init(adapter_name)\\n+        \\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -28,6 +28,7 @@ DPOTrainer_metrics = [\\n     \"logits/rejected\",\\n     \"logits/chosen\",\\n ]\\n+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)\\n \\n \\n def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):\\n@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n     if args.evaluation_strategy == IntervalStrategy.NO and \"loss\" in logs:\\n         values = {\"Training Loss\": logs[\"loss\"]}\\n         for metric in DPOTrainer_metrics:\\n-            if metric in logs:\\n-                values[metric.replace(\"/\", \" / \")] = logs[metric]\\n-            else:\\n-                # Maybe not a DPO Trainer anymore? Redo the tracker\\n-                column_names = [self.first_column] + [\"Training Loss\"]\\n-                if args.evaluation_strategy != IntervalStrategy.NO:\\n-                    column_names.append(\"Validation Loss\")\\n-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)\\n-                break\\n-            pass\\n+            values[metric.replace(\"/\", \" / \")] = logs[metric]\\n         pass\\n         # First column is necessarily Step since we\\'re not in epoch eval strategy\\n         values[\"Step\"] = state.global_step\\n@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):\\n         self.inner_table = [list(values.keys()), list(values.values())]\\n     else:\\n         columns = self.inner_table[0]\\n-        print(columns)\\n-        for key in values.keys():\\n-            if key not in columns:\\n-                columns.append(key)\\n+        new_values = {}\\n+        for key, value in values.items():\\n+            lowered = key.lower()\\n+            if lowered in set_DPOTrainer_metrics:\\n+                new_values[lowered.replace(\"/\", \" / \")] = value\\n+            else:\\n+                new_values[key] = value\\n+        pass\\n+        values = new_values\\n+\\n         self.inner_table[0] = columns\\n         if len(self.inner_table) > 1:\\n             last_values = self.inner_table[-1]\\n@@ -104,7 +102,7 @@ pass\\n \\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -912,14 +912,13 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n-                # We do it ourselves!\\n-                new_alpha = lora_alpha / (r**0.5)\\n+                # We manually check for PEFT\\n                 import peft\\n-                logger.warning_once(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n-                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n                 )\\n-                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"push_to_hub\"]  = False # We save ourselves\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"save_method\"]   = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n',\n",
       " '@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n         self.scaling[adapter_name] = lora_alpha / r\\n \\n     if init_lora_weights == \"loftq\":\\n+        # We manually check for PEFT\\n+        if not hasattr(self, \"loftq_init\"):\\n+            import peft\\n+            raise RuntimeError(\\n+                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+            )\\n+        pass\\n         self.loftq_init(adapter_name)\\n+        \\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -28,6 +28,7 @@ DPOTrainer_metrics = [\\n     \"logits/rejected\",\\n     \"logits/chosen\",\\n ]\\n+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)\\n \\n \\n def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):\\n@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n     if args.evaluation_strategy == IntervalStrategy.NO and \"loss\" in logs:\\n         values = {\"Training Loss\": logs[\"loss\"]}\\n         for metric in DPOTrainer_metrics:\\n-            if metric in logs:\\n-                values[metric.replace(\"/\", \" / \")] = logs[metric]\\n-            else:\\n-                # Maybe not a DPO Trainer anymore? Redo the tracker\\n-                column_names = [self.first_column] + [\"Training Loss\"]\\n-                if args.evaluation_strategy != IntervalStrategy.NO:\\n-                    column_names.append(\"Validation Loss\")\\n-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)\\n-                break\\n-            pass\\n+            values[metric.replace(\"/\", \" / \")] = logs[metric]\\n         pass\\n         # First column is necessarily Step since we\\'re not in epoch eval strategy\\n         values[\"Step\"] = state.global_step\\n@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):\\n         self.inner_table = [list(values.keys()), list(values.values())]\\n     else:\\n         columns = self.inner_table[0]\\n-        print(columns)\\n-        for key in values.keys():\\n-            if key not in columns:\\n-                columns.append(key)\\n+        new_values = {}\\n+        for key, value in values.items():\\n+            lowered = key.lower()\\n+            if lowered in set_DPOTrainer_metrics:\\n+                new_values[lowered.replace(\"/\", \" / \")] = value\\n+            else:\\n+                new_values[key] = value\\n+        pass\\n+        values = new_values\\n+\\n         self.inner_table[0] = columns\\n         if len(self.inner_table) > 1:\\n             last_values = self.inner_table[-1]\\n@@ -104,7 +102,7 @@ pass\\n \\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -912,14 +912,13 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n-                # We do it ourselves!\\n-                new_alpha = lora_alpha / (r**0.5)\\n+                # We manually check for PEFT\\n                 import peft\\n-                logger.warning_once(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n-                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n                 )\\n-                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"push_to_hub\"]  = False # We save ourselves\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"save_method\"]   = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n',\n",
       " '@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n         self.scaling[adapter_name] = lora_alpha / r\\n \\n     if init_lora_weights == \"loftq\":\\n+        # We manually check for PEFT\\n+        if not hasattr(self, \"loftq_init\"):\\n+            import peft\\n+            raise RuntimeError(\\n+                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+            )\\n+        pass\\n         self.loftq_init(adapter_name)\\n+        \\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -28,6 +28,7 @@ DPOTrainer_metrics = [\\n     \"logits/rejected\",\\n     \"logits/chosen\",\\n ]\\n+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)\\n \\n \\n def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):\\n@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n     if args.evaluation_strategy == IntervalStrategy.NO and \"loss\" in logs:\\n         values = {\"Training Loss\": logs[\"loss\"]}\\n         for metric in DPOTrainer_metrics:\\n-            if metric in logs:\\n-                values[metric.replace(\"/\", \" / \")] = logs[metric]\\n-            else:\\n-                # Maybe not a DPO Trainer anymore? Redo the tracker\\n-                column_names = [self.first_column] + [\"Training Loss\"]\\n-                if args.evaluation_strategy != IntervalStrategy.NO:\\n-                    column_names.append(\"Validation Loss\")\\n-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)\\n-                break\\n-            pass\\n+            values[metric.replace(\"/\", \" / \")] = logs[metric]\\n         pass\\n         # First column is necessarily Step since we\\'re not in epoch eval strategy\\n         values[\"Step\"] = state.global_step\\n@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):\\n         self.inner_table = [list(values.keys()), list(values.values())]\\n     else:\\n         columns = self.inner_table[0]\\n-        print(columns)\\n-        for key in values.keys():\\n-            if key not in columns:\\n-                columns.append(key)\\n+        new_values = {}\\n+        for key, value in values.items():\\n+            lowered = key.lower()\\n+            if lowered in set_DPOTrainer_metrics:\\n+                new_values[lowered.replace(\"/\", \" / \")] = value\\n+            else:\\n+                new_values[key] = value\\n+        pass\\n+        values = new_values\\n+\\n         self.inner_table[0] = columns\\n         if len(self.inner_table) > 1:\\n             last_values = self.inner_table[-1]\\n@@ -104,7 +102,7 @@ pass\\n \\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -912,14 +912,13 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n-                # We do it ourselves!\\n-                new_alpha = lora_alpha / (r**0.5)\\n+                # We manually check for PEFT\\n                 import peft\\n-                logger.warning_once(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n-                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n                 )\\n-                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"push_to_hub\"]  = False # We save ourselves\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"save_method\"]   = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n',\n",
       " '@@ -242,7 +242,17 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n         self.scaling[adapter_name] = lora_alpha / r\\n \\n     if init_lora_weights == \"loftq\":\\n+        # We manually check for PEFT\\n+        if not hasattr(self, \"loftq_init\"):\\n+            import peft\\n+            raise RuntimeError(\\n+                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n+                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n+            )\\n+        pass\\n         self.loftq_init(adapter_name)\\n+        \\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -28,6 +28,7 @@ DPOTrainer_metrics = [\\n     \"logits/rejected\",\\n     \"logits/chosen\",\\n ]\\n+set_DPOTrainer_metrics = frozenset(DPOTrainer_metrics)\\n \\n \\n def NotebookProgressCallback_on_train_begin(self, args, state, control, **kwargs):\\n@@ -47,16 +48,7 @@ def NotebookProgressCallback_on_log(self, args, state, control, logs=None, **kwa\\n     if args.evaluation_strategy == IntervalStrategy.NO and \"loss\" in logs:\\n         values = {\"Training Loss\": logs[\"loss\"]}\\n         for metric in DPOTrainer_metrics:\\n-            if metric in logs:\\n-                values[metric.replace(\"/\", \" / \")] = logs[metric]\\n-            else:\\n-                # Maybe not a DPO Trainer anymore? Redo the tracker\\n-                column_names = [self.first_column] + [\"Training Loss\"]\\n-                if args.evaluation_strategy != IntervalStrategy.NO:\\n-                    column_names.append(\"Validation Loss\")\\n-                    self.training_tracker = NotebookTrainingTracker(state.max_steps, column_names)\\n-                break\\n-            pass\\n+            values[metric.replace(\"/\", \" / \")] = logs[metric]\\n         pass\\n         # First column is necessarily Step since we\\'re not in epoch eval strategy\\n         values[\"Step\"] = state.global_step\\n@@ -76,10 +68,16 @@ def NotebookTrainingTracker_write_line(self, values):\\n         self.inner_table = [list(values.keys()), list(values.values())]\\n     else:\\n         columns = self.inner_table[0]\\n-        print(columns)\\n-        for key in values.keys():\\n-            if key not in columns:\\n-                columns.append(key)\\n+        new_values = {}\\n+        for key, value in values.items():\\n+            lowered = key.lower()\\n+            if lowered in set_DPOTrainer_metrics:\\n+                new_values[lowered.replace(\"/\", \" / \")] = value\\n+            else:\\n+                new_values[key] = value\\n+        pass\\n+        values = new_values\\n+\\n         self.inner_table[0] = columns\\n         if len(self.inner_table) > 1:\\n             last_values = self.inner_table[-1]\\n@@ -104,7 +102,7 @@ pass\\n \\n def PatchDPOTrainer():\\n     # Patch DPO notebook printing\\n-    # NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n     from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n     DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n     DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n',\n",
       " '@@ -912,14 +912,13 @@ class FastLlamaModel:\\n         assert(type(use_rslora) is bool)\\n         if use_rslora:\\n             if not SUPPORTS_RSLORA:\\n-                # We do it ourselves!\\n-                new_alpha = lora_alpha / (r**0.5)\\n+                # We manually check for PEFT\\n                 import peft\\n-                logger.warning_once(\\n-                    f\"Unsloth: Your PEFT version of {peft.__version__} (0.7.2 needed) does not support `use_rslora` natively.\\\\n\"\\\\\\n-                    f\"But, we do it ourselves by setting `alpha = {new_alpha}.`\"\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your PEFT version of {peft.__version__} does not support `use_rslora`.\\\\n\"\\\\\\n+                    \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n+                    \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n                 )\\n-                lora_alpha = new_alpha\\n             pass\\n         pass\\n \\n',\n",
       " '@@ -555,7 +555,7 @@ def unsloth_push_to_hub_merged(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -601,7 +601,7 @@ def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -649,7 +649,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"push_to_hub\"]  = False # We save ourselves\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n@@ -699,7 +699,7 @@ def unsloth_push_to_hub_gguf(\\n     self,\\n     repo_id              : str,\\n     tokenizer            = None,\\n-    quantization_method         : str = \"fast_quantized\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -749,7 +749,7 @@ def unsloth_push_to_hub_gguf(\\n     arguments[\"save_method\"]   = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n-    del arguments[\"quantization\"]\\n+    del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n     git_clone = install_llama_cpp_clone_non_blocking()\\n',\n",
       " '@@ -54,6 +54,11 @@ __all__ = [\\n ]\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(\\n             future Pytorch versions.\\n     \"\"\"\\n \\n-    # Freeze all parameters\\n-    for param in model.parameters():\\n-        param.requires_grad_(False)\\n+    # Freeze all parameters except LoRA\\n+    for name, param in model.named_parameters():\\n+        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+            param.requires_grad_(True)\\n+        else:\\n+            param.requires_grad_(False)\\n+    pass\\n \\n     if use_gradient_checkpointing:\\n         model.gradient_checkpointing_enable()\\n@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n             )\\n         pass\\n         self.loftq_init(adapter_name)\\n-        \\n+\\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -844,9 +844,11 @@ class FastLlamaModel:\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n         init_lora_weights   = True,\\n-        loftq_config        = None,\\n+        loftq_config        = {},\\n         **kwargs,\\n     ):\\n+        transformers_set_seed(random_state)\\n+\\n         if isinstance(model, PeftModelForCausalLM):\\n             raise TypeError(\\n                 \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n@@ -892,7 +894,7 @@ class FastLlamaModel:\\n                 )\\n             pass\\n \\n-            if loftq_config is None:\\n+            if loftq_config == {}:\\n                 from peft import LoftQConfig\\n                 logger.warning_once(\\n                     f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n@@ -922,8 +924,6 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n-        transformers_set_seed(random_state)\\n-\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n@@ -949,22 +949,40 @@ class FastLlamaModel:\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n         lora_config = LoraConfig(**arguments)\\n+        model = _get_peft_model(model, lora_config)\\n+\\n+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+        return model\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def patch_peft_model(\\n+        model,\\n+        use_gradient_checkpointing = True,\\n+    ):\\n+        if not isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model needs to call `.get_peft_model` first!\"\\n+            )\\n+        pass\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n             use_reentrant = True,\\n         )\\n-        model = _get_peft_model(model, lora_config)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.peft_config[\"default\"].base_model_name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        for active_adapter in model.peft_config.keys():\\n+            name = model.peft_config[active_adapter].base_model_name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.peft_config[active_adapter].base_model_name_or_path = name\\n+            pass\\n+            # Add revision to enable future fast inference paths\\n+            model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n-        # Add revision to enable future fast inference paths\\n-        model.peft_config[\"default\"].revision = f\"unsloth\"\\n \\n         # Do patching\\n         n_mlp = 0\\n@@ -972,6 +990,13 @@ class FastLlamaModel:\\n         n_o   = 0\\n         import types\\n \\n+        active_adapter = model.active_adapters[0] if \\\\\\n+            hasattr(model, \"active_adapters\") else model.active_adapter\\n+\\n+        # Get dropout and bias\\n+        lora_dropout = model.peft_config[active_adapter].lora_dropout\\n+        bias         = model.peft_config[active_adapter].bias\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n',\n",
       " '@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):\\n         if is_peft:\\n             # Now add PEFT adapters\\n             model = PeftModel.from_pretrained(model, old_model_name)\\n+            # Patch it as well!\\n+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -221,6 +221,17 @@ def unsloth_save_model(\\n         save_pretrained_settings[\"save_directory\"] = new_save_directory\\n         save_directory = new_save_directory\\n     pass\\n+\\n+    # Tokenizer has different saving arguments\\n+    tokenizer_save_settings = \\\\\\n+    {\\n+        \"save_directory\"  : save_pretrained_settings[\"save_directory\"],\\n+        \"legacy_format\"   : None,\\n+        \"filename_prefix\" : None,\\n+        \"push_to_hub\"     : save_pretrained_settings[\"push_to_hub\"],\\n+        \"private\"         : save_pretrained_settings[\"private\"],\\n+        \"token\"           : save_pretrained_settings[\"token\"],\\n+    }\\n     \\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n@@ -240,7 +251,7 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            tokenizer.save_pretrained(**tokenizer_save_settings)\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -360,13 +371,34 @@ def unsloth_save_model(\\n \\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+\\n+    # Since merged, edit quantization_config\\n+    old_config = model.config\\n+    new_config = model.config.to_dict()\\n+    if \"quantization_config\" in new_config:\\n+        del new_config[\"quantization_config\"]\\n+    original_model = model\\n+    new_config = type(model.config).from_dict(new_config)\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = new_config\\n+    model.config = new_config\\n+\\n+    # Save!\\n     model.model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Revert config back\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = old_config\\n+    model.config = old_config\\n     print(\"Done.\")\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -446,7 +478,7 @@ def save_to_gguf(\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +488,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -491,11 +523,11 @@ def save_to_gguf(\\n \\n     if quantization_method != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization} {n_cpus}\"\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    # Check for username\\n+    if \"/\" not in save_directory:\\n+        from huggingface_hub import whoami\\n+        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n+        except: pass\\n+    pass\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    # Create model card\\n+    from huggingface_hub import ModelCard, ModelCardData\\n+    card_data = ModelCardData(\\n+        language = \"en\",\\n+        license  = \"apache-2.0\",\\n+        library  = \"unsloth\",\\n+        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    )\\n+\\n+    content = f\"\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n+    f\"\\\\n\"\\n+    \\n+    card = ModelCard(content)\\n+    card.push_to_hub(save_directory, token = token)\\n+\\n+    # Now upload file\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # And save to HF\\n-    if push_to_hub:\\n-        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-        from huggingface_hub import create_repo\\n-        create_repo(\\n-            repo_id   = save_directory,\\n-            token     = token,\\n-            repo_type = \"model\",\\n-            exist_ok  = True,\\n-        )\\n-\\n-        from huggingface_hub import HfApi\\n-        hf_api = HfApi(token = token)\\n-\\n-        if \"/\" in file_location:\\n-            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-        else:\\n-            uploaded_location = file_location\\n-        pass\\n-\\n-        hf_api.upload_file(\\n-            path_or_fileobj = file_location,\\n-            path_in_repo    = uploaded_location,\\n-            repo_id         = save_directory,\\n-            repo_type       = \"model\",\\n-        )\\n-    pass\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # Save to hub\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-    from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        private   = private,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n-\\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n-\\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n',\n",
       " '@@ -54,6 +54,11 @@ __all__ = [\\n ]\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(\\n             future Pytorch versions.\\n     \"\"\"\\n \\n-    # Freeze all parameters\\n-    for param in model.parameters():\\n-        param.requires_grad_(False)\\n+    # Freeze all parameters except LoRA\\n+    for name, param in model.named_parameters():\\n+        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+            param.requires_grad_(True)\\n+        else:\\n+            param.requires_grad_(False)\\n+    pass\\n \\n     if use_gradient_checkpointing:\\n         model.gradient_checkpointing_enable()\\n@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n             )\\n         pass\\n         self.loftq_init(adapter_name)\\n-        \\n+\\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -844,9 +844,11 @@ class FastLlamaModel:\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n         init_lora_weights   = True,\\n-        loftq_config        = None,\\n+        loftq_config        = {},\\n         **kwargs,\\n     ):\\n+        transformers_set_seed(random_state)\\n+\\n         if isinstance(model, PeftModelForCausalLM):\\n             raise TypeError(\\n                 \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n@@ -892,7 +894,7 @@ class FastLlamaModel:\\n                 )\\n             pass\\n \\n-            if loftq_config is None:\\n+            if loftq_config == {}:\\n                 from peft import LoftQConfig\\n                 logger.warning_once(\\n                     f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n@@ -922,8 +924,6 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n-        transformers_set_seed(random_state)\\n-\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n@@ -949,22 +949,40 @@ class FastLlamaModel:\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n         lora_config = LoraConfig(**arguments)\\n+        model = _get_peft_model(model, lora_config)\\n+\\n+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+        return model\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def patch_peft_model(\\n+        model,\\n+        use_gradient_checkpointing = True,\\n+    ):\\n+        if not isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model needs to call `.get_peft_model` first!\"\\n+            )\\n+        pass\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n             use_reentrant = True,\\n         )\\n-        model = _get_peft_model(model, lora_config)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.peft_config[\"default\"].base_model_name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        for active_adapter in model.peft_config.keys():\\n+            name = model.peft_config[active_adapter].base_model_name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.peft_config[active_adapter].base_model_name_or_path = name\\n+            pass\\n+            # Add revision to enable future fast inference paths\\n+            model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n-        # Add revision to enable future fast inference paths\\n-        model.peft_config[\"default\"].revision = f\"unsloth\"\\n \\n         # Do patching\\n         n_mlp = 0\\n@@ -972,6 +990,13 @@ class FastLlamaModel:\\n         n_o   = 0\\n         import types\\n \\n+        active_adapter = model.active_adapters[0] if \\\\\\n+            hasattr(model, \"active_adapters\") else model.active_adapter\\n+\\n+        # Get dropout and bias\\n+        lora_dropout = model.peft_config[active_adapter].lora_dropout\\n+        bias         = model.peft_config[active_adapter].bias\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n',\n",
       " '@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):\\n         if is_peft:\\n             # Now add PEFT adapters\\n             model = PeftModel.from_pretrained(model, old_model_name)\\n+            # Patch it as well!\\n+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -221,6 +221,17 @@ def unsloth_save_model(\\n         save_pretrained_settings[\"save_directory\"] = new_save_directory\\n         save_directory = new_save_directory\\n     pass\\n+\\n+    # Tokenizer has different saving arguments\\n+    tokenizer_save_settings = \\\\\\n+    {\\n+        \"save_directory\"  : save_pretrained_settings[\"save_directory\"],\\n+        \"legacy_format\"   : None,\\n+        \"filename_prefix\" : None,\\n+        \"push_to_hub\"     : save_pretrained_settings[\"push_to_hub\"],\\n+        \"private\"         : save_pretrained_settings[\"private\"],\\n+        \"token\"           : save_pretrained_settings[\"token\"],\\n+    }\\n     \\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n@@ -240,7 +251,7 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            tokenizer.save_pretrained(**tokenizer_save_settings)\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -360,13 +371,34 @@ def unsloth_save_model(\\n \\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+\\n+    # Since merged, edit quantization_config\\n+    old_config = model.config\\n+    new_config = model.config.to_dict()\\n+    if \"quantization_config\" in new_config:\\n+        del new_config[\"quantization_config\"]\\n+    original_model = model\\n+    new_config = type(model.config).from_dict(new_config)\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = new_config\\n+    model.config = new_config\\n+\\n+    # Save!\\n     model.model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Revert config back\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = old_config\\n+    model.config = old_config\\n     print(\"Done.\")\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -446,7 +478,7 @@ def save_to_gguf(\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +488,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -491,11 +523,11 @@ def save_to_gguf(\\n \\n     if quantization_method != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization} {n_cpus}\"\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    # Check for username\\n+    if \"/\" not in save_directory:\\n+        from huggingface_hub import whoami\\n+        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n+        except: pass\\n+    pass\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    # Create model card\\n+    from huggingface_hub import ModelCard, ModelCardData\\n+    card_data = ModelCardData(\\n+        language = \"en\",\\n+        license  = \"apache-2.0\",\\n+        library  = \"unsloth\",\\n+        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    )\\n+\\n+    content = f\"\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n+    f\"\\\\n\"\\n+    \\n+    card = ModelCard(content)\\n+    card.push_to_hub(save_directory, token = token)\\n+\\n+    # Now upload file\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # And save to HF\\n-    if push_to_hub:\\n-        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-        from huggingface_hub import create_repo\\n-        create_repo(\\n-            repo_id   = save_directory,\\n-            token     = token,\\n-            repo_type = \"model\",\\n-            exist_ok  = True,\\n-        )\\n-\\n-        from huggingface_hub import HfApi\\n-        hf_api = HfApi(token = token)\\n-\\n-        if \"/\" in file_location:\\n-            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-        else:\\n-            uploaded_location = file_location\\n-        pass\\n-\\n-        hf_api.upload_file(\\n-            path_or_fileobj = file_location,\\n-            path_in_repo    = uploaded_location,\\n-            repo_id         = save_directory,\\n-            repo_type       = \"model\",\\n-        )\\n-    pass\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # Save to hub\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-    from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        private   = private,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n-\\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n-\\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n',\n",
       " '@@ -54,6 +54,11 @@ __all__ = [\\n ]\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(\\n             future Pytorch versions.\\n     \"\"\"\\n \\n-    # Freeze all parameters\\n-    for param in model.parameters():\\n-        param.requires_grad_(False)\\n+    # Freeze all parameters except LoRA\\n+    for name, param in model.named_parameters():\\n+        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+            param.requires_grad_(True)\\n+        else:\\n+            param.requires_grad_(False)\\n+    pass\\n \\n     if use_gradient_checkpointing:\\n         model.gradient_checkpointing_enable()\\n@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n             )\\n         pass\\n         self.loftq_init(adapter_name)\\n-        \\n+\\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -844,9 +844,11 @@ class FastLlamaModel:\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n         init_lora_weights   = True,\\n-        loftq_config        = None,\\n+        loftq_config        = {},\\n         **kwargs,\\n     ):\\n+        transformers_set_seed(random_state)\\n+\\n         if isinstance(model, PeftModelForCausalLM):\\n             raise TypeError(\\n                 \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n@@ -892,7 +894,7 @@ class FastLlamaModel:\\n                 )\\n             pass\\n \\n-            if loftq_config is None:\\n+            if loftq_config == {}:\\n                 from peft import LoftQConfig\\n                 logger.warning_once(\\n                     f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n@@ -922,8 +924,6 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n-        transformers_set_seed(random_state)\\n-\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n@@ -949,22 +949,40 @@ class FastLlamaModel:\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n         lora_config = LoraConfig(**arguments)\\n+        model = _get_peft_model(model, lora_config)\\n+\\n+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+        return model\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def patch_peft_model(\\n+        model,\\n+        use_gradient_checkpointing = True,\\n+    ):\\n+        if not isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model needs to call `.get_peft_model` first!\"\\n+            )\\n+        pass\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n             use_reentrant = True,\\n         )\\n-        model = _get_peft_model(model, lora_config)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.peft_config[\"default\"].base_model_name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        for active_adapter in model.peft_config.keys():\\n+            name = model.peft_config[active_adapter].base_model_name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.peft_config[active_adapter].base_model_name_or_path = name\\n+            pass\\n+            # Add revision to enable future fast inference paths\\n+            model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n-        # Add revision to enable future fast inference paths\\n-        model.peft_config[\"default\"].revision = f\"unsloth\"\\n \\n         # Do patching\\n         n_mlp = 0\\n@@ -972,6 +990,13 @@ class FastLlamaModel:\\n         n_o   = 0\\n         import types\\n \\n+        active_adapter = model.active_adapters[0] if \\\\\\n+            hasattr(model, \"active_adapters\") else model.active_adapter\\n+\\n+        # Get dropout and bias\\n+        lora_dropout = model.peft_config[active_adapter].lora_dropout\\n+        bias         = model.peft_config[active_adapter].bias\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n',\n",
       " '@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):\\n         if is_peft:\\n             # Now add PEFT adapters\\n             model = PeftModel.from_pretrained(model, old_model_name)\\n+            # Patch it as well!\\n+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -221,6 +221,17 @@ def unsloth_save_model(\\n         save_pretrained_settings[\"save_directory\"] = new_save_directory\\n         save_directory = new_save_directory\\n     pass\\n+\\n+    # Tokenizer has different saving arguments\\n+    tokenizer_save_settings = \\\\\\n+    {\\n+        \"save_directory\"  : save_pretrained_settings[\"save_directory\"],\\n+        \"legacy_format\"   : None,\\n+        \"filename_prefix\" : None,\\n+        \"push_to_hub\"     : save_pretrained_settings[\"push_to_hub\"],\\n+        \"private\"         : save_pretrained_settings[\"private\"],\\n+        \"token\"           : save_pretrained_settings[\"token\"],\\n+    }\\n     \\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n@@ -240,7 +251,7 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            tokenizer.save_pretrained(**tokenizer_save_settings)\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -360,13 +371,34 @@ def unsloth_save_model(\\n \\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+\\n+    # Since merged, edit quantization_config\\n+    old_config = model.config\\n+    new_config = model.config.to_dict()\\n+    if \"quantization_config\" in new_config:\\n+        del new_config[\"quantization_config\"]\\n+    original_model = model\\n+    new_config = type(model.config).from_dict(new_config)\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = new_config\\n+    model.config = new_config\\n+\\n+    # Save!\\n     model.model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Revert config back\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = old_config\\n+    model.config = old_config\\n     print(\"Done.\")\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -446,7 +478,7 @@ def save_to_gguf(\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +488,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -491,11 +523,11 @@ def save_to_gguf(\\n \\n     if quantization_method != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization} {n_cpus}\"\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    # Check for username\\n+    if \"/\" not in save_directory:\\n+        from huggingface_hub import whoami\\n+        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n+        except: pass\\n+    pass\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    # Create model card\\n+    from huggingface_hub import ModelCard, ModelCardData\\n+    card_data = ModelCardData(\\n+        language = \"en\",\\n+        license  = \"apache-2.0\",\\n+        library  = \"unsloth\",\\n+        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    )\\n+\\n+    content = f\"\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n+    f\"\\\\n\"\\n+    \\n+    card = ModelCard(content)\\n+    card.push_to_hub(save_directory, token = token)\\n+\\n+    # Now upload file\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # And save to HF\\n-    if push_to_hub:\\n-        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-        from huggingface_hub import create_repo\\n-        create_repo(\\n-            repo_id   = save_directory,\\n-            token     = token,\\n-            repo_type = \"model\",\\n-            exist_ok  = True,\\n-        )\\n-\\n-        from huggingface_hub import HfApi\\n-        hf_api = HfApi(token = token)\\n-\\n-        if \"/\" in file_location:\\n-            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-        else:\\n-            uploaded_location = file_location\\n-        pass\\n-\\n-        hf_api.upload_file(\\n-            path_or_fileobj = file_location,\\n-            path_in_repo    = uploaded_location,\\n-            repo_id         = save_directory,\\n-            repo_type       = \"model\",\\n-        )\\n-    pass\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # Save to hub\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-    from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        private   = private,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n-\\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n-\\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n',\n",
       " '@@ -54,6 +54,11 @@ __all__ = [\\n ]\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(\\n             future Pytorch versions.\\n     \"\"\"\\n \\n-    # Freeze all parameters\\n-    for param in model.parameters():\\n-        param.requires_grad_(False)\\n+    # Freeze all parameters except LoRA\\n+    for name, param in model.named_parameters():\\n+        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+            param.requires_grad_(True)\\n+        else:\\n+            param.requires_grad_(False)\\n+    pass\\n \\n     if use_gradient_checkpointing:\\n         model.gradient_checkpointing_enable()\\n@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n             )\\n         pass\\n         self.loftq_init(adapter_name)\\n-        \\n+\\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -844,9 +844,11 @@ class FastLlamaModel:\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n         init_lora_weights   = True,\\n-        loftq_config        = None,\\n+        loftq_config        = {},\\n         **kwargs,\\n     ):\\n+        transformers_set_seed(random_state)\\n+\\n         if isinstance(model, PeftModelForCausalLM):\\n             raise TypeError(\\n                 \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n@@ -892,7 +894,7 @@ class FastLlamaModel:\\n                 )\\n             pass\\n \\n-            if loftq_config is None:\\n+            if loftq_config == {}:\\n                 from peft import LoftQConfig\\n                 logger.warning_once(\\n                     f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n@@ -922,8 +924,6 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n-        transformers_set_seed(random_state)\\n-\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n@@ -949,22 +949,40 @@ class FastLlamaModel:\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n         lora_config = LoraConfig(**arguments)\\n+        model = _get_peft_model(model, lora_config)\\n+\\n+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+        return model\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def patch_peft_model(\\n+        model,\\n+        use_gradient_checkpointing = True,\\n+    ):\\n+        if not isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model needs to call `.get_peft_model` first!\"\\n+            )\\n+        pass\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n             use_reentrant = True,\\n         )\\n-        model = _get_peft_model(model, lora_config)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.peft_config[\"default\"].base_model_name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        for active_adapter in model.peft_config.keys():\\n+            name = model.peft_config[active_adapter].base_model_name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.peft_config[active_adapter].base_model_name_or_path = name\\n+            pass\\n+            # Add revision to enable future fast inference paths\\n+            model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n-        # Add revision to enable future fast inference paths\\n-        model.peft_config[\"default\"].revision = f\"unsloth\"\\n \\n         # Do patching\\n         n_mlp = 0\\n@@ -972,6 +990,13 @@ class FastLlamaModel:\\n         n_o   = 0\\n         import types\\n \\n+        active_adapter = model.active_adapters[0] if \\\\\\n+            hasattr(model, \"active_adapters\") else model.active_adapter\\n+\\n+        # Get dropout and bias\\n+        lora_dropout = model.peft_config[active_adapter].lora_dropout\\n+        bias         = model.peft_config[active_adapter].bias\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n',\n",
       " '@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):\\n         if is_peft:\\n             # Now add PEFT adapters\\n             model = PeftModel.from_pretrained(model, old_model_name)\\n+            # Patch it as well!\\n+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -221,6 +221,17 @@ def unsloth_save_model(\\n         save_pretrained_settings[\"save_directory\"] = new_save_directory\\n         save_directory = new_save_directory\\n     pass\\n+\\n+    # Tokenizer has different saving arguments\\n+    tokenizer_save_settings = \\\\\\n+    {\\n+        \"save_directory\"  : save_pretrained_settings[\"save_directory\"],\\n+        \"legacy_format\"   : None,\\n+        \"filename_prefix\" : None,\\n+        \"push_to_hub\"     : save_pretrained_settings[\"push_to_hub\"],\\n+        \"private\"         : save_pretrained_settings[\"private\"],\\n+        \"token\"           : save_pretrained_settings[\"token\"],\\n+    }\\n     \\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n@@ -240,7 +251,7 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            tokenizer.save_pretrained(**tokenizer_save_settings)\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -360,13 +371,34 @@ def unsloth_save_model(\\n \\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+\\n+    # Since merged, edit quantization_config\\n+    old_config = model.config\\n+    new_config = model.config.to_dict()\\n+    if \"quantization_config\" in new_config:\\n+        del new_config[\"quantization_config\"]\\n+    original_model = model\\n+    new_config = type(model.config).from_dict(new_config)\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = new_config\\n+    model.config = new_config\\n+\\n+    # Save!\\n     model.model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Revert config back\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = old_config\\n+    model.config = old_config\\n     print(\"Done.\")\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -446,7 +478,7 @@ def save_to_gguf(\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +488,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -491,11 +523,11 @@ def save_to_gguf(\\n \\n     if quantization_method != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization} {n_cpus}\"\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    # Check for username\\n+    if \"/\" not in save_directory:\\n+        from huggingface_hub import whoami\\n+        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n+        except: pass\\n+    pass\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    # Create model card\\n+    from huggingface_hub import ModelCard, ModelCardData\\n+    card_data = ModelCardData(\\n+        language = \"en\",\\n+        license  = \"apache-2.0\",\\n+        library  = \"unsloth\",\\n+        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    )\\n+\\n+    content = f\"\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n+    f\"\\\\n\"\\n+    \\n+    card = ModelCard(content)\\n+    card.push_to_hub(save_directory, token = token)\\n+\\n+    # Now upload file\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # And save to HF\\n-    if push_to_hub:\\n-        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-        from huggingface_hub import create_repo\\n-        create_repo(\\n-            repo_id   = save_directory,\\n-            token     = token,\\n-            repo_type = \"model\",\\n-            exist_ok  = True,\\n-        )\\n-\\n-        from huggingface_hub import HfApi\\n-        hf_api = HfApi(token = token)\\n-\\n-        if \"/\" in file_location:\\n-            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-        else:\\n-            uploaded_location = file_location\\n-        pass\\n-\\n-        hf_api.upload_file(\\n-            path_or_fileobj = file_location,\\n-            path_in_repo    = uploaded_location,\\n-            repo_id         = save_directory,\\n-            repo_type       = \"model\",\\n-        )\\n-    pass\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # Save to hub\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-    from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        private   = private,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n-\\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n-\\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n',\n",
       " '@@ -54,6 +54,11 @@ __all__ = [\\n ]\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(\\n             future Pytorch versions.\\n     \"\"\"\\n \\n-    # Freeze all parameters\\n-    for param in model.parameters():\\n-        param.requires_grad_(False)\\n+    # Freeze all parameters except LoRA\\n+    for name, param in model.named_parameters():\\n+        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+            param.requires_grad_(True)\\n+        else:\\n+            param.requires_grad_(False)\\n+    pass\\n \\n     if use_gradient_checkpointing:\\n         model.gradient_checkpointing_enable()\\n@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n             )\\n         pass\\n         self.loftq_init(adapter_name)\\n-        \\n+\\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -844,9 +844,11 @@ class FastLlamaModel:\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n         init_lora_weights   = True,\\n-        loftq_config        = None,\\n+        loftq_config        = {},\\n         **kwargs,\\n     ):\\n+        transformers_set_seed(random_state)\\n+\\n         if isinstance(model, PeftModelForCausalLM):\\n             raise TypeError(\\n                 \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n@@ -892,7 +894,7 @@ class FastLlamaModel:\\n                 )\\n             pass\\n \\n-            if loftq_config is None:\\n+            if loftq_config == {}:\\n                 from peft import LoftQConfig\\n                 logger.warning_once(\\n                     f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n@@ -922,8 +924,6 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n-        transformers_set_seed(random_state)\\n-\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n@@ -949,22 +949,40 @@ class FastLlamaModel:\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n         lora_config = LoraConfig(**arguments)\\n+        model = _get_peft_model(model, lora_config)\\n+\\n+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+        return model\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def patch_peft_model(\\n+        model,\\n+        use_gradient_checkpointing = True,\\n+    ):\\n+        if not isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model needs to call `.get_peft_model` first!\"\\n+            )\\n+        pass\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n             use_reentrant = True,\\n         )\\n-        model = _get_peft_model(model, lora_config)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.peft_config[\"default\"].base_model_name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        for active_adapter in model.peft_config.keys():\\n+            name = model.peft_config[active_adapter].base_model_name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.peft_config[active_adapter].base_model_name_or_path = name\\n+            pass\\n+            # Add revision to enable future fast inference paths\\n+            model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n-        # Add revision to enable future fast inference paths\\n-        model.peft_config[\"default\"].revision = f\"unsloth\"\\n \\n         # Do patching\\n         n_mlp = 0\\n@@ -972,6 +990,13 @@ class FastLlamaModel:\\n         n_o   = 0\\n         import types\\n \\n+        active_adapter = model.active_adapters[0] if \\\\\\n+            hasattr(model, \"active_adapters\") else model.active_adapter\\n+\\n+        # Get dropout and bias\\n+        lora_dropout = model.peft_config[active_adapter].lora_dropout\\n+        bias         = model.peft_config[active_adapter].bias\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n',\n",
       " '@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):\\n         if is_peft:\\n             # Now add PEFT adapters\\n             model = PeftModel.from_pretrained(model, old_model_name)\\n+            # Patch it as well!\\n+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -221,6 +221,17 @@ def unsloth_save_model(\\n         save_pretrained_settings[\"save_directory\"] = new_save_directory\\n         save_directory = new_save_directory\\n     pass\\n+\\n+    # Tokenizer has different saving arguments\\n+    tokenizer_save_settings = \\\\\\n+    {\\n+        \"save_directory\"  : save_pretrained_settings[\"save_directory\"],\\n+        \"legacy_format\"   : None,\\n+        \"filename_prefix\" : None,\\n+        \"push_to_hub\"     : save_pretrained_settings[\"push_to_hub\"],\\n+        \"private\"         : save_pretrained_settings[\"private\"],\\n+        \"token\"           : save_pretrained_settings[\"token\"],\\n+    }\\n     \\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n@@ -240,7 +251,7 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            tokenizer.save_pretrained(**tokenizer_save_settings)\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -360,13 +371,34 @@ def unsloth_save_model(\\n \\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+\\n+    # Since merged, edit quantization_config\\n+    old_config = model.config\\n+    new_config = model.config.to_dict()\\n+    if \"quantization_config\" in new_config:\\n+        del new_config[\"quantization_config\"]\\n+    original_model = model\\n+    new_config = type(model.config).from_dict(new_config)\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = new_config\\n+    model.config = new_config\\n+\\n+    # Save!\\n     model.model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Revert config back\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = old_config\\n+    model.config = old_config\\n     print(\"Done.\")\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -446,7 +478,7 @@ def save_to_gguf(\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +488,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -491,11 +523,11 @@ def save_to_gguf(\\n \\n     if quantization_method != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization} {n_cpus}\"\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    # Check for username\\n+    if \"/\" not in save_directory:\\n+        from huggingface_hub import whoami\\n+        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n+        except: pass\\n+    pass\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    # Create model card\\n+    from huggingface_hub import ModelCard, ModelCardData\\n+    card_data = ModelCardData(\\n+        language = \"en\",\\n+        license  = \"apache-2.0\",\\n+        library  = \"unsloth\",\\n+        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    )\\n+\\n+    content = f\"\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n+    f\"\\\\n\"\\n+    \\n+    card = ModelCard(content)\\n+    card.push_to_hub(save_directory, token = token)\\n+\\n+    # Now upload file\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # And save to HF\\n-    if push_to_hub:\\n-        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-        from huggingface_hub import create_repo\\n-        create_repo(\\n-            repo_id   = save_directory,\\n-            token     = token,\\n-            repo_type = \"model\",\\n-            exist_ok  = True,\\n-        )\\n-\\n-        from huggingface_hub import HfApi\\n-        hf_api = HfApi(token = token)\\n-\\n-        if \"/\" in file_location:\\n-            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-        else:\\n-            uploaded_location = file_location\\n-        pass\\n-\\n-        hf_api.upload_file(\\n-            path_or_fileobj = file_location,\\n-            path_in_repo    = uploaded_location,\\n-            repo_id         = save_directory,\\n-            repo_type       = \"model\",\\n-        )\\n-    pass\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # Save to hub\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-    from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        private   = private,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n-\\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n-\\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n',\n",
       " '@@ -54,6 +54,11 @@ __all__ = [\\n ]\\n \\n \\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -74,9 +79,13 @@ def prepare_model_for_kbit_training(\\n             future Pytorch versions.\\n     \"\"\"\\n \\n-    # Freeze all parameters\\n-    for param in model.parameters():\\n-        param.requires_grad_(False)\\n+    # Freeze all parameters except LoRA\\n+    for name, param in model.named_parameters():\\n+        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+            param.requires_grad_(True)\\n+        else:\\n+            param.requires_grad_(False)\\n+    pass\\n \\n     if use_gradient_checkpointing:\\n         model.gradient_checkpointing_enable()\\n@@ -115,11 +124,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def check_tokenizer(\\n     model,\\n     tokenizer,\\n@@ -252,7 +256,7 @@ def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init\\n             )\\n         pass\\n         self.loftq_init(adapter_name)\\n-        \\n+\\n     elif init_lora_weights:\\n         self.reset_lora_parameters(adapter_name, init_lora_weights)\\n \\n',\n",
       " '@@ -844,9 +844,11 @@ class FastLlamaModel:\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n         init_lora_weights   = True,\\n-        loftq_config        = None,\\n+        loftq_config        = {},\\n         **kwargs,\\n     ):\\n+        transformers_set_seed(random_state)\\n+\\n         if isinstance(model, PeftModelForCausalLM):\\n             raise TypeError(\\n                 \"Unsloth: Your model already has LoRA adapters. No need to run this again!\"\\n@@ -892,7 +894,7 @@ class FastLlamaModel:\\n                 )\\n             pass\\n \\n-            if loftq_config is None:\\n+            if loftq_config == {}:\\n                 from peft import LoftQConfig\\n                 logger.warning_once(\\n                     f\"Unsloth: init_lora_weights = `loftq` is set, but `loftq_config` is None.\\\\n\"\\\\\\n@@ -922,8 +924,6 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n-        transformers_set_seed(random_state)\\n-\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n@@ -949,22 +949,40 @@ class FastLlamaModel:\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n         lora_config = LoraConfig(**arguments)\\n+        model = _get_peft_model(model, lora_config)\\n+\\n+        model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+        return model\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def patch_peft_model(\\n+        model,\\n+        use_gradient_checkpointing = True,\\n+    ):\\n+        if not isinstance(model, PeftModelForCausalLM):\\n+            raise TypeError(\\n+                \"Unsloth: Your model needs to call `.get_peft_model` first!\"\\n+            )\\n+        pass\\n \\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n             use_reentrant = True,\\n         )\\n-        model = _get_peft_model(model, lora_config)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.peft_config[\"default\"].base_model_name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.peft_config[\"default\"].base_model_name_or_path = name\\n+        for active_adapter in model.peft_config.keys():\\n+            name = model.peft_config[active_adapter].base_model_name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.peft_config[active_adapter].base_model_name_or_path = name\\n+            pass\\n+            # Add revision to enable future fast inference paths\\n+            model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n-        # Add revision to enable future fast inference paths\\n-        model.peft_config[\"default\"].revision = f\"unsloth\"\\n \\n         # Do patching\\n         n_mlp = 0\\n@@ -972,6 +990,13 @@ class FastLlamaModel:\\n         n_o   = 0\\n         import types\\n \\n+        active_adapter = model.active_adapters[0] if \\\\\\n+            hasattr(model, \"active_adapters\") else model.active_adapter\\n+\\n+        # Get dropout and bias\\n+        lora_dropout = model.peft_config[active_adapter].lora_dropout\\n+        bias         = model.peft_config[active_adapter].bias\\n+\\n         if lora_dropout == 0 and bias == \"none\":\\n             for idx, layer in enumerate(model.model.model.layers):\\n \\n',\n",
       " '@@ -71,6 +71,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n         old_model_name = model_name\\n@@ -139,6 +140,8 @@ class FastLanguageModel(FastLlamaModel):\\n         if is_peft:\\n             # Now add PEFT adapters\\n             model = PeftModel.from_pretrained(model, old_model_name)\\n+            # Patch it as well!\\n+            model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -221,6 +221,17 @@ def unsloth_save_model(\\n         save_pretrained_settings[\"save_directory\"] = new_save_directory\\n         save_directory = new_save_directory\\n     pass\\n+\\n+    # Tokenizer has different saving arguments\\n+    tokenizer_save_settings = \\\\\\n+    {\\n+        \"save_directory\"  : save_pretrained_settings[\"save_directory\"],\\n+        \"legacy_format\"   : None,\\n+        \"filename_prefix\" : None,\\n+        \"push_to_hub\"     : save_pretrained_settings[\"push_to_hub\"],\\n+        \"private\"         : save_pretrained_settings[\"private\"],\\n+        \"token\"           : save_pretrained_settings[\"token\"],\\n+    }\\n     \\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n@@ -240,7 +251,7 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-            tokenizer.save_pretrained(**save_pretrained_settings)\\n+            tokenizer.save_pretrained(**tokenizer_save_settings)\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -360,13 +371,34 @@ def unsloth_save_model(\\n \\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n-        tokenizer.save_pretrained(**save_pretrained_settings)\\n+        tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n+\\n+    # Since merged, edit quantization_config\\n+    old_config = model.config\\n+    new_config = model.config.to_dict()\\n+    if \"quantization_config\" in new_config:\\n+        del new_config[\"quantization_config\"]\\n+    original_model = model\\n+    new_config = type(model.config).from_dict(new_config)\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = new_config\\n+    model.config = new_config\\n+\\n+    # Save!\\n     model.model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Revert config back\\n+    original_model = model\\n+    while hasattr(original_model, \"model\"):\\n+        original_model = original_model.model\\n+        original_model.config = old_config\\n+    model.config = old_config\\n     print(\"Done.\")\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -446,7 +478,7 @@ def save_to_gguf(\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n-        error = f\"Unsloth: Quant method = [{quantization}] not supported. Choose from below:\\\\n\"\\n+        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n         for key, value in ALLOWED_QUANTS.items():\\n             error += f\"[{key}] => {value}\\\\n\"\\n         raise RuntimeError(error)\\n@@ -456,7 +488,7 @@ def save_to_gguf(\\n         f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\\\n\"\\\\\\n         f\"   \\\\\\\\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\\\n\"\\\\\\n         f\"O^O/ \\\\_/ \\\\\\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\\\n\"\\\\\\n-        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization} will take 20 minutes.\\\\n\"\\\\\\n+        f\"\\\\        /    [2] Converting GGUF 16bits to {quantization_method} will take 20 minutes.\\\\n\"\\\\\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n@@ -491,11 +523,11 @@ def save_to_gguf(\\n \\n     if quantization_method != first_conversion:\\n         old_location = final_location\\n-        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization}. This will take 20 minutes...\")\\n-        final_location = f\"./{model_directory}-unsloth.{quantization.upper()}.gguf\"\\n+        print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n+        final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n         command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n-            f\"{final_location} {quantization} {n_cpus}\"\\n+            f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -597,6 +629,65 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n+def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+\\n+    # Check for username\\n+    if \"/\" not in save_directory:\\n+        from huggingface_hub import whoami\\n+        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n+        except: pass\\n+    pass\\n+\\n+    from huggingface_hub import create_repo\\n+    create_repo(\\n+        repo_id   = save_directory,\\n+        token     = token,\\n+        repo_type = \"model\",\\n+        exist_ok  = True,\\n+    )\\n+\\n+    # Create model card\\n+    from huggingface_hub import ModelCard, ModelCardData\\n+    card_data = ModelCardData(\\n+        language = \"en\",\\n+        license  = \"apache-2.0\",\\n+        library  = \"unsloth\",\\n+        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    )\\n+\\n+    content = f\"\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n+    f\"---\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n+    f\"\\\\n\"\\\\\\n+    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n+    f\"\\\\n\"\\n+    \\n+    card = ModelCard(content)\\n+    card.push_to_hub(save_directory, token = token)\\n+\\n+    # Now upload file\\n+    from huggingface_hub import HfApi\\n+    hf_api = HfApi(token = token)\\n+\\n+    if \"/\" in file_location:\\n+        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+    else:\\n+        uploaded_location = file_location\\n+    pass\\n+\\n+    hf_api.upload_file(\\n+        path_or_fileobj = file_location,\\n+        path_in_repo    = uploaded_location,\\n+        repo_id         = save_directory,\\n+        repo_type       = \"model\",\\n+    )\\n+pass\\n+\\n+\\n def unsloth_save_pretrained_gguf(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n@@ -619,7 +710,7 @@ def unsloth_save_pretrained_gguf(\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -662,36 +753,9 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # And save to HF\\n-    if push_to_hub:\\n-        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-        from huggingface_hub import create_repo\\n-        create_repo(\\n-            repo_id   = save_directory,\\n-            token     = token,\\n-            repo_type = \"model\",\\n-            exist_ok  = True,\\n-        )\\n-\\n-        from huggingface_hub import HfApi\\n-        hf_api = HfApi(token = token)\\n-\\n-        if \"/\" in file_location:\\n-            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-        else:\\n-            uploaded_location = file_location\\n-        pass\\n-\\n-        hf_api.upload_file(\\n-            path_or_fileobj = file_location,\\n-            path_in_repo    = uploaded_location,\\n-            repo_id         = save_directory,\\n-            repo_type       = \"model\",\\n-        )\\n-    pass\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n@@ -717,7 +781,7 @@ def unsloth_push_to_hub_gguf(\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n         converted to float16 then converted to GGUF / llama.cpp format.\\n \\n-        Choose for `quantization` to be:\\n+        Choose for `quantization_method` to be:\\n         \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\\n         \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\\n         \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\\n@@ -762,35 +826,9 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     python_install.wait()\\n-    file_location = save_to_gguf(new_save_directory, quantization, makefile)\\n-\\n-    # Save to hub\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n-\\n-    from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        private   = private,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n-\\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n-\\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    model_type = self.config.model_type\\n+    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n pass\\n \\n \\n',\n",
       " '@@ -208,6 +208,15 @@ def unsloth_save_model(\\n         return save_directory\\n     pass\\n \\n+    # Update model tag\\n+    username = \"\"\\n+    if push_to_hub:\\n+        username = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+        )\\n+    pass\\n+\\n     # If push_to_hub, we must remove the .../ part of a repo\\n     if push_to_hub and \"/\" in save_directory:\\n \\n@@ -331,6 +340,7 @@ def unsloth_save_model(\\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n                 state_dict[name] = W\\n+            # [TODO] Saving to RAM seems to leak memory???\\n             # elif (max_ram - W.nbytes) > 0:\\n             #     # Save to CPU memory\\n             #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n@@ -401,27 +411,32 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n+    # Print location\\n+    if push_to_hub:\\n+        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n+\\n     save_pretrained_settings[\"state_dict\"] = None\\n \\n-    # for j, (key, value) in enumerate(state_dict.items()):\\n-    #     state_dict[key] = None\\n-    #     if j % 10 == 0:\\n-    #         torch.cuda.empty_cache()\\n-    #         gc.collect()\\n-    #     pass\\n-    # pass\\n-    # state_dict = None\\n-    # del state_dict\\n-    # torch.cuda.empty_cache()\\n-    # gc.collect()\\n+    for j, (key, value) in enumerate(state_dict.items()):\\n+        state_dict[key] = None\\n+        if j % 10 == 0:\\n+            torch.cuda.empty_cache()\\n+            gc.collect()\\n+        pass\\n+    pass\\n+    state_dict = None\\n+    del state_dict\\n+    torch.cuda.empty_cache()\\n+    gc.collect()\\n \\n     # Remove temporary location\\n     import shutil\\n     shutil.rmtree(temporary_location)\\n \\n-    # for _ in range(3):\\n-    #     torch.cuda.empty_cache()\\n-    #     gc.collect()\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n     return save_directory\\n pass\\n \\n@@ -629,14 +644,44 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n-def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+MODEL_CARD = \\\\\\n+\"\"\"---\\n+base_model: {base_model}\\n+tags:\\n+- text-generation-inference\\n+- transformers\\n+- unsloth\\n+- {model_type}\\n+- {extra}\\n+license: apache-2.0\\n+language:\\n+- en\\n+---\\n+\\n+# Uploaded {method} model\\n+\\n+- **Developed by:** {username}\\n+- **License:** apache-2.0\\n+- **Finetuned from model :** {base_model}\\n+\\n+This {model_type} model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n+\\n+[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n+\"\"\"\\n+\\n \\n+def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n     # Check for username\\n+    username = \"\"\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n-        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n-        except: pass\\n+        try: \\n+            username = whoami()[\\'name\\']\\n+            save_directory = f\"{save_directory}/{username}\"\\n+        except:\\n+            raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n+    else:\\n+        username = save_directory.split(\"/\")[0]\\n     pass\\n \\n     from huggingface_hub import create_repo\\n@@ -648,43 +693,36 @@ def upload_gguf_to_huggingface(save_directory, file_location, token, model_type)\\n     )\\n \\n     # Create model card\\n-    from huggingface_hub import ModelCard, ModelCardData\\n-    card_data = ModelCardData(\\n-        language = \"en\",\\n-        license  = \"apache-2.0\",\\n-        library  = \"unsloth\",\\n-        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    from huggingface_hub import ModelCard\\n+    content = MODEL_CARD.format(\\n+        username   = username,\\n+        base_model = model.config._name_or_path,\\n+        model_type = model.config.model_type,\\n+        method     = \"\",\\n+        extra      = extra,\\n     )\\n-\\n-    content = f\"\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n-    f\"\\\\n\"\\n-    \\n     card = ModelCard(content)\\n     card.push_to_hub(save_directory, token = token)\\n \\n-    # Now upload file\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n+    if file_location is not None:\\n+        # Now upload file\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n \\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n \\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n+    pass\\n+    return username\\n pass\\n \\n \\n@@ -754,8 +792,15 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+        username = upload_to_huggingface(\\n+            self, model, new_save_directory, token,\\n+            \"GGUF converted\", \"gguf\", file_location,\\n+        )\\n+        print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n pass\\n \\n \\n@@ -827,8 +872,13 @@ def unsloth_push_to_hub_gguf(\\n \\n     python_install.wait()\\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, model, new_save_directory, token,\\n+        \"GGUF converted\", \"gguf\", file_location,\\n+    )\\n+    print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n pass\\n \\n \\n',\n",
       " '@@ -208,6 +208,15 @@ def unsloth_save_model(\\n         return save_directory\\n     pass\\n \\n+    # Update model tag\\n+    username = \"\"\\n+    if push_to_hub:\\n+        username = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+        )\\n+    pass\\n+\\n     # If push_to_hub, we must remove the .../ part of a repo\\n     if push_to_hub and \"/\" in save_directory:\\n \\n@@ -331,6 +340,7 @@ def unsloth_save_model(\\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n                 state_dict[name] = W\\n+            # [TODO] Saving to RAM seems to leak memory???\\n             # elif (max_ram - W.nbytes) > 0:\\n             #     # Save to CPU memory\\n             #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n@@ -401,27 +411,32 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n+    # Print location\\n+    if push_to_hub:\\n+        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n+\\n     save_pretrained_settings[\"state_dict\"] = None\\n \\n-    # for j, (key, value) in enumerate(state_dict.items()):\\n-    #     state_dict[key] = None\\n-    #     if j % 10 == 0:\\n-    #         torch.cuda.empty_cache()\\n-    #         gc.collect()\\n-    #     pass\\n-    # pass\\n-    # state_dict = None\\n-    # del state_dict\\n-    # torch.cuda.empty_cache()\\n-    # gc.collect()\\n+    for j, (key, value) in enumerate(state_dict.items()):\\n+        state_dict[key] = None\\n+        if j % 10 == 0:\\n+            torch.cuda.empty_cache()\\n+            gc.collect()\\n+        pass\\n+    pass\\n+    state_dict = None\\n+    del state_dict\\n+    torch.cuda.empty_cache()\\n+    gc.collect()\\n \\n     # Remove temporary location\\n     import shutil\\n     shutil.rmtree(temporary_location)\\n \\n-    # for _ in range(3):\\n-    #     torch.cuda.empty_cache()\\n-    #     gc.collect()\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n     return save_directory\\n pass\\n \\n@@ -629,14 +644,44 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n-def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+MODEL_CARD = \\\\\\n+\"\"\"---\\n+base_model: {base_model}\\n+tags:\\n+- text-generation-inference\\n+- transformers\\n+- unsloth\\n+- {model_type}\\n+- {extra}\\n+license: apache-2.0\\n+language:\\n+- en\\n+---\\n+\\n+# Uploaded {method} model\\n+\\n+- **Developed by:** {username}\\n+- **License:** apache-2.0\\n+- **Finetuned from model :** {base_model}\\n+\\n+This {model_type} model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n+\\n+[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n+\"\"\"\\n+\\n \\n+def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n     # Check for username\\n+    username = \"\"\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n-        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n-        except: pass\\n+        try: \\n+            username = whoami()[\\'name\\']\\n+            save_directory = f\"{save_directory}/{username}\"\\n+        except:\\n+            raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n+    else:\\n+        username = save_directory.split(\"/\")[0]\\n     pass\\n \\n     from huggingface_hub import create_repo\\n@@ -648,43 +693,36 @@ def upload_gguf_to_huggingface(save_directory, file_location, token, model_type)\\n     )\\n \\n     # Create model card\\n-    from huggingface_hub import ModelCard, ModelCardData\\n-    card_data = ModelCardData(\\n-        language = \"en\",\\n-        license  = \"apache-2.0\",\\n-        library  = \"unsloth\",\\n-        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    from huggingface_hub import ModelCard\\n+    content = MODEL_CARD.format(\\n+        username   = username,\\n+        base_model = model.config._name_or_path,\\n+        model_type = model.config.model_type,\\n+        method     = \"\",\\n+        extra      = extra,\\n     )\\n-\\n-    content = f\"\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n-    f\"\\\\n\"\\n-    \\n     card = ModelCard(content)\\n     card.push_to_hub(save_directory, token = token)\\n \\n-    # Now upload file\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n+    if file_location is not None:\\n+        # Now upload file\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n \\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n \\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n+    pass\\n+    return username\\n pass\\n \\n \\n@@ -754,8 +792,15 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+        username = upload_to_huggingface(\\n+            self, model, new_save_directory, token,\\n+            \"GGUF converted\", \"gguf\", file_location,\\n+        )\\n+        print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n pass\\n \\n \\n@@ -827,8 +872,13 @@ def unsloth_push_to_hub_gguf(\\n \\n     python_install.wait()\\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, model, new_save_directory, token,\\n+        \"GGUF converted\", \"gguf\", file_location,\\n+    )\\n+    print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n pass\\n \\n \\n',\n",
       " '@@ -208,6 +208,15 @@ def unsloth_save_model(\\n         return save_directory\\n     pass\\n \\n+    # Update model tag\\n+    username = \"\"\\n+    if push_to_hub:\\n+        username = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+        )\\n+    pass\\n+\\n     # If push_to_hub, we must remove the .../ part of a repo\\n     if push_to_hub and \"/\" in save_directory:\\n \\n@@ -331,6 +340,7 @@ def unsloth_save_model(\\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n                 state_dict[name] = W\\n+            # [TODO] Saving to RAM seems to leak memory???\\n             # elif (max_ram - W.nbytes) > 0:\\n             #     # Save to CPU memory\\n             #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n@@ -401,27 +411,32 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n+    # Print location\\n+    if push_to_hub:\\n+        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n+\\n     save_pretrained_settings[\"state_dict\"] = None\\n \\n-    # for j, (key, value) in enumerate(state_dict.items()):\\n-    #     state_dict[key] = None\\n-    #     if j % 10 == 0:\\n-    #         torch.cuda.empty_cache()\\n-    #         gc.collect()\\n-    #     pass\\n-    # pass\\n-    # state_dict = None\\n-    # del state_dict\\n-    # torch.cuda.empty_cache()\\n-    # gc.collect()\\n+    for j, (key, value) in enumerate(state_dict.items()):\\n+        state_dict[key] = None\\n+        if j % 10 == 0:\\n+            torch.cuda.empty_cache()\\n+            gc.collect()\\n+        pass\\n+    pass\\n+    state_dict = None\\n+    del state_dict\\n+    torch.cuda.empty_cache()\\n+    gc.collect()\\n \\n     # Remove temporary location\\n     import shutil\\n     shutil.rmtree(temporary_location)\\n \\n-    # for _ in range(3):\\n-    #     torch.cuda.empty_cache()\\n-    #     gc.collect()\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n     return save_directory\\n pass\\n \\n@@ -629,14 +644,44 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n-def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+MODEL_CARD = \\\\\\n+\"\"\"---\\n+base_model: {base_model}\\n+tags:\\n+- text-generation-inference\\n+- transformers\\n+- unsloth\\n+- {model_type}\\n+- {extra}\\n+license: apache-2.0\\n+language:\\n+- en\\n+---\\n+\\n+# Uploaded {method} model\\n+\\n+- **Developed by:** {username}\\n+- **License:** apache-2.0\\n+- **Finetuned from model :** {base_model}\\n+\\n+This {model_type} model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n+\\n+[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n+\"\"\"\\n+\\n \\n+def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n     # Check for username\\n+    username = \"\"\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n-        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n-        except: pass\\n+        try: \\n+            username = whoami()[\\'name\\']\\n+            save_directory = f\"{save_directory}/{username}\"\\n+        except:\\n+            raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n+    else:\\n+        username = save_directory.split(\"/\")[0]\\n     pass\\n \\n     from huggingface_hub import create_repo\\n@@ -648,43 +693,36 @@ def upload_gguf_to_huggingface(save_directory, file_location, token, model_type)\\n     )\\n \\n     # Create model card\\n-    from huggingface_hub import ModelCard, ModelCardData\\n-    card_data = ModelCardData(\\n-        language = \"en\",\\n-        license  = \"apache-2.0\",\\n-        library  = \"unsloth\",\\n-        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    from huggingface_hub import ModelCard\\n+    content = MODEL_CARD.format(\\n+        username   = username,\\n+        base_model = model.config._name_or_path,\\n+        model_type = model.config.model_type,\\n+        method     = \"\",\\n+        extra      = extra,\\n     )\\n-\\n-    content = f\"\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n-    f\"\\\\n\"\\n-    \\n     card = ModelCard(content)\\n     card.push_to_hub(save_directory, token = token)\\n \\n-    # Now upload file\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n+    if file_location is not None:\\n+        # Now upload file\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n \\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n \\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n+    pass\\n+    return username\\n pass\\n \\n \\n@@ -754,8 +792,15 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+        username = upload_to_huggingface(\\n+            self, model, new_save_directory, token,\\n+            \"GGUF converted\", \"gguf\", file_location,\\n+        )\\n+        print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n pass\\n \\n \\n@@ -827,8 +872,13 @@ def unsloth_push_to_hub_gguf(\\n \\n     python_install.wait()\\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, model, new_save_directory, token,\\n+        \"GGUF converted\", \"gguf\", file_location,\\n+    )\\n+    print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n pass\\n \\n \\n',\n",
       " '@@ -208,6 +208,15 @@ def unsloth_save_model(\\n         return save_directory\\n     pass\\n \\n+    # Update model tag\\n+    username = \"\"\\n+    if push_to_hub:\\n+        username = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+        )\\n+    pass\\n+\\n     # If push_to_hub, we must remove the .../ part of a repo\\n     if push_to_hub and \"/\" in save_directory:\\n \\n@@ -331,6 +340,7 @@ def unsloth_save_model(\\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n                 state_dict[name] = W\\n+            # [TODO] Saving to RAM seems to leak memory???\\n             # elif (max_ram - W.nbytes) > 0:\\n             #     # Save to CPU memory\\n             #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n@@ -401,27 +411,32 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n+    # Print location\\n+    if push_to_hub:\\n+        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n+\\n     save_pretrained_settings[\"state_dict\"] = None\\n \\n-    # for j, (key, value) in enumerate(state_dict.items()):\\n-    #     state_dict[key] = None\\n-    #     if j % 10 == 0:\\n-    #         torch.cuda.empty_cache()\\n-    #         gc.collect()\\n-    #     pass\\n-    # pass\\n-    # state_dict = None\\n-    # del state_dict\\n-    # torch.cuda.empty_cache()\\n-    # gc.collect()\\n+    for j, (key, value) in enumerate(state_dict.items()):\\n+        state_dict[key] = None\\n+        if j % 10 == 0:\\n+            torch.cuda.empty_cache()\\n+            gc.collect()\\n+        pass\\n+    pass\\n+    state_dict = None\\n+    del state_dict\\n+    torch.cuda.empty_cache()\\n+    gc.collect()\\n \\n     # Remove temporary location\\n     import shutil\\n     shutil.rmtree(temporary_location)\\n \\n-    # for _ in range(3):\\n-    #     torch.cuda.empty_cache()\\n-    #     gc.collect()\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n     return save_directory\\n pass\\n \\n@@ -629,14 +644,44 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n-def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+MODEL_CARD = \\\\\\n+\"\"\"---\\n+base_model: {base_model}\\n+tags:\\n+- text-generation-inference\\n+- transformers\\n+- unsloth\\n+- {model_type}\\n+- {extra}\\n+license: apache-2.0\\n+language:\\n+- en\\n+---\\n+\\n+# Uploaded {method} model\\n+\\n+- **Developed by:** {username}\\n+- **License:** apache-2.0\\n+- **Finetuned from model :** {base_model}\\n+\\n+This {model_type} model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n+\\n+[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n+\"\"\"\\n+\\n \\n+def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n     # Check for username\\n+    username = \"\"\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n-        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n-        except: pass\\n+        try: \\n+            username = whoami()[\\'name\\']\\n+            save_directory = f\"{save_directory}/{username}\"\\n+        except:\\n+            raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n+    else:\\n+        username = save_directory.split(\"/\")[0]\\n     pass\\n \\n     from huggingface_hub import create_repo\\n@@ -648,43 +693,36 @@ def upload_gguf_to_huggingface(save_directory, file_location, token, model_type)\\n     )\\n \\n     # Create model card\\n-    from huggingface_hub import ModelCard, ModelCardData\\n-    card_data = ModelCardData(\\n-        language = \"en\",\\n-        license  = \"apache-2.0\",\\n-        library  = \"unsloth\",\\n-        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    from huggingface_hub import ModelCard\\n+    content = MODEL_CARD.format(\\n+        username   = username,\\n+        base_model = model.config._name_or_path,\\n+        model_type = model.config.model_type,\\n+        method     = \"\",\\n+        extra      = extra,\\n     )\\n-\\n-    content = f\"\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n-    f\"\\\\n\"\\n-    \\n     card = ModelCard(content)\\n     card.push_to_hub(save_directory, token = token)\\n \\n-    # Now upload file\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n+    if file_location is not None:\\n+        # Now upload file\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n \\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n \\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n+    pass\\n+    return username\\n pass\\n \\n \\n@@ -754,8 +792,15 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+        username = upload_to_huggingface(\\n+            self, model, new_save_directory, token,\\n+            \"GGUF converted\", \"gguf\", file_location,\\n+        )\\n+        print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n pass\\n \\n \\n@@ -827,8 +872,13 @@ def unsloth_push_to_hub_gguf(\\n \\n     python_install.wait()\\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, model, new_save_directory, token,\\n+        \"GGUF converted\", \"gguf\", file_location,\\n+    )\\n+    print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n pass\\n \\n \\n',\n",
       " '@@ -208,6 +208,15 @@ def unsloth_save_model(\\n         return save_directory\\n     pass\\n \\n+    # Update model tag\\n+    username = \"\"\\n+    if push_to_hub:\\n+        username = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+        )\\n+    pass\\n+\\n     # If push_to_hub, we must remove the .../ part of a repo\\n     if push_to_hub and \"/\" in save_directory:\\n \\n@@ -331,6 +340,7 @@ def unsloth_save_model(\\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n                 state_dict[name] = W\\n+            # [TODO] Saving to RAM seems to leak memory???\\n             # elif (max_ram - W.nbytes) > 0:\\n             #     # Save to CPU memory\\n             #     logger.warning_once(f\"We will save to RAM and not VRAM now.\")\\n@@ -401,27 +411,32 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n+    # Print location\\n+    if push_to_hub:\\n+        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n+\\n     save_pretrained_settings[\"state_dict\"] = None\\n \\n-    # for j, (key, value) in enumerate(state_dict.items()):\\n-    #     state_dict[key] = None\\n-    #     if j % 10 == 0:\\n-    #         torch.cuda.empty_cache()\\n-    #         gc.collect()\\n-    #     pass\\n-    # pass\\n-    # state_dict = None\\n-    # del state_dict\\n-    # torch.cuda.empty_cache()\\n-    # gc.collect()\\n+    for j, (key, value) in enumerate(state_dict.items()):\\n+        state_dict[key] = None\\n+        if j % 10 == 0:\\n+            torch.cuda.empty_cache()\\n+            gc.collect()\\n+        pass\\n+    pass\\n+    state_dict = None\\n+    del state_dict\\n+    torch.cuda.empty_cache()\\n+    gc.collect()\\n \\n     # Remove temporary location\\n     import shutil\\n     shutil.rmtree(temporary_location)\\n \\n-    # for _ in range(3):\\n-    #     torch.cuda.empty_cache()\\n-    #     gc.collect()\\n+    for _ in range(3):\\n+        torch.cuda.empty_cache()\\n+        gc.collect()\\n     return save_directory\\n pass\\n \\n@@ -629,14 +644,44 @@ def unsloth_push_to_hub_merged(\\n pass\\n \\n \\n-def upload_gguf_to_huggingface(save_directory, file_location, token, model_type):\\n-    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+MODEL_CARD = \\\\\\n+\"\"\"---\\n+base_model: {base_model}\\n+tags:\\n+- text-generation-inference\\n+- transformers\\n+- unsloth\\n+- {model_type}\\n+- {extra}\\n+license: apache-2.0\\n+language:\\n+- en\\n+---\\n+\\n+# Uploaded {method} model\\n+\\n+- **Developed by:** {username}\\n+- **License:** apache-2.0\\n+- **Finetuned from model :** {base_model}\\n+\\n+This {model_type} model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n+\\n+[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n+\"\"\"\\n+\\n \\n+def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n     # Check for username\\n+    username = \"\"\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n-        try: save_directory = f\"{save_directory}/{whoami()[\\'name\\']}\"\\n-        except: pass\\n+        try: \\n+            username = whoami()[\\'name\\']\\n+            save_directory = f\"{save_directory}/{username}\"\\n+        except:\\n+            raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n+    else:\\n+        username = save_directory.split(\"/\")[0]\\n     pass\\n \\n     from huggingface_hub import create_repo\\n@@ -648,43 +693,36 @@ def upload_gguf_to_huggingface(save_directory, file_location, token, model_type)\\n     )\\n \\n     # Create model card\\n-    from huggingface_hub import ModelCard, ModelCardData\\n-    card_data = ModelCardData(\\n-        language = \"en\",\\n-        license  = \"apache-2.0\",\\n-        library  = \"unsloth\",\\n-        tags     = [\"gguf\", \"unsloth\", \"text-generation-inference\", \"transformers\",],\\n+    from huggingface_hub import ModelCard\\n+    content = MODEL_CARD.format(\\n+        username   = username,\\n+        base_model = model.config._name_or_path,\\n+        model_type = model.config.model_type,\\n+        method     = \"\",\\n+        extra      = extra,\\n     )\\n-\\n-    content = f\"\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"{ card_data.to_yaml() }\\\\n\"\\\\\\n-    f\"---\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"# My Model Card for {file_location}\\\\n\"\\\\\\n-    f\"\\\\n\"\\\\\\n-    f\"\\\\nThis {model_type.title()} model was trained by [Unsloth](https://github.com/unslothai/unsloth) then saved to GGUF.\\\\n\"\\\\\\n-    f\"\\\\n\"\\n-    \\n     card = ModelCard(content)\\n     card.push_to_hub(save_directory, token = token)\\n \\n-    # Now upload file\\n-    from huggingface_hub import HfApi\\n-    hf_api = HfApi(token = token)\\n+    if file_location is not None:\\n+        # Now upload file\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = token)\\n \\n-    if \"/\" in file_location:\\n-        uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n-    else:\\n-        uploaded_location = file_location\\n-    pass\\n+        if \"/\" in file_location:\\n+            uploaded_location = file_location[file_location.rfind(\"/\")+1:]\\n+        else:\\n+            uploaded_location = file_location\\n+        pass\\n \\n-    hf_api.upload_file(\\n-        path_or_fileobj = file_location,\\n-        path_in_repo    = uploaded_location,\\n-        repo_id         = save_directory,\\n-        repo_type       = \"model\",\\n-    )\\n+        hf_api.upload_file(\\n+            path_or_fileobj = file_location,\\n+            path_in_repo    = uploaded_location,\\n+            repo_id         = save_directory,\\n+            repo_type       = \"model\",\\n+        )\\n+    pass\\n+    return username\\n pass\\n \\n \\n@@ -754,8 +792,15 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    if push_to_hub: upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    if push_to_hub:\\n+        print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+        username = upload_to_huggingface(\\n+            self, model, new_save_directory, token,\\n+            \"GGUF converted\", \"gguf\", file_location,\\n+        )\\n+        print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n+    pass\\n pass\\n \\n \\n@@ -827,8 +872,13 @@ def unsloth_push_to_hub_gguf(\\n \\n     python_install.wait()\\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n-    model_type = self.config.model_type\\n-    upload_gguf_to_huggingface(new_save_directory, file_location, token, model_type)\\n+\\n+    print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, model, new_save_directory, token,\\n+        \"GGUF converted\", \"gguf\", file_location,\\n+    )\\n+    print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n pass\\n \\n \\n',\n",
       " '@@ -144,7 +144,7 @@ def LlamaAttention_fast_forward_inference(\\n     A = torch.matmul(A, Vnn)\\n     A = A.transpose(1, 2)\\n     A = A.reshape(bsz, 1, self.hidden_size)\\n-    A = original_apply_o(self, A)\\n+    A = self.o_proj(A)\\n     return A, (Kn, Vn)\\n pass\\n \\n@@ -187,10 +187,9 @@ def LlamaAttention_fast_forward(\\n ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\\n     \\n     bsz, q_len, _ = hidden_states.size()\\n-    Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n     # Check for inference\\n-    if use_cache and past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -206,6 +205,7 @@ def LlamaAttention_fast_forward(\\n     head_dim   = self.head_dim\\n     assert(n_kv_heads * n_groups == n_heads)\\n \\n+    Q, K, V = self.apply_qkv(self, hidden_states)\\n     Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)\\n     K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)\\n     V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)\\n@@ -304,11 +304,10 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-\\n-    if (self.training):\\n+    if (past_key_value is not None and q_len == 1):\\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n             causal_mask=causal_mask,\\n@@ -319,17 +318,16 @@ def LlamaDecoderLayer_fast_forward(\\n             use_cache=use_cache,\\n             padding_mask=padding_mask,\\n         )\\n-        hidden_states = residual + hidden_states\\n+        hidden_states += residual\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n-        hidden_states = self.mlp(hidden_states)\\n-        hidden_states = residual + hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n+        hidden_states += residual\\n     else:\\n-        # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n             causal_mask=causal_mask,\\n@@ -340,13 +338,13 @@ def LlamaDecoderLayer_fast_forward(\\n             use_cache=use_cache,\\n             padding_mask=padding_mask,\\n         )\\n-        hidden_states += residual\\n+        hidden_states = residual + hidden_states\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n-        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n-        hidden_states += residual\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = self.mlp(hidden_states)\\n+        hidden_states = residual + hidden_states\\n     pass\\n \\n     outputs = (hidden_states,)\\n@@ -445,7 +443,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif self.training:\\n+    elif True:#self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n@@ -524,10 +522,11 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n \\n-    if (self.training):\\n-        hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n-    else:\\n+    bsz, q_len, hd = hidden_states.size()\\n+    if (past_key_value is not None and q_len == 1):\\n         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n     pass\\n \\n     # add hidden states from the last decoder layer\\n',\n",
       " '@@ -47,10 +47,9 @@ def MistralAttention_fast_forward(\\n ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\\n     \\n     bsz, q_len, _ = hidden_states.size()\\n-    Q, K, V = self.apply_qkv(self, hidden_states)\\n \\n     # Check for inference\\n-    if use_cache and past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -66,6 +65,7 @@ def MistralAttention_fast_forward(\\n     head_dim   = self.head_dim\\n     assert(n_kv_heads * n_groups == n_heads)\\n \\n+    Q, K, V = self.apply_qkv(self, hidden_states)\\n     Q = Q.view(bsz, q_len, n_heads,    head_dim).transpose(1, 2)\\n     K = K.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)\\n     V = V.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)\\n',\n",
       " '@@ -94,8 +94,9 @@ def fast_save_pickle(shard, name):\\n     torch.save(\\n         shard,\\n         name,\\n-        pickle_module   = pickle,\\n-        pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n+        # HIGHEST_PROTOCOL seems to not work with Pytorch!\\n+        # pickle_module   = pickle,\\n+        # pickle_protocol = pickle.HIGHEST_PROTOCOL,\\n     )\\n     return\\n pass\\n@@ -783,12 +784,27 @@ def unsloth_save_pretrained_gguf(\\n     del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n-    git_clone = install_llama_cpp_clone_non_blocking()\\n-    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-    git_clone.wait()\\n-    makefile  = install_llama_cpp_make_non_blocking()\\n-    new_save_directory = unsloth_save_model(**arguments)\\n-    python_install.wait()\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        git_clone = install_llama_cpp_clone_non_blocking()\\n+        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+        git_clone.wait()\\n+        makefile  = install_llama_cpp_make_non_blocking()\\n+        new_save_directory = unsloth_save_model(**arguments)\\n+        python_install.wait()\\n+    else:\\n+        try:\\n+            new_save_directory = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            git_clone.wait()\\n+            makefile  = install_llama_cpp_make_non_blocking()\\n+            new_save_directory = unsloth_save_model(**arguments)\\n+            python_install.wait()\\n+        pass\\n+    pass\\n \\n     for _ in range(3):\\n         gc.collect()\\n@@ -801,7 +817,10 @@ def unsloth_save_pretrained_gguf(\\n             self, save_directory, token,\\n             \"GGUF converted\", \"gguf\", file_location,\\n         )\\n-        print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/.\\')}\")\\n+        link = f\"{username}/{new_save_directory.lstrip(\\'/.\\')}\" \\\\\\n+            if username not in new_save_directory else \\\\\\n+            new_save_directory.lstrip(\\'/.\\')\\n+        print(f\"Saved to https://huggingface.co/{link}\")\\n     pass\\n pass\\n \\n@@ -863,16 +882,31 @@ def unsloth_push_to_hub_gguf(\\n     del arguments[\"quantization_method\"]\\n \\n     # Non blocking install GGUF first\\n-    git_clone = install_llama_cpp_clone_non_blocking()\\n-    python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-    git_clone.wait()\\n-    makefile  = install_llama_cpp_make_non_blocking()\\n-    new_save_directory = unsloth_save_model(**arguments)\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        git_clone = install_llama_cpp_clone_non_blocking()\\n+        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+        git_clone.wait()\\n+        makefile  = install_llama_cpp_make_non_blocking()\\n+        new_save_directory = unsloth_save_model(**arguments)\\n+        python_install.wait()\\n+    else:\\n+        try:\\n+            new_save_directory = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            git_clone.wait()\\n+            makefile  = install_llama_cpp_make_non_blocking()\\n+            new_save_directory = unsloth_save_model(**arguments)\\n+            python_install.wait()\\n+        pass\\n+    pass\\n \\n     for _ in range(3):\\n         gc.collect()\\n \\n-    python_install.wait()\\n     file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -880,7 +914,10 @@ def unsloth_push_to_hub_gguf(\\n         self, repo_id, token,\\n         \"GGUF converted\", \"gguf\", file_location,\\n     )\\n-    print(f\"Saved to https://huggingface.co/{username}/{new_save_directory.lstrip(\\'/\\')}\")\\n+    link = f\"{username}/{new_save_directory.lstrip(\\'/.\\')}\" \\\\\\n+        if username not in new_save_directory else \\\\\\n+        new_save_directory.lstrip(\\'/.\\')\\n+    print(f\"Saved to https://huggingface.co/{link}\")\\n pass\\n \\n \\n',\n",
       " '@@ -32,18 +32,8 @@ include-package-data = false\\n exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n-huggingfacedev = [\\n-    \"transformers @ git+https://github.com/huggingface/transformers\",\\n-    \"datasets\",\\n-    \"sentencepiece\",\\n-    \"accelerate\",\\n-    \"trl>=0.7.9\",\\n-    \"peft\",\\n-    \"tqdm\",\\n-    \"psutil\",\\n-]\\n huggingface = [\\n-    \"transformers\",\\n+    \"transformers>=4.37.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n@@ -107,15 +97,15 @@ colab_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_dev = [\\n-    \"unsloth[huggingfacedev]\",\\n+colab_torch211 = [\\n+    \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n-    \"unsloth[cu121only]\",\\n+    \"unsloth[cu121onlytorch211]\",\\n ]\\n-colab_ampere_dev = [\\n-    \"unsloth[huggingfacedev]\",\\n+colab_ampere_torch211 = [\\n+    \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n-    \"unsloth[cu121only]\",\\n+    \"unsloth[cu121onlytorch211]\",\\n     \"packaging\",\\n     \"ninja\",\\n     \"flash-attn\",\\n',\n",
       " '@@ -141,7 +141,7 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         # Gate projection LoRA weights\\n         d_gateA = X.t() @ (DW_dfg @ gateB.t())\\n-        d_gateB = (gateA.t() @ X.t() @ DW_dfg)\\n+        d_gateB = (gateA.t() @ X.t()) @ DW_dfg\\n         d_gateA *= gateS\\n         d_gateB *= gateS\\n \\n',\n",
       " '@@ -30,7 +30,19 @@ major_version, minor_version = torch.cuda.get_device_capability()\\n if major_version >= 8:\\n     try:\\n         from flash_attn import flash_attn_func\\n-        HAS_FLASH_ATTENTION = True\\n+        # Check for CUDA linking errors \"undefined symbol: _ZNK3c106SymIntltEl\"\\n+        try:\\n+            from flash_attn.flash_attn_interface import flash_attn_cuda\\n+            HAS_FLASH_ATTENTION = True\\n+        except:\\n+            logger.warning_once(\\n+                \"Unsloth: Your Flash Attention 2 installation seems to be broken?\\\\n\"\\\\\\n+                \"A possible explanation is you have a new CUDA version which isn\\'t\\\\n\"\\\\\\n+                \"yet compatible with FA2? Please file a ticket to Unsloth or FA2.\\\\n\"\\\\\\n+                \"We shall now use Xformers instead, which gets a 0.01% performance hit.\\\\n\"\\\\\\n+                \"We found this negligible impact by benchmarking on 1x A100.\"\\n+            )\\n+            HAS_FLASH_ATTENTION = False\\n     except:\\n         HAS_FLASH_ATTENTION = False\\n else:\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -271,6 +271,7 @@ def LlamaAttention_fast_forward(\\n     if past_key_value is not None:\\n         K = torch.cat([past_key_value[0], K], dim = 2)\\n         V = torch.cat([past_key_value[1], V], dim = 2)\\n+    pass\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n@@ -283,13 +284,13 @@ def LlamaAttention_fast_forward(\\n \\n         # Group query attention\\n         if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+            V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+            K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n+            V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n             if hidden_states.requires_grad:\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim)\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim)\\n+                K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n+                V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n             else:\\n                 Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n         pass\\n@@ -304,10 +305,10 @@ def LlamaAttention_fast_forward(\\n     else:\\n         # Grouped query attention\\n         if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+            K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n+            V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n         pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n@@ -349,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -722,9 +723,9 @@ class FastLlamaModel:\\n         statistics = \\\\\\n            f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n+           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         logger.warning_once(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n@@ -813,10 +814,13 @@ class FastLlamaModel:\\n         patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.config._name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.config.update({\"_name_or_path\" : name})\\n+        # Not necessary anymore since we require transformers>=4.37!\\n+        if False:\\n+            name = model.config._name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.config.update({\"_name_or_path\" : name})\\n+            pass\\n         pass\\n \\n         # Log Unsloth version for future fastpaths for inference\\n@@ -1019,11 +1023,13 @@ class FastLlamaModel:\\n \\n         # Fix up config for transformers uploading PEFT\\n         for active_adapter in model.peft_config.keys():\\n-            name = model.peft_config[active_adapter].base_model_name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.peft_config[active_adapter].base_model_name_or_path = name\\n-            pass\\n+            # Not necessary since we requires transformers >= 4.37\\n+            if False:\\n+                name = model.peft_config[active_adapter].base_model_name_or_path\\n+                if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                    name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                    model.peft_config[active_adapter].base_model_name_or_path = name\\n+                pass\\n             # Add revision to enable future fast inference paths\\n             model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n',\n",
       " '@@ -34,7 +34,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\\'Try `pip install --upgrade \"transformers>=4.37\"`\\\\n\\'\\\\\\n             f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n             f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n         )\\n',\n",
       " '@@ -36,7 +36,7 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     ),\\n     \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n         \"unsloth/zephyr-sft\",\\n-        \"alignment-handbook/zephyr-7b-sft-full\",\\n+        \"HuggingFaceH4/mistral-7b-sft-beta\",\\n     ),\\n     \"unsloth/tinyllama-bnb-4bit\"     : (\\n         \"unsloth/tinyllama\",\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -84,9 +84,9 @@ def MistralAttention_fast_forward(\\n     pass\\n \\n     if past_key_value is not None:\\n-        # reuse k, v, self_attention\\n         K = torch.cat([past_key_value[0], K], dim = 2)\\n         V = torch.cat([past_key_value[1], V], dim = 2)\\n+    pass\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n@@ -95,32 +95,33 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        M = bsz * q_len\\n+        K_M = V_M = bsz * kv_seq_len\\n+        Q_M = bsz * q_len\\n \\n         has_swa = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n         if hidden_states.requires_grad:\\n-            K = K.reshape(bsz, q_len, n_heads, head_dim)\\n-            V = V.reshape(bsz, q_len, n_heads, head_dim)\\n+            K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n+            V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n \\n             if has_swa:\\n-                Q = Q.view(1, M, n_heads, head_dim)\\n-                K = K.view(1, M, n_heads, head_dim)\\n-                V = V.view(1, M, n_heads, head_dim)\\n+                Q = Q.view(1, Q_M, n_heads, head_dim)\\n+                K = K.view(1, K_M, n_heads, head_dim)\\n+                V = V.view(1, V_M, n_heads, head_dim)\\n             pass\\n         else:\\n             # Xformers does support the forward pass though\\n             Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n \\n             if has_swa:\\n-                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n-                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n-                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                Q = Q.view(1, Q_M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, K_M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, V_M, n_kv_heads, n_groups, head_dim)\\n             pass\\n         pass\\n \\n@@ -132,16 +133,16 @@ def MistralAttention_fast_forward(\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n         sw = getattr(self.config, \"sliding_window\", None)\\n-        sw = q_len if (sw is None or sw == \"null\") else sw\\n-        window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n+        sw = kv_seq_len if (sw is None or sw == \"null\") else sw\\n+        window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n         # if n_groups != 1:\\n-        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n         # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n@@ -278,7 +279,7 @@ class FastMistralModel(FastLlamaModel):\\n         statistics = \\\\\\n            f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n         logger.warning_once(statistics)\\n@@ -363,11 +364,13 @@ class FastMistralModel(FastLlamaModel):\\n         patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.config._name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.config.update({\"_name_or_path\" : name})\\n-        pass\\n+        # Not necessary anymore since we require transformers>=4.37\\n+        if False:\\n+            name = model.config._name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.config.update({\"_name_or_path\" : name})\\n+            pass\\n         \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n',\n",
       " '@@ -135,6 +135,17 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if save_method == \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\\\\n\"\\\\\\n+            \"to merge to GGUF or others later on. I suggest you to do this as a final step\\\\n\"\\\\\\n+            \"if you\\'re planning to do multiple saves.\\\\n\"\\\\\\n+            \"If you are certain, change `save_method` to `merged_4bit_forced`.\"\\n+        )\\n+    elif save_method == \"merged_4bit_forced\":\\n+        save_method = \"merged_4bit\"\\n+    pass\\n+\\n     save_pretrained_settings = dict(locals())\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n@@ -457,6 +468,8 @@ pass\\n def install_llama_cpp_make_non_blocking():\\n     env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    # Force make clean\\n+    os.system(\"make clean -C llama.cpp\")\\n     full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n@@ -487,8 +500,8 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory : str = \"unsloth_finetuned_model\",\\n-    quantization_method    : str = \"fast_quantized\",\\n+    model_directory      : str = \"unsloth_finetuned_model\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n@@ -566,7 +579,7 @@ def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n',\n",
       " '@@ -32,18 +32,8 @@ include-package-data = false\\n exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n-huggingfacedev = [\\n-    \"transformers @ git+https://github.com/huggingface/transformers\",\\n-    \"datasets\",\\n-    \"sentencepiece\",\\n-    \"accelerate\",\\n-    \"trl>=0.7.9\",\\n-    \"peft\",\\n-    \"tqdm\",\\n-    \"psutil\",\\n-]\\n huggingface = [\\n-    \"transformers\",\\n+    \"transformers>=4.37.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n@@ -107,15 +97,15 @@ colab_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_dev = [\\n-    \"unsloth[huggingfacedev]\",\\n+colab_torch211 = [\\n+    \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n-    \"unsloth[cu121only]\",\\n+    \"unsloth[cu121onlytorch211]\",\\n ]\\n-colab_ampere_dev = [\\n-    \"unsloth[huggingfacedev]\",\\n+colab_ampere_torch211 = [\\n+    \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n-    \"unsloth[cu121only]\",\\n+    \"unsloth[cu121onlytorch211]\",\\n     \"packaging\",\\n     \"ninja\",\\n     \"flash-attn\",\\n',\n",
       " '@@ -141,7 +141,7 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         # Gate projection LoRA weights\\n         d_gateA = X.t() @ (DW_dfg @ gateB.t())\\n-        d_gateB = (gateA.t() @ X.t() @ DW_dfg)\\n+        d_gateB = (gateA.t() @ X.t()) @ DW_dfg\\n         d_gateA *= gateS\\n         d_gateB *= gateS\\n \\n',\n",
       " '@@ -30,7 +30,19 @@ major_version, minor_version = torch.cuda.get_device_capability()\\n if major_version >= 8:\\n     try:\\n         from flash_attn import flash_attn_func\\n-        HAS_FLASH_ATTENTION = True\\n+        # Check for CUDA linking errors \"undefined symbol: _ZNK3c106SymIntltEl\"\\n+        try:\\n+            from flash_attn.flash_attn_interface import flash_attn_cuda\\n+            HAS_FLASH_ATTENTION = True\\n+        except:\\n+            logger.warning_once(\\n+                \"Unsloth: Your Flash Attention 2 installation seems to be broken?\\\\n\"\\\\\\n+                \"A possible explanation is you have a new CUDA version which isn\\'t\\\\n\"\\\\\\n+                \"yet compatible with FA2? Please file a ticket to Unsloth or FA2.\\\\n\"\\\\\\n+                \"We shall now use Xformers instead, which gets a 0.01% performance hit.\\\\n\"\\\\\\n+                \"We found this negligible impact by benchmarking on 1x A100.\"\\n+            )\\n+            HAS_FLASH_ATTENTION = False\\n     except:\\n         HAS_FLASH_ATTENTION = False\\n else:\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -271,6 +271,7 @@ def LlamaAttention_fast_forward(\\n     if past_key_value is not None:\\n         K = torch.cat([past_key_value[0], K], dim = 2)\\n         V = torch.cat([past_key_value[1], V], dim = 2)\\n+    pass\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n@@ -283,13 +284,13 @@ def LlamaAttention_fast_forward(\\n \\n         # Group query attention\\n         if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+            V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+            K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n+            V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n             if hidden_states.requires_grad:\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim)\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim)\\n+                K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n+                V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n             else:\\n                 Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n         pass\\n@@ -304,10 +305,10 @@ def LlamaAttention_fast_forward(\\n     else:\\n         # Grouped query attention\\n         if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+            K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n+            V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n         pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n@@ -349,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -722,9 +723,9 @@ class FastLlamaModel:\\n         statistics = \\\\\\n            f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n+           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         logger.warning_once(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n@@ -813,10 +814,13 @@ class FastLlamaModel:\\n         patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.config._name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.config.update({\"_name_or_path\" : name})\\n+        # Not necessary anymore since we require transformers>=4.37!\\n+        if False:\\n+            name = model.config._name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.config.update({\"_name_or_path\" : name})\\n+            pass\\n         pass\\n \\n         # Log Unsloth version for future fastpaths for inference\\n@@ -1019,11 +1023,13 @@ class FastLlamaModel:\\n \\n         # Fix up config for transformers uploading PEFT\\n         for active_adapter in model.peft_config.keys():\\n-            name = model.peft_config[active_adapter].base_model_name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.peft_config[active_adapter].base_model_name_or_path = name\\n-            pass\\n+            # Not necessary since we requires transformers >= 4.37\\n+            if False:\\n+                name = model.peft_config[active_adapter].base_model_name_or_path\\n+                if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                    name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                    model.peft_config[active_adapter].base_model_name_or_path = name\\n+                pass\\n             # Add revision to enable future fast inference paths\\n             model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n',\n",
       " '@@ -34,7 +34,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\\'Try `pip install --upgrade \"transformers>=4.37\"`\\\\n\\'\\\\\\n             f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n             f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n         )\\n',\n",
       " '@@ -36,7 +36,7 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     ),\\n     \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n         \"unsloth/zephyr-sft\",\\n-        \"alignment-handbook/zephyr-7b-sft-full\",\\n+        \"HuggingFaceH4/mistral-7b-sft-beta\",\\n     ),\\n     \"unsloth/tinyllama-bnb-4bit\"     : (\\n         \"unsloth/tinyllama\",\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -84,9 +84,9 @@ def MistralAttention_fast_forward(\\n     pass\\n \\n     if past_key_value is not None:\\n-        # reuse k, v, self_attention\\n         K = torch.cat([past_key_value[0], K], dim = 2)\\n         V = torch.cat([past_key_value[1], V], dim = 2)\\n+    pass\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n@@ -95,32 +95,33 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        M = bsz * q_len\\n+        K_M = V_M = bsz * kv_seq_len\\n+        Q_M = bsz * q_len\\n \\n         has_swa = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n         if hidden_states.requires_grad:\\n-            K = K.reshape(bsz, q_len, n_heads, head_dim)\\n-            V = V.reshape(bsz, q_len, n_heads, head_dim)\\n+            K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n+            V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n \\n             if has_swa:\\n-                Q = Q.view(1, M, n_heads, head_dim)\\n-                K = K.view(1, M, n_heads, head_dim)\\n-                V = V.view(1, M, n_heads, head_dim)\\n+                Q = Q.view(1, Q_M, n_heads, head_dim)\\n+                K = K.view(1, K_M, n_heads, head_dim)\\n+                V = V.view(1, V_M, n_heads, head_dim)\\n             pass\\n         else:\\n             # Xformers does support the forward pass though\\n             Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n \\n             if has_swa:\\n-                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n-                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n-                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                Q = Q.view(1, Q_M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, K_M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, V_M, n_kv_heads, n_groups, head_dim)\\n             pass\\n         pass\\n \\n@@ -132,16 +133,16 @@ def MistralAttention_fast_forward(\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n         sw = getattr(self.config, \"sliding_window\", None)\\n-        sw = q_len if (sw is None or sw == \"null\") else sw\\n-        window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n+        sw = kv_seq_len if (sw is None or sw == \"null\") else sw\\n+        window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n         # if n_groups != 1:\\n-        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n         # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n@@ -278,7 +279,7 @@ class FastMistralModel(FastLlamaModel):\\n         statistics = \\\\\\n            f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n         logger.warning_once(statistics)\\n@@ -363,11 +364,13 @@ class FastMistralModel(FastLlamaModel):\\n         patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.config._name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.config.update({\"_name_or_path\" : name})\\n-        pass\\n+        # Not necessary anymore since we require transformers>=4.37\\n+        if False:\\n+            name = model.config._name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.config.update({\"_name_or_path\" : name})\\n+            pass\\n         \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n',\n",
       " '@@ -135,6 +135,17 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if save_method == \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\\\\n\"\\\\\\n+            \"to merge to GGUF or others later on. I suggest you to do this as a final step\\\\n\"\\\\\\n+            \"if you\\'re planning to do multiple saves.\\\\n\"\\\\\\n+            \"If you are certain, change `save_method` to `merged_4bit_forced`.\"\\n+        )\\n+    elif save_method == \"merged_4bit_forced\":\\n+        save_method = \"merged_4bit\"\\n+    pass\\n+\\n     save_pretrained_settings = dict(locals())\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n@@ -457,6 +468,8 @@ pass\\n def install_llama_cpp_make_non_blocking():\\n     env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    # Force make clean\\n+    os.system(\"make clean -C llama.cpp\")\\n     full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n@@ -487,8 +500,8 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory : str = \"unsloth_finetuned_model\",\\n-    quantization_method    : str = \"fast_quantized\",\\n+    model_directory      : str = \"unsloth_finetuned_model\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n@@ -566,7 +579,7 @@ def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n',\n",
       " '@@ -32,18 +32,8 @@ include-package-data = false\\n exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n-huggingfacedev = [\\n-    \"transformers @ git+https://github.com/huggingface/transformers\",\\n-    \"datasets\",\\n-    \"sentencepiece\",\\n-    \"accelerate\",\\n-    \"trl>=0.7.9\",\\n-    \"peft\",\\n-    \"tqdm\",\\n-    \"psutil\",\\n-]\\n huggingface = [\\n-    \"transformers\",\\n+    \"transformers>=4.37.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate\",\\n@@ -107,15 +97,15 @@ colab_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_dev = [\\n-    \"unsloth[huggingfacedev]\",\\n+colab_torch211 = [\\n+    \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n-    \"unsloth[cu121only]\",\\n+    \"unsloth[cu121onlytorch211]\",\\n ]\\n-colab_ampere_dev = [\\n-    \"unsloth[huggingfacedev]\",\\n+colab_ampere_torch211 = [\\n+    \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n-    \"unsloth[cu121only]\",\\n+    \"unsloth[cu121onlytorch211]\",\\n     \"packaging\",\\n     \"ninja\",\\n     \"flash-attn\",\\n',\n",
       " '@@ -141,7 +141,7 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         # Gate projection LoRA weights\\n         d_gateA = X.t() @ (DW_dfg @ gateB.t())\\n-        d_gateB = (gateA.t() @ X.t() @ DW_dfg)\\n+        d_gateB = (gateA.t() @ X.t()) @ DW_dfg\\n         d_gateA *= gateS\\n         d_gateB *= gateS\\n \\n',\n",
       " '@@ -30,7 +30,19 @@ major_version, minor_version = torch.cuda.get_device_capability()\\n if major_version >= 8:\\n     try:\\n         from flash_attn import flash_attn_func\\n-        HAS_FLASH_ATTENTION = True\\n+        # Check for CUDA linking errors \"undefined symbol: _ZNK3c106SymIntltEl\"\\n+        try:\\n+            from flash_attn.flash_attn_interface import flash_attn_cuda\\n+            HAS_FLASH_ATTENTION = True\\n+        except:\\n+            logger.warning_once(\\n+                \"Unsloth: Your Flash Attention 2 installation seems to be broken?\\\\n\"\\\\\\n+                \"A possible explanation is you have a new CUDA version which isn\\'t\\\\n\"\\\\\\n+                \"yet compatible with FA2? Please file a ticket to Unsloth or FA2.\\\\n\"\\\\\\n+                \"We shall now use Xformers instead, which gets a 0.01% performance hit.\\\\n\"\\\\\\n+                \"We found this negligible impact by benchmarking on 1x A100.\"\\n+            )\\n+            HAS_FLASH_ATTENTION = False\\n     except:\\n         HAS_FLASH_ATTENTION = False\\n else:\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -271,6 +271,7 @@ def LlamaAttention_fast_forward(\\n     if past_key_value is not None:\\n         K = torch.cat([past_key_value[0], K], dim = 2)\\n         V = torch.cat([past_key_value[1], V], dim = 2)\\n+    pass\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n@@ -283,13 +284,13 @@ def LlamaAttention_fast_forward(\\n \\n         # Group query attention\\n         if n_groups != 1:\\n-            K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-            K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-            V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+            K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+            V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+            K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n+            V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n             if hidden_states.requires_grad:\\n-                K = K.reshape(bsz, q_len, n_heads, head_dim)\\n-                V = V.reshape(bsz, q_len, n_heads, head_dim)\\n+                K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n+                V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n             else:\\n                 Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n         pass\\n@@ -304,10 +305,10 @@ def LlamaAttention_fast_forward(\\n     else:\\n         # Grouped query attention\\n         if n_groups != 1:\\n-            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-            K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-            V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+            K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+            V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+            K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n+            V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n         pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n@@ -349,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -722,9 +723,9 @@ class FastLlamaModel:\\n         statistics = \\\\\\n            f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n+           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         logger.warning_once(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n@@ -813,10 +814,13 @@ class FastLlamaModel:\\n         patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.config._name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.config.update({\"_name_or_path\" : name})\\n+        # Not necessary anymore since we require transformers>=4.37!\\n+        if False:\\n+            name = model.config._name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.config.update({\"_name_or_path\" : name})\\n+            pass\\n         pass\\n \\n         # Log Unsloth version for future fastpaths for inference\\n@@ -1019,11 +1023,13 @@ class FastLlamaModel:\\n \\n         # Fix up config for transformers uploading PEFT\\n         for active_adapter in model.peft_config.keys():\\n-            name = model.peft_config[active_adapter].base_model_name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.peft_config[active_adapter].base_model_name_or_path = name\\n-            pass\\n+            # Not necessary since we requires transformers >= 4.37\\n+            if False:\\n+                name = model.peft_config[active_adapter].base_model_name_or_path\\n+                if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                    name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                    model.peft_config[active_adapter].base_model_name_or_path = name\\n+                pass\\n             # Add revision to enable future fast inference paths\\n             model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n',\n",
       " '@@ -34,7 +34,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n-            f\\'Try `pip install \"git+https://github.com/huggingface/transformers.git\"`\\\\n\\'\\\\\\n+            f\\'Try `pip install --upgrade \"transformers>=4.37\"`\\\\n\\'\\\\\\n             f\"to obtain the latest transformers build, then restart this session.\\\\n\"\\\\\\n             f\"For now, we shall load `{model_name}` instead (still 4bit, just slower downloading).\"\\n         )\\n',\n",
       " '@@ -36,7 +36,7 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     ),\\n     \"unsloth/zephyr-sft-bnb-4bit\"    : (\\n         \"unsloth/zephyr-sft\",\\n-        \"alignment-handbook/zephyr-7b-sft-full\",\\n+        \"HuggingFaceH4/mistral-7b-sft-beta\",\\n     ),\\n     \"unsloth/tinyllama-bnb-4bit\"     : (\\n         \"unsloth/tinyllama\",\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1:\\n+    if past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -84,9 +84,9 @@ def MistralAttention_fast_forward(\\n     pass\\n \\n     if past_key_value is not None:\\n-        # reuse k, v, self_attention\\n         K = torch.cat([past_key_value[0], K], dim = 2)\\n         V = torch.cat([past_key_value[1], V], dim = 2)\\n+    pass\\n     past_key_value = (K, V) if use_cache else None\\n \\n     # Attention module\\n@@ -95,32 +95,33 @@ def MistralAttention_fast_forward(\\n         Q = Q.transpose(1, 2)\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n-        M = bsz * q_len\\n+        K_M = V_M = bsz * kv_seq_len\\n+        Q_M = bsz * q_len\\n \\n         has_swa = isinstance(causal_mask, xformers.attn_bias.BlockDiagonalCausalMask)\\n \\n         # Group query attention\\n-        K = K  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-        V = V  .view(bsz, q_len, n_kv_heads,        1, head_dim)\\n-        K = K.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n-        V = V.expand(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n+        K = K  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+        V = V  .view(bsz, kv_seq_len, n_kv_heads,        1, head_dim)\\n+        K = K.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n+        V = V.expand(bsz, kv_seq_len, n_kv_heads, n_groups, head_dim)\\n         if hidden_states.requires_grad:\\n-            K = K.reshape(bsz, q_len, n_heads, head_dim)\\n-            V = V.reshape(bsz, q_len, n_heads, head_dim)\\n+            K = K.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n+            V = V.reshape(bsz, kv_seq_len, n_heads, head_dim)\\n \\n             if has_swa:\\n-                Q = Q.view(1, M, n_heads, head_dim)\\n-                K = K.view(1, M, n_heads, head_dim)\\n-                V = V.view(1, M, n_heads, head_dim)\\n+                Q = Q.view(1, Q_M, n_heads, head_dim)\\n+                K = K.view(1, K_M, n_heads, head_dim)\\n+                V = V.view(1, V_M, n_heads, head_dim)\\n             pass\\n         else:\\n             # Xformers does support the forward pass though\\n             Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\\n \\n             if has_swa:\\n-                Q = Q.view(1, M, n_kv_heads, n_groups, head_dim)\\n-                K = K.view(1, M, n_kv_heads, n_groups, head_dim)\\n-                V = V.view(1, M, n_kv_heads, n_groups, head_dim)\\n+                Q = Q.view(1, Q_M, n_kv_heads, n_groups, head_dim)\\n+                K = K.view(1, K_M, n_kv_heads, n_groups, head_dim)\\n+                V = V.view(1, V_M, n_kv_heads, n_groups, head_dim)\\n             pass\\n         pass\\n \\n@@ -132,16 +133,16 @@ def MistralAttention_fast_forward(\\n         K = K.transpose(1, 2)\\n         V = V.transpose(1, 2)\\n         sw = getattr(self.config, \"sliding_window\", None)\\n-        sw = q_len if (sw is None or sw == \"null\") else sw\\n-        window = (-1, -1) if (q_len <= sw) else (sw, sw)\\n+        sw = kv_seq_len if (sw is None or sw == \"null\") else sw\\n+        window = (-1, -1) if (kv_seq_len <= sw) else (sw, sw)\\n         A = flash_attn_func(Q, K, V, causal = True, window_size = window)\\n     else:\\n         # Grouped query attention\\n         # if n_groups != 1:\\n-        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, q_len, head_dim)\\n-        K = K.reshape(bsz, n_heads, q_len, head_dim)\\n-        V = V.reshape(bsz, n_heads, q_len, head_dim)\\n+        K = K[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+        V = V[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, kv_seq_len, head_dim)\\n+        K = K.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n+        V = V.reshape(bsz, n_heads, kv_seq_len, head_dim)\\n         # pass\\n         # Needs (batch_size, n_heads, seq_len, head_dim)\\n         # is_casual and attention_mask must not be both set!\\n@@ -278,7 +279,7 @@ class FastMistralModel(FastLlamaModel):\\n         statistics = \\\\\\n            f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\     Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n+           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n         logger.warning_once(statistics)\\n@@ -363,11 +364,13 @@ class FastMistralModel(FastLlamaModel):\\n         patch_saving_functions(tokenizer)\\n \\n         # Fix up config for transformers uploading PEFT\\n-        name = model.config._name_or_path\\n-        if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-            name = name[:len(name) - len(\"-bnb-4bit\")]\\n-            model.config.update({\"_name_or_path\" : name})\\n-        pass\\n+        # Not necessary anymore since we require transformers>=4.37\\n+        if False:\\n+            name = model.config._name_or_path\\n+            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n+                name = name[:len(name) - len(\"-bnb-4bit\")]\\n+                model.config.update({\"_name_or_path\" : name})\\n+            pass\\n         \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n',\n",
       " '@@ -135,6 +135,17 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if save_method == \"merged_4bit\":\\n+        raise RuntimeError(\\n+            \"Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\\\\n\"\\\\\\n+            \"to merge to GGUF or others later on. I suggest you to do this as a final step\\\\n\"\\\\\\n+            \"if you\\'re planning to do multiple saves.\\\\n\"\\\\\\n+            \"If you are certain, change `save_method` to `merged_4bit_forced`.\"\\n+        )\\n+    elif save_method == \"merged_4bit_forced\":\\n+        save_method = \"merged_4bit\"\\n+    pass\\n+\\n     save_pretrained_settings = dict(locals())\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n@@ -457,6 +468,8 @@ pass\\n def install_llama_cpp_make_non_blocking():\\n     env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n+    # Force make clean\\n+    os.system(\"make clean -C llama.cpp\")\\n     full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n@@ -487,8 +500,8 @@ pass\\n \\n \\n def save_to_gguf(\\n-    model_directory : str = \"unsloth_finetuned_model\",\\n-    quantization_method    : str = \"fast_quantized\",\\n+    model_directory      : str = \"unsloth_finetuned_model\",\\n+    quantization_method  : str = \"fast_quantized\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n@@ -566,7 +579,7 @@ def unsloth_save_pretrained_merged(\\n     self,\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n-    save_method         : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n+    save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n',\n",
       " '@@ -179,7 +179,10 @@ pass\\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-    out = fast_gemv(X, W, W_quant, out = out)\\n+    if W_quant is None:\\n+        out = torch.matmul(X, W.t())\\n+    else:\\n+        out = fast_gemv(X, W, W_quant, out = out)\\n     if lora_A is not None:\\n \\n         # Save LoRAs for inference to stop data movement costs\\n',\n",
       " '@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif True:#self.training:\\n+    elif self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n',\n",
       " '@@ -258,11 +258,19 @@ def unsloth_save_model(\\n         \"private\"         : save_pretrained_settings[\"private\"],\\n         \"token\"           : save_pretrained_settings[\"token\"],\\n     }\\n-    \\n+\\n+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.\\n+    from peft import PeftModelForCausalLM\\n+    if isinstance(model, PeftModelForCausalLM):\\n+        internal_model = model.model\\n+    else:\\n+        internal_model = model\\n+    pass\\n+        \\n+    # Cannot be converted properly!\\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n-        not hasattr(model.model, \"model\") or \\\\\\n-        not hasattr(model.model.model, \"layers\")\\n+        not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n         \\n@@ -343,12 +351,12 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n     from tqdm import tqdm as ProgressBar\\n-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n@@ -375,8 +383,8 @@ def unsloth_save_model(\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n+    state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -418,7 +426,7 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    model.model.save_pretrained(**save_pretrained_settings)\\n+    internal_model.save_pretrained(**save_pretrained_settings)\\n \\n     # Revert config back\\n     original_model = model\\n',\n",
       " '@@ -179,7 +179,10 @@ pass\\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-    out = fast_gemv(X, W, W_quant, out = out)\\n+    if W_quant is None:\\n+        out = torch.matmul(X, W.t())\\n+    else:\\n+        out = fast_gemv(X, W, W_quant, out = out)\\n     if lora_A is not None:\\n \\n         # Save LoRAs for inference to stop data movement costs\\n',\n",
       " '@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif True:#self.training:\\n+    elif self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n',\n",
       " '@@ -258,11 +258,19 @@ def unsloth_save_model(\\n         \"private\"         : save_pretrained_settings[\"private\"],\\n         \"token\"           : save_pretrained_settings[\"token\"],\\n     }\\n-    \\n+\\n+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.\\n+    from peft import PeftModelForCausalLM\\n+    if isinstance(model, PeftModelForCausalLM):\\n+        internal_model = model.model\\n+    else:\\n+        internal_model = model\\n+    pass\\n+        \\n+    # Cannot be converted properly!\\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n-        not hasattr(model.model, \"model\") or \\\\\\n-        not hasattr(model.model.model, \"layers\")\\n+        not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n         \\n@@ -343,12 +351,12 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n     from tqdm import tqdm as ProgressBar\\n-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n@@ -375,8 +383,8 @@ def unsloth_save_model(\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n+    state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -418,7 +426,7 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    model.model.save_pretrained(**save_pretrained_settings)\\n+    internal_model.save_pretrained(**save_pretrained_settings)\\n \\n     # Revert config back\\n     original_model = model\\n',\n",
       " '@@ -179,7 +179,10 @@ pass\\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-    out = fast_gemv(X, W, W_quant, out = out)\\n+    if W_quant is None:\\n+        out = torch.matmul(X, W.t())\\n+    else:\\n+        out = fast_gemv(X, W, W_quant, out = out)\\n     if lora_A is not None:\\n \\n         # Save LoRAs for inference to stop data movement costs\\n',\n",
       " '@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif True:#self.training:\\n+    elif self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n',\n",
       " '@@ -258,11 +258,19 @@ def unsloth_save_model(\\n         \"private\"         : save_pretrained_settings[\"private\"],\\n         \"token\"           : save_pretrained_settings[\"token\"],\\n     }\\n-    \\n+\\n+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.\\n+    from peft import PeftModelForCausalLM\\n+    if isinstance(model, PeftModelForCausalLM):\\n+        internal_model = model.model\\n+    else:\\n+        internal_model = model\\n+    pass\\n+        \\n+    # Cannot be converted properly!\\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n-        not hasattr(model.model, \"model\") or \\\\\\n-        not hasattr(model.model.model, \"layers\")\\n+        not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n         \\n@@ -343,12 +351,12 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n     from tqdm import tqdm as ProgressBar\\n-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n@@ -375,8 +383,8 @@ def unsloth_save_model(\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n+    state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -418,7 +426,7 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    model.model.save_pretrained(**save_pretrained_settings)\\n+    internal_model.save_pretrained(**save_pretrained_settings)\\n \\n     # Revert config back\\n     original_model = model\\n',\n",
       " '@@ -179,7 +179,10 @@ pass\\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-    out = fast_gemv(X, W, W_quant, out = out)\\n+    if W_quant is None:\\n+        out = torch.matmul(X, W.t())\\n+    else:\\n+        out = fast_gemv(X, W, W_quant, out = out)\\n     if lora_A is not None:\\n \\n         # Save LoRAs for inference to stop data movement costs\\n',\n",
       " '@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif True:#self.training:\\n+    elif self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n',\n",
       " '@@ -258,11 +258,19 @@ def unsloth_save_model(\\n         \"private\"         : save_pretrained_settings[\"private\"],\\n         \"token\"           : save_pretrained_settings[\"token\"],\\n     }\\n-    \\n+\\n+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.\\n+    from peft import PeftModelForCausalLM\\n+    if isinstance(model, PeftModelForCausalLM):\\n+        internal_model = model.model\\n+    else:\\n+        internal_model = model\\n+    pass\\n+        \\n+    # Cannot be converted properly!\\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n-        not hasattr(model.model, \"model\") or \\\\\\n-        not hasattr(model.model.model, \"layers\")\\n+        not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n         \\n@@ -343,12 +351,12 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n     from tqdm import tqdm as ProgressBar\\n-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n@@ -375,8 +383,8 @@ def unsloth_save_model(\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n+    state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -418,7 +426,7 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    model.model.save_pretrained(**save_pretrained_settings)\\n+    internal_model.save_pretrained(**save_pretrained_settings)\\n \\n     # Revert config back\\n     original_model = model\\n',\n",
       " '@@ -179,7 +179,10 @@ pass\\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-    out = fast_gemv(X, W, W_quant, out = out)\\n+    if W_quant is None:\\n+        out = torch.matmul(X, W.t())\\n+    else:\\n+        out = fast_gemv(X, W, W_quant, out = out)\\n     if lora_A is not None:\\n \\n         # Save LoRAs for inference to stop data movement costs\\n',\n",
       " '@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif True:#self.training:\\n+    elif self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n',\n",
       " '@@ -258,11 +258,19 @@ def unsloth_save_model(\\n         \"private\"         : save_pretrained_settings[\"private\"],\\n         \"token\"           : save_pretrained_settings[\"token\"],\\n     }\\n-    \\n+\\n+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.\\n+    from peft import PeftModelForCausalLM\\n+    if isinstance(model, PeftModelForCausalLM):\\n+        internal_model = model.model\\n+    else:\\n+        internal_model = model\\n+    pass\\n+        \\n+    # Cannot be converted properly!\\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n-        not hasattr(model.model, \"model\") or \\\\\\n-        not hasattr(model.model.model, \"layers\")\\n+        not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n         \\n@@ -343,12 +351,12 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n     from tqdm import tqdm as ProgressBar\\n-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n@@ -375,8 +383,8 @@ def unsloth_save_model(\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n+    state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -418,7 +426,7 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    model.model.save_pretrained(**save_pretrained_settings)\\n+    internal_model.save_pretrained(**save_pretrained_settings)\\n \\n     # Revert config back\\n     original_model = model\\n',\n",
       " '@@ -179,7 +179,10 @@ pass\\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-    out = fast_gemv(X, W, W_quant, out = out)\\n+    if W_quant is None:\\n+        out = torch.matmul(X, W.t())\\n+    else:\\n+        out = fast_gemv(X, W, W_quant, out = out)\\n     if lora_A is not None:\\n \\n         # Save LoRAs for inference to stop data movement costs\\n',\n",
       " '@@ -489,7 +489,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif True:#self.training:\\n+    elif self.training:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n',\n",
       " '@@ -258,11 +258,19 @@ def unsloth_save_model(\\n         \"private\"         : save_pretrained_settings[\"private\"],\\n         \"token\"           : save_pretrained_settings[\"token\"],\\n     }\\n-    \\n+\\n+    # Check if PEFT Model or not - if yes, 3 levels. If not 2 levels.\\n+    from peft import PeftModelForCausalLM\\n+    if isinstance(model, PeftModelForCausalLM):\\n+        internal_model = model.model\\n+    else:\\n+        internal_model = model\\n+    pass\\n+        \\n+    # Cannot be converted properly!\\n     if (save_method == \"merged_4bit\") or (save_method == \"lora\") or (\\n         not hasattr(model, \"model\") or \\\\\\n-        not hasattr(model.model, \"model\") or \\\\\\n-        not hasattr(model.model.model, \"layers\")\\n+        not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n         \\n@@ -343,12 +351,12 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = model.model.model.embed_tokens.weight.data\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n     from tqdm import tqdm as ProgressBar\\n-    for j, layer in enumerate(ProgressBar(model.model.model.layers)):\\n+    for j, layer in enumerate(ProgressBar(internal_model.model.layers)):\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n@@ -375,8 +383,8 @@ def unsloth_save_model(\\n         pass\\n     pass\\n \\n-    state_dict[\"model.norm.weight\"] = model.model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = model.model.lm_head.weight.data\\n+    state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n+    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -418,7 +426,7 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    model.model.save_pretrained(**save_pretrained_settings)\\n+    internal_model.save_pretrained(**save_pretrained_settings)\\n \\n     # Revert config back\\n     original_model = model\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n',\n",
       " '@@ -234,7 +234,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +350,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n',\n",
       " '@@ -36,9 +36,9 @@ huggingface = [\\n     \"transformers>=4.37.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n-    \"accelerate\",\\n+    \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n-    \"peft\",\\n+    \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n ]\\n',\n",
       " '@@ -90,6 +90,8 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)\\n         g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\\n+        # f = torch.nn.functional.silu(e)\\n+        # h = f * g\\n         h = swiglu_fg_kernel(e, g)\\n         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)\\n \\n@@ -103,6 +105,7 @@ class LoRA_MLP(torch.autograd.Function):\\n         return i\\n     pass\\n \\n+\\n     @staticmethod\\n     @torch.cuda.amp.custom_bwd\\n     def backward(ctx, dY : torch.Tensor):\\n@@ -121,11 +124,16 @@ class LoRA_MLP(torch.autograd.Function):\\n         g  = g .view(-1, g .shape[-1])\\n         dtype = X.dtype\\n \\n-        # DW_f   = (D @ W.T * f)\\n-        # DW_dfg = (D @ W.T * df * g)\\n         DW = matmul_lora(dY, downW.t(), downW_quant, downB, downA, downS)\\n+        # e = e.float()\\n+        # se = 1.0 / (1.0 + torch.exp(-e))\\n+        # f = (se * e).to(dtype)\\n+        # h = f * g\\n+        # df = DW * f\\n+        # dg = DW * g\\n+        # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n         DW, e, g = swiglu_DWf_DW_dfg_kernel(DW, e, g)\\n-        h, DW_f, DW_dfg = DW, e, g\\n+        h, df, de = DW, e, g\\n \\n         # Down projection LoRA weights\\n         d_downA = h.t() @ (dY @ downB.t())\\n@@ -134,31 +142,29 @@ class LoRA_MLP(torch.autograd.Function):\\n         d_downB *= downS\\n \\n         # Up projection LoRA weights\\n-        d_upA   = X.t() @ (DW_f @ upB.t())\\n-        d_upB   = (upA.t() @ X.t()) @ DW_f\\n+        d_upA   = X.t() @ (df @ upB.t())\\n+        d_upB   = (upA.t() @ X.t()) @ df\\n         d_upA  *= upS\\n         d_upB  *= upS\\n \\n         # Gate projection LoRA weights\\n-        d_gateA = X.t() @ (DW_dfg @ gateB.t())\\n-        d_gateB = (gateA.t() @ X.t()) @ DW_dfg\\n+        d_gateA = X.t() @ (de @ gateB.t())\\n+        d_gateB = (gateA.t() @ X.t()) @ de\\n         d_gateA *= gateS\\n         d_gateB *= gateS\\n \\n-        # Final derivatives to backpropagate backwards.\\n-        # See our blogpost for more details.\\n-        # (D @ W.T * f) @ U.T\\n+        # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)\\n+        # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)\\n+\\n         upW = fast_dequantize(upW.t(), upW_quant)\\n-        # (D @ W.T * f) @ (U.T + B.T @ A.T)\\n-        dX = torch.matmul(DW_f, upW.t(), out = X)\\n+        dX = torch.matmul(df, upW.t(), out = X)\\n         del upW\\n-        dX += DW_f @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())\\n+        dX += df @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())\\n \\n-        # And add the derivative for the gate projection\\n         gateW = fast_dequantize(gateW.t(), gateW_quant)\\n-        dX += DW_dfg @ gateW.t()\\n+        dX += de @ gateW.t()\\n         del gateW\\n-        dX += DW_dfg @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())\\n+        dX += de @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())\\n \\n         # gateW, gateW_quant, gateA, gateB, gateS,\\n         #  upW,    upW_quant,   upA,   upB,   upS,\\n@@ -172,6 +178,11 @@ pass\\n \\n \\n def apply_lora_mlp(self, X):\\n+    # gate = self.gate_proj(X)\\n+    # up   = self.  up_proj(X)\\n+    # h = torch.nn.functional.silu(gate) * up\\n+    # down = self.down_proj(h)\\n+    # return down\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n',\n",
       " '@@ -28,7 +28,7 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = e_row * tl.sigmoid(e_row) # e_row / (1 + tl.exp(-e_row))\\n     f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n@@ -50,30 +50,43 @@ pass\\n \\n @triton.jit\\n def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    e = e.float()\\n+    se = 1.0 / (1.0 + torch.exp(-e))\\n+    f = (se * e).to(dtype)\\n+    h = f * g\\n+    df = DW * f\\n+    dg = DW * g\\n+    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n+    \"\"\"\\n     block_idx = tl.program_id(0)\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n     DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n     g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n-    # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n-    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n-    # f = e * se\\n-    f_row = e_row * se_row\\n+    # e = e.float()\\n+    # se = 1.0 / (1.0 + torch.exp(-e))\\n+    se_row = tl.sigmoid(e_row) # 1.0 / (1.0 + tl.exp(-e_row))\\n+    # f = (se * e).to(dtype)\\n+    f_row = se_row * e_row\\n+    f_row = f_row.to(DW_row.dtype)\\n     # h = f * g\\n-    h_row = f_row * g_row\\n-    # DW_f = DW * f\\n-    DWf_row = DW_row * f_row\\n-    # DW_dfg = DW * (se*(g - h) + h)\\n-    DW_dfg_row = DW_row * (se_row*(g_row - h_row) + h_row)\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+    # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n+    de_row = dg_row.to(tl.float32) * se_row * (1.0 + e_row * (1.0 - se_row))\\n+    de_row = de_row.to(DW_row.dtype)\\n \\n     # Store derivatives in buffers\\n-    tl.store(DW + offsets, h_row,      mask = mask)\\n-    tl.store(e  + offsets, DWf_row,    mask = mask)\\n-    tl.store(g  + offsets, DW_dfg_row, mask = mask)\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n pass\\n \\n \\n',\n",
       " '@@ -14,6 +14,7 @@\\n \\n from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n@@ -72,11 +73,15 @@ pass\\n \\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n+\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n-        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n-        W = fast_dequantize(W, quant_state).to(torch.float32).t()\\n+        if quant_state is not None:\\n+            dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n+            W = fast_dequantize(W, quant_state)\\n+        pass\\n+        W = W.to(torch.float32).t()\\n \\n         if A is not None:\\n             sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n@@ -84,7 +89,6 @@ def _merge_lora(layer, name):\\n             if not torch.isfinite(W).all():\\n                 raise ValueError(f\"Unsloth: Merge failed.\\\\n{name} has some elements = infinity.\")\\n         pass\\n-        \\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n',\n",
       " '@@ -36,9 +36,9 @@ huggingface = [\\n     \"transformers>=4.37.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n-    \"accelerate\",\\n+    \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n-    \"peft\",\\n+    \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n ]\\n',\n",
       " '@@ -90,6 +90,8 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)\\n         g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\\n+        # f = torch.nn.functional.silu(e)\\n+        # h = f * g\\n         h = swiglu_fg_kernel(e, g)\\n         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)\\n \\n@@ -103,6 +105,7 @@ class LoRA_MLP(torch.autograd.Function):\\n         return i\\n     pass\\n \\n+\\n     @staticmethod\\n     @torch.cuda.amp.custom_bwd\\n     def backward(ctx, dY : torch.Tensor):\\n@@ -121,11 +124,16 @@ class LoRA_MLP(torch.autograd.Function):\\n         g  = g .view(-1, g .shape[-1])\\n         dtype = X.dtype\\n \\n-        # DW_f   = (D @ W.T * f)\\n-        # DW_dfg = (D @ W.T * df * g)\\n         DW = matmul_lora(dY, downW.t(), downW_quant, downB, downA, downS)\\n+        # e = e.float()\\n+        # se = 1.0 / (1.0 + torch.exp(-e))\\n+        # f = (se * e).to(dtype)\\n+        # h = f * g\\n+        # df = DW * f\\n+        # dg = DW * g\\n+        # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n         DW, e, g = swiglu_DWf_DW_dfg_kernel(DW, e, g)\\n-        h, DW_f, DW_dfg = DW, e, g\\n+        h, df, de = DW, e, g\\n \\n         # Down projection LoRA weights\\n         d_downA = h.t() @ (dY @ downB.t())\\n@@ -134,31 +142,29 @@ class LoRA_MLP(torch.autograd.Function):\\n         d_downB *= downS\\n \\n         # Up projection LoRA weights\\n-        d_upA   = X.t() @ (DW_f @ upB.t())\\n-        d_upB   = (upA.t() @ X.t()) @ DW_f\\n+        d_upA   = X.t() @ (df @ upB.t())\\n+        d_upB   = (upA.t() @ X.t()) @ df\\n         d_upA  *= upS\\n         d_upB  *= upS\\n \\n         # Gate projection LoRA weights\\n-        d_gateA = X.t() @ (DW_dfg @ gateB.t())\\n-        d_gateB = (gateA.t() @ X.t()) @ DW_dfg\\n+        d_gateA = X.t() @ (de @ gateB.t())\\n+        d_gateB = (gateA.t() @ X.t()) @ de\\n         d_gateA *= gateS\\n         d_gateB *= gateS\\n \\n-        # Final derivatives to backpropagate backwards.\\n-        # See our blogpost for more details.\\n-        # (D @ W.T * f) @ U.T\\n+        # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)\\n+        # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)\\n+\\n         upW = fast_dequantize(upW.t(), upW_quant)\\n-        # (D @ W.T * f) @ (U.T + B.T @ A.T)\\n-        dX = torch.matmul(DW_f, upW.t(), out = X)\\n+        dX = torch.matmul(df, upW.t(), out = X)\\n         del upW\\n-        dX += DW_f @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())\\n+        dX += df @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())\\n \\n-        # And add the derivative for the gate projection\\n         gateW = fast_dequantize(gateW.t(), gateW_quant)\\n-        dX += DW_dfg @ gateW.t()\\n+        dX += de @ gateW.t()\\n         del gateW\\n-        dX += DW_dfg @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())\\n+        dX += de @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())\\n \\n         # gateW, gateW_quant, gateA, gateB, gateS,\\n         #  upW,    upW_quant,   upA,   upB,   upS,\\n@@ -172,6 +178,11 @@ pass\\n \\n \\n def apply_lora_mlp(self, X):\\n+    # gate = self.gate_proj(X)\\n+    # up   = self.  up_proj(X)\\n+    # h = torch.nn.functional.silu(gate) * up\\n+    # down = self.down_proj(h)\\n+    # return down\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n',\n",
       " '@@ -28,7 +28,7 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = e_row * tl.sigmoid(e_row) # e_row / (1 + tl.exp(-e_row))\\n     f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n@@ -50,30 +50,43 @@ pass\\n \\n @triton.jit\\n def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    e = e.float()\\n+    se = 1.0 / (1.0 + torch.exp(-e))\\n+    f = (se * e).to(dtype)\\n+    h = f * g\\n+    df = DW * f\\n+    dg = DW * g\\n+    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n+    \"\"\"\\n     block_idx = tl.program_id(0)\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n     DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n     g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n-    # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n-    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n-    # f = e * se\\n-    f_row = e_row * se_row\\n+    # e = e.float()\\n+    # se = 1.0 / (1.0 + torch.exp(-e))\\n+    se_row = tl.sigmoid(e_row) # 1.0 / (1.0 + tl.exp(-e_row))\\n+    # f = (se * e).to(dtype)\\n+    f_row = se_row * e_row\\n+    f_row = f_row.to(DW_row.dtype)\\n     # h = f * g\\n-    h_row = f_row * g_row\\n-    # DW_f = DW * f\\n-    DWf_row = DW_row * f_row\\n-    # DW_dfg = DW * (se*(g - h) + h)\\n-    DW_dfg_row = DW_row * (se_row*(g_row - h_row) + h_row)\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+    # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n+    de_row = dg_row.to(tl.float32) * se_row * (1.0 + e_row * (1.0 - se_row))\\n+    de_row = de_row.to(DW_row.dtype)\\n \\n     # Store derivatives in buffers\\n-    tl.store(DW + offsets, h_row,      mask = mask)\\n-    tl.store(e  + offsets, DWf_row,    mask = mask)\\n-    tl.store(g  + offsets, DW_dfg_row, mask = mask)\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n pass\\n \\n \\n',\n",
       " '@@ -14,6 +14,7 @@\\n \\n from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n@@ -72,11 +73,15 @@ pass\\n \\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n+\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n-        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n-        W = fast_dequantize(W, quant_state).to(torch.float32).t()\\n+        if quant_state is not None:\\n+            dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n+            W = fast_dequantize(W, quant_state)\\n+        pass\\n+        W = W.to(torch.float32).t()\\n \\n         if A is not None:\\n             sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n@@ -84,7 +89,6 @@ def _merge_lora(layer, name):\\n             if not torch.isfinite(W).all():\\n                 raise ValueError(f\"Unsloth: Merge failed.\\\\n{name} has some elements = infinity.\")\\n         pass\\n-        \\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n',\n",
       " '@@ -36,9 +36,9 @@ huggingface = [\\n     \"transformers>=4.37.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n-    \"accelerate\",\\n+    \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n-    \"peft\",\\n+    \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n ]\\n',\n",
       " '@@ -90,6 +90,8 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)\\n         g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\\n+        # f = torch.nn.functional.silu(e)\\n+        # h = f * g\\n         h = swiglu_fg_kernel(e, g)\\n         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)\\n \\n@@ -103,6 +105,7 @@ class LoRA_MLP(torch.autograd.Function):\\n         return i\\n     pass\\n \\n+\\n     @staticmethod\\n     @torch.cuda.amp.custom_bwd\\n     def backward(ctx, dY : torch.Tensor):\\n@@ -121,11 +124,16 @@ class LoRA_MLP(torch.autograd.Function):\\n         g  = g .view(-1, g .shape[-1])\\n         dtype = X.dtype\\n \\n-        # DW_f   = (D @ W.T * f)\\n-        # DW_dfg = (D @ W.T * df * g)\\n         DW = matmul_lora(dY, downW.t(), downW_quant, downB, downA, downS)\\n+        # e = e.float()\\n+        # se = 1.0 / (1.0 + torch.exp(-e))\\n+        # f = (se * e).to(dtype)\\n+        # h = f * g\\n+        # df = DW * f\\n+        # dg = DW * g\\n+        # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n         DW, e, g = swiglu_DWf_DW_dfg_kernel(DW, e, g)\\n-        h, DW_f, DW_dfg = DW, e, g\\n+        h, df, de = DW, e, g\\n \\n         # Down projection LoRA weights\\n         d_downA = h.t() @ (dY @ downB.t())\\n@@ -134,31 +142,29 @@ class LoRA_MLP(torch.autograd.Function):\\n         d_downB *= downS\\n \\n         # Up projection LoRA weights\\n-        d_upA   = X.t() @ (DW_f @ upB.t())\\n-        d_upB   = (upA.t() @ X.t()) @ DW_f\\n+        d_upA   = X.t() @ (df @ upB.t())\\n+        d_upB   = (upA.t() @ X.t()) @ df\\n         d_upA  *= upS\\n         d_upB  *= upS\\n \\n         # Gate projection LoRA weights\\n-        d_gateA = X.t() @ (DW_dfg @ gateB.t())\\n-        d_gateB = (gateA.t() @ X.t()) @ DW_dfg\\n+        d_gateA = X.t() @ (de @ gateB.t())\\n+        d_gateB = (gateA.t() @ X.t()) @ de\\n         d_gateA *= gateS\\n         d_gateB *= gateS\\n \\n-        # Final derivatives to backpropagate backwards.\\n-        # See our blogpost for more details.\\n-        # (D @ W.T * f) @ U.T\\n+        # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)\\n+        # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)\\n+\\n         upW = fast_dequantize(upW.t(), upW_quant)\\n-        # (D @ W.T * f) @ (U.T + B.T @ A.T)\\n-        dX = torch.matmul(DW_f, upW.t(), out = X)\\n+        dX = torch.matmul(df, upW.t(), out = X)\\n         del upW\\n-        dX += DW_f @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())\\n+        dX += df @ upB.to(dtype).t() @ (upS * upA.to(dtype).t())\\n \\n-        # And add the derivative for the gate projection\\n         gateW = fast_dequantize(gateW.t(), gateW_quant)\\n-        dX += DW_dfg @ gateW.t()\\n+        dX += de @ gateW.t()\\n         del gateW\\n-        dX += DW_dfg @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())\\n+        dX += de @ gateB.to(dtype).t() @ (gateS * gateA.to(dtype).t())\\n \\n         # gateW, gateW_quant, gateA, gateB, gateS,\\n         #  upW,    upW_quant,   upA,   upB,   upS,\\n@@ -172,6 +178,11 @@ pass\\n \\n \\n def apply_lora_mlp(self, X):\\n+    # gate = self.gate_proj(X)\\n+    # up   = self.  up_proj(X)\\n+    # h = torch.nn.functional.silu(gate) * up\\n+    # down = self.down_proj(h)\\n+    # return down\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n',\n",
       " '@@ -28,7 +28,7 @@ def _fg_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     # f = e * sigmoid(e)\\n-    f_row = e_row / (1 + tl.exp(-e_row))\\n+    f_row = e_row * tl.sigmoid(e_row) # e_row / (1 + tl.exp(-e_row))\\n     f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n     # h = f * g\\n     h_row = f_row * g_row\\n@@ -50,30 +50,43 @@ pass\\n \\n @triton.jit\\n def _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    e = e.float()\\n+    se = 1.0 / (1.0 + torch.exp(-e))\\n+    f = (se * e).to(dtype)\\n+    h = f * g\\n+    df = DW * f\\n+    dg = DW * g\\n+    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n+    \"\"\"\\n     block_idx = tl.program_id(0)\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n \\n     DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n-    e_row  = tl.load(e  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n     g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n-    # f = e * sigmoid(e)\\n-    se_row = 1 / (1 + tl.exp(-e_row.to(tl.float32)))\\n-    se_row = se_row.to(e_row.dtype) # Exact copy from HF\\n-    # f = e * se\\n-    f_row = e_row * se_row\\n+    # e = e.float()\\n+    # se = 1.0 / (1.0 + torch.exp(-e))\\n+    se_row = tl.sigmoid(e_row) # 1.0 / (1.0 + tl.exp(-e_row))\\n+    # f = (se * e).to(dtype)\\n+    f_row = se_row * e_row\\n+    f_row = f_row.to(DW_row.dtype)\\n     # h = f * g\\n-    h_row = f_row * g_row\\n-    # DW_f = DW * f\\n-    DWf_row = DW_row * f_row\\n-    # DW_dfg = DW * (se*(g - h) + h)\\n-    DW_dfg_row = DW_row * (se_row*(g_row - h_row) + h_row)\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+    # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n+    de_row = dg_row.to(tl.float32) * se_row * (1.0 + e_row * (1.0 - se_row))\\n+    de_row = de_row.to(DW_row.dtype)\\n \\n     # Store derivatives in buffers\\n-    tl.store(DW + offsets, h_row,      mask = mask)\\n-    tl.store(e  + offsets, DWf_row,    mask = mask)\\n-    tl.store(g  + offsets, DW_dfg_row, mask = mask)\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n pass\\n \\n \\n',\n",
       " '@@ -14,6 +14,7 @@\\n \\n from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n+from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n@@ -72,11 +73,15 @@ pass\\n \\n \\n def _merge_lora(layer, name):\\n-    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit)):\\n+\\n+    if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n         W, quant_state, A, B, s = get_lora_parameters(layer)\\n-        dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n-        W = fast_dequantize(W, quant_state).to(torch.float32).t()\\n+        if quant_state is not None:\\n+            dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n+            W = fast_dequantize(W, quant_state)\\n+        pass\\n+        W = W.to(torch.float32).t()\\n \\n         if A is not None:\\n             sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n@@ -84,7 +89,6 @@ def _merge_lora(layer, name):\\n             if not torch.isfinite(W).all():\\n                 raise ValueError(f\"Unsloth: Merge failed.\\\\n{name} has some elements = infinity.\")\\n         pass\\n-        \\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n',\n",
       " '@@ -80,7 +80,8 @@ def _merge_lora(layer, name):\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n-        pass\\n+        else:\\n+            dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n \\n         if A is not None:\\n',\n",
       " '@@ -80,7 +80,8 @@ def _merge_lora(layer, name):\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n-        pass\\n+        else:\\n+            dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n \\n         if A is not None:\\n',\n",
       " '@@ -80,7 +80,8 @@ def _merge_lora(layer, name):\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n-        pass\\n+        else:\\n+            dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n \\n         if A is not None:\\n',\n",
       " '@@ -101,10 +101,13 @@ pass\\n \\n \\n def PatchDPOTrainer():\\n-    # Patch DPO notebook printing\\n-    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n-    from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n-    DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n-    DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    from transformers.trainer import is_in_notebook\\n+    if is_in_notebook():\\n+        # Patch DPO notebook printing\\n+        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n+        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n+        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    pass\\n pass\\n \\n',\n",
       " '@@ -486,6 +486,15 @@ def LlamaModel_fast_forward(\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n+    # Fix up attention mask by setting elements to 0\\n+    # Specifically for DPO\\n+    if self._has_no_labels and attention_mask is not None:\\n+        inputs_requires_grad = inputs_embeds.requires_grad\\n+        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n+        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+    pass\\n+\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n@@ -617,6 +626,7 @@ def LlamaForCausalLM_fast_forward(\\n     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n     outputs = self.model(\\n         input_ids=input_ids,\\n         causal_mask=causal_mask,\\n@@ -726,7 +736,7 @@ class FastLlamaModel:\\n            f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        logger.warning_once(statistics)\\n+        print(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -826,6 +836,9 @@ class FastLlamaModel:\\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n+        # Add save modules\\n+        patch_saving_functions(model)\\n+\\n         return model, tokenizer\\n     pass\\n \\n',\n",
       " '@@ -195,6 +195,7 @@ def MistralForCausalLM_fast_forward(\\n     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n     outputs = self.model(\\n         input_ids=input_ids,\\n         causal_mask=causal_mask,\\n@@ -282,7 +283,7 @@ class FastMistralModel(FastLlamaModel):\\n            f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n-        logger.warning_once(statistics)\\n+        print(statistics)\\n         FastMistralModel.pre_patch()\\n \\n         if dtype is None:\\n',\n",
       " '@@ -278,7 +278,7 @@ def unsloth_save_model(\\n         not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n-        \\n+        print(type(model))\\n         # Edit save_pretrained_settings\\n         # [TODO] _create_repo has errors due to **kwargs getting accepted\\n         for deletion in \\\\\\n@@ -483,7 +483,7 @@ def install_llama_cpp_make_non_blocking():\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n-    full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    full_command = [\"make\", \"all\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n pass\\n@@ -499,7 +499,7 @@ pass\\n def install_llama_cpp_blocking():\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j {psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -515,6 +515,7 @@ pass\\n def save_to_gguf(\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n+    first_conversion     : str = \"f16\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n@@ -539,6 +540,16 @@ def save_to_gguf(\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n+    # Check first_conversion format\\n+    if   first_conversion == \"f16\" : pass\\n+    elif first_conversion == \"f32\" : pass\\n+    elif first_conversion == \"q8_0\": pass\\n+    else:\\n+        raise RuntimeError(\\n+            f\"Unsloth: `first_conversion` can only be one of [\\'f16\\', \\'f32\\', \\'q8_0\\'] and not `{first_conversion}`.\"\\n+        )\\n+    pass\\n+\\n     print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n     if _run_installer is not None:\\n         _run_installer.wait()\\n@@ -546,11 +557,19 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n \\n-    print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n-    first_conversion = \"f16\"\\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n     elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    else:\\n+        # Quantized models must have f16 as the default argument\\n+        if   first_conversion == \"f32\" : pass\\n+        elif first_conversion == \"f16\" : pass\\n+        elif first_conversion == \"q8_0\":\\n+            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n+            first_conversion = \"f16\"\\n+        pass\\n+    pass\\n+    print(f\"Unsloth: [1] Converting HF into {first_conversion} GGUF format. This will take 3 minutes...\")\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -566,6 +585,17 @@ def save_to_gguf(\\n             print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n     pass\\n \\n+    # Check if quantization succeeded!\\n+    if not os.path.isfile(final_location):\\n+        raise RuntimeError(\\n+            \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+            \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+            \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+            \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"Once that\\'s done, redo the quantization.\"\\n+        )\\n+    pass\\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n     if quantization_method != first_conversion:\\n@@ -581,6 +611,19 @@ def save_to_gguf(\\n             for line in sp.stderr:\\n                 print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n         pass\\n+\\n+        # Check if quantization succeeded!\\n+        if not os.path.isfile(final_location):\\n+            raise RuntimeError(\\n+                \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"Once that\\'s done, redo the quantization.\"\\n+            )\\n+        pass\\n+\\n         print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n     pass\\n \\n@@ -765,6 +808,7 @@ def unsloth_save_pretrained_gguf(\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n     quantization_method  : str = \"fast_quantized\",\\n+    first_conversion     : str = \"f16\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -813,6 +857,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"quantization_method\"]\\n+    del arguments[\"first_conversion\"]\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n@@ -840,7 +885,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -861,6 +906,7 @@ def unsloth_push_to_hub_gguf(\\n     repo_id              : str,\\n     tokenizer            = None,\\n     quantization_method  : str = \"fast_quantized\",\\n+    first_conversion     : str = \"f16\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -911,6 +957,7 @@ def unsloth_push_to_hub_gguf(\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n     del arguments[\"quantization_method\"]\\n+    del arguments[\"first_conversion\"]\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n@@ -938,7 +985,7 @@ def unsloth_push_to_hub_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n@@ -960,6 +1007,23 @@ def patch_saving_functions(model):\\n \\n     if hasattr(model, \"_original_push_to_hub\"): return\\n \\n+    # First check if this has already been called, and revert it\\n+    original_model = model\\n+    while True:\\n+        if hasattr(original_model, \"_original_push_to_hub\"):\\n+            original_model.push_to_hub = original_model._original_push_to_hub\\n+            del original_model._original_push_to_hub\\n+            if hasattr(original_model, \"push_to_hub_merged\"):     del original_model.push_to_hub_merged\\n+            if hasattr(original_model, \"save_pretrained_merged\"): del original_model.save_pretrained_merged\\n+            if hasattr(original_model, \"push_to_hub_gguf\"):       del original_model.push_to_hub_gguf\\n+            if hasattr(original_model, \"save_pretrained_gguf\"):   del original_model.save_pretrained_gguf\\n+        pass\\n+\\n+        if hasattr(original_model, \"model\"): original_model = original_model.model\\n+        else: break\\n+    pass\\n+\\n+    # And now re add our saving methods!\\n     original_push_to_hub = model.push_to_hub\\n     signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n     signature = signature[1:]\\n@@ -988,49 +1052,29 @@ def patch_saving_functions(model):\\n     pass\\n     \\'\\'\\'\\n     exec(push_to_hub_text, globals())\\n-    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)\\n \\n-    if hasattr(model, \"add_model_tags\"):\\n-        model.add_model_tags([\"unsloth\",])\\n+    original_model = model\\n+    while True:\\n+\\n+        if not hasattr(original_model, \"_original_push_to_hub\"):\\n+            original_model._original_push_to_hub = original_model.push_to_hub\\n+            original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n+\\n+            if hasattr(original_model, \"add_model_tags\"):\\n+                original_model.add_model_tags([\"unsloth\",])\\n+        pass\\n \\n+        if hasattr(original_model, \"model\"): original_model = original_model.model\\n+        else: break\\n+    pass\\n+\\n+    # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n         model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n         model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n         model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n         model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n-    else:\\n-        model.push_to_hub_merged     = model.push_to_hub\\n-        model.save_pretrained_merged = model.save_pretrained\\n-        model.push_to_hub_gguf       = model.push_to_hub\\n-        model.save_pretrained_gguf   = model.save_pretrained\\n-    pass\\n-\\n-    original_model = model\\n-    while hasattr(original_model, \"model\"):\\n-        original_model = original_model.model\\n-        if hasattr(original_model, \"_original_push_to_hub\"): continue\\n-        \\n-        original_model._original_push_to_hub = original_model.push_to_hub\\n-        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n-\\n-        if hasattr(original_model, \"add_model_tags\"):\\n-            original_model.add_model_tags([\"unsloth\",])\\n-\\n-        if hasattr(original_model, \"config\"):\\n-            # Counteract tokenizers\\n-            original_model.push_to_hub_merged     = \\\\\\n-                types.MethodType(unsloth_push_to_hub_merged,     original_model)\\n-\\n-            original_model.save_pretrained_merged = \\\\\\n-                types.MethodType(unsloth_save_pretrained_merged, original_model)\\n-\\n-            original_model.push_to_hub_gguf       = \\\\\\n-                types.MethodType(unsloth_push_to_hub_gguf,       original_model)\\n-\\n-            original_model.save_pretrained_gguf   = \\\\\\n-                types.MethodType(unsloth_save_pretrained_gguf,   original_model)\\n-        pass\\n     pass\\n-    return\\n+    return model\\n pass\\n',\n",
       " '@@ -101,10 +101,13 @@ pass\\n \\n \\n def PatchDPOTrainer():\\n-    # Patch DPO notebook printing\\n-    NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n-    from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n-    DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n-    DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    from transformers.trainer import is_in_notebook\\n+    if is_in_notebook():\\n+        # Patch DPO notebook printing\\n+        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n+        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n+        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    pass\\n pass\\n \\n',\n",
       " '@@ -486,6 +486,15 @@ def LlamaModel_fast_forward(\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n+    # Fix up attention mask by setting elements to 0\\n+    # Specifically for DPO\\n+    if self._has_no_labels and attention_mask is not None:\\n+        inputs_requires_grad = inputs_embeds.requires_grad\\n+        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n+        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+    pass\\n+\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n@@ -617,6 +626,7 @@ def LlamaForCausalLM_fast_forward(\\n     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n     outputs = self.model(\\n         input_ids=input_ids,\\n         causal_mask=causal_mask,\\n@@ -726,7 +736,7 @@ class FastLlamaModel:\\n            f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        logger.warning_once(statistics)\\n+        print(statistics)\\n         FastLlamaModel.pre_patch()\\n \\n         if dtype is None:\\n@@ -826,6 +836,9 @@ class FastLlamaModel:\\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n+        # Add save modules\\n+        patch_saving_functions(model)\\n+\\n         return model, tokenizer\\n     pass\\n \\n',\n",
       " '@@ -195,6 +195,7 @@ def MistralForCausalLM_fast_forward(\\n     return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n     outputs = self.model(\\n         input_ids=input_ids,\\n         causal_mask=causal_mask,\\n@@ -282,7 +283,7 @@ class FastMistralModel(FastLlamaModel):\\n            f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n-        logger.warning_once(statistics)\\n+        print(statistics)\\n         FastMistralModel.pre_patch()\\n \\n         if dtype is None:\\n',\n",
       " '@@ -278,7 +278,7 @@ def unsloth_save_model(\\n         not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n-        \\n+        print(type(model))\\n         # Edit save_pretrained_settings\\n         # [TODO] _create_repo has errors due to **kwargs getting accepted\\n         for deletion in \\\\\\n@@ -483,7 +483,7 @@ def install_llama_cpp_make_non_blocking():\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n-    full_command = [\"make\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    full_command = [\"make\", \"all\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n pass\\n@@ -499,7 +499,7 @@ pass\\n def install_llama_cpp_blocking():\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j {psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j {psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -515,6 +515,7 @@ pass\\n def save_to_gguf(\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n+    first_conversion     : str = \"f16\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n@@ -539,6 +540,16 @@ def save_to_gguf(\\n         f\\' \"-____-\"     In total, you will have to wait around 26 minutes.\\\\n\\'\\n     print(print_info)\\n \\n+    # Check first_conversion format\\n+    if   first_conversion == \"f16\" : pass\\n+    elif first_conversion == \"f32\" : pass\\n+    elif first_conversion == \"q8_0\": pass\\n+    else:\\n+        raise RuntimeError(\\n+            f\"Unsloth: `first_conversion` can only be one of [\\'f16\\', \\'f32\\', \\'q8_0\\'] and not `{first_conversion}`.\"\\n+        )\\n+    pass\\n+\\n     print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n     if _run_installer is not None:\\n         _run_installer.wait()\\n@@ -546,11 +557,19 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n \\n-    print(\"Unsloth: [1] Converting HF into GGUF format. This will take 3 minutes...\")\\n-    first_conversion = \"f16\"\\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n     elif quantization_method == \"q8_0\": first_conversion = \"q8_0\"\\n+    else:\\n+        # Quantized models must have f16 as the default argument\\n+        if   first_conversion == \"f32\" : pass\\n+        elif first_conversion == \"f16\" : pass\\n+        elif first_conversion == \"q8_0\":\\n+            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n+            first_conversion = \"f16\"\\n+        pass\\n+    pass\\n+    print(f\"Unsloth: [1] Converting HF into {first_conversion} GGUF format. This will take 3 minutes...\")\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n@@ -566,6 +585,17 @@ def save_to_gguf(\\n             print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n     pass\\n \\n+    # Check if quantization succeeded!\\n+    if not os.path.isfile(final_location):\\n+        raise RuntimeError(\\n+            \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+            \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+            \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+            \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"Once that\\'s done, redo the quantization.\"\\n+        )\\n+    pass\\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n     if quantization_method != first_conversion:\\n@@ -581,6 +611,19 @@ def save_to_gguf(\\n             for line in sp.stderr:\\n                 print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n         pass\\n+\\n+        # Check if quantization succeeded!\\n+        if not os.path.isfile(final_location):\\n+            raise RuntimeError(\\n+                \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"Once that\\'s done, redo the quantization.\"\\n+            )\\n+        pass\\n+\\n         print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n     pass\\n \\n@@ -765,6 +808,7 @@ def unsloth_save_pretrained_gguf(\\n     save_directory       : Union[str, os.PathLike],\\n     tokenizer            = None,\\n     quantization_method  : str = \"fast_quantized\",\\n+    first_conversion     : str = \"f16\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n     is_main_process      : bool = True,\\n@@ -813,6 +857,7 @@ def unsloth_save_pretrained_gguf(\\n     arguments[\"save_method\"] = \"merged_16bit\" # Must be 16bit\\n     del arguments[\"self\"]\\n     del arguments[\"quantization_method\"]\\n+    del arguments[\"first_conversion\"]\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n@@ -840,7 +885,7 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -861,6 +906,7 @@ def unsloth_push_to_hub_gguf(\\n     repo_id              : str,\\n     tokenizer            = None,\\n     quantization_method  : str = \"fast_quantized\",\\n+    first_conversion     : str = \"f16\",\\n     use_temp_dir         : Optional[bool] = None,\\n     commit_message       : Optional[str] = None,\\n     private              : Optional[bool] = None,\\n@@ -911,6 +957,7 @@ def unsloth_push_to_hub_gguf(\\n     del arguments[\"self\"]\\n     del arguments[\"repo_id\"]\\n     del arguments[\"quantization_method\"]\\n+    del arguments[\"first_conversion\"]\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n@@ -938,7 +985,7 @@ def unsloth_push_to_hub_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, makefile)\\n+    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n@@ -960,6 +1007,23 @@ def patch_saving_functions(model):\\n \\n     if hasattr(model, \"_original_push_to_hub\"): return\\n \\n+    # First check if this has already been called, and revert it\\n+    original_model = model\\n+    while True:\\n+        if hasattr(original_model, \"_original_push_to_hub\"):\\n+            original_model.push_to_hub = original_model._original_push_to_hub\\n+            del original_model._original_push_to_hub\\n+            if hasattr(original_model, \"push_to_hub_merged\"):     del original_model.push_to_hub_merged\\n+            if hasattr(original_model, \"save_pretrained_merged\"): del original_model.save_pretrained_merged\\n+            if hasattr(original_model, \"push_to_hub_gguf\"):       del original_model.push_to_hub_gguf\\n+            if hasattr(original_model, \"save_pretrained_gguf\"):   del original_model.save_pretrained_gguf\\n+        pass\\n+\\n+        if hasattr(original_model, \"model\"): original_model = original_model.model\\n+        else: break\\n+    pass\\n+\\n+    # And now re add our saving methods!\\n     original_push_to_hub = model.push_to_hub\\n     signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n     signature = signature[1:]\\n@@ -988,49 +1052,29 @@ def patch_saving_functions(model):\\n     pass\\n     \\'\\'\\'\\n     exec(push_to_hub_text, globals())\\n-    model.push_to_hub = types.MethodType(unsloth_push_to_hub, model)\\n \\n-    if hasattr(model, \"add_model_tags\"):\\n-        model.add_model_tags([\"unsloth\",])\\n+    original_model = model\\n+    while True:\\n+\\n+        if not hasattr(original_model, \"_original_push_to_hub\"):\\n+            original_model._original_push_to_hub = original_model.push_to_hub\\n+            original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n+\\n+            if hasattr(original_model, \"add_model_tags\"):\\n+                original_model.add_model_tags([\"unsloth\",])\\n+        pass\\n \\n+        if hasattr(original_model, \"model\"): original_model = original_model.model\\n+        else: break\\n+    pass\\n+\\n+    # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n         model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n         model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n         model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n         model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n-    else:\\n-        model.push_to_hub_merged     = model.push_to_hub\\n-        model.save_pretrained_merged = model.save_pretrained\\n-        model.push_to_hub_gguf       = model.push_to_hub\\n-        model.save_pretrained_gguf   = model.save_pretrained\\n-    pass\\n-\\n-    original_model = model\\n-    while hasattr(original_model, \"model\"):\\n-        original_model = original_model.model\\n-        if hasattr(original_model, \"_original_push_to_hub\"): continue\\n-        \\n-        original_model._original_push_to_hub = original_model.push_to_hub\\n-        original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n-\\n-        if hasattr(original_model, \"add_model_tags\"):\\n-            original_model.add_model_tags([\"unsloth\",])\\n-\\n-        if hasattr(original_model, \"config\"):\\n-            # Counteract tokenizers\\n-            original_model.push_to_hub_merged     = \\\\\\n-                types.MethodType(unsloth_push_to_hub_merged,     original_model)\\n-\\n-            original_model.save_pretrained_merged = \\\\\\n-                types.MethodType(unsloth_save_pretrained_merged, original_model)\\n-\\n-            original_model.push_to_hub_gguf       = \\\\\\n-                types.MethodType(unsloth_push_to_hub_gguf,       original_model)\\n-\\n-            original_model.save_pretrained_gguf   = \\\\\\n-                types.MethodType(unsloth_save_pretrained_gguf,   original_model)\\n-        pass\\n     pass\\n-    return\\n+    return model\\n pass\\n',\n",
       " '@@ -375,6 +375,9 @@ class FastMistralModel(FastLlamaModel):\\n         \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        # Add save modules\\n+        patch_saving_functions(model)\\n         \\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -278,7 +278,6 @@ def unsloth_save_model(\\n         not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n-        print(type(model))\\n         # Edit save_pretrained_settings\\n         # [TODO] _create_repo has errors due to **kwargs getting accepted\\n         for deletion in \\\\\\n',\n",
       " '@@ -375,6 +375,9 @@ class FastMistralModel(FastLlamaModel):\\n         \\n         # Log Unsloth version for future fastpaths for inference\\n         model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        # Add save modules\\n+        patch_saving_functions(model)\\n         \\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -278,7 +278,6 @@ def unsloth_save_model(\\n         not hasattr(internal_model.model, \"layers\")\\n     ):\\n         # Do general saving\\n-        print(type(model))\\n         # Edit save_pretrained_settings\\n         # [TODO] _create_repo has errors due to **kwargs getting accepted\\n         for deletion in \\\\\\n',\n",
       " '@@ -488,7 +488,10 @@ def LlamaModel_fast_forward(\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and attention_mask is not None:\\n+    if self._has_no_labels and attention_mask is not None and \\\\\\n+        attention_mask.shape[1] == seq_length:\\n+        # Careful for inference the attention_mask is size (1, kv_seq_len)\\n+        # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n',\n",
       " '@@ -488,7 +488,10 @@ def LlamaModel_fast_forward(\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and attention_mask is not None:\\n+    if self._has_no_labels and attention_mask is not None and \\\\\\n+        attention_mask.shape[1] == seq_length:\\n+        # Careful for inference the attention_mask is size (1, kv_seq_len)\\n+        # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n',\n",
       " '@@ -22,4 +22,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n',\n",
       " '@@ -134,9 +134,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = Q.shape[-1]//2\\n         RH_Q = torch.cat((-Q[..., half:], Q[..., :half]), dim = -1)\\n         Q *= cos\\n-        Q.addcmul_(RH_Q, sin)\\n-        # RH_Q *= sin\\n-        # Q += RH_Q\\n+        # Q.addcmul_(RH_Q, sin)\\n+        RH_Q *= sin\\n+        Q += RH_Q\\n         ctx.save_for_backward(cos, sin)\\n         return Q\\n     pass\\n@@ -148,9 +148,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = dY.shape[-1]//2\\n         RH_dY = torch.cat((dY[..., half:], -dY[..., :half]), dim = -1)\\n         dY *= cos\\n-        dY.addcmul_(RH_dY, sin)\\n-        # RH_dY *= sin\\n-        # dY += RH_dY\\n+        # dY.addcmul_(RH_dY, sin)\\n+        RH_dY *= sin\\n+        dY += RH_dY\\n         return dY, None, None, None\\n     pass\\n pass\\n',\n",
       " '@@ -114,11 +114,12 @@ def fast_dequantize(W, quant_state = None, out = None):\\n pass\\n \\n \\n-def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n-    quant_state = W.quant_state\\n-    bsz = 1\\n-    q_len = 1\\n-    hd = X.shape[0]\\n+def fast_gemv(X, W, quant_state, out = None):\\n+    if quant_state is None: return torch.matmul(X, W, out = out)\\n+    # For fast X @ W where seq_len == 1\\n+    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n+    bsz, q_len, hd = X.shape\\n+    assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n         # https://github.com/TimDettmers/bitsandbytes/pull/763/files\\n@@ -137,9 +138,14 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n         offset, state2 = compressed_stats\\n         absmax2, code2, blocksize2, _, _, _, _ = state2\\n     pass\\n+    assert(dtype == X.dtype)\\n     bout = shape[0]\\n-    if out is None: out = torch.empty(bout, dtype = dtype, device = \"cuda\")\\n-    else: assert(out.shape[0] == bout)\\n+\\n+    if out is None:\\n+        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    else:\\n+        assert(out.shape == (bsz, 1, bout,))\\n+    pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -170,30 +176,46 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n     ptr_stats  = get_ptr(stats)\\n     blocksize  = ctypes.c_int32(blocksize)\\n \\n-    fx(m, n, k, get_ptr(X), ptr_W, ptr_absmax, ptr_stats, get_ptr(out),\\n-        lda, ldb, ldc, blocksize)\\n+    for row in range(bsz):\\n+        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n+           lda, ldb, ldc, blocksize)\\n+    pass\\n \\n     return out\\n pass\\n \\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n+\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n+\\n+    bsz, _, in_dim = X.shape\\n+\\n     if W_quant is None:\\n         out = torch.matmul(X, W.t())\\n-    else:\\n+    elif bsz <= 4:\\n+        # Only batches of 4 are faster with Gemv\\n         out = fast_gemv(X, W, W_quant, out = out)\\n-    if lora_A is not None:\\n+    else:\\n+        W = fast_dequantize(W.t(), W_quant)\\n+        out = torch.matmul(X, W, out = out)\\n+    pass\\n \\n-        # Save LoRAs for inference to stop data movement costs\\n-        if not hasattr(lora_A, \"_fast_lora\"):\\n-            dtype = X.dtype\\n-            lora_A._fast_lora = lora_A.to(dtype).t()\\n-            lora_B._fast_lora = lora_B.to(dtype)\\n+    # Add in LoRA weights\\n+    if lora_A is not None:\\n+        out_dim = out.shape[2]\\n+        dtype = X.dtype\\n+        if bsz == 1:\\n+            out = out.view(out_dim)\\n+            temp_lora = torch.mv(lora_A.to(dtype), X.ravel(), out = temp_lora)\\n+            out.addmv_(lora_B.to(dtype), temp_lora, alpha = lora_S)\\n+        else:\\n+            out = out.view(bsz, out_dim)\\n+            temp_lora = torch.mm(X.view(bsz, in_dim), lora_A.to(dtype).t(), out = temp_lora)\\n+            out.addmm_(temp_lora, lora_B.to(dtype).t(), alpha = lora_S)\\n         pass\\n-\\n-        temp_lora = torch.matmul(X, lora_A._fast_lora, out = temp_lora)\\n-        out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)\\n+        out = out.view(bsz, 1, out_dim)\\n     pass\\n+\\n     return out\\n pass\\n',\n",
       " '@@ -69,7 +69,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-def LlamaAttention_fast_forward_inference(\\n+def _LlamaAttention_fast_forward_inference(\\n     self,\\n     hidden_states:  torch.Tensor,\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n@@ -185,11 +185,89 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+def LlamaAttention_fast_forward_inference(\\n+    self,\\n+    hidden_states:  torch.Tensor,\\n+    past_key_value: Optional[Tuple[torch.Tensor]],\\n+    position_ids,\\n+):\\n+    \"\"\"\\n+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n+        Fast inference using KV cache.\\n+        QK^T can be computed in 4 chunks\\n+\\n+        [Q, q] @ [K, k].T where q, k are the new tokens.\\n+        [QK^T, Qk^T]\\n+        [qK^T, qk^T]\\n+\\n+        Since the attention mask wipes Qk^T, we just get\\n+        [QK^T,    0]\\n+        [qK^T, qk^T]\\n+\\n+        Since softmax is row-wise, we get\\n+        softmax([QK^T,    0])\\n+        softmax([qK^T, qk^T])\\n+\\n+        We then multiply by   [V]\\n+                              [v]\\n+        softmax([QK^T,    0]) [softmax(QK^T)V] *\\n+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]\\n+\\n+        But notice * [softmax(QK^T)V] is just the last attention.\\n+        We just need to compute the last final row.\\n+\\n+        This means we can pass in a row of Q, but we need to\\n+        remember K and V, which are called the KV cache.\\n+    \"\"\"\\n+    Xn = hidden_states\\n+    bsz, _, _ = hidden_states.size()\\n+    K1, V1 = past_key_value\\n+\\n+    n_heads    = self.num_heads\\n+    n_groups   = self.num_key_value_groups\\n+    n_kv_heads = self.num_key_value_heads\\n+    head_dim   = self.head_dim\\n+    assert(n_kv_heads * n_groups == n_heads)\\n+\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+\\n+    kv_seq_len = K1.shape[-2] + 1\\n+    cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n+    Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n+    \\n+    # New KV cache\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+\\n+    # Grouped query attention\\n+    if n_groups != 1:\\n+        _, _, cached_len, _ = Kn.shape\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n+\\n+    # Attention\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n+    A *= 1.0 / (self.head_dim**0.5)\\n+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n+    A = torch.matmul(A, Vnn)\\n+    A = A.transpose(1, 2)\\n+    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = original_apply_o(self, A)\\n+    return A, (Kn, Vn)\\n+pass\\n+\\n+\\n torch_silu = torch.nn.functional.silu\\n def fast_mlp_inference(self, X):\\n-    hidden_size = self.hidden_size\\n-    X = X.view(hidden_size)\\n-\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     gate = fast_linear_forward(self.gate_proj, X)\\n@@ -198,20 +276,18 @@ def fast_mlp_inference(self, X):\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n-    down = fast_linear_forward(self.down_proj, gate, out = up[:hidden_size])\\n-    X = down.view(1, 1, hidden_size)\\n-\\n-    return X\\n+    down = fast_linear_forward(self.down_proj, gate)\\n+    return down\\n pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n     old_dtype = X.dtype\\n-    X = X.to(torch.float32)\\n-    variance = X.square().mean(-1, keepdim = True)\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n-    X *= variance.rsqrt_()\\n-    X = X.to(old_dtype)\\n+    XX *= variance.rsqrt_()\\n+    X = XX.to(old_dtype) # Must preserve due to residual\\n     X *= self.weight\\n     return X\\n pass\\n@@ -234,7 +310,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +426,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if past_key_value is not None:\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -488,8 +564,7 @@ def LlamaModel_fast_forward(\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and attention_mask is not None and \\\\\\n-        attention_mask.shape[1] == seq_length:\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -501,7 +576,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif self.training:\\n+    elif False:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n@@ -522,7 +597,7 @@ def LlamaModel_fast_forward(\\n \\n     hidden_states = inputs_embeds\\n \\n-    if self.gradient_checkpointing and self.training:\\n+    if past_key_values is None and self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n                 \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n@@ -581,7 +656,7 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if past_key_values is not None:\\n         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n     else:\\n         hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n@@ -644,7 +719,13 @@ def LlamaForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -210,7 +210,13 @@ def MistralForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -22,4 +22,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n',\n",
       " '@@ -134,9 +134,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = Q.shape[-1]//2\\n         RH_Q = torch.cat((-Q[..., half:], Q[..., :half]), dim = -1)\\n         Q *= cos\\n-        Q.addcmul_(RH_Q, sin)\\n-        # RH_Q *= sin\\n-        # Q += RH_Q\\n+        # Q.addcmul_(RH_Q, sin)\\n+        RH_Q *= sin\\n+        Q += RH_Q\\n         ctx.save_for_backward(cos, sin)\\n         return Q\\n     pass\\n@@ -148,9 +148,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = dY.shape[-1]//2\\n         RH_dY = torch.cat((dY[..., half:], -dY[..., :half]), dim = -1)\\n         dY *= cos\\n-        dY.addcmul_(RH_dY, sin)\\n-        # RH_dY *= sin\\n-        # dY += RH_dY\\n+        # dY.addcmul_(RH_dY, sin)\\n+        RH_dY *= sin\\n+        dY += RH_dY\\n         return dY, None, None, None\\n     pass\\n pass\\n',\n",
       " '@@ -114,11 +114,12 @@ def fast_dequantize(W, quant_state = None, out = None):\\n pass\\n \\n \\n-def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n-    quant_state = W.quant_state\\n-    bsz = 1\\n-    q_len = 1\\n-    hd = X.shape[0]\\n+def fast_gemv(X, W, quant_state, out = None):\\n+    if quant_state is None: return torch.matmul(X, W, out = out)\\n+    # For fast X @ W where seq_len == 1\\n+    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n+    bsz, q_len, hd = X.shape\\n+    assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n         # https://github.com/TimDettmers/bitsandbytes/pull/763/files\\n@@ -137,9 +138,14 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n         offset, state2 = compressed_stats\\n         absmax2, code2, blocksize2, _, _, _, _ = state2\\n     pass\\n+    assert(dtype == X.dtype)\\n     bout = shape[0]\\n-    if out is None: out = torch.empty(bout, dtype = dtype, device = \"cuda\")\\n-    else: assert(out.shape[0] == bout)\\n+\\n+    if out is None:\\n+        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    else:\\n+        assert(out.shape == (bsz, 1, bout,))\\n+    pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -170,30 +176,46 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n     ptr_stats  = get_ptr(stats)\\n     blocksize  = ctypes.c_int32(blocksize)\\n \\n-    fx(m, n, k, get_ptr(X), ptr_W, ptr_absmax, ptr_stats, get_ptr(out),\\n-        lda, ldb, ldc, blocksize)\\n+    for row in range(bsz):\\n+        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n+           lda, ldb, ldc, blocksize)\\n+    pass\\n \\n     return out\\n pass\\n \\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n+\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n+\\n+    bsz, _, in_dim = X.shape\\n+\\n     if W_quant is None:\\n         out = torch.matmul(X, W.t())\\n-    else:\\n+    elif bsz <= 4:\\n+        # Only batches of 4 are faster with Gemv\\n         out = fast_gemv(X, W, W_quant, out = out)\\n-    if lora_A is not None:\\n+    else:\\n+        W = fast_dequantize(W.t(), W_quant)\\n+        out = torch.matmul(X, W, out = out)\\n+    pass\\n \\n-        # Save LoRAs for inference to stop data movement costs\\n-        if not hasattr(lora_A, \"_fast_lora\"):\\n-            dtype = X.dtype\\n-            lora_A._fast_lora = lora_A.to(dtype).t()\\n-            lora_B._fast_lora = lora_B.to(dtype)\\n+    # Add in LoRA weights\\n+    if lora_A is not None:\\n+        out_dim = out.shape[2]\\n+        dtype = X.dtype\\n+        if bsz == 1:\\n+            out = out.view(out_dim)\\n+            temp_lora = torch.mv(lora_A.to(dtype), X.ravel(), out = temp_lora)\\n+            out.addmv_(lora_B.to(dtype), temp_lora, alpha = lora_S)\\n+        else:\\n+            out = out.view(bsz, out_dim)\\n+            temp_lora = torch.mm(X.view(bsz, in_dim), lora_A.to(dtype).t(), out = temp_lora)\\n+            out.addmm_(temp_lora, lora_B.to(dtype).t(), alpha = lora_S)\\n         pass\\n-\\n-        temp_lora = torch.matmul(X, lora_A._fast_lora, out = temp_lora)\\n-        out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)\\n+        out = out.view(bsz, 1, out_dim)\\n     pass\\n+\\n     return out\\n pass\\n',\n",
       " '@@ -69,7 +69,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-def LlamaAttention_fast_forward_inference(\\n+def _LlamaAttention_fast_forward_inference(\\n     self,\\n     hidden_states:  torch.Tensor,\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n@@ -185,11 +185,89 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+def LlamaAttention_fast_forward_inference(\\n+    self,\\n+    hidden_states:  torch.Tensor,\\n+    past_key_value: Optional[Tuple[torch.Tensor]],\\n+    position_ids,\\n+):\\n+    \"\"\"\\n+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n+        Fast inference using KV cache.\\n+        QK^T can be computed in 4 chunks\\n+\\n+        [Q, q] @ [K, k].T where q, k are the new tokens.\\n+        [QK^T, Qk^T]\\n+        [qK^T, qk^T]\\n+\\n+        Since the attention mask wipes Qk^T, we just get\\n+        [QK^T,    0]\\n+        [qK^T, qk^T]\\n+\\n+        Since softmax is row-wise, we get\\n+        softmax([QK^T,    0])\\n+        softmax([qK^T, qk^T])\\n+\\n+        We then multiply by   [V]\\n+                              [v]\\n+        softmax([QK^T,    0]) [softmax(QK^T)V] *\\n+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]\\n+\\n+        But notice * [softmax(QK^T)V] is just the last attention.\\n+        We just need to compute the last final row.\\n+\\n+        This means we can pass in a row of Q, but we need to\\n+        remember K and V, which are called the KV cache.\\n+    \"\"\"\\n+    Xn = hidden_states\\n+    bsz, _, _ = hidden_states.size()\\n+    K1, V1 = past_key_value\\n+\\n+    n_heads    = self.num_heads\\n+    n_groups   = self.num_key_value_groups\\n+    n_kv_heads = self.num_key_value_heads\\n+    head_dim   = self.head_dim\\n+    assert(n_kv_heads * n_groups == n_heads)\\n+\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+\\n+    kv_seq_len = K1.shape[-2] + 1\\n+    cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n+    Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n+    \\n+    # New KV cache\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+\\n+    # Grouped query attention\\n+    if n_groups != 1:\\n+        _, _, cached_len, _ = Kn.shape\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n+\\n+    # Attention\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n+    A *= 1.0 / (self.head_dim**0.5)\\n+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n+    A = torch.matmul(A, Vnn)\\n+    A = A.transpose(1, 2)\\n+    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = original_apply_o(self, A)\\n+    return A, (Kn, Vn)\\n+pass\\n+\\n+\\n torch_silu = torch.nn.functional.silu\\n def fast_mlp_inference(self, X):\\n-    hidden_size = self.hidden_size\\n-    X = X.view(hidden_size)\\n-\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     gate = fast_linear_forward(self.gate_proj, X)\\n@@ -198,20 +276,18 @@ def fast_mlp_inference(self, X):\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n-    down = fast_linear_forward(self.down_proj, gate, out = up[:hidden_size])\\n-    X = down.view(1, 1, hidden_size)\\n-\\n-    return X\\n+    down = fast_linear_forward(self.down_proj, gate)\\n+    return down\\n pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n     old_dtype = X.dtype\\n-    X = X.to(torch.float32)\\n-    variance = X.square().mean(-1, keepdim = True)\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n-    X *= variance.rsqrt_()\\n-    X = X.to(old_dtype)\\n+    XX *= variance.rsqrt_()\\n+    X = XX.to(old_dtype) # Must preserve due to residual\\n     X *= self.weight\\n     return X\\n pass\\n@@ -234,7 +310,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +426,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if past_key_value is not None:\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -488,8 +564,7 @@ def LlamaModel_fast_forward(\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and attention_mask is not None and \\\\\\n-        attention_mask.shape[1] == seq_length:\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -501,7 +576,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif self.training:\\n+    elif False:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n@@ -522,7 +597,7 @@ def LlamaModel_fast_forward(\\n \\n     hidden_states = inputs_embeds\\n \\n-    if self.gradient_checkpointing and self.training:\\n+    if past_key_values is None and self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n                 \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n@@ -581,7 +656,7 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if past_key_values is not None:\\n         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n     else:\\n         hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n@@ -644,7 +719,13 @@ def LlamaForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -210,7 +210,13 @@ def MistralForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -22,4 +22,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n',\n",
       " '@@ -134,9 +134,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = Q.shape[-1]//2\\n         RH_Q = torch.cat((-Q[..., half:], Q[..., :half]), dim = -1)\\n         Q *= cos\\n-        Q.addcmul_(RH_Q, sin)\\n-        # RH_Q *= sin\\n-        # Q += RH_Q\\n+        # Q.addcmul_(RH_Q, sin)\\n+        RH_Q *= sin\\n+        Q += RH_Q\\n         ctx.save_for_backward(cos, sin)\\n         return Q\\n     pass\\n@@ -148,9 +148,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = dY.shape[-1]//2\\n         RH_dY = torch.cat((dY[..., half:], -dY[..., :half]), dim = -1)\\n         dY *= cos\\n-        dY.addcmul_(RH_dY, sin)\\n-        # RH_dY *= sin\\n-        # dY += RH_dY\\n+        # dY.addcmul_(RH_dY, sin)\\n+        RH_dY *= sin\\n+        dY += RH_dY\\n         return dY, None, None, None\\n     pass\\n pass\\n',\n",
       " '@@ -114,11 +114,12 @@ def fast_dequantize(W, quant_state = None, out = None):\\n pass\\n \\n \\n-def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n-    quant_state = W.quant_state\\n-    bsz = 1\\n-    q_len = 1\\n-    hd = X.shape[0]\\n+def fast_gemv(X, W, quant_state, out = None):\\n+    if quant_state is None: return torch.matmul(X, W, out = out)\\n+    # For fast X @ W where seq_len == 1\\n+    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n+    bsz, q_len, hd = X.shape\\n+    assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n         # https://github.com/TimDettmers/bitsandbytes/pull/763/files\\n@@ -137,9 +138,14 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n         offset, state2 = compressed_stats\\n         absmax2, code2, blocksize2, _, _, _, _ = state2\\n     pass\\n+    assert(dtype == X.dtype)\\n     bout = shape[0]\\n-    if out is None: out = torch.empty(bout, dtype = dtype, device = \"cuda\")\\n-    else: assert(out.shape[0] == bout)\\n+\\n+    if out is None:\\n+        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    else:\\n+        assert(out.shape == (bsz, 1, bout,))\\n+    pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -170,30 +176,46 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n     ptr_stats  = get_ptr(stats)\\n     blocksize  = ctypes.c_int32(blocksize)\\n \\n-    fx(m, n, k, get_ptr(X), ptr_W, ptr_absmax, ptr_stats, get_ptr(out),\\n-        lda, ldb, ldc, blocksize)\\n+    for row in range(bsz):\\n+        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n+           lda, ldb, ldc, blocksize)\\n+    pass\\n \\n     return out\\n pass\\n \\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n+\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n+\\n+    bsz, _, in_dim = X.shape\\n+\\n     if W_quant is None:\\n         out = torch.matmul(X, W.t())\\n-    else:\\n+    elif bsz <= 4:\\n+        # Only batches of 4 are faster with Gemv\\n         out = fast_gemv(X, W, W_quant, out = out)\\n-    if lora_A is not None:\\n+    else:\\n+        W = fast_dequantize(W.t(), W_quant)\\n+        out = torch.matmul(X, W, out = out)\\n+    pass\\n \\n-        # Save LoRAs for inference to stop data movement costs\\n-        if not hasattr(lora_A, \"_fast_lora\"):\\n-            dtype = X.dtype\\n-            lora_A._fast_lora = lora_A.to(dtype).t()\\n-            lora_B._fast_lora = lora_B.to(dtype)\\n+    # Add in LoRA weights\\n+    if lora_A is not None:\\n+        out_dim = out.shape[2]\\n+        dtype = X.dtype\\n+        if bsz == 1:\\n+            out = out.view(out_dim)\\n+            temp_lora = torch.mv(lora_A.to(dtype), X.ravel(), out = temp_lora)\\n+            out.addmv_(lora_B.to(dtype), temp_lora, alpha = lora_S)\\n+        else:\\n+            out = out.view(bsz, out_dim)\\n+            temp_lora = torch.mm(X.view(bsz, in_dim), lora_A.to(dtype).t(), out = temp_lora)\\n+            out.addmm_(temp_lora, lora_B.to(dtype).t(), alpha = lora_S)\\n         pass\\n-\\n-        temp_lora = torch.matmul(X, lora_A._fast_lora, out = temp_lora)\\n-        out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)\\n+        out = out.view(bsz, 1, out_dim)\\n     pass\\n+\\n     return out\\n pass\\n',\n",
       " '@@ -69,7 +69,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-def LlamaAttention_fast_forward_inference(\\n+def _LlamaAttention_fast_forward_inference(\\n     self,\\n     hidden_states:  torch.Tensor,\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n@@ -185,11 +185,89 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+def LlamaAttention_fast_forward_inference(\\n+    self,\\n+    hidden_states:  torch.Tensor,\\n+    past_key_value: Optional[Tuple[torch.Tensor]],\\n+    position_ids,\\n+):\\n+    \"\"\"\\n+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n+        Fast inference using KV cache.\\n+        QK^T can be computed in 4 chunks\\n+\\n+        [Q, q] @ [K, k].T where q, k are the new tokens.\\n+        [QK^T, Qk^T]\\n+        [qK^T, qk^T]\\n+\\n+        Since the attention mask wipes Qk^T, we just get\\n+        [QK^T,    0]\\n+        [qK^T, qk^T]\\n+\\n+        Since softmax is row-wise, we get\\n+        softmax([QK^T,    0])\\n+        softmax([qK^T, qk^T])\\n+\\n+        We then multiply by   [V]\\n+                              [v]\\n+        softmax([QK^T,    0]) [softmax(QK^T)V] *\\n+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]\\n+\\n+        But notice * [softmax(QK^T)V] is just the last attention.\\n+        We just need to compute the last final row.\\n+\\n+        This means we can pass in a row of Q, but we need to\\n+        remember K and V, which are called the KV cache.\\n+    \"\"\"\\n+    Xn = hidden_states\\n+    bsz, _, _ = hidden_states.size()\\n+    K1, V1 = past_key_value\\n+\\n+    n_heads    = self.num_heads\\n+    n_groups   = self.num_key_value_groups\\n+    n_kv_heads = self.num_key_value_heads\\n+    head_dim   = self.head_dim\\n+    assert(n_kv_heads * n_groups == n_heads)\\n+\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+\\n+    kv_seq_len = K1.shape[-2] + 1\\n+    cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n+    Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n+    \\n+    # New KV cache\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+\\n+    # Grouped query attention\\n+    if n_groups != 1:\\n+        _, _, cached_len, _ = Kn.shape\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n+\\n+    # Attention\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n+    A *= 1.0 / (self.head_dim**0.5)\\n+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n+    A = torch.matmul(A, Vnn)\\n+    A = A.transpose(1, 2)\\n+    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = original_apply_o(self, A)\\n+    return A, (Kn, Vn)\\n+pass\\n+\\n+\\n torch_silu = torch.nn.functional.silu\\n def fast_mlp_inference(self, X):\\n-    hidden_size = self.hidden_size\\n-    X = X.view(hidden_size)\\n-\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     gate = fast_linear_forward(self.gate_proj, X)\\n@@ -198,20 +276,18 @@ def fast_mlp_inference(self, X):\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n-    down = fast_linear_forward(self.down_proj, gate, out = up[:hidden_size])\\n-    X = down.view(1, 1, hidden_size)\\n-\\n-    return X\\n+    down = fast_linear_forward(self.down_proj, gate)\\n+    return down\\n pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n     old_dtype = X.dtype\\n-    X = X.to(torch.float32)\\n-    variance = X.square().mean(-1, keepdim = True)\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n-    X *= variance.rsqrt_()\\n-    X = X.to(old_dtype)\\n+    XX *= variance.rsqrt_()\\n+    X = XX.to(old_dtype) # Must preserve due to residual\\n     X *= self.weight\\n     return X\\n pass\\n@@ -234,7 +310,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +426,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if past_key_value is not None:\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -488,8 +564,7 @@ def LlamaModel_fast_forward(\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and attention_mask is not None and \\\\\\n-        attention_mask.shape[1] == seq_length:\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -501,7 +576,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif self.training:\\n+    elif False:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n@@ -522,7 +597,7 @@ def LlamaModel_fast_forward(\\n \\n     hidden_states = inputs_embeds\\n \\n-    if self.gradient_checkpointing and self.training:\\n+    if past_key_values is None and self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n                 \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n@@ -581,7 +656,7 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if past_key_values is not None:\\n         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n     else:\\n         hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n@@ -644,7 +719,13 @@ def LlamaForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -210,7 +210,13 @@ def MistralForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -22,4 +22,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n',\n",
       " '@@ -134,9 +134,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = Q.shape[-1]//2\\n         RH_Q = torch.cat((-Q[..., half:], Q[..., :half]), dim = -1)\\n         Q *= cos\\n-        Q.addcmul_(RH_Q, sin)\\n-        # RH_Q *= sin\\n-        # Q += RH_Q\\n+        # Q.addcmul_(RH_Q, sin)\\n+        RH_Q *= sin\\n+        Q += RH_Q\\n         ctx.save_for_backward(cos, sin)\\n         return Q\\n     pass\\n@@ -148,9 +148,9 @@ class Slow_RoPE_Embedding(torch.autograd.Function):\\n         half = dY.shape[-1]//2\\n         RH_dY = torch.cat((dY[..., half:], -dY[..., :half]), dim = -1)\\n         dY *= cos\\n-        dY.addcmul_(RH_dY, sin)\\n-        # RH_dY *= sin\\n-        # dY += RH_dY\\n+        # dY.addcmul_(RH_dY, sin)\\n+        RH_dY *= sin\\n+        dY += RH_dY\\n         return dY, None, None, None\\n     pass\\n pass\\n',\n",
       " '@@ -114,11 +114,12 @@ def fast_dequantize(W, quant_state = None, out = None):\\n pass\\n \\n \\n-def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n-    quant_state = W.quant_state\\n-    bsz = 1\\n-    q_len = 1\\n-    hd = X.shape[0]\\n+def fast_gemv(X, W, quant_state, out = None):\\n+    if quant_state is None: return torch.matmul(X, W, out = out)\\n+    # For fast X @ W where seq_len == 1\\n+    # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n+    bsz, q_len, hd = X.shape\\n+    assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n         # https://github.com/TimDettmers/bitsandbytes/pull/763/files\\n@@ -137,9 +138,14 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n         offset, state2 = compressed_stats\\n         absmax2, code2, blocksize2, _, _, _, _ = state2\\n     pass\\n+    assert(dtype == X.dtype)\\n     bout = shape[0]\\n-    if out is None: out = torch.empty(bout, dtype = dtype, device = \"cuda\")\\n-    else: assert(out.shape[0] == bout)\\n+\\n+    if out is None:\\n+        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    else:\\n+        assert(out.shape == (bsz, 1, bout,))\\n+    pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -170,30 +176,46 @@ def fast_gemv(X, W, quant_state, out = None, out_W = None):\\n     ptr_stats  = get_ptr(stats)\\n     blocksize  = ctypes.c_int32(blocksize)\\n \\n-    fx(m, n, k, get_ptr(X), ptr_W, ptr_absmax, ptr_stats, get_ptr(out),\\n-        lda, ldb, ldc, blocksize)\\n+    for row in range(bsz):\\n+        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n+           lda, ldb, ldc, blocksize)\\n+    pass\\n \\n     return out\\n pass\\n \\n \\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n+\\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n+\\n+    bsz, _, in_dim = X.shape\\n+\\n     if W_quant is None:\\n         out = torch.matmul(X, W.t())\\n-    else:\\n+    elif bsz <= 4:\\n+        # Only batches of 4 are faster with Gemv\\n         out = fast_gemv(X, W, W_quant, out = out)\\n-    if lora_A is not None:\\n+    else:\\n+        W = fast_dequantize(W.t(), W_quant)\\n+        out = torch.matmul(X, W, out = out)\\n+    pass\\n \\n-        # Save LoRAs for inference to stop data movement costs\\n-        if not hasattr(lora_A, \"_fast_lora\"):\\n-            dtype = X.dtype\\n-            lora_A._fast_lora = lora_A.to(dtype).t()\\n-            lora_B._fast_lora = lora_B.to(dtype)\\n+    # Add in LoRA weights\\n+    if lora_A is not None:\\n+        out_dim = out.shape[2]\\n+        dtype = X.dtype\\n+        if bsz == 1:\\n+            out = out.view(out_dim)\\n+            temp_lora = torch.mv(lora_A.to(dtype), X.ravel(), out = temp_lora)\\n+            out.addmv_(lora_B.to(dtype), temp_lora, alpha = lora_S)\\n+        else:\\n+            out = out.view(bsz, out_dim)\\n+            temp_lora = torch.mm(X.view(bsz, in_dim), lora_A.to(dtype).t(), out = temp_lora)\\n+            out.addmm_(temp_lora, lora_B.to(dtype).t(), alpha = lora_S)\\n         pass\\n-\\n-        temp_lora = torch.matmul(X, lora_A._fast_lora, out = temp_lora)\\n-        out.addmv_(lora_B._fast_lora, temp_lora, alpha = lora_S)\\n+        out = out.view(bsz, 1, out_dim)\\n     pass\\n+\\n     return out\\n pass\\n',\n",
       " '@@ -69,7 +69,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-def LlamaAttention_fast_forward_inference(\\n+def _LlamaAttention_fast_forward_inference(\\n     self,\\n     hidden_states:  torch.Tensor,\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n@@ -185,11 +185,89 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+def LlamaAttention_fast_forward_inference(\\n+    self,\\n+    hidden_states:  torch.Tensor,\\n+    past_key_value: Optional[Tuple[torch.Tensor]],\\n+    position_ids,\\n+):\\n+    \"\"\"\\n+        https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n+        Fast inference using KV cache.\\n+        QK^T can be computed in 4 chunks\\n+\\n+        [Q, q] @ [K, k].T where q, k are the new tokens.\\n+        [QK^T, Qk^T]\\n+        [qK^T, qk^T]\\n+\\n+        Since the attention mask wipes Qk^T, we just get\\n+        [QK^T,    0]\\n+        [qK^T, qk^T]\\n+\\n+        Since softmax is row-wise, we get\\n+        softmax([QK^T,    0])\\n+        softmax([qK^T, qk^T])\\n+\\n+        We then multiply by   [V]\\n+                              [v]\\n+        softmax([QK^T,    0]) [softmax(QK^T)V] *\\n+        softmax([qK^T, qk^T]) [softmax([qK^T, qk^T]) @ [V, v]]\\n+\\n+        But notice * [softmax(QK^T)V] is just the last attention.\\n+        We just need to compute the last final row.\\n+\\n+        This means we can pass in a row of Q, but we need to\\n+        remember K and V, which are called the KV cache.\\n+    \"\"\"\\n+    Xn = hidden_states\\n+    bsz, _, _ = hidden_states.size()\\n+    K1, V1 = past_key_value\\n+\\n+    n_heads    = self.num_heads\\n+    n_groups   = self.num_key_value_groups\\n+    n_kv_heads = self.num_key_value_heads\\n+    head_dim   = self.head_dim\\n+    assert(n_kv_heads * n_groups == n_heads)\\n+\\n+    Qn = self.q_proj(Xn)\\n+    Kn = self.k_proj(Xn)\\n+    Vn = self.v_proj(Xn)\\n+    Qn = Qn.view(bsz, 1, n_heads,    head_dim).transpose(1, 2)\\n+    Kn = Kn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+    Vn = Vn.view(bsz, 1, n_kv_heads, head_dim).transpose(1, 2)\\n+\\n+    kv_seq_len = K1.shape[-2] + 1\\n+    cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n+    Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n+    \\n+    # New KV cache\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+\\n+    # Grouped query attention\\n+    if n_groups != 1:\\n+        _, _, cached_len, _ = Kn.shape\\n+        Knn = Kn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Vnn = Vn[:, :, None, :, :].expand(bsz, n_kv_heads, n_groups, cached_len, head_dim)\\n+        Knn = Knn.reshape(bsz, n_heads, cached_len, head_dim)\\n+        Vnn = Vnn.reshape(bsz, n_heads, cached_len, head_dim)\\n+    else:\\n+        Knn, Vnn = Kn, Vn\\n+\\n+    # Attention\\n+    A = torch.matmul(Qn, Knn.transpose(2, 3))\\n+    A *= 1.0 / (self.head_dim**0.5)\\n+    A = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32).to(A.dtype)\\n+    A = torch.matmul(A, Vnn)\\n+    A = A.transpose(1, 2)\\n+    A = A.reshape(bsz, 1, self.hidden_size)\\n+    A = original_apply_o(self, A)\\n+    return A, (Kn, Vn)\\n+pass\\n+\\n+\\n torch_silu = torch.nn.functional.silu\\n def fast_mlp_inference(self, X):\\n-    hidden_size = self.hidden_size\\n-    X = X.view(hidden_size)\\n-\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     gate = fast_linear_forward(self.gate_proj, X)\\n@@ -198,20 +276,18 @@ def fast_mlp_inference(self, X):\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n-    down = fast_linear_forward(self.down_proj, gate, out = up[:hidden_size])\\n-    X = down.view(1, 1, hidden_size)\\n-\\n-    return X\\n+    down = fast_linear_forward(self.down_proj, gate)\\n+    return down\\n pass\\n \\n \\n def fast_rms_layernorm_inference(self, X):\\n     old_dtype = X.dtype\\n-    X = X.to(torch.float32)\\n-    variance = X.square().mean(-1, keepdim = True)\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n     variance += self.variance_epsilon\\n-    X *= variance.rsqrt_()\\n-    X = X.to(old_dtype)\\n+    XX *= variance.rsqrt_()\\n+    X = XX.to(old_dtype) # Must preserve due to residual\\n     X *= self.weight\\n     return X\\n pass\\n@@ -234,7 +310,7 @@ def LlamaAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if False: #past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -350,7 +426,7 @@ def LlamaDecoderLayer_fast_forward(\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n     bsz, q_len, hd = hidden_states.size()\\n-    if False: #(past_key_value is not None and q_len == 1 and bsz == 1):\\n+    if past_key_value is not None:\\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n@@ -488,8 +564,7 @@ def LlamaModel_fast_forward(\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and attention_mask is not None and \\\\\\n-        attention_mask.shape[1] == seq_length:\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -501,7 +576,7 @@ def LlamaModel_fast_forward(\\n     # Ignore attention_mask\\n     if attention_mask is None:\\n         padding_mask = None\\n-    elif self.training:\\n+    elif False:\\n         attention_mask = None\\n         padding_mask = None\\n     else:\\n@@ -522,7 +597,7 @@ def LlamaModel_fast_forward(\\n \\n     hidden_states = inputs_embeds\\n \\n-    if self.gradient_checkpointing and self.training:\\n+    if past_key_values is None and self.gradient_checkpointing and self.training:\\n         if use_cache:\\n             logger.warning_once(\\n                 \"Unsloth: `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\"\\n@@ -581,7 +656,7 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     bsz, q_len, hd = hidden_states.size()\\n-    if (past_key_value is not None and q_len == 1):\\n+    if past_key_values is not None:\\n         hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n     else:\\n         hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n@@ -644,7 +719,13 @@ def LlamaForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -49,7 +49,7 @@ def MistralAttention_fast_forward(\\n     bsz, q_len, _ = hidden_states.size()\\n \\n     # Check for inference\\n-    if past_key_value is not None and q_len == 1 and bsz == 1:\\n+    if past_key_value is not None:\\n         A, past_key_value = LlamaAttention_fast_forward_inference(\\n             self,\\n             hidden_states,\\n@@ -210,7 +210,13 @@ def MistralForCausalLM_fast_forward(\\n     )\\n \\n     hidden_states = outputs[0]\\n-    logits = self.lm_head(hidden_states)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n \\n     loss = None\\n     if labels is not None:\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -1,68 +1,105 @@\\n-<p align=\"center\">\\n-  <picture>\\n-    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n-    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/shimmyshimmer/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n-    <img alt=\"unsloth logo\" src=\"./images/unsloth%20logo%20black%20text.png\" height=\"120\" style=\"max-width: 100%;\">\\n-  </picture>\\n-</p>\\n-<p align=\"center\">\\n-  <a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"./images/Free version button.png\" height=\"50\"></a>\\n-  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"./images/Discord button.png\" height=\"50\"></a>\\n-  <a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-</p>\\n-\\n-<h2 align=\"center\">\\n-    Finetune Mistral, Llama 2-5x faster with 50% less memory!\\n-</h2>\\n-<br>\\n-\\n-| Llama 2 7b                    | Mistral 7b                  | CodeLlama 34b           | Llama 7b Kaggle 2x T4  |\\n-|-----------------------------|-----------------------------|-------------------------|------------------------|\\n-| **2.2x faster 43% less VRAM**     | **2.2x faster 62% less VRAM**     | **1.9x faster 27% less VRAM**  | **5.5x faster 44% less VRAM** |\\n-| [⭐Llama **free** Colab notebook](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing\") | [⭐Mistral **free** Colab notebook](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing) | [CodeLlama A100 Colab notebook](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing) | [⭐Kaggle **free** Alpaca notebook](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\\n-| [Llama A100 Colab notebook](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) | [Mistral A100 Colab notebook](https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing) | 50+ more examples below! | [⭐Kaggle **free** Slim Orca notebook](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) |\\n-\\n-* **NEW!** [DPO](https://arxiv.org/abs/2305.18290) support. ⭐**Free!** DPO Zephyr, Mistral example! <a href=\"https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">  [More info](#DPO) on DPO\\n-* **NEW!** [TinyLlama 1.1b](https://github.com/jzhang38/TinyLlama) on 3T tokens! ⭐**Free!** example <a href=\"https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing\"><img src=\"./images/Colab.png\" height=\"20\">\\n-* **NEW!** We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n-* Supports Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek and their derived models (Open Hermes etc).\\n-* All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n-* **0% loss in accuracy** - no approximation methods - all exact.\\n-* No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n-* Works on **Linux** and **Windows** via WSL.\\n-* **NEW!** Download 4 bit models 4x faster from 🤗 Huggingface! Eg: `unsloth/mistral-7b-bnb-4bit`\\n-* Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-* **NEW!** Want a UI for finetuning? Try [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory) and use `--use_unsloth`!\\n-* Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n-\\n-| 1 A100 40GB  | 🤗 Hugging Face | Flash Attention | 🦥 Unsloth Open Source | [🦥 Unsloth Pro](https://unsloth.ai/pricing) |\\n+<div align=\"center\">\\n+\\n+  <a href=\"https://unsloth.ai\"><picture>\\n+    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\\n+    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\\n+    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\\n+  </picture></a>\\n+  \\n+<a href=\"https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\\n+<a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n+<a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n+\\n+### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+</div>\\n+\\n+## ✨ Finetune for Free\\n+\\n+All notebooks are **beginner friendly**! Colab provides a free GPU. Kaggle provides 30 hours for free per week.\\n+| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n+|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n+| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n+| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n+| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n+| **Mistral 7b** 2xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster | 60% less |\\n+\\n+- This [conversational notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- Our [raw text notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for text completion.\\n+\\n+## 🦥 Unsloth.ai News\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n+- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face! We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n+- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n+- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+\\n+## 🔗 Links and Resources\\n+| Type                            | Links                               |\\n+| ------------------------------- | --------------------------------------- |\\n+| 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n+| 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n+| <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n+| 🥇 **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)\\n+| 🌐 **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|\\n+| ✍️ **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\\n+\\n+## ⭐ Key Features\\n+- All kernels written in [OpenAI\\'s Triton](https://openai.com/research/triton) language. **Manual backprop engine**.\\n+- **0% loss in accuracy** - no approximation methods - all exact.\\n+- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n+- Works on **Linux** and **Windows** via WSL.\\n+- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n+\\n+\\n+## 🥇 Performance Benchmarking\\n+- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+\\n+| 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n | Alpaca       | 1x           | 1.04x           | 1.98x               | **15.64x**      |\\n | LAION Chip2  | 1x           | 0.92x           | 1.61x               | **20.73x**      |\\n | OASST        | 1x           | 1.19x           | 2.17x               | **14.83x**      |\\n | Slim Orca    | 1x           | 1.18x           | 2.22x               | **14.82x**      |\\n \\n-Join our [Discord](https://discord.gg/nsS4V5Z6ge)!\\n+- Benchmarking table below was conducted by [🤗Hugging Face](https://huggingface.co/blog/unsloth-trl).\\n \\n-<img src=\"./images/unsloth made with love.png\" width=\"200\" />\\n-If you trained a model with 🦥 Unsloth, we made a cool sticker if you want to use it!\\n+| Free Colab T4 | Dataset | 🤗Hugging Face | Pytorch 2.1.1 | 🦥Unsloth | 🦥 VRAM reduction |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Llama-2 7b | OASST | 1x | 1.19x | 1.95x | -43.3% |\\n+| Mistral 7b | Alpaca | 1x | 1.07x | 1.56x | -13.7% |\\n+| Tiny Llama 1.1b | Alpaca | 1x | 2.06x | 3.87x | -73.8% |\\n+| DPO with Zephyr | Ultra Chat | 1x | 1.09x | 1.55x | -18.6% |\\n \\n-# Installation Instructions - Conda\\n-Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1.\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+\\n+## 💾 Installation Instructions\\n+### Conda Installation\\n+Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \\\\\\n-  -c pytorch -c nvidia -c xformers -c conda-forge -y\\n+conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers -y\\n+\\n+pip install bitsandbytes\\n+\\n pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n \\n-# Installation Instructions - Pip\\n+### Pip Installation\\n Do **NOT** use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.\\n \\n 1. Find your CUDA version via\\n ```python\\n import torch; torch.version.cuda\\n ```\\n-2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: got to step 3.\\n+2. For Pytorch 2.1.0: You can update Pytorch via Pip (interchange `cu121` / `cu118`). Go to https://pytorch.org/ to learn more. Select either `cu118` for CUDA 11.8 or `cu121` for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the `\"ampere\"` path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.\\n ```bash\\n pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n@@ -84,16 +121,25 @@ pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.\\n pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n-4. We\\'re working on Pytorch 2.1.2 support.\\n+4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n+  --index-url https://download.pytorch.org/whl/cu121\\n+```\\n+```bash\\n+pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n pip install --upgrade pip\\n ```\\n \\n-# Documentation\\n-We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n-\\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+## 📜 Documentation\\n+- We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n+- We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel\\n@@ -159,10 +205,10 @@ trainer.train()\\n ```\\n \\n <a name=\"DPO\"></a>\\n-# DPO (Direct Preference Optimization) Support\\n-DPO, PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n+## DPO Support\\n+DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory). We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: [notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing).\\n \\n-We\\'re in 🤗 Huggingface\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n+We\\'re in 🤗Hugging Face\\'s official docs! We\\'re on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n ```python\\n from unsloth import FastLanguageModel, PatchDPOTrainer\\n@@ -217,60 +263,21 @@ dpo_trainer = DPOTrainer(\\n dpo_trainer.train()\\n ```\\n \\n-# Support us!\\n-We\\'re currently 2 brothers trying to make LLMs for everyone! It\\'ll be super cool if you can support our work!!\\n-<a href=\"https://ko-fi.com/unsloth\"><img src=\"./images/Kofi button.png\" height=\"50\"></a>\\n-\\n-# Future Milestones and limitations\\n-1. Support Mixtral.\\n-2. Supports all Mistral, Llama type models, but some are unoptimized (Qwen with biases)\\n-3. Dropout, bias in LoRA matrices are supported, just not optimized.\\n-\\n-# Performance comparisons on 1 Tesla T4 GPU:\\n-**Time taken for 1 epoch**\\n-\\n-One Tesla T4 on Google Colab\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n-| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n-| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n-| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n-\\n-**Peak Memory Usage**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n-| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n-| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n-| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n-\\n-# Performance comparisons on 2 Tesla T4 GPUs via DDP:\\n-**Time taken for 1 epoch**\\n-\\n-Two Tesla T4s on Kaggle\\n-`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n-| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n-| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n-\\n-**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n-\\n-| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n-| --- | --- | --- | --- | --- | --- |\\n-| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n-| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n-| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n-\\n-* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+## 🥇 Detailed Benchmarking Tables\\n+- Click \"Code\" for fully reproducible examples\\n+- \"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n+- For the full list of benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+  \\n+| 1 A100 40GB | 🤗Hugging Face | Flash Attention 2 | 🦥Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n+|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n+| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n+| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n+| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n+| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n+| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n \\n-# Llama-Factory 3rd party benchmarking\\n+### Llama-Factory 3rd party benchmarking\\n+- [Link to performance table.](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n \\n | Method | Bits | TGS | GRAM | Speed |\\n | --- | --- | --- | --- | --- |\\n@@ -280,58 +287,10 @@ Two Tesla T4s on Kaggle\\n | HF | 4 | 2415 | 9GB | 101% |\\n | Unsloth+FA2 | 4 | 3726 | 7GB | **160%** |\\n \\n-[Link](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison) to performance table. TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.\\n-\\n-# How did we make it faster?\\n-Manual autograd, Triton kernels etc. See our [Benchmark Breakdown](https://unsloth.ai/blog/mistral-benchmark) for more info!\\n-\\n-# Troubleshooting\\n-1. Sometimes `bitsandbytes` or `xformers` does not link properly. Try running:\\n-```bash\\n-!ldconfig /usr/lib64-nvidia\\n-```\\n-2. Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.\\n-\\n-3. If it doesn\\'t install - maybe try updating `pip`.\\n-\\n-\\n-# Full benchmarking tables\\n-Click  \"Code\" for a fully reproducible example.\\n-\"Unsloth Equal\" is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Alpaca       | 1x          | 1.04x       | 1.98x           | 2.48x        | 5.32x         | **15.64x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing) |    [Code](https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing) |    [Code](https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing) |    [Code](https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing) | | |\\n-| seconds| 1040 | 1001 | 525 | 419 | 196 | 67  |\\n-| memory MB| 18235 | 15365 | 9631 | 8525 | | |\\n-| % saved| | 15.74 | 47.18 | 53.25 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.92x       | 1.61x           | 1.84x        | 7.05x         | **20.73x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1gjL1TaKwc_xv2TcxJC8QWEWBG1msh3g2?usp=sharing) |    [Code](https://colab.research.google.com/drive/15vlPjMr8xDj5BFhGdqunGaOQSMqXPEXU?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zPwvf-BmHyHlPMBxDsY8zS0BnQ-KKbCc?usp=sharing) |    [Code](https://colab.research.google.com/drive/1X2uHy-arRsZxqWHvKHwwW102JaMwChD2?usp=sharing) | | |\\n-| seconds| 581  | 631  | 361 | 315 | 82  | 28  |\\n-| memory MB| 7763  | 8047  | 7763 | 6441 | | |\\n-| % saved| | -3.66 | 0.00  | 17.03 | | | |\\n-\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x       | 2.17x           | 2.66x        | 5.04x         | **14.83x**      |\\n-| code |[Code](https://colab.research.google.com/drive/10NzDreFbuWELGUuBv0MOoC7y3MBewaNx?usp=sharing) |    [Code](https://colab.research.google.com/drive/1TwdkJ1sHsuEH-kgeCPqSFeCpOnCfz6Ou?usp=sharing) |    [Code](https://colab.research.google.com/drive/1AkwjUkOF0XeRBMT_S8Uhh74kitEsZHla?usp=sharing) |    [Code](https://colab.research.google.com/drive/1roMkp2UjbeK2t3DkNz50cRs1MT92RPFT?usp=sharing) | | |\\n-| seconds| 1852 | 1558 | 852 | 696 | 367 | 125 |\\n-| memory MB| 26431 | 16565 | 12267| 11223| | |\\n-| % saved| | 37.33 | 53.59 | 57.54 | | |\\n-\\n-| 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.18x       | 2.22x           | 2.64x        | 5.04x         | **14.82x**      |\\n-| code |[Code](https://colab.research.google.com/drive/1UNo1xsMl8YH7xnWnIVjDFnCAPfc0RGgu?usp=sharing) |    [Code](https://colab.research.google.com/drive/1zbphER-SKhbSWGjHTfnBLPFyTgIVvaeH?usp=sharing) |    [Code](https://colab.research.google.com/drive/156si33585iv4Uh-VILFglUmIMrNCNuc2?usp=sharing) |    [Code](https://colab.research.google.com/drive/1_mhZy7dfl9jEnJRuJBZJ5y3OwW06jgQA?usp=sharing) | | |\\n-| seconds| 1824 | 1545 | 821 | 691 | 362 | 123 |\\n-| memory MB| 24557 | 15681 | 10595| 9007 | | |\\n-| % saved| | 36.14 | 56.86 | 63.32 | | |\\n-\\n+### Performance comparisons between popular models\\n+<details>\\n+  <summary>Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)</summary>\\n+  \\n ### Mistral 7b\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n@@ -345,7 +304,7 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 A100 40GB | Hugging Face | Flash Attention 2 | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|-------------|-------------|-----------------|--------------|---------------|-------------|\\n | Code Llama 34B   | OOM ❌         | 0.99x        | 1.87x           | 2.61x        | 4.27x      | 12.82x      |\\n-| code | [Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing) | [Code](https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing) | [Code](https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing) | [Code](https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing) | |\\n | seconds      | 1953  | 1982  | 1043  | 748   | 458   | 152   |\\n | memory MB    | 40000 | 33217 | 27413 | 22161 |       | |\\n | % saved|    | 16.96| 31.47 | 44.60 |       | | |\\n@@ -355,87 +314,74 @@ Click  \"Code\" for a fully reproducible example.\\n | 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n |--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n | Alpaca       | 1x          | 1.09x           | 1.69x           | 1.79x         | 2.93x          | **8.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n+| code | [▶️ Code](https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing) |    [Code](https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing) |    [Code](https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing) | | |\\n | seconds       | 1599        | 1468        | 942             | 894          | 545           | 193         |\\n | memory MB       | 7199        | 7059        | 6459            | 5443         |               |             |\\n | % saved        |         | 1.94        | 10.28           | 24.39        |               | |\\n \\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| LAION Chip2  | 1x          | 0.99x           | 1.80x           | 1.75x         | 4.15x         | **11.75x**      |\\n-| code | [Code](https://colab.research.google.com/drive/1EtdStADehE4FVJnU2Cu6O8p9jDYdqG2L?usp=sharing) |    [Code](https://colab.research.google.com/drive/1Ik4jO68odUiQIJ_szZ3xok5fk58WpA5Q?usp=sharing) |    [Code](https://colab.research.google.com/drive/1E2nR4V3bXIWBQIUE7uR39lYPr3UikzqH?usp=sharing) |    [Code](https://colab.research.google.com/drive/13jbj8D8FOt9KyXwZt9Yf2MsYkD8CyCVR?usp=sharing) | | |\\n-| seconds  | 952         | 955         | 529             | 543          | 229           | 81          | \\n-| memory MB  | 6037        | 6033        | 5797            | 4855         |               | |\\n-| % saved   |         | 0.07        | 3.98            | 19.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| OASST        | 1x          | 1.19x           | 1.95x           | 1.86x         | 2.58x         | **7.3x**        |\\n-| code | [Code](https://colab.research.google.com/drive/1aXzGgEM3yYB6SWy_XR81nQFWME40ksSy?usp=sharing) |    [Code](https://colab.research.google.com/drive/1-5MdIOp0cM0scC-CdRZhh8OYhnGHqct4?usp=sharing) |    [Code](https://colab.research.google.com/drive/1n-fgduZhRUsSjgpqNtVkXA3rSfE7iBdg?usp=sharing) |    [Code](https://colab.research.google.com/drive/1z_GlHr2M_bB4lQrPhdWC7dseZv23cBIy?usp=sharing) | | |\\n-| seconds        | 2640        | 2222        | 1355            | 1421         | 1024          | 362         |\\n-| memory MB        | 14827       | 10391       | 8413            | 7031         |               | |\\n-| % saved         |         | 29.92       | 43.26           | 52.58        |               | |\\n-\\n-| 1 T4 16GB  | Hugging Face | Flash Attention | Unsloth Open    | Unsloth Pro Equal | Unsloth Pro   | Unsloth Max |\\n-|--------------|-------------|-----------------|-----------------|---------------|---------------|-------------|\\n-| Slim Orca    | 1x          | 1.21x           | 1.77x           | 1.85x         | 2.71x         | **7.67x**       |\\n-| code | [Code](https://colab.research.google.com/drive/15yLlJx9IE84kzx7ikky45pRcarPyUtEs?usp=sharing) |    [Code](https://colab.research.google.com/drive/16IShIBmjKULWy87I-xURpj4nztTkAF13?usp=sharing) |    [Code](https://colab.research.google.com/drive/1CJG3XLg_OQpCz71eB7Uqx7wuK_n2b-a8?usp=sharing) |    [Code](https://colab.research.google.com/drive/1UmwuWHtlrC6MAfl9mX7A_TRfo5iSHDa-?usp=sharing) | | |\\n-| seconds    | 2735        | 2262        | 1545            | 1478         | 1009          | 356         |\\n-| memory MB    | 13933       | 10489       | 7661            | 6563         |               | |\\n-| % saved    |         | 24.72       | 45.02           | 52.90        |               | |\\n-\\n ### 2 Tesla T4s via DDP\\n \\n  | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n |--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n | Alpaca       | 1x       | 0.99x       | 4.95x           | 4.44x        | 7.28x         | **20.61x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n+| code | [▶️ Code](https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp) | | |\\n | seconds       | 9882     | 9946        | 1996            | 2227         | 1357          | 480         |\\n | memory MB| 9176 | 9128 | 6904 | 6782 |  | |\\n | % saved |     | 0.52 | 24.76 | 26.09 |  | | |\\n+</details>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| LAION Chip2  | 1x       | 1.12x       | 5.28x           | 4.21x        | 10.01x        | **28.32x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-laion-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-laion-t4-ddp) | | |\\n-| seconds  | 5418     | 4854        | 1027            | 1286         | 541           | 191         |\\n-| memory MB| 7316 | 7316 | 5732 | 5934 |  | |\\n-| % saved |     | 0.00 | 21.65 | 18.89 |  | |\\n+### Performance comparisons on 1 Tesla T4 GPU:\\n+<details>\\n+  <summary>Click for Time taken for 1 epoch</summary>\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=1)        | 1x       | 1.14x       | 5.56x           | 5.09x        | 5.64x         | **15.97x**      |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-bsz1-t4-ddp) |   [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-bsz1-t4-ddp) | | | |\\n-| seconds        | 4503 | 3955 | 811 | 885 | 798 | 282 |\\n-| memory MB | 11896 | 11628 | 6616 | 7105 |  | |\\n-| % saved |     | 2.25 | 44.38 | 40.27 |  | |\\n+One Tesla T4 on Google Colab\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=1)    | 1x       | 0.97x       | 5.54x           | 4.68x        | 6.88x         | **19.46x**       |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-bsz1-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-bsz1-t4-ddp) | | |\\n-| seconds | 4042 | 4158 | 729 | 863 | 588 | 208 |\\n-| memory MB| 11010 | 11042 | 6492 | 7410 |  | |\\n-| % saved |     | -0.29| 41.04 | 32.70 |  | | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 23h 15m | 56h 28m | 8h 38m | 391h 41m |\\n+| Unsloth Open | 1 T4 | 13h 7m (1.8x) | 31h 47m (1.8x) | 4h 27m (1.9x) | 240h 4m (1.6x) |\\n+| Unsloth Pro | 1 T4 | 3h 6m (7.5x) | 5h 17m (10.7x) | 1h 7m (7.7x) | 59h 53m (6.5x) |\\n+| Unsloth Max | 1 T4 | 2h 39m (8.8x) | 4h 31m (12.5x) | 0h 58m (8.9x) | 51h 30m (7.6x) |\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| OASST (bsz=2)        | OOM ❌      | OOM ❌       |  ✓          | ✓         | ✓         | ✓ |\\n-| code | [Code](https://www.kaggle.com/danielhanchen/hf-original-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-oasst-t4-ddp) |    [Code](https://www.kaggle.com/danielhanchen/unsloth-oasst-t4-ddp) | | | |\\n-| seconds        | OOM      | OOM         | 2719            | 3391         | 2794          | 987         |\\n-| memory MB| OOM  | OOM  | 8134 | 9600 |  | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+**Peak Memory Usage**\\n \\n- | 2 T4 DDP | Hugging Face | Flash Attention | Unsloth Open | Unsloth Equal | Unsloth Pro | Unsloth Max |\\n-|--------------|----------|-------------|-----------------|--------------|---------------|-------------|\\n-| Slim Orca (bsz=2)    | OOM ❌       | OOM ❌       |  ✓          | ✓        | ✓         |✓ |\\n-| code  | [Code](https://www.kaggle.com/danielhanchen/hf-original-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/hf-sdpa-slimorca-t4-ddp) |     [Code](https://www.kaggle.com/danielhanchen/unsloth-slimorca-t4-ddp) | | |\\n-| seconds    | OOM      | OOM         | 2990            | 3444         | 2351          | 831         |\\n-| memory MB| OOM  | OOM  | 7594 | 8881 | | |\\n-| % saved | OOM  | OOM  |       |       |  | |\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 1 T4 | 7.3GB | 5.9GB | 14.0GB | 13.3GB |\\n+| Unsloth Open | 1 T4 | 6.8GB | 5.7GB | 7.8GB | 7.7GB |\\n+| Unsloth Pro | 1 T4 | 6.4GB | 6.4GB | 6.4GB | 6.4GB |\\n+| Unsloth Max | 1 T4 | 11.4GB | 12.4GB | 11.9GB | 14.4GB |\\n+</details>\\n+\\n+<details>\\n+  <summary>Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:</summary>\\n+**Time taken for 1 epoch**\\n+\\n+Two Tesla T4s on Kaggle\\n+`bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = \"adamw_8bit\", schedule = \"linear\", schedule_steps = 10`\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 84h 47m | 163h 48m | 30h 51m | 1301h 24m * |\\n+| Unsloth Pro | 2 T4 | 3h 20m (25.4x) | 5h 43m (28.7x) | 1h 12m (25.7x) | 71h 40m (18.1x) * |\\n+| Unsloth Max | 2 T4 | 3h 4m (27.6x) | 5h 14m (31.3x) | 1h 6m (28.1x) | 54h 20m (23.9x) * |\\n+\\n+**Peak Memory Usage on a Multi GPU System (2 GPUs)**\\n+\\n+| System | GPU | Alpaca (52K) | LAION OIG (210K) | Open Assistant (10K) | SlimOrca (518K) * |\\n+| --- | --- | --- | --- | --- | --- |\\n+| Huggingface | 2 T4 | 8.4GB \\\\| 6GB | 7.2GB \\\\| 5.3GB | 14.3GB \\\\| 6.6GB | 10.9GB \\\\| 5.9GB * |\\n+| Unsloth Pro | 2 T4 | 7.7GB \\\\| 4.9GB | 7.5GB \\\\| 4.9GB | 8.5GB \\\\| 4.9GB | 6.2GB \\\\| 4.7GB * |\\n+| Unsloth Max | 2 T4 | 10.5GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.6GB \\\\| 5GB | 10.5GB \\\\| 5GB * |\\n+\\n+* Slim Orca `bsz=1` for all benchmarks since `bsz=2` OOMs. We can handle `bsz=2`, but we benchmark it with `bsz=1` for consistency.\\n+</details>\\n+\\n+![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n+<br>\\n \\n-# Credits\\n+### Credits\\n 1. [RandomInternetPreson](https://github.com/RandomInternetPreson) for confirming WSL support\\n 2. [152334H](https://github.com/152334H) for experimental DPO support\\n 3. [atgctg](https://github.com/atgctg) for syntax highlighting\\n-<img src=\"./images/unsloth loading page render.png\" width=\"300\" />\\n',\n",
       " 'Binary files /dev/null and b/images/buy me a coffee button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/made with unsloth.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/start free finetune button.png differ\\n',\n",
       " 'Binary files /dev/null and b/images/unsloth end.png differ\\n',\n",
       " '@@ -29,7 +29,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n | **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n | **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\\\* | 62% less |\\n \\n-- This [conversational notebook](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing) is useful for ShareGPT ChatML datatsets.\\n+- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.\\n - Our [raw text notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for text completion.\\n - Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.\\n - \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.\\n',\n",
       " '@@ -41,6 +41,7 @@ huggingface = [\\n     \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n+    \"wheel>=0.42.0\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n',\n",
       " '@@ -82,3 +82,4 @@ pass\\n \\n from .models import *\\n from .save import *\\n+from .chat_templates import *\\n',\n",
       " '@@ -0,0 +1,384 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+__all__ = [\\n+    \"get_chat_template\",\\n+    \"test_chat_templates\",\\n+]\\n+\\n+from transformers import StoppingCriteria, StoppingCriteriaList\\n+from torch import LongTensor, FloatTensor\\n+from transformers.models.llama.modeling_llama import logger\\n+from .models._utils import patch_tokenizer\\n+\\n+CHAT_TEMPLATES = {}\\n+\\n+# Unsloth efficient template leverages from Zephyr\\n+unsloth_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% if messages[0][\\'role\\'] == \\'system\\' %}\"\\\\\\n+        \"{{ messages[0][\\'content\\'] + \\'\\\\n\\' }}\"\\\\\\n+        \"{% set loop_messages = messages[1:] %}\"\\\\\\n+    \"{% else %}\"\\\\\\n+        \"{{ \\'You are a helpful assistant to the user\\\\n\\' }}\"\\\\\\n+        \"{% set loop_messages = messages %}\"\\\\\\n+    \"{% endif %}\"\\\\\\n+    \"{% for message in loop_messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'>>> User: \\' + message[\\'content\\'] + \\'\\\\n\\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ \\'>>> Assistant: \\' + message[\\'content\\'] + eos_token + \\'\\\\n\\' }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ raise_exception(\\'Only user and assistant roles are supported!\\') }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'>>> Assistant: \\' }}\"\\\\\\n+    \"{% endif %}\"\\n+unsloth_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"unsloth\"] = (unsloth_template, unsloth_eos_token,)\\n+\\n+\\n+# Zephyr has no BOS!\\n+zephyr_template = \\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'<|user|>\\\\n\\' + message[\\'content\\'] + eos_token + \\'\\\\n\\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ \\'<|assistant|>\\\\n\\' + message[\\'content\\'] + eos_token + \\'\\\\n\\' }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ \\'<|system|>\\\\n\\' + message[\\'content\\'] + eos_token + \\'\\\\n\\' }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'<|assistant|>\\\\n\\' }}\"\\\\\\n+    \"{% endif %}\"\\n+zephyr_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"zephyr\"] = (zephyr_template, zephyr_eos_token,)\\n+\\n+\\n+# ChatML has no BOS and not EOS! Rather <|im_start|> and <|im_end|> acts as BOS / EOS.\\n+chatml_template = \\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{\\'<|im_start|>user\\\\n\\' + message[\\'content\\'] + \\'<|im_end|>\\\\n\\'}}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{\\'<|im_start|>assistant\\\\n\\' + message[\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ \\'<|im_start|>system\\\\n\\' + message[\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'<|im_start|>assistant\\\\n\\' }}\"\\\\\\n+    \"{% endif %}\"\\n+chatml_eos_token = \"<|im_end|>\"\\n+CHAT_TEMPLATES[\"chatml\"] = (chatml_template, chatml_eos_token,)\\n+\\n+\\n+# Mistral Instruct doesn\\'t allow system prompts, so we append it to the user message.\\n+mistral_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% if messages[0][\\'role\\'] == \\'system\\' %}\"\\\\\\n+        \"{% if messages[1][\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'[INST] \\' + messages[0][\\'content\\'] + \\' \\' + messages[1][\\'content\\'] + \\' [/INST]\\' }}\"\\\\\\n+            \"{% set loop_messages = messages[2:] %}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ \\'[INST] \\' + messages[0][\\'content\\'] + \\' [/INST]\\' }}\"\\\\\\n+            \"{% set loop_messages = messages[1:] %}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% else %}\"\\\\\\n+        \"{% set loop_messages = messages %}\"\\\\\\n+    \"{% endif %}\"\\\\\\n+    \"{% for message in loop_messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'[INST] \\' + message[\\'content\\'] + \\' [/INST]\\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ message[\\'content\\'] + eos_token }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ raise_exception(\\'Only user and assistant roles are supported!\\') }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\n+mistral_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"mistral\"] = (mistral_template, mistral_eos_token,)\\n+\\n+\\n+# Adds BOS to every convo! And weird <<SYS>> system messages.\\n+llama_template = \\\\\\n+    \"{% if messages[0][\\'role\\'] == \\'system\\' %}\"\\\\\\n+        \"{% if messages[1][\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ bos_token + \\'[INST] <<SYS>>\\\\n\\' + messages[0][\\'content\\'] + \\'\\\\n<</SYS>>\\\\n\\\\n\\' + messages[1][\\'content\\'] + \\' [/INST]\\' }}\"\\\\\\n+            \"{% set loop_messages = messages[2:] %}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ bos_token + \\'[INST] \\' + messages[0][\\'content\\'] + \\' [/INST]\\' }}\"\\\\\\n+            \"{% set loop_messages = messages[1:] %}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% else %}\"\\\\\\n+        \"{% set loop_messages = messages %}\"\\\\\\n+    \"{% endif %}\"\\\\\\n+    \"{% for message in loop_messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ bos_token + \\'[INST] \\' + message[\\'content\\'].strip() + \\' [/INST]\\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ \\' \\' + message[\\'content\\'].strip() + \\' \\' + eos_token }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ raise_exception(\\'Only user and assistant roles are supported!\\') }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\n+llama_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"llama\"] = (llama_template, llama_eos_token,)\\n+\\n+\\n+# https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template\\n+vicuna_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% if messages[0][\\'role\\'] == \\'system\\' %}\"\\\\\\n+        \"{{ messages[0][\\'content\\'] + \\' \\' }}\"\\\\\\n+        \"{% set loop_messages = messages[1:] %}\"\\\\\\n+    \"{% else %}\"\\\\\\n+        \"{{ \\'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\\\\\\\\\'s questions.\\' + \\' \\' }}\"\\\\\\n+        \"{% set loop_messages = messages %}\"\\\\\\n+    \"{% endif %}\"\\\\\\n+    \"{% for message in loop_messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'USER: \\' + message[\\'content\\'] + \\' \\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ \\'ASSISTANT: \\' + message[\\'content\\'] + eos_token }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ raise_exception(\\'Only user and assistant roles are supported!\\') }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'ASSISTANT:\\' }}\"\\\\\\n+    \"{% endif %}\"\\n+vicuna_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"vicuna\"] = (vicuna_template, vicuna_eos_token,)\\n+\\n+\\n+# https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template\\n+vicuna_old_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% if messages[0][\\'role\\'] == \\'system\\' %}\"\\\\\\n+        \"{{ messages[0][\\'content\\'] + \\'\\\\n\\' }}\"\\\\\\n+        \"{% set loop_messages = messages[1:] %}\"\\\\\\n+    \"{% else %}\"\\\\\\n+        \"{{ \\'A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human\\\\\\\\\\'s questions.\\' + \\'\\\\n\\' }}\"\\\\\\n+        \"{% set loop_messages = messages %}\"\\\\\\n+    \"{% endif %}\"\\\\\\n+    \"{% for message in loop_messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'### Human: \\' + message[\\'content\\'] + \\'\\\\n\\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ \\'### Assistant: \\' + message[\\'content\\'] + eos_token + \\'\\\\n\\' }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ raise_exception(\\'Only user and assistant roles are supported!\\') }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'### Assistant:\\' }}\"\\\\\\n+    \"{% endif %}\"\\n+vicuna_old_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"vicuna_old\"] = (vicuna_old_template, vicuna_old_eos_token,)\\n+\\n+\\n+# https://github.com/tatsu-lab/stanford_alpaca Changed for multi-turn convos\\n+alpaca_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% if messages[0][\\'role\\'] == \\'system\\' %}\"\\\\\\n+        \"{{ messages[0][\\'content\\'] + \\'\\\\n\\\\n\\' }}\"\\\\\\n+        \"{% set loop_messages = messages[1:] %}\"\\\\\\n+    \"{% else %}\"\\\\\\n+        \"{{ \\'Below are some instructions that describes some tasks. Write responses that appropriately completes each request.\\\\n\\\\n\\' }}\"\\\\\\n+        \"{% set loop_messages = messages %}\"\\\\\\n+    \"{% endif %}\"\\\\\\n+    \"{% for message in loop_messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{ \\'### Instruction:\\\\n\\' + message[\\'content\\'] + \\'\\\\n\\\\n\\' }}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{ \\'### Response:\\\\n\\' + message[\\'content\\'] + eos_token + \\'\\\\n\\\\n\\' }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ raise_exception(\\'Only user and assistant roles are supported!\\') }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'### Response:\\\\n\\' }}\"\\\\\\n+    \"{% endif %}\"\\n+alpaca_eos_token = \"eos_token\"\\n+CHAT_TEMPLATES[\"alpaca\"] = (alpaca_template, alpaca_eos_token,)\\n+\\n+\\n+def get_chat_template(\\n+    tokenizer,\\n+    chat_template = \"chatml\",\\n+    mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n+    map_eos_token = True,\\n+):\\n+    if map_eos_token is False:\\n+        assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n+    pass\\n+\\n+    old_padding_side = tokenizer.padding_side\\n+\\n+    if type(chat_template) in (list, tuple):\\n+        chat_template, stop_word = chat_template\\n+        assert(type(chat_template) is str)\\n+        assert(type(stop_word) is str)\\n+\\n+    elif type(chat_template) is str:\\n+\\n+        chat_template, stop_word = CHAT_TEMPLATES[chat_template]\\n+\\n+        if stop_word != \"eos_token\":\\n+            logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n+\\n+            # Replaces the old EOS token with a new one.\\n+            # Useful for ChatML <|im_end|> for example.\\n+            # Usually we train 2 more tokens <|im_start|> and <|im_end|>\\n+            # But training the lm_head and embeddings are slow!\\n+            # This is a HACK!\\n+            # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n+            string_vocab = tokenizer._tokenizer.to_str()\\n+            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n+            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+        pass\\n+    else:\\n+        raise TypeError(\\n+            f\"Unsloth: `chat_template` must be a tuple of (your_template, eos_token,) or one of\\\\n\"\\\\\\n+            f\"{CHAT_TEMPLATES.keys()}\"\\n+        )\\n+    pass\\n+\\n+    # For ShareGPT role -> from and content -> value\\n+    chat_template = chat_template\\\\\\n+        .replace(\"\\'role\\'\",      \"\\'\" + mapping[\"role\"]      + \"\\'\")\\\\\\n+        .replace(\"\\'content\\'\",   \"\\'\" + mapping[\"content\"]   + \"\\'\")\\\\\\n+        .replace(\"\\'user\\'\",      \"\\'\" + mapping[\"user\"]      + \"\\'\")\\\\\\n+        .replace(\"\\'assistant\\'\", \"\\'\" + mapping[\"assistant\"] + \"\\'\")\\n+\\n+    _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)\\n+    tokenizer.padding_side  = old_padding_side\\n+    tokenizer.chat_template = chat_template\\n+\\n+    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+\\n+    return tokenizer#, stopping_criteria\\n+pass\\n+\\n+\\n+def create_stopping_criteria(tokenizer, stop_word = \"eos_token\"):\\n+    class StoppingCriteriaSub(StoppingCriteria):\\n+        __slots__ = \"stop_token\", \"single_match\", \"length\",\\n+\\n+        def __init__(self, stops = \"eos_token\", device = \"cuda\", encounters = 1):\\n+            super().__init__()\\n+            if stops == \"eos_token\":\\n+                self.stop_token = torch.tensor(tokenizer.eos_token_id, device = \"cuda\")\\n+                self.length = 1\\n+            else:\\n+                self.stop_token = tokenizer([\"\\\\n\" + stops], add_special_tokens = False, return_tensors = \"pt\")\\n+                self.stop_token = self.stop_token.input_ids.ravel()[1:].to(\"cuda\")\\n+                self.length = self.stop_token.shape[0]\\n+            pass\\n+            self.single_match = self.length == 1\\n+        pass\\n+\\n+        def __call__(self, input_ids: LongTensor, scores: FloatTensor) -> bool:\\n+            input_ids = input_ids.ravel()\\n+            last_token = input_ids[-1]\\n+            if self.single_match and (last_token == self.stop_token): return True\\n+\\n+            if input_ids.shape[0] >= self.length and \\\\\\n+                (input_ids[-self.length:] == self.stop_token).all(): return True\\n+            return False\\n+        pass\\n+    pass\\n+    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = stop_word)])\\n+    return stopping_criteria\\n+pass\\n+\\n+\\n+def test_chat_templates():\\n+    messages = [\\n+        {\"role\": \"system\",\"content\": \" You are a friendly chatbot.\",},\\n+        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n+        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n+        {\"role\": \"user\", \"content\": \"  But 2+2 is equal to 5. \"},\\n+        {\"role\": \"assistant\", \"content\": \"No I\\'m sure its 4.\"},\\n+        {\"role\": \"user\", \"content\": \"  No it\\'s 100% 5! \"},\\n+    ]\\n+\\n+    from transformers import AutoTokenizer\\n+    template = zephyr_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n+\\n+    template = chatml_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n+\\n+    template = mistral_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n+\\n+    template = llama_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-2-7b-chat\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n+\\n+    try:\\n+        from fastchat.conversation import get_conv_template\\n+    except:\\n+        os.system(\"pip -qqq install git+https://github.com/lm-sys/FastChat.git\")\\n+        from fastchat.conversation import get_conv_template\\n+    correct_prompt = get_conv_template(\"vicuna_v1.1\")\\n+    for j in range(len(messages)-1):\\n+        correct_prompt.append_message(correct_prompt.roles[j%2==1], messages[j+1][\"content\"])\\n+    correct_prompt.append_message(correct_prompt.roles[1], \"\")\\n+    correct_prompt = tokenizer.bos_token + correct_prompt.get_prompt()\\n+\\n+    template = vicuna_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n+\\n+    try:\\n+        from fastchat.conversation import get_conv_template\\n+    except:\\n+        os.system(\"pip -qqq install git+https://github.com/lm-sys/FastChat.git\")\\n+        from fastchat.conversation import get_conv_template\\n+    correct_prompt = get_conv_template(\"zero_shot\")\\n+    for j in range(len(messages)-1):\\n+        correct_prompt.append_message(correct_prompt.roles[j%2==1], messages[j+1][\"content\"])\\n+    correct_prompt.append_message(correct_prompt.roles[1], \"\")\\n+    correct_prompt = tokenizer.bos_token + correct_prompt.get_prompt()\\n+\\n+    template = vicuna_old_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n+    # We add </s> ourselves\\n+    assert(correct_prompt == our_prompt.replace(\"</s>\", \"\"))\\n+pass\\n',\n",
       " '@@ -16,6 +16,7 @@ import torch\\n from typing import Union, Optional, List, Any, Callable\\n import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n@@ -116,21 +117,24 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n-    model.config.update({\"unsloth_version\" : __version__})\\n+    if model is not None:\\n+        model.config.update({\"unsloth_version\" : __version__})\\n     if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n         # Fixes https://github.com/unslothai/unsloth/issues/5\\n         if hasattr(tokenizer, \"unk_token\"):\\n             tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n             tokenizer.pad_token = tokenizer.unk_token\\n         else:\\n+            name = model.config._name_or_path if model is not None else \"Model\"\\n             logger.warning_one(\\n-                f\"{model.config._name_or_path} does not have a padding or unknown token!\\\\n\"\\\\\\n+                f\"{name} does not have a padding or unknown token!\\\\n\"\\\\\\n                 f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n             )\\n             assert(hasattr(tokenizer, \"eos_token\"))\\n             tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n             tokenizer.pad_token = tokenizer.eos_token\\n-        config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+        if model is not None:\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n     pass\\n     return model, tokenizer\\n pass\\n',\n",
       " '@@ -540,7 +540,7 @@ def LlamaModel_fast_forward(\\n \\n     hidden_states = inputs_embeds\\n \\n-    if past_key_values is None and self.gradient_checkpointing and self.training:\\n+    if past_key_values is None and self.training:\\n         use_cache = False\\n         # if use_cache:\\n         #     logger.warning_once(\\n@@ -776,6 +776,73 @@ def PeftModelForCausalLM_fast_forward(\\n pass\\n \\n \\n+# Solves https://github.com/unslothai/unsloth/issues/168\\n+# Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n+# Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.\\n+# https://github.com/huggingface/transformers/pull/27931\\n+# https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n+class LlamaRotaryEmbedding(torch.nn.Module):\\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\\n+        super().__init__()\\n+\\n+        self.dim = dim\\n+        self.max_position_embeddings = max_position_embeddings\\n+        self.base = base\\n+        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\\n+\\n+        # Build here to make `torch.jit.trace` work.\\n+        self._set_cos_sin_cache(\\n+            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\\n+        )\\n+    pass\\n+\\n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        self.max_seq_len_cached = seq_len\\n+        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\\n+\\n+        freqs = torch.outer(t, self.inv_freq)\\n+        # Different from paper, but it uses a different permutation in order to obtain the same calculation\\n+        emb = torch.cat((freqs, freqs), dim=-1)\\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\\n+    pass\\n+\\n+    def forward(self, x, seq_len=None):\\n+        # x: [bs, num_attention_heads, seq_len, head_size]\\n+        if seq_len > self.max_seq_len_cached:\\n+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\\n+\\n+        return (\\n+            self.cos_cached[:seq_len].to(dtype=x.dtype),\\n+            self.sin_cached[:seq_len].to(dtype=x.dtype),\\n+        )\\n+    pass\\n+pass\\n+\\n+\\n+class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n+    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\\n+\\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\\n+        self.scaling_factor = scaling_factor\\n+        super().__init__(dim, max_position_embeddings, base, device)\\n+    pass\\n+\\n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        self.max_seq_len_cached = seq_len\\n+        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\\n+        t = t / self.scaling_factor\\n+\\n+        freqs = torch.outer(t, self.inv_freq)\\n+        # Different from paper, but it uses a different permutation in order to obtain the same calculation\\n+        emb = torch.cat((freqs, freqs), dim=-1)\\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\\n+    pass\\n+pass\\n+\\n+\\n class FastLlamaModel:\\n \\n     @staticmethod\\n@@ -787,6 +854,15 @@ class FastLlamaModel:\\n         LlamaModel          .forward = LlamaModel_fast_forward\\n         LlamaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n+\\n+        # Solves https://github.com/unslothai/unsloth/issues/168\\n+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.\\n+        # https://github.com/huggingface/transformers/pull/27931\\n+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n+        import transformers.models.llama.modeling_llama\\n+        transformers.models.llama.modeling_llama.LlamaRotaryEmbedding = LlamaRotaryEmbedding\\n+        transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding = LlamaLinearScalingRotaryEmbedding\\n         return\\n     pass\\n \\n',\n",
       " '@@ -271,6 +271,14 @@ class FastMistralModel(FastLlamaModel):\\n         MistralModel          .forward = LlamaModel_fast_forward\\n         MistralForCausalLM    .forward = MistralForCausalLM_fast_forward\\n         PeftModelForCausalLM  .forward = PeftModelForCausalLM_fast_forward\\n+\\n+        # Solves https://github.com/unslothai/unsloth/issues/168\\n+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.\\n+        # https://github.com/huggingface/transformers/pull/27931\\n+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n+        import transformers.models.mistral.modeling_mistral\\n+        transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding = LlamaRotaryEmbedding\\n         return\\n     pass\\n \\n',\n",
       " '@@ -41,6 +41,7 @@ LLAMA_LAYERNORMS = (\\n     \"input_layernorm\", \"post_attention_layernorm\",\\n )\\n \\n+# https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp#L19\\n # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\\n ALLOWED_QUANTS = \\\\\\n {\\n@@ -59,10 +60,16 @@ ALLOWED_QUANTS = \\\\\\n     \"q4_0\"    : \"Original quant method, 4-bit.\",\\n     \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n     \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+    \"q4_k\"    : \"alias for q4_k_m\",\\n+    \"q5_k\"    : \"alias for q5_k_m\",\\n     \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n     \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n     \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n     \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+    \"iq2_xxs\" : \"2.06 bpw quantization\",\\n+    \"iq2_xs\"  : \"2.31 bpw quantization\",\\n+    \"iq3_xxs\" : \"3.06 bpw quantization\",\\n+    \"q3_k_xs\" : \"3-bit extra small quantization\",\\n }\\n \\n def print_quantization_methods():\\n@@ -246,7 +253,8 @@ def unsloth_save_model(\\n     # If push_to_hub, we must remove the .../ part of a repo\\n     if push_to_hub and \"/\" in save_directory:\\n \\n-        new_save_directory = save_directory[save_directory.find(\"/\"):]\\n+        # +1 solves absolute path issues\\n+        new_save_directory = save_directory[save_directory.find(\"/\")+1:]\\n \\n         logger.warning_once(\\n             f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n@@ -861,10 +869,16 @@ def unsloth_save_pretrained_gguf(\\n         \"q4_0\"    : \"Original quant method, 4-bit.\",\\n         \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\\n         \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\\n+        \"q4_k\"    : \"alias for q4_k_m\",\\n+        \"q5_k\"    : \"alias for q5_k_m\",\\n         \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\\n         \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n         \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n         \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n+        \"iq2_xxs\" : \"2.06 bpw quantization\",\\n+        \"iq2_xs\"  : \"2.31 bpw quantization\",\\n+        \"iq3_xxs\" : \"3.06 bpw quantization\",\\n+        \"q3_k_xs\" : \"3-bit extra small quantization\",\\n     \"\"\"\\n     if tokenizer is None:\\n         raise ValueError(\"Unsloth: Saving to GGUF must have a tokenizer.\")\\n',\n",
       " '@@ -30,7 +30,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n | **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\\\* | 62% less |\\n \\n - This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.\\n-- Our [raw text notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for text completion.\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\\n - Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.\\n - \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.\\n \\n@@ -86,9 +86,12 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda create --name unsloth_env python=3.10\\n+conda activate unsloth_env\\n \\n-conda install xformers -c xformers -y\\n+conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers\\n \\n pip install bitsandbytes\\n \\n@@ -141,6 +144,7 @@ pip install --upgrade pip\\n ```\\n \\n ## 📜 Documentation\\n+- Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n - We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n - We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n@@ -162,7 +166,8 @@ fourbit_models = [\\n     \"unsloth/llama-2-13b-bnb-4bit\",\\n     \"unsloth/codellama-34b-bnb-4bit\",\\n     \"unsloth/tinyllama-bnb-4bit\",\\n-]\\n+] # Go to https://huggingface.co/unsloth for more 4-bit models!\\n+\\n # Load Llama model\\n model, tokenizer = FastLanguageModel.from_pretrained(\\n     model_name = \"unsloth/mistral-7b-bnb-4bit\", # Supports Llama, Mistral - replace this!\\n@@ -183,6 +188,8 @@ model = FastLanguageModel.get_peft_model(\\n     use_gradient_checkpointing = True,\\n     random_state = 3407,\\n     max_seq_length = max_seq_length,\\n+    use_rslora = False,  # We support rank stabilized LoRA\\n+    loftq_config = None, # And LoftQ\\n )\\n \\n trainer = SFTTrainer(\\n@@ -205,6 +212,12 @@ trainer = SFTTrainer(\\n     ),\\n )\\n trainer.train()\\n+\\n+# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like\\n+# (1) Saving to GGUF / merging to 16bit for vLLM\\n+# (2) Continued training from a saved LoRA adapter\\n+# (3) Adding an evaluation loop / OOMs\\n+# (4) Cutomized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -42,6 +42,7 @@ huggingface = [\\n     \"tqdm\",\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n+    \"numpy\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -83,22 +84,22 @@ cu121 = [\\n     \"bitsandbytes\",\\n     \"unsloth[cu121only]\",\\n ]\\n-cu118_torch211 = [\\n+cu118-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch211]\",\\n ]\\n-cu121_torch211 = [\\n+cu121-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n-cu118_torch220 = [\\n+cu118-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch220]\",\\n ]\\n-cu121_torch220 = [\\n+cu121-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n@@ -112,18 +113,18 @@ conda = [\\n colab = [\\n     \"unsloth[cu121]\",\\n ]\\n-colab_ampere = [\\n+colab-ampere = [\\n     \"unsloth[cu121]\",\\n     \"packaging\",\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_torch211 = [\\n+colab-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n-colab_ampere_torch211 = [\\n+colab-ampere-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n@@ -131,12 +132,12 @@ colab_ampere_torch211 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_torch220 = [\\n+colab-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n ]\\n-colab_ampere_torch220 = [\\n+colab-ampere-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n@@ -144,7 +145,7 @@ colab_ampere_torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu118_ampere = [\\n+cu118-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118only]\",\\n@@ -152,7 +153,7 @@ cu118_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu121_ampere = [\\n+cu121-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121only]\",\\n@@ -160,7 +161,7 @@ cu121_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu118_ampere_torch211 = [\\n+cu118-ampere-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch211]\",\\n@@ -168,7 +169,7 @@ cu118_ampere_torch211 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu121_ampere_torch211 = [\\n+cu121-ampere-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n@@ -176,7 +177,7 @@ cu121_ampere_torch211 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu118_ampere_torch220 = [\\n+cu118-ampere-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch220]\",\\n@@ -184,7 +185,7 @@ cu118_ampere_torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu121_ampere_torch220 = [\\n+cu121-ampere-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n',\n",
       " '@@ -59,14 +59,38 @@ if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n import bitsandbytes as bnb\\n import triton\\n from triton.common.build import libcuda_dirs\\n+import os\\n+import re\\n+import numpy as np\\n+import subprocess\\n+\\n try:\\n     cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n     libcuda_dirs()\\n except:\\n     warnings.warn(\\n-        \"Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n+        \"Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n     )\\n-    os.system(\"ldconfig /usr/lib64-nvidia\")\\n+\\n+    if os.path.exists(\"/usr/lib64-nvidia\"):\\n+        os.system(\"ldconfig /usr/lib64-nvidia\")\\n+    elif os.path.exists(\"/usr/local\"):\\n+        # Sometimes bitsandbytes cannot be linked properly in Runpod for example\\n+        possible_cudas = subprocess.check_output([\"ls\", \"-al\", \"/usr/local\"]).decode(\"utf-8\").split(\"\\\\n\")\\n+        find_cuda = re.compile(r\"[\\\\s](cuda\\\\-[\\\\d\\\\.]{2,})$\")\\n+        possible_cudas = [find_cuda.search(x) for x in possible_cudas]\\n+        possible_cudas = [x.group(1) for x in possible_cudas if x is not None]\\n+\\n+        # Try linking cuda folder, or everything in local\\n+        if len(possible_cudas) == 0:\\n+            os.system(f\"ldconfig /usr/local/\")\\n+        else:\\n+            find_number = re.compile(r\"([\\\\d\\\\.]{2,})\")\\n+            latest_cuda = np.argsort([float(find_number.search(x).group(1)) for x in possible_cudas])[::-1][0]\\n+            latest_cuda = possible_cudas[latest_cuda]\\n+            os.system(f\"ldconfig /usr/local/{latest_cuda}\")\\n+    pass\\n+\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n@@ -75,9 +99,10 @@ except:\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n-        raise ImportError(\"CUDA is not linked properly.\\\\n\"\\\\\\n+        raise ImportError(\"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n                           \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n-                          \"You need to run in your terminal `ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\")\\n+                          \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n+                          \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\")\\n pass\\n \\n from .models import *\\n',\n",
       " '@@ -17,6 +17,7 @@ from typing import Union, Optional, List, Any, Callable\\n import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n+warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -55,6 +55,7 @@ from peft import PeftModelForCausalLM\\n from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n from ..save import patch_saving_functions\\n+import re, os, inspect, math, sys\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -782,30 +783,33 @@ pass\\n # https://github.com/huggingface/transformers/pull/27931\\n # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n class LlamaRotaryEmbedding(torch.nn.Module):\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\\n         super().__init__()\\n-\\n         self.dim = dim\\n         self.max_position_embeddings = max_position_embeddings\\n         self.base = base\\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\\n \\n         # Build here to make `torch.jit.trace` work.\\n-        self._set_cos_sin_cache(\\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\\n-        )\\n+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())\\n     pass\\n \\n     def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and\\n+        # in FP32. They are applied (multiplied) in FP32 as well.\\n         self.max_seq_len_cached = seq_len\\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\\n+        inv_freq = 1.0 / (\\n+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=\"cpu\").float() / self.dim)\\n+        )\\n+        t = torch.arange(self.max_seq_len_cached, device=\"cpu\", dtype=torch.int64).float()\\n \\n-        freqs = torch.outer(t, self.inv_freq)\\n+        freqs = torch.outer(t, inv_freq)\\n         # Different from paper, but it uses a different permutation in order to obtain the same calculation\\n         emb = torch.cat((freqs, freqs), dim=-1)\\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n     pass\\n \\n     def forward(self, x, seq_len=None):\\n@@ -823,7 +827,9 @@ pass\\n \\n class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n     \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\\n-\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\\n         self.scaling_factor = scaling_factor\\n         super().__init__(dim, max_position_embeddings, base, device)\\n@@ -831,14 +837,17 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n \\n     def _set_cos_sin_cache(self, seq_len, device, dtype):\\n         self.max_seq_len_cached = seq_len\\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\\n+        inv_freq = 1.0 / (\\n+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=\"cpu\").float() / self.dim)\\n+        )\\n+        t = torch.arange(self.max_seq_len_cached, device=\"cpu\", dtype=torch.int64).float()\\n         t = t / self.scaling_factor\\n \\n-        freqs = torch.outer(t, self.inv_freq)\\n+        freqs = torch.outer(t, inv_freq)\\n         # Different from paper, but it uses a different permutation in order to obtain the same calculation\\n         emb = torch.cat((freqs, freqs), dim=-1)\\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n     pass\\n pass\\n \\n@@ -954,6 +963,125 @@ class FastLlamaModel:\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n+        # Patch Trainer\\n+        from transformers.trainer import Trainer\\n+        try:\\n+            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n+                Trainer._original_training_loop = inner_training_loop\\n+            else:\\n+                inner_training_loop = Trainer._original_training_loop\\n+        except:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+\\n+        import transformers.trainer\\n+        items_in_trainer = dir(transformers.trainer)\\n+        good_items = []\\n+        for item in items_in_trainer:\\n+            # TODO: Support Deepspeed\\n+            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n+            if item in inner_training_loop: good_items.append(item)\\n+        pass\\n+        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n+\\n+        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n+        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n+        original_debug = inner_training_loop[start:end]\\n+        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n+        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n+\\n+        debug_info = \"\"\"debug_info = \\\\\\\\\\n+        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n+        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n+        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n+        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n+        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n+        logger.warning_once(debug_info)\"\"\"\\n+\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n+\\n+        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n+            args.gradient_accumulation_steps // self._train_batch_size\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        debug_info =\"\"\"\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n+\\n+        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n+        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n+            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"self.accelerator.free_memory()\",\\n+            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n+            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n+            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n+        )\\n+\\n+        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n+        ga  = args.gradient_accumulation_steps\\n+        bsz = self._train_batch_size\\n+        total_batches = bsz * ga * args.world_size\\n+        n_total_devices = total_batches // ga // bsz\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Please consider a commercial license - Unsloth was designed for the GPU Poor.\\\\\\\\n\"\\n+                \"The OSS currently works on 4 GPUs - we\\'re a 2 person team, so please help fund\\\\\\\\n\"\\n+                \"our development costs by supporting us through Ko-fi or buying a license! Thanks!\",\\n+            )\\n+            divisor = n_total_devices / 2\\n+            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n+            if total_batches // ga // bsz > 2:\\n+                divisor = n_total_devices / 2\\n+                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n+        check_batches = check_batches.split(\\'\\\\n\\')\\n+        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = self.get_train_dataloader()\",\\n+            check_batches, 1,\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"_inner_training_loop\",\\n+            \"_fast_inner_training_loop\", 1,\\n+        )\\n+        exec(inner_training_loop, globals())\\n+\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_torch_tpu_available()\",\\n+            \"False\",\\n+        )\\n+        if \"n_total_devices >\" not in inner_training_loop:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_sagemaker_mp_enabled()\",\\n+            \"False\",\\n+        )\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+\\n         # Save max_seq_length\\n         model.max_seq_length = max_position_embeddings\\n         internal_model = model\\n@@ -1073,7 +1201,7 @@ class FastLlamaModel:\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n-\\n+        \\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n@@ -1200,6 +1328,28 @@ class FastLlamaModel:\\n             model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n \\n+        from transformers.trainer import Trainer \\n+        if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+\\n+        # Fix loftq issues\\n+        # loftq_config must not = None, but rather {}\\n+        all_configs = model.peft_config\\n+        for key, current_config in all_configs.items():\\n+            if hasattr(current_config, \"loftq_config\") and current_config.loftq_config is None:\\n+                new_args = current_config.__dict__\\n+                new_args[\"loftq_config\"] = {}\\n+                current_config = current_config.__class__(**new_args)\\n+                all_configs[key] = current_config\\n+            pass\\n+        pass\\n+\\n         # Do patching\\n         n_mlp = 0\\n         n_qkv = 0\\n',\n",
       " '@@ -118,9 +118,13 @@ class FastLanguageModel(FastLlamaModel):\\n             *args, **kwargs,\\n         )\\n \\n-        # in case the model supports tagging, add the unsloth tag.\\n+        # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n-            model.add_model_tags([\"unsloth\"])\\n+            model.add_model_tags([\"unsloth\",])\\n+        pass\\n+        if hasattr(tokenizer, \"add_model_tags\"):\\n+            tokenizer.add_model_tags([\"unsloth\",])\\n+        pass\\n \\n         if load_in_4bit:\\n             # Fix up bitsandbytes config\\n@@ -143,7 +147,7 @@ class FastLanguageModel(FastLlamaModel):\\n \\n         if is_peft:\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name)\\n+            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -42,6 +42,10 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/tinyllama\",\\n         \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\\n     ),\\n+    \"unsloth/tinyllama-chat-bnb-4bit\" : (\\n+        \"unsloth/tinyllama-chat\",\\n+        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\\n+    ),\\n     \"unsloth/mistral-7b-instruct-v0.1-bnb-4bit\" : (\\n         \"mistralai/Mistral-7B-Instruct-v0.1\",\\n     ),\\n',\n",
       " '@@ -368,6 +368,140 @@ class FastMistralModel(FastLlamaModel):\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n+        # Patch Trainer\\n+        from transformers.trainer import Trainer\\n+        if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+            try:\\n+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n+            except:\\n+                raise RuntimeError(\\n+                    \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                    \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                    \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                    \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+                )\\n+            pass\\n+        pass\\n+\\n+        # Patch Trainer\\n+        from transformers.trainer import Trainer\\n+        try:\\n+            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n+                Trainer._original_training_loop = inner_training_loop\\n+            else:\\n+                inner_training_loop = Trainer._original_training_loop\\n+        except:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+\\n+        import transformers.trainer\\n+        items_in_trainer = dir(transformers.trainer)\\n+        good_items = []\\n+        for item in items_in_trainer:\\n+            # TODO: Support Deepspeed\\n+            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n+            if item in inner_training_loop: good_items.append(item)\\n+        pass\\n+        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n+\\n+        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n+        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n+        original_debug = inner_training_loop[start:end]\\n+        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n+        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n+\\n+        debug_info = \"\"\"debug_info = \\\\\\\\\\n+        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n+        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n+        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n+        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n+        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n+        logger.warning_once(debug_info)\"\"\"\\n+\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n+\\n+        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n+            args.gradient_accumulation_steps // self._train_batch_size\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        debug_info =\"\"\"\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n+\\n+        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n+        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n+            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"self.accelerator.free_memory()\",\\n+            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n+            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n+            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n+        )\\n+\\n+        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n+        ga  = args.gradient_accumulation_steps\\n+        bsz = self._train_batch_size\\n+        total_batches = bsz * ga * args.world_size\\n+        n_total_devices = total_batches // ga // bsz\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Please consider a commercial license - Unsloth was designed for the GPU Poor.\\\\\\\\n\"\\n+                \"The OSS currently works on 4 GPUs - we\\'re a 2 person team, so please help fund\\\\\\\\n\"\\n+                \"our development costs by supporting us through Ko-fi or buying a license! Thanks!\",\\n+            )\\n+            divisor = n_total_devices / 2\\n+            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n+            if total_batches // ga // bsz > 2:\\n+                divisor = n_total_devices / 2\\n+                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n+        check_batches = check_batches.split(\\'\\\\n\\')\\n+        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = self.get_train_dataloader()\",\\n+            check_batches, 1,\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"_inner_training_loop\",\\n+            \"_fast_inner_training_loop\", 1,\\n+        )\\n+        exec(inner_training_loop, globals())\\n+\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_torch_tpu_available()\",\\n+            \"False\",\\n+        )\\n+        if \"n_total_devices >\" not in inner_training_loop:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_sagemaker_mp_enabled()\",\\n+            \"False\",\\n+        )\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+\\n         # Save max_seq_length\\n         max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n         model.max_seq_length = max_position_embeddings\\n',\n",
       " '@@ -140,17 +140,28 @@ def unsloth_save_model(\\n \\n     # Push to hub\\n     use_temp_dir         : Optional[bool] = None,\\n-    commit_message       : Optional[str] = None,\\n+    commit_message       : Optional[str] = \"Trained with Unsloth\",\\n     private              : Optional[bool] = None,\\n     create_pr            : bool = False,\\n     revision             : str = None,\\n-    commit_description   : str = None,\\n+    commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : List[str] = None,\\n \\n     # Our functions\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if commit_message is None: commit_message = \"\"\\n+    if \"Unsloth\" not in commit_message:\\n+        commit_message += \" (Trained with Unsloth)\"\\n+    commit_message = commit_message.lstrip()\\n+\\n+    if commit_description is None:\\n+        commit_description = \"Upload model trained with Unsloth 2x faster\"\\n+    elif \"Unsloth 2x faster\" not in commit_description:\\n+        commit_description += \" (Trained with Unsloth 2x faster)\"\\n+    pass\\n+\\n     if save_method == \"merged_4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\\\\n\"\\\\\\n@@ -202,7 +213,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if ((save_method == \"lora\") or (save_method == \"merged_4bit\")) and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -210,7 +221,20 @@ def unsloth_save_model(\\n             )\\n         pass\\n \\n-        model.push_to_hub(\\n+        if save_method == \"lora\":\\n+            print(\"Unsloth: Saving LoRA adapters. Please wait...\")\\n+        elif save_method == \"merged_4bit\":\\n+            print(\"Unsloth: Saving 4bit Bitsandbytes model. Please wait...\")\\n+        pass\\n+\\n+        # Update model tag\\n+        _ = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+            old_username = None, private = private,\\n+        )\\n+\\n+        model.original_push_to_hub(\\n             repo_id            = save_directory,\\n             use_temp_dir       = use_temp_dir,\\n             commit_message     = commit_message,\\n@@ -224,7 +248,7 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n-            tokenizer.push_to_hub(\\n+            tokenizer.original_push_to_hub(\\n                 repo_id            = save_directory,\\n                 use_temp_dir       = use_temp_dir,\\n                 commit_message     = commit_message,\\n@@ -238,31 +262,11 @@ def unsloth_save_model(\\n                 tags               = tags,\\n             )\\n         pass\\n-        return save_directory\\n-    pass\\n-\\n-    # Update model tag\\n-    username = \"\"\\n-    if push_to_hub:\\n-        username = upload_to_huggingface(\\n-            model, save_directory, token,\\n-            \"finetuned\", \"trl\", file_location = None,\\n-        )\\n-    pass\\n-\\n-    # If push_to_hub, we must remove the .../ part of a repo\\n-    if push_to_hub and \"/\" in save_directory:\\n-\\n-        # +1 solves absolute path issues\\n-        new_save_directory = save_directory[save_directory.find(\"/\")+1:]\\n-\\n-        logger.warning_once(\\n-            f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n-            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n-        )\\n \\n-        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n-        save_directory = new_save_directory\\n+        if hasattr(model, \"config\"):\\n+            print(f\"Saved {save_method} model to https://huggingface.co/\" + save_directory)\\n+        pass\\n+        return save_directory\\n     pass\\n \\n     # Tokenizer has different saving arguments\\n@@ -292,13 +296,25 @@ def unsloth_save_model(\\n         # Do general saving\\n         # Edit save_pretrained_settings\\n         # [TODO] _create_repo has errors due to **kwargs getting accepted\\n-        for deletion in \\\\\\n-            (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+        # commit_description does not seem to work?\\n+        what_to_delete = (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",) \\\\\\n+            if save_pretrained_settings[\"push_to_hub\"] is False else \\\\\\n+            (\"use_temp_dir\", \"create_pr\", \"revision\", \"tags\", \"commit_description\",)\\n+        for deletion in what_to_delete:\\n             del save_pretrained_settings[deletion]\\n         pass\\n         if hasattr(model, \"add_model_tags\"):\\n             model.add_model_tags([\"unsloth\",])\\n \\n+        # Update model tag\\n+        if push_to_hub:\\n+             _ = upload_to_huggingface(\\n+                model, save_pretrained_settings[\"save_directory\"], token,\\n+                \"finetuned\", \"trl\", file_location = None,\\n+                old_username = None, private = private,\\n+            )\\n+        pass\\n+\\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n@@ -310,10 +326,33 @@ def unsloth_save_model(\\n         if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n+\\n+        if push_to_hub and hasattr(model, \"config\"):\\n+            print(\"Saved to https://huggingface.co/\" + save_pretrained_settings[\"save_directory\"])\\n+        pass\\n+\\n         print(\" Done.\")\\n         return save_directory\\n     pass\\n \\n+    # If push_to_hub, we must remove the .../ part of a repo\\n+    username = None\\n+    if push_to_hub and \"/\" in save_directory:\\n+\\n+        # +1 solves absolute path issues\\n+        username = save_directory[:save_directory.find(\"/\")]\\n+        new_save_directory = save_directory[save_directory.find(\"/\")+1:]\\n+\\n+        logger.warning_once(\\n+            f\"Unsloth: You are pushing to hub, but you passed your HF username = {username}.\\\\n\"\\\\\\n+            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n+        )\\n+\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        tokenizer_save_settings [\"save_directory\"] = new_save_directory\\n+        save_directory = new_save_directory\\n+    pass\\n+\\n     print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n \\n     # Determine max RAM usage minus sharding\\n@@ -339,7 +378,7 @@ def unsloth_save_model(\\n         logger.warning_once(\\n             f\"Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\\\\n\"\\\\\\n             f\"We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\\\\n\"\\\\\\n-            f\"To force `safe_serialization`, set it to None instead.\",\\n+            f\"To force `safe_serialization`, set it to `None` instead.\",\\n         )\\n         safe_serialization = False\\n         save_function = fast_save_pickle\\n@@ -413,13 +452,26 @@ def unsloth_save_model(\\n     # Edit save_pretrained_settings\\n     # [TODO] _create_repo has errors due to **kwargs getting accepted\\n     save_pretrained_settings[\"state_dict\"] = state_dict\\n-    for deletion in \\\\\\n-        (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+    \\n+    # commit_description does not seem to work?\\n+    what_to_delete = (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",) \\\\\\n+        if not push_to_hub else \\\\\\n+        (\"use_temp_dir\", \"create_pr\", \"revision\", \"tags\", \"commit_description\",)\\n+    for deletion in what_to_delete:\\n         del save_pretrained_settings[deletion]\\n     pass\\n     if hasattr(model, \"add_model_tags\"):\\n         model.add_model_tags([\"unsloth\",])\\n \\n+    # Update model tag\\n+    if push_to_hub:\\n+        _ = upload_to_huggingface(\\n+            model, save_pretrained_settings[\"save_directory\"], token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+            old_username = username, private = private,\\n+        )\\n+    pass\\n+\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n@@ -452,9 +504,8 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n-    # Print location\\n-    if push_to_hub:\\n-        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    if push_to_hub and hasattr(model, \"config\"):\\n+        print(f\"Saved merged model to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n     pass\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -478,7 +529,7 @@ def unsloth_save_model(\\n     for _ in range(3):\\n         torch.cuda.empty_cache()\\n         gc.collect()\\n-    return save_directory\\n+    return save_directory, username\\n pass\\n \\n \\n@@ -494,7 +545,7 @@ def install_llama_cpp_make_non_blocking():\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n-    full_command = [\"make\", \"all\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    full_command = [\"make\", \"all\", \"-j\"+str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n pass\\n@@ -507,10 +558,44 @@ def install_python_non_blocking(packages = []):\\n pass\\n \\n \\n+def install_llama_cpp_old(version = -10):\\n+    # Download the 10th latest release since the latest might be broken!\\n+    # FALLBACK mechanism\\n+    releases = subprocess.check_output([\"git\", \"ls-remote\", \"--tags\", \"https://github.com/ggerganov/llama.cpp.git\"])\\n+    releases = releases.decode(\"utf-8\").replace(\"\\\\t\", \" \").split(\"\\\\n\")\\n+    for i, x in enumerate(releases):\\n+        if \"refs/tags/b\" not in x: break\\n+    releases = releases[:i]\\n+    latest = releases[-1]\\n+    version = releases[version].split(\" \")[0]\\n+\\n+    # Clone a specific commit\\n+    commands = [\\n+        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n+        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        \"pip install gguf protobuf\",\\n+    ]\\n+    for command in commands:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+    pass\\n+    # Check if successful\\n+    if not os.path.exists(\"llama.cpp/quantize\"):\\n+        raise RuntimeError(\\n+            \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n+            \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n+        )\\n+    pass\\n+pass\\n+\\n+\\n def install_llama_cpp_blocking():\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j {psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -563,10 +648,13 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n     if _run_installer is not None:\\n-        _run_installer.wait()\\n+        error = _run_installer.wait()\\n     else:\\n+        error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+    # Check if successful. If not install 10th latest release\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -580,15 +668,18 @@ def save_to_gguf(\\n             first_conversion = \"f16\"\\n         pass\\n     pass\\n-    print(f\"Unsloth: [1] Converting HF into {first_conversion} GGUF format. This will take 3 minutes...\")\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n     \\n     final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n \\n+    print(f\"Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\\\\n\"\\\\\\n+          f\"The output location will be {final_location}\\\\n\"\\\\\\n+          \"This will take 3 minutes...\")\\n+\\n     command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-        f\"--outfile {final_location} \"\\\\\\n+        f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n         f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n@@ -601,7 +692,8 @@ def save_to_gguf(\\n     # Check if quantization succeeded!\\n     if not os.path.isfile(final_location):\\n         raise RuntimeError(\\n-            \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+            f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+            \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n@@ -662,7 +754,7 @@ def unsloth_save_pretrained_merged(\\n     save_peft_format     : bool = True,\\n     tags                 : List[str] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n-    maximum_memory_usage : float = 0.85,   \\n+    maximum_memory_usage : float = 0.85,\\n ):\\n     \"\"\"\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n@@ -695,14 +787,14 @@ def unsloth_push_to_hub_merged(\\n     tokenizer            = None,\\n     save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n-    commit_message       : Optional[str] = None,\\n+    commit_message       : Optional[str] = \"Trained with Unsloth\",\\n     private              : Optional[bool] = None,\\n     token                : Union[bool, str, None] = None,\\n     max_shard_size       : Union[int, str, None] = \"5GB\",\\n     create_pr            : bool = False,\\n     safe_serialization   : bool = True,\\n     revision             : str = None,\\n-    commit_description   : str = None,\\n+    commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : Optional[List[str]] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.85,\\n@@ -760,15 +852,27 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/\\n [<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n \"\"\"\\n \\n-def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n+def upload_to_huggingface(\\n+    model,\\n+    save_directory,\\n+    token,\\n+    method,\\n+    extra = \"\",\\n+    file_location = None,\\n+    old_username = None,\\n+    private = None,\\n+):\\n     # Check for username\\n     username = \"\"\\n     save_directory = save_directory.lstrip(\"./\")\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n         try: \\n-            username = whoami()[\\'name\\']\\n-            save_directory = f\"{save_directory}/{username}\"\\n+            username = whoami(token = token)[\"name\"]\\n+            if type(old_username) is str and username != old_username:\\n+                username = old_username\\n+            pass\\n+            save_directory = f\"{username}/{save_directory}\"\\n         except:\\n             raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n     else:\\n@@ -776,24 +880,28 @@ def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file\\n     pass\\n \\n     from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    # Create model card\\n-    from huggingface_hub import ModelCard\\n-    content = MODEL_CARD.format(\\n-        username   = username,\\n-        base_model = model.config._name_or_path,\\n-        model_type = model.config.model_type,\\n-        method     = \"\",\\n-        extra      = extra,\\n-    )\\n-    card = ModelCard(content)\\n-    card.push_to_hub(save_directory, token = token)\\n+    try:\\n+        create_repo(\\n+            repo_id   = save_directory,\\n+            token     = token,\\n+            repo_type = \"model\",\\n+            exist_ok  = False,\\n+            private   = private,\\n+        ) \\n+\\n+        # Create model card\\n+        from huggingface_hub import ModelCard\\n+        content = MODEL_CARD.format(\\n+            username   = username,\\n+            base_model = model.config._name_or_path,\\n+            model_type = model.config.model_type,\\n+            method     = \"\",\\n+            extra      = extra,\\n+        )\\n+        card = ModelCard(content)\\n+        card.push_to_hub(save_directory, token = token)\\n+    except:\\n+        pass\\n \\n     if file_location is not None:\\n         # Now upload file\\n@@ -811,6 +919,7 @@ def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file\\n             path_in_repo    = uploaded_location,\\n             repo_id         = save_directory,\\n             repo_type       = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n         )\\n \\n         # We also upload a config.json file\\n@@ -823,6 +932,7 @@ def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file\\n             path_in_repo    = \"config.json\",\\n             repo_id         = save_directory,\\n             repo_type       = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n         )\\n         os.remove(\"_temporary_unsloth_config.json\")\\n     pass\\n@@ -838,6 +948,7 @@ def unsloth_save_pretrained_gguf(\\n     first_conversion     : str = \"f16\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n+    private              : Optional[bool] = None,\\n     is_main_process      : bool = True,\\n     state_dict           : Optional[dict] = None,\\n     save_function        : Callable = torch.save,\\n@@ -847,7 +958,7 @@ def unsloth_save_pretrained_gguf(\\n     save_peft_format     : bool = True,\\n     tags                 : List[str] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n-    maximum_memory_usage : float = 0.85,   \\n+    maximum_memory_usage : float = 0.85,\\n ):\\n     \"\"\"\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n@@ -898,11 +1009,11 @@ def unsloth_save_pretrained_gguf(\\n         python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n         git_clone.wait()\\n         makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory = unsloth_save_model(**arguments)\\n+        new_save_directory, old_username = unsloth_save_model(**arguments)\\n         python_install.wait()\\n     else:\\n         try:\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n@@ -910,7 +1021,7 @@ def unsloth_save_pretrained_gguf(\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n             makefile  = install_llama_cpp_make_non_blocking()\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n     pass\\n@@ -924,12 +1035,12 @@ def unsloth_save_pretrained_gguf(\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n         username = upload_to_huggingface(\\n             self, save_directory, token,\\n-            \"GGUF converted\", \"gguf\", file_location,\\n+            \"GGUF converted\", \"gguf\", file_location, old_username, private,\\n         )\\n         link = f\"{username}/{new_save_directory.lstrip(\\'/.\\')}\" \\\\\\n             if username not in new_save_directory else \\\\\\n             new_save_directory.lstrip(\\'/.\\')\\n-        print(f\"Saved to https://huggingface.co/{link}\")\\n+        print(f\"Saved GGUF to https://huggingface.co/{link}\")\\n     pass\\n pass\\n \\n@@ -941,14 +1052,14 @@ def unsloth_push_to_hub_gguf(\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n     use_temp_dir         : Optional[bool] = None,\\n-    commit_message       : Optional[str] = None,\\n+    commit_message       : Optional[str] = \"Trained with Unsloth\",\\n     private              : Optional[bool] = None,\\n     token                : Union[bool, str, None] = None,\\n     max_shard_size       : Union[int, str, None] = \"5GB\",\\n     create_pr            : bool = False,\\n     safe_serialization   : bool = True,\\n     revision             : str = None,\\n-    commit_description   : str = None,\\n+    commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : Optional[List[str]] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.85,\\n@@ -998,19 +1109,19 @@ def unsloth_push_to_hub_gguf(\\n         python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n         git_clone.wait()\\n         makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory = unsloth_save_model(**arguments)\\n+        new_save_directory, old_username = unsloth_save_model(**arguments)\\n         python_install.wait()\\n     else:\\n         try:\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n-            makefile  = install_llama_cpp_make_non_blocking()\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n     pass\\n@@ -1023,12 +1134,12 @@ def unsloth_push_to_hub_gguf(\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n         self, repo_id, token,\\n-        \"GGUF converted\", \"gguf\", file_location,\\n+        \"GGUF converted\", \"gguf\", file_location, old_username, private,\\n     )\\n     link = f\"{username}/{new_save_directory.lstrip(\\'/.\\')}\" \\\\\\n         if username not in new_save_directory else \\\\\\n         new_save_directory.lstrip(\\'/.\\')\\n-    print(f\"Saved to https://huggingface.co/{link}\")\\n+    print(f\"Saved GGUF to https://huggingface.co/{link}\")\\n pass\\n \\n \\n@@ -1038,31 +1149,17 @@ def patch_saving_functions(model):\\n     import types\\n     from typing import Callable, Optional, Union, List\\n \\n-    if hasattr(model, \"_original_push_to_hub\"): return\\n-\\n-    # First check if this has already been called, and revert it\\n-    original_model = model\\n-    while True:\\n-        if hasattr(original_model, \"_original_push_to_hub\"):\\n-            original_model.push_to_hub = original_model._original_push_to_hub\\n-            del original_model._original_push_to_hub\\n-            if hasattr(original_model, \"push_to_hub_merged\"):     del original_model.push_to_hub_merged\\n-            if hasattr(original_model, \"save_pretrained_merged\"): del original_model.save_pretrained_merged\\n-            if hasattr(original_model, \"push_to_hub_gguf\"):       del original_model.push_to_hub_gguf\\n-            if hasattr(original_model, \"save_pretrained_gguf\"):   del original_model.save_pretrained_gguf\\n-        pass\\n-\\n-        if hasattr(original_model, \"model\"): original_model = original_model.model\\n-        else: break\\n+    # And now re add our saving methods!\\n+    if model.push_to_hub.__name__ == \"unsloth_push_to_hub\":\\n+        original_push_to_hub = model.original_push_to_hub\\n+    else:\\n+        original_push_to_hub = model.push_to_hub\\n     pass\\n \\n-    # And now re add our saving methods!\\n-    original_push_to_hub = model.push_to_hub\\n     signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n     signature = signature[1:]\\n     signature = re.sub(\"<function save at .+?>\", \"torch.save\", signature)\\n     docs = original_push_to_hub.__doc__.encode(\"utf-8\").decode(\"utf-8\")\\n-    model._original_push_to_hub = original_push_to_hub\\n \\n     push_to_hub_text = f\\'\\'\\'def unsloth_push_to_hub(self, {signature}:\\n     \"\"\"\\n@@ -1077,11 +1174,45 @@ def patch_saving_functions(model):\\n         arguments[\"tags\"] = [\"unsloth\",]\\n     elif hasattr(self, \"add_model_tags\"):\\n         self.add_model_tags([\"unsloth\",])\\n+\\n+    if \"commit_message\" in arguments:\\n+        commit_message = arguments[\"commit_message\"]\\n+        if commit_message is not None:\\n+            if not commit_message.endswith(\" \"): commit_message += \" \"\\n+            if \"Unsloth\" not in commit_message:\\n+                commit_message += \"(Trained with Unsloth)\"\\n+        else:\\n+            commit_message = \"Upload model trained with Unsloth\"\\n+        arguments[\"commit_message\"] = commit_message\\n+\\n+    if \"commit_description\" in arguments:\\n+        commit_description = arguments[\"commit_description\"]\\n+        if commit_description is not None:\\n+            if not commit_description.endswith(\" \"): commit_description += \" \"\\n+            if \"Unsloth\" not in commit_description:\\n+                commit_description += \"(Trained with Unsloth 2x faster)\"\\n+        else:\\n+            commit_description = \"Upload model trained with Unsloth 2x faster\"\\n+        arguments[\"commit_description\"] = commit_description\\n+\\n+    # Update model tag\\n+    if hasattr(self, \"config\"):\\n+        _ = upload_to_huggingface(\\n+            self, arguments[\"repo_id\"], arguments[\"token\"],\\n+            \"finetuned\", \"trl\", file_location = None,\\n+            old_username = None, private = arguments[\"private\"],\\n+        )\\n+    pass\\n+\\n     try:\\n-        return self._original_push_to_hub(**arguments)\\n+        self.original_push_to_hub(**arguments)\\n     except:\\n         del arguments[\"tags\"]\\n-        return self._original_push_to_hub(**arguments)\\n+        self.original_push_to_hub(**arguments)\\n+    pass\\n+\\n+    if hasattr(self, \"config\"):\\n+        print(\"Saved model to https://huggingface.co/\" + arguments[\"repo_id\"])\\n     pass\\n     \\'\\'\\'\\n     exec(push_to_hub_text, globals())\\n@@ -1089,12 +1220,12 @@ def patch_saving_functions(model):\\n     original_model = model\\n     while True:\\n \\n-        if not hasattr(original_model, \"_original_push_to_hub\"):\\n-            original_model._original_push_to_hub = original_model.push_to_hub\\n+        if original_model.push_to_hub.__name__ != \"unsloth_push_to_hub\":\\n+            original_model.original_push_to_hub = original_model.push_to_hub\\n             original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n-\\n             if hasattr(original_model, \"add_model_tags\"):\\n                 original_model.add_model_tags([\"unsloth\",])\\n+            pass\\n         pass\\n \\n         if hasattr(original_model, \"model\"): original_model = original_model.model\\n',\n",
       " '@@ -30,7 +30,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n | **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\\\* | 62% less |\\n \\n - This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.\\n-- Our [raw text notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is useful for text completion.\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\\n - Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.\\n - \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.\\n \\n@@ -86,9 +86,12 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda create --name unsloth_env python=3.10\\n+conda activate unsloth_env\\n \\n-conda install xformers -c xformers -y\\n+conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+\\n+conda install xformers -c xformers\\n \\n pip install bitsandbytes\\n \\n@@ -141,6 +144,7 @@ pip install --upgrade pip\\n ```\\n \\n ## 📜 Documentation\\n+- Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n - We support Huggingface\\'s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\\n - We\\'re in 🤗Hugging Face\\'s official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\\n \\n@@ -162,7 +166,8 @@ fourbit_models = [\\n     \"unsloth/llama-2-13b-bnb-4bit\",\\n     \"unsloth/codellama-34b-bnb-4bit\",\\n     \"unsloth/tinyllama-bnb-4bit\",\\n-]\\n+] # Go to https://huggingface.co/unsloth for more 4-bit models!\\n+\\n # Load Llama model\\n model, tokenizer = FastLanguageModel.from_pretrained(\\n     model_name = \"unsloth/mistral-7b-bnb-4bit\", # Supports Llama, Mistral - replace this!\\n@@ -183,6 +188,8 @@ model = FastLanguageModel.get_peft_model(\\n     use_gradient_checkpointing = True,\\n     random_state = 3407,\\n     max_seq_length = max_seq_length,\\n+    use_rslora = False,  # We support rank stabilized LoRA\\n+    loftq_config = None, # And LoftQ\\n )\\n \\n trainer = SFTTrainer(\\n@@ -205,6 +212,12 @@ trainer = SFTTrainer(\\n     ),\\n )\\n trainer.train()\\n+\\n+# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like\\n+# (1) Saving to GGUF / merging to 16bit for vLLM\\n+# (2) Continued training from a saved LoRA adapter\\n+# (3) Adding an evaluation loop / OOMs\\n+# (4) Cutomized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -42,6 +42,7 @@ huggingface = [\\n     \"tqdm\",\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n+    \"numpy\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -83,22 +84,22 @@ cu121 = [\\n     \"bitsandbytes\",\\n     \"unsloth[cu121only]\",\\n ]\\n-cu118_torch211 = [\\n+cu118-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch211]\",\\n ]\\n-cu121_torch211 = [\\n+cu121-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n-cu118_torch220 = [\\n+cu118-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch220]\",\\n ]\\n-cu121_torch220 = [\\n+cu121-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n@@ -112,18 +113,18 @@ conda = [\\n colab = [\\n     \"unsloth[cu121]\",\\n ]\\n-colab_ampere = [\\n+colab-ampere = [\\n     \"unsloth[cu121]\",\\n     \"packaging\",\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_torch211 = [\\n+colab-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n-colab_ampere_torch211 = [\\n+colab-ampere-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n@@ -131,12 +132,12 @@ colab_ampere_torch211 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-colab_torch220 = [\\n+colab-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n ]\\n-colab_ampere_torch220 = [\\n+colab-ampere-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n@@ -144,7 +145,7 @@ colab_ampere_torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu118_ampere = [\\n+cu118-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118only]\",\\n@@ -152,7 +153,7 @@ cu118_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu121_ampere = [\\n+cu121-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121only]\",\\n@@ -160,7 +161,7 @@ cu121_ampere = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu118_ampere_torch211 = [\\n+cu118-ampere-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch211]\",\\n@@ -168,7 +169,7 @@ cu118_ampere_torch211 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu121_ampere_torch211 = [\\n+cu121-ampere-torch211 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n@@ -176,7 +177,7 @@ cu121_ampere_torch211 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu118_ampere_torch220 = [\\n+cu118-ampere-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu118onlytorch220]\",\\n@@ -184,7 +185,7 @@ cu118_ampere_torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n-cu121_ampere_torch220 = [\\n+cu121-ampere-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n',\n",
       " '@@ -59,14 +59,38 @@ if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n import bitsandbytes as bnb\\n import triton\\n from triton.common.build import libcuda_dirs\\n+import os\\n+import re\\n+import numpy as np\\n+import subprocess\\n+\\n try:\\n     cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n     libcuda_dirs()\\n except:\\n     warnings.warn(\\n-        \"Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n+        \"Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\"\\\\\\n     )\\n-    os.system(\"ldconfig /usr/lib64-nvidia\")\\n+\\n+    if os.path.exists(\"/usr/lib64-nvidia\"):\\n+        os.system(\"ldconfig /usr/lib64-nvidia\")\\n+    elif os.path.exists(\"/usr/local\"):\\n+        # Sometimes bitsandbytes cannot be linked properly in Runpod for example\\n+        possible_cudas = subprocess.check_output([\"ls\", \"-al\", \"/usr/local\"]).decode(\"utf-8\").split(\"\\\\n\")\\n+        find_cuda = re.compile(r\"[\\\\s](cuda\\\\-[\\\\d\\\\.]{2,})$\")\\n+        possible_cudas = [find_cuda.search(x) for x in possible_cudas]\\n+        possible_cudas = [x.group(1) for x in possible_cudas if x is not None]\\n+\\n+        # Try linking cuda folder, or everything in local\\n+        if len(possible_cudas) == 0:\\n+            os.system(f\"ldconfig /usr/local/\")\\n+        else:\\n+            find_number = re.compile(r\"([\\\\d\\\\.]{2,})\")\\n+            latest_cuda = np.argsort([float(find_number.search(x).group(1)) for x in possible_cudas])[::-1][0]\\n+            latest_cuda = possible_cudas[latest_cuda]\\n+            os.system(f\"ldconfig /usr/local/{latest_cuda}\")\\n+    pass\\n+\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n@@ -75,9 +99,10 @@ except:\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n-        raise ImportError(\"CUDA is not linked properly.\\\\n\"\\\\\\n+        raise ImportError(\"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n                           \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n-                          \"You need to run in your terminal `ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\")\\n+                          \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n+                          \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\")\\n pass\\n \\n from .models import *\\n',\n",
       " '@@ -17,6 +17,7 @@ from typing import Union, Optional, List, Any, Callable\\n import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n+warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -55,6 +55,7 @@ from peft import PeftModelForCausalLM\\n from bitsandbytes.nn import Linear4bit as Bnb_Linear4bit\\n from peft.tuners.lora import Linear4bit as Peft_Linear4bit\\n from ..save import patch_saving_functions\\n+import re, os, inspect, math, sys\\n \\n \\n def original_apply_qkv(self, X):\\n@@ -782,30 +783,33 @@ pass\\n # https://github.com/huggingface/transformers/pull/27931\\n # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n class LlamaRotaryEmbedding(torch.nn.Module):\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\\n         super().__init__()\\n-\\n         self.dim = dim\\n         self.max_position_embeddings = max_position_embeddings\\n         self.base = base\\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\\n \\n         # Build here to make `torch.jit.trace` work.\\n-        self._set_cos_sin_cache(\\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\\n-        )\\n+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())\\n     pass\\n \\n     def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and\\n+        # in FP32. They are applied (multiplied) in FP32 as well.\\n         self.max_seq_len_cached = seq_len\\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\\n+        inv_freq = 1.0 / (\\n+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=\"cpu\").float() / self.dim)\\n+        )\\n+        t = torch.arange(self.max_seq_len_cached, device=\"cpu\", dtype=torch.int64).float()\\n \\n-        freqs = torch.outer(t, self.inv_freq)\\n+        freqs = torch.outer(t, inv_freq)\\n         # Different from paper, but it uses a different permutation in order to obtain the same calculation\\n         emb = torch.cat((freqs, freqs), dim=-1)\\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n     pass\\n \\n     def forward(self, x, seq_len=None):\\n@@ -823,7 +827,9 @@ pass\\n \\n class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n     \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\\n-\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\\n         self.scaling_factor = scaling_factor\\n         super().__init__(dim, max_position_embeddings, base, device)\\n@@ -831,14 +837,17 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n \\n     def _set_cos_sin_cache(self, seq_len, device, dtype):\\n         self.max_seq_len_cached = seq_len\\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\\n+        inv_freq = 1.0 / (\\n+            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64, device=\"cpu\").float() / self.dim)\\n+        )\\n+        t = torch.arange(self.max_seq_len_cached, device=\"cpu\", dtype=torch.int64).float()\\n         t = t / self.scaling_factor\\n \\n-        freqs = torch.outer(t, self.inv_freq)\\n+        freqs = torch.outer(t, inv_freq)\\n         # Different from paper, but it uses a different permutation in order to obtain the same calculation\\n         emb = torch.cat((freqs, freqs), dim=-1)\\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\\n+        self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n+        self.register_buffer(\"sin_cached\", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n     pass\\n pass\\n \\n@@ -954,6 +963,125 @@ class FastLlamaModel:\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n+        # Patch Trainer\\n+        from transformers.trainer import Trainer\\n+        try:\\n+            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n+                Trainer._original_training_loop = inner_training_loop\\n+            else:\\n+                inner_training_loop = Trainer._original_training_loop\\n+        except:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+\\n+        import transformers.trainer\\n+        items_in_trainer = dir(transformers.trainer)\\n+        good_items = []\\n+        for item in items_in_trainer:\\n+            # TODO: Support Deepspeed\\n+            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n+            if item in inner_training_loop: good_items.append(item)\\n+        pass\\n+        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n+\\n+        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n+        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n+        original_debug = inner_training_loop[start:end]\\n+        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n+        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n+\\n+        debug_info = \"\"\"debug_info = \\\\\\\\\\n+        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n+        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n+        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n+        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n+        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n+        logger.warning_once(debug_info)\"\"\"\\n+\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n+\\n+        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n+            args.gradient_accumulation_steps // self._train_batch_size\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        debug_info =\"\"\"\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n+\\n+        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n+        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n+            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"self.accelerator.free_memory()\",\\n+            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n+            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n+            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n+        )\\n+\\n+        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n+        ga  = args.gradient_accumulation_steps\\n+        bsz = self._train_batch_size\\n+        total_batches = bsz * ga * args.world_size\\n+        n_total_devices = total_batches // ga // bsz\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Please consider a commercial license - Unsloth was designed for the GPU Poor.\\\\\\\\n\"\\n+                \"The OSS currently works on 4 GPUs - we\\'re a 2 person team, so please help fund\\\\\\\\n\"\\n+                \"our development costs by supporting us through Ko-fi or buying a license! Thanks!\",\\n+            )\\n+            divisor = n_total_devices / 2\\n+            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n+            if total_batches // ga // bsz > 2:\\n+                divisor = n_total_devices / 2\\n+                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n+        check_batches = check_batches.split(\\'\\\\n\\')\\n+        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = self.get_train_dataloader()\",\\n+            check_batches, 1,\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"_inner_training_loop\",\\n+            \"_fast_inner_training_loop\", 1,\\n+        )\\n+        exec(inner_training_loop, globals())\\n+\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_torch_tpu_available()\",\\n+            \"False\",\\n+        )\\n+        if \"n_total_devices >\" not in inner_training_loop:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_sagemaker_mp_enabled()\",\\n+            \"False\",\\n+        )\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+\\n         # Save max_seq_length\\n         model.max_seq_length = max_position_embeddings\\n         internal_model = model\\n@@ -1073,7 +1201,7 @@ class FastLlamaModel:\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n-\\n+        \\n         assert(max_seq_length <= model.max_seq_length)\\n \\n         if lora_dropout != 0:\\n@@ -1200,6 +1328,28 @@ class FastLlamaModel:\\n             model.peft_config[active_adapter].revision = f\"unsloth\"\\n         pass\\n \\n+        from transformers.trainer import Trainer \\n+        if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+\\n+        # Fix loftq issues\\n+        # loftq_config must not = None, but rather {}\\n+        all_configs = model.peft_config\\n+        for key, current_config in all_configs.items():\\n+            if hasattr(current_config, \"loftq_config\") and current_config.loftq_config is None:\\n+                new_args = current_config.__dict__\\n+                new_args[\"loftq_config\"] = {}\\n+                current_config = current_config.__class__(**new_args)\\n+                all_configs[key] = current_config\\n+            pass\\n+        pass\\n+\\n         # Do patching\\n         n_mlp = 0\\n         n_qkv = 0\\n',\n",
       " '@@ -118,9 +118,13 @@ class FastLanguageModel(FastLlamaModel):\\n             *args, **kwargs,\\n         )\\n \\n-        # in case the model supports tagging, add the unsloth tag.\\n+        # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n-            model.add_model_tags([\"unsloth\"])\\n+            model.add_model_tags([\"unsloth\",])\\n+        pass\\n+        if hasattr(tokenizer, \"add_model_tags\"):\\n+            tokenizer.add_model_tags([\"unsloth\",])\\n+        pass\\n \\n         if load_in_4bit:\\n             # Fix up bitsandbytes config\\n@@ -143,7 +147,7 @@ class FastLanguageModel(FastLlamaModel):\\n \\n         if is_peft:\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name)\\n+            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -42,6 +42,10 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/tinyllama\",\\n         \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\\n     ),\\n+    \"unsloth/tinyllama-chat-bnb-4bit\" : (\\n+        \"unsloth/tinyllama-chat\",\\n+        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\\n+    ),\\n     \"unsloth/mistral-7b-instruct-v0.1-bnb-4bit\" : (\\n         \"mistralai/Mistral-7B-Instruct-v0.1\",\\n     ),\\n',\n",
       " '@@ -368,6 +368,140 @@ class FastMistralModel(FastLlamaModel):\\n             layer.self_attn.apply_o   = original_apply_o\\n         pass\\n \\n+        # Patch Trainer\\n+        from transformers.trainer import Trainer\\n+        if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+            try:\\n+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n+            except:\\n+                raise RuntimeError(\\n+                    \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                    \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                    \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                    \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+                )\\n+            pass\\n+        pass\\n+\\n+        # Patch Trainer\\n+        from transformers.trainer import Trainer\\n+        try:\\n+            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n+                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n+                Trainer._original_training_loop = inner_training_loop\\n+            else:\\n+                inner_training_loop = Trainer._original_training_loop\\n+        except:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+\\n+        import transformers.trainer\\n+        items_in_trainer = dir(transformers.trainer)\\n+        good_items = []\\n+        for item in items_in_trainer:\\n+            # TODO: Support Deepspeed\\n+            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n+            if item in inner_training_loop: good_items.append(item)\\n+        pass\\n+        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n+\\n+        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n+        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n+        original_debug = inner_training_loop[start:end]\\n+        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n+        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n+\\n+        debug_info = \"\"\"debug_info = \\\\\\\\\\n+        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n+        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n+        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n+        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n+        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n+        logger.warning_once(debug_info)\"\"\"\\n+\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n+\\n+        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n+            args.gradient_accumulation_steps // self._train_batch_size\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        debug_info =\"\"\"\\n+        debug_info = debug_info.split(\\'\\\\n\\')\\n+        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n+\\n+        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n+        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n+            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"self.accelerator.free_memory()\",\\n+            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n+            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n+            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n+        )\\n+\\n+        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n+        ga  = args.gradient_accumulation_steps\\n+        bsz = self._train_batch_size\\n+        total_batches = bsz * ga * args.world_size\\n+        n_total_devices = total_batches // ga // bsz\\n+        if n_total_devices > 2:\\n+            logger.warning_once(\\n+                \"Please consider a commercial license - Unsloth was designed for the GPU Poor.\\\\\\\\n\"\\n+                \"The OSS currently works on 4 GPUs - we\\'re a 2 person team, so please help fund\\\\\\\\n\"\\n+                \"our development costs by supporting us through Ko-fi or buying a license! Thanks!\",\\n+            )\\n+            divisor = n_total_devices / 2\\n+            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n+            if total_batches // ga // bsz > 2:\\n+                divisor = n_total_devices / 2\\n+                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n+        check_batches = check_batches.split(\\'\\\\n\\')\\n+        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"train_dataloader = self.get_train_dataloader()\",\\n+            check_batches, 1,\\n+        )\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"_inner_training_loop\",\\n+            \"_fast_inner_training_loop\", 1,\\n+        )\\n+        exec(inner_training_loop, globals())\\n+\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_torch_tpu_available()\",\\n+            \"False\",\\n+        )\\n+        if \"n_total_devices >\" not in inner_training_loop:\\n+            raise RuntimeError(\\n+                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n+                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n+                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n+            )\\n+        pass\\n+        inner_training_loop = inner_training_loop.replace(\\n+            \"is_sagemaker_mp_enabled()\",\\n+            \"False\",\\n+        )\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n+\\n         # Save max_seq_length\\n         max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n         model.max_seq_length = max_position_embeddings\\n',\n",
       " '@@ -140,17 +140,28 @@ def unsloth_save_model(\\n \\n     # Push to hub\\n     use_temp_dir         : Optional[bool] = None,\\n-    commit_message       : Optional[str] = None,\\n+    commit_message       : Optional[str] = \"Trained with Unsloth\",\\n     private              : Optional[bool] = None,\\n     create_pr            : bool = False,\\n     revision             : str = None,\\n-    commit_description   : str = None,\\n+    commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : List[str] = None,\\n \\n     # Our functions\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if commit_message is None: commit_message = \"\"\\n+    if \"Unsloth\" not in commit_message:\\n+        commit_message += \" (Trained with Unsloth)\"\\n+    commit_message = commit_message.lstrip()\\n+\\n+    if commit_description is None:\\n+        commit_description = \"Upload model trained with Unsloth 2x faster\"\\n+    elif \"Unsloth 2x faster\" not in commit_description:\\n+        commit_description += \" (Trained with Unsloth 2x faster)\"\\n+    pass\\n+\\n     if save_method == \"merged_4bit\":\\n         raise RuntimeError(\\n             \"Unsloth: Merging into 4bit will cause your model to lose accuracy if you plan\\\\n\"\\\\\\n@@ -202,7 +213,7 @@ def unsloth_save_model(\\n     pass\\n     save_pretrained_settings[\"tags\"] = tags\\n \\n-    if (save_method == \"lora\") and push_to_hub:\\n+    if ((save_method == \"lora\") or (save_method == \"merged_4bit\")) and push_to_hub:\\n         if token is None:\\n             raise RuntimeError(\\n                 \"Unsloth: Pushing to HF requires a token. Pass `token = \\'hf_....\\'`\\\\n\"\\\\\\n@@ -210,7 +221,20 @@ def unsloth_save_model(\\n             )\\n         pass\\n \\n-        model.push_to_hub(\\n+        if save_method == \"lora\":\\n+            print(\"Unsloth: Saving LoRA adapters. Please wait...\")\\n+        elif save_method == \"merged_4bit\":\\n+            print(\"Unsloth: Saving 4bit Bitsandbytes model. Please wait...\")\\n+        pass\\n+\\n+        # Update model tag\\n+        _ = upload_to_huggingface(\\n+            model, save_directory, token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+            old_username = None, private = private,\\n+        )\\n+\\n+        model.original_push_to_hub(\\n             repo_id            = save_directory,\\n             use_temp_dir       = use_temp_dir,\\n             commit_message     = commit_message,\\n@@ -224,7 +248,7 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n-            tokenizer.push_to_hub(\\n+            tokenizer.original_push_to_hub(\\n                 repo_id            = save_directory,\\n                 use_temp_dir       = use_temp_dir,\\n                 commit_message     = commit_message,\\n@@ -238,31 +262,11 @@ def unsloth_save_model(\\n                 tags               = tags,\\n             )\\n         pass\\n-        return save_directory\\n-    pass\\n-\\n-    # Update model tag\\n-    username = \"\"\\n-    if push_to_hub:\\n-        username = upload_to_huggingface(\\n-            model, save_directory, token,\\n-            \"finetuned\", \"trl\", file_location = None,\\n-        )\\n-    pass\\n-\\n-    # If push_to_hub, we must remove the .../ part of a repo\\n-    if push_to_hub and \"/\" in save_directory:\\n-\\n-        # +1 solves absolute path issues\\n-        new_save_directory = save_directory[save_directory.find(\"/\")+1:]\\n-\\n-        logger.warning_once(\\n-            f\"Unsloth: You are pushing to hub, but you passed your HF username.\\\\n\"\\\\\\n-            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n-        )\\n \\n-        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n-        save_directory = new_save_directory\\n+        if hasattr(model, \"config\"):\\n+            print(f\"Saved {save_method} model to https://huggingface.co/\" + save_directory)\\n+        pass\\n+        return save_directory\\n     pass\\n \\n     # Tokenizer has different saving arguments\\n@@ -292,13 +296,25 @@ def unsloth_save_model(\\n         # Do general saving\\n         # Edit save_pretrained_settings\\n         # [TODO] _create_repo has errors due to **kwargs getting accepted\\n-        for deletion in \\\\\\n-            (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+        # commit_description does not seem to work?\\n+        what_to_delete = (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",) \\\\\\n+            if save_pretrained_settings[\"push_to_hub\"] is False else \\\\\\n+            (\"use_temp_dir\", \"create_pr\", \"revision\", \"tags\", \"commit_description\",)\\n+        for deletion in what_to_delete:\\n             del save_pretrained_settings[deletion]\\n         pass\\n         if hasattr(model, \"add_model_tags\"):\\n             model.add_model_tags([\"unsloth\",])\\n \\n+        # Update model tag\\n+        if push_to_hub:\\n+             _ = upload_to_huggingface(\\n+                model, save_pretrained_settings[\"save_directory\"], token,\\n+                \"finetuned\", \"trl\", file_location = None,\\n+                old_username = None, private = private,\\n+            )\\n+        pass\\n+\\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n@@ -310,10 +326,33 @@ def unsloth_save_model(\\n         if save_method != \"lora\": print(\" This might take 10 minutes for Llama-7b...\", end = \"\")\\n \\n         model.save_pretrained(**save_pretrained_settings)\\n+\\n+        if push_to_hub and hasattr(model, \"config\"):\\n+            print(\"Saved to https://huggingface.co/\" + save_pretrained_settings[\"save_directory\"])\\n+        pass\\n+\\n         print(\" Done.\")\\n         return save_directory\\n     pass\\n \\n+    # If push_to_hub, we must remove the .../ part of a repo\\n+    username = None\\n+    if push_to_hub and \"/\" in save_directory:\\n+\\n+        # +1 solves absolute path issues\\n+        username = save_directory[:save_directory.find(\"/\")]\\n+        new_save_directory = save_directory[save_directory.find(\"/\")+1:]\\n+\\n+        logger.warning_once(\\n+            f\"Unsloth: You are pushing to hub, but you passed your HF username = {username}.\\\\n\"\\\\\\n+            f\"We shall truncate {save_directory} to {new_save_directory}\"\\n+        )\\n+\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        tokenizer_save_settings [\"save_directory\"] = new_save_directory\\n+        save_directory = new_save_directory\\n+    pass\\n+\\n     print(\"Unsloth: Merging 4bit and LoRA weights to 16bit...\")\\n \\n     # Determine max RAM usage minus sharding\\n@@ -339,7 +378,7 @@ def unsloth_save_model(\\n         logger.warning_once(\\n             f\"Unsloth: You have {n_cpus} CPUs. Using `safe_serialization` is 10x slower.\\\\n\"\\\\\\n             f\"We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\\\\n\"\\\\\\n-            f\"To force `safe_serialization`, set it to None instead.\",\\n+            f\"To force `safe_serialization`, set it to `None` instead.\",\\n         )\\n         safe_serialization = False\\n         save_function = fast_save_pickle\\n@@ -413,13 +452,26 @@ def unsloth_save_model(\\n     # Edit save_pretrained_settings\\n     # [TODO] _create_repo has errors due to **kwargs getting accepted\\n     save_pretrained_settings[\"state_dict\"] = state_dict\\n-    for deletion in \\\\\\n-        (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",):\\n+    \\n+    # commit_description does not seem to work?\\n+    what_to_delete = (\"use_temp_dir\", \"commit_message\", \"create_pr\", \"revision\", \"commit_description\", \"tags\",) \\\\\\n+        if not push_to_hub else \\\\\\n+        (\"use_temp_dir\", \"create_pr\", \"revision\", \"tags\", \"commit_description\",)\\n+    for deletion in what_to_delete:\\n         del save_pretrained_settings[deletion]\\n     pass\\n     if hasattr(model, \"add_model_tags\"):\\n         model.add_model_tags([\"unsloth\",])\\n \\n+    # Update model tag\\n+    if push_to_hub:\\n+        _ = upload_to_huggingface(\\n+            model, save_pretrained_settings[\"save_directory\"], token,\\n+            \"finetuned\", \"trl\", file_location = None,\\n+            old_username = username, private = private,\\n+        )\\n+    pass\\n+\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n@@ -452,9 +504,8 @@ def unsloth_save_model(\\n     model.config = old_config\\n     print(\"Done.\")\\n \\n-    # Print location\\n-    if push_to_hub:\\n-        print(f\"Saved to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n+    if push_to_hub and hasattr(model, \"config\"):\\n+        print(f\"Saved merged model to https://huggingface.co/{username}/{save_directory.lstrip(\\'/\\')}\")\\n     pass\\n \\n     save_pretrained_settings[\"state_dict\"] = None\\n@@ -478,7 +529,7 @@ def unsloth_save_model(\\n     for _ in range(3):\\n         torch.cuda.empty_cache()\\n         gc.collect()\\n-    return save_directory\\n+    return save_directory, username\\n pass\\n \\n \\n@@ -494,7 +545,7 @@ def install_llama_cpp_make_non_blocking():\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n-    full_command = [\"make\", \"all\", \"-j\", str(n_jobs), \"-C\", \"llama.cpp\"]\\n+    full_command = [\"make\", \"all\", \"-j\"+str(n_jobs), \"-C\", \"llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n pass\\n@@ -507,10 +558,44 @@ def install_python_non_blocking(packages = []):\\n pass\\n \\n \\n+def install_llama_cpp_old(version = -10):\\n+    # Download the 10th latest release since the latest might be broken!\\n+    # FALLBACK mechanism\\n+    releases = subprocess.check_output([\"git\", \"ls-remote\", \"--tags\", \"https://github.com/ggerganov/llama.cpp.git\"])\\n+    releases = releases.decode(\"utf-8\").replace(\"\\\\t\", \" \").split(\"\\\\n\")\\n+    for i, x in enumerate(releases):\\n+        if \"refs/tags/b\" not in x: break\\n+    releases = releases[:i]\\n+    latest = releases[-1]\\n+    version = releases[version].split(\" \")[0]\\n+\\n+    # Clone a specific commit\\n+    commands = [\\n+        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n+        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        \"pip install gguf protobuf\",\\n+    ]\\n+    for command in commands:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+        pass\\n+    pass\\n+    # Check if successful\\n+    if not os.path.exists(\"llama.cpp/quantize\"):\\n+        raise RuntimeError(\\n+            \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n+            \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n+        )\\n+    pass\\n+pass\\n+\\n+\\n def install_llama_cpp_blocking():\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j {psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -563,10 +648,13 @@ def save_to_gguf(\\n \\n     print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\\n     if _run_installer is not None:\\n-        _run_installer.wait()\\n+        error = _run_installer.wait()\\n     else:\\n+        error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+    # Check if successful. If not install 10th latest release\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -580,15 +668,18 @@ def save_to_gguf(\\n             first_conversion = \"f16\"\\n         pass\\n     pass\\n-    print(f\"Unsloth: [1] Converting HF into {first_conversion} GGUF format. This will take 3 minutes...\")\\n \\n     n_cpus = psutil.cpu_count()*2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n     \\n     final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n \\n+    print(f\"Unsloth: [1] Converting model at {model_directory} into {first_conversion} GGUF format.\\\\n\"\\\\\\n+          f\"The output location will be {final_location}\\\\n\"\\\\\\n+          \"This will take 3 minutes...\")\\n+\\n     command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-        f\"--outfile {final_location} \"\\\\\\n+        f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n         f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n@@ -601,7 +692,8 @@ def save_to_gguf(\\n     # Check if quantization succeeded!\\n     if not os.path.isfile(final_location):\\n         raise RuntimeError(\\n-            \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+            f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+            \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n@@ -662,7 +754,7 @@ def unsloth_save_pretrained_merged(\\n     save_peft_format     : bool = True,\\n     tags                 : List[str] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n-    maximum_memory_usage : float = 0.85,   \\n+    maximum_memory_usage : float = 0.85,\\n ):\\n     \"\"\"\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n@@ -695,14 +787,14 @@ def unsloth_push_to_hub_merged(\\n     tokenizer            = None,\\n     save_method          : str = \"merged_16bit\", # [\"lora\", \"merged_16bit\", \"merged_4bit\"]\\n     use_temp_dir         : Optional[bool] = None,\\n-    commit_message       : Optional[str] = None,\\n+    commit_message       : Optional[str] = \"Trained with Unsloth\",\\n     private              : Optional[bool] = None,\\n     token                : Union[bool, str, None] = None,\\n     max_shard_size       : Union[int, str, None] = \"5GB\",\\n     create_pr            : bool = False,\\n     safe_serialization   : bool = True,\\n     revision             : str = None,\\n-    commit_description   : str = None,\\n+    commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : Optional[List[str]] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.85,\\n@@ -760,15 +852,27 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/\\n [<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n \"\"\"\\n \\n-def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file_location = None):\\n+def upload_to_huggingface(\\n+    model,\\n+    save_directory,\\n+    token,\\n+    method,\\n+    extra = \"\",\\n+    file_location = None,\\n+    old_username = None,\\n+    private = None,\\n+):\\n     # Check for username\\n     username = \"\"\\n     save_directory = save_directory.lstrip(\"./\")\\n     if \"/\" not in save_directory:\\n         from huggingface_hub import whoami\\n         try: \\n-            username = whoami()[\\'name\\']\\n-            save_directory = f\"{save_directory}/{username}\"\\n+            username = whoami(token = token)[\"name\"]\\n+            if type(old_username) is str and username != old_username:\\n+                username = old_username\\n+            pass\\n+            save_directory = f\"{username}/{save_directory}\"\\n         except:\\n             raise RuntimeError(f\"Unsloth: {save_directory} is not a Huggingface directory.\")\\n     else:\\n@@ -776,24 +880,28 @@ def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file\\n     pass\\n \\n     from huggingface_hub import create_repo\\n-    create_repo(\\n-        repo_id   = save_directory,\\n-        token     = token,\\n-        repo_type = \"model\",\\n-        exist_ok  = True,\\n-    )\\n-\\n-    # Create model card\\n-    from huggingface_hub import ModelCard\\n-    content = MODEL_CARD.format(\\n-        username   = username,\\n-        base_model = model.config._name_or_path,\\n-        model_type = model.config.model_type,\\n-        method     = \"\",\\n-        extra      = extra,\\n-    )\\n-    card = ModelCard(content)\\n-    card.push_to_hub(save_directory, token = token)\\n+    try:\\n+        create_repo(\\n+            repo_id   = save_directory,\\n+            token     = token,\\n+            repo_type = \"model\",\\n+            exist_ok  = False,\\n+            private   = private,\\n+        ) \\n+\\n+        # Create model card\\n+        from huggingface_hub import ModelCard\\n+        content = MODEL_CARD.format(\\n+            username   = username,\\n+            base_model = model.config._name_or_path,\\n+            model_type = model.config.model_type,\\n+            method     = \"\",\\n+            extra      = extra,\\n+        )\\n+        card = ModelCard(content)\\n+        card.push_to_hub(save_directory, token = token)\\n+    except:\\n+        pass\\n \\n     if file_location is not None:\\n         # Now upload file\\n@@ -811,6 +919,7 @@ def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file\\n             path_in_repo    = uploaded_location,\\n             repo_id         = save_directory,\\n             repo_type       = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n         )\\n \\n         # We also upload a config.json file\\n@@ -823,6 +932,7 @@ def upload_to_huggingface(model, save_directory, token, method, extra = \"\", file\\n             path_in_repo    = \"config.json\",\\n             repo_id         = save_directory,\\n             repo_type       = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n         )\\n         os.remove(\"_temporary_unsloth_config.json\")\\n     pass\\n@@ -838,6 +948,7 @@ def unsloth_save_pretrained_gguf(\\n     first_conversion     : str = \"f16\",\\n     push_to_hub          : bool = False,\\n     token                : Optional[Union[str, bool]] = None,\\n+    private              : Optional[bool] = None,\\n     is_main_process      : bool = True,\\n     state_dict           : Optional[dict] = None,\\n     save_function        : Callable = torch.save,\\n@@ -847,7 +958,7 @@ def unsloth_save_pretrained_gguf(\\n     save_peft_format     : bool = True,\\n     tags                 : List[str] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n-    maximum_memory_usage : float = 0.85,   \\n+    maximum_memory_usage : float = 0.85,\\n ):\\n     \"\"\"\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n@@ -898,11 +1009,11 @@ def unsloth_save_pretrained_gguf(\\n         python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n         git_clone.wait()\\n         makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory = unsloth_save_model(**arguments)\\n+        new_save_directory, old_username = unsloth_save_model(**arguments)\\n         python_install.wait()\\n     else:\\n         try:\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n@@ -910,7 +1021,7 @@ def unsloth_save_pretrained_gguf(\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n             makefile  = install_llama_cpp_make_non_blocking()\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n     pass\\n@@ -924,12 +1035,12 @@ def unsloth_save_pretrained_gguf(\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n         username = upload_to_huggingface(\\n             self, save_directory, token,\\n-            \"GGUF converted\", \"gguf\", file_location,\\n+            \"GGUF converted\", \"gguf\", file_location, old_username, private,\\n         )\\n         link = f\"{username}/{new_save_directory.lstrip(\\'/.\\')}\" \\\\\\n             if username not in new_save_directory else \\\\\\n             new_save_directory.lstrip(\\'/.\\')\\n-        print(f\"Saved to https://huggingface.co/{link}\")\\n+        print(f\"Saved GGUF to https://huggingface.co/{link}\")\\n     pass\\n pass\\n \\n@@ -941,14 +1052,14 @@ def unsloth_push_to_hub_gguf(\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n     use_temp_dir         : Optional[bool] = None,\\n-    commit_message       : Optional[str] = None,\\n+    commit_message       : Optional[str] = \"Trained with Unsloth\",\\n     private              : Optional[bool] = None,\\n     token                : Union[bool, str, None] = None,\\n     max_shard_size       : Union[int, str, None] = \"5GB\",\\n     create_pr            : bool = False,\\n     safe_serialization   : bool = True,\\n     revision             : str = None,\\n-    commit_description   : str = None,\\n+    commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : Optional[List[str]] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.85,\\n@@ -998,19 +1109,19 @@ def unsloth_push_to_hub_gguf(\\n         python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n         git_clone.wait()\\n         makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory = unsloth_save_model(**arguments)\\n+        new_save_directory, old_username = unsloth_save_model(**arguments)\\n         python_install.wait()\\n     else:\\n         try:\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n-            makefile  = install_llama_cpp_make_non_blocking()\\n-            new_save_directory = unsloth_save_model(**arguments)\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n     pass\\n@@ -1023,12 +1134,12 @@ def unsloth_push_to_hub_gguf(\\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n         self, repo_id, token,\\n-        \"GGUF converted\", \"gguf\", file_location,\\n+        \"GGUF converted\", \"gguf\", file_location, old_username, private,\\n     )\\n     link = f\"{username}/{new_save_directory.lstrip(\\'/.\\')}\" \\\\\\n         if username not in new_save_directory else \\\\\\n         new_save_directory.lstrip(\\'/.\\')\\n-    print(f\"Saved to https://huggingface.co/{link}\")\\n+    print(f\"Saved GGUF to https://huggingface.co/{link}\")\\n pass\\n \\n \\n@@ -1038,31 +1149,17 @@ def patch_saving_functions(model):\\n     import types\\n     from typing import Callable, Optional, Union, List\\n \\n-    if hasattr(model, \"_original_push_to_hub\"): return\\n-\\n-    # First check if this has already been called, and revert it\\n-    original_model = model\\n-    while True:\\n-        if hasattr(original_model, \"_original_push_to_hub\"):\\n-            original_model.push_to_hub = original_model._original_push_to_hub\\n-            del original_model._original_push_to_hub\\n-            if hasattr(original_model, \"push_to_hub_merged\"):     del original_model.push_to_hub_merged\\n-            if hasattr(original_model, \"save_pretrained_merged\"): del original_model.save_pretrained_merged\\n-            if hasattr(original_model, \"push_to_hub_gguf\"):       del original_model.push_to_hub_gguf\\n-            if hasattr(original_model, \"save_pretrained_gguf\"):   del original_model.save_pretrained_gguf\\n-        pass\\n-\\n-        if hasattr(original_model, \"model\"): original_model = original_model.model\\n-        else: break\\n+    # And now re add our saving methods!\\n+    if model.push_to_hub.__name__ == \"unsloth_push_to_hub\":\\n+        original_push_to_hub = model.original_push_to_hub\\n+    else:\\n+        original_push_to_hub = model.push_to_hub\\n     pass\\n \\n-    # And now re add our saving methods!\\n-    original_push_to_hub = model.push_to_hub\\n     signature = str(inspect.signature(original_push_to_hub)).replace(\"NoneType\", \"None\")\\n     signature = signature[1:]\\n     signature = re.sub(\"<function save at .+?>\", \"torch.save\", signature)\\n     docs = original_push_to_hub.__doc__.encode(\"utf-8\").decode(\"utf-8\")\\n-    model._original_push_to_hub = original_push_to_hub\\n \\n     push_to_hub_text = f\\'\\'\\'def unsloth_push_to_hub(self, {signature}:\\n     \"\"\"\\n@@ -1077,11 +1174,45 @@ def patch_saving_functions(model):\\n         arguments[\"tags\"] = [\"unsloth\",]\\n     elif hasattr(self, \"add_model_tags\"):\\n         self.add_model_tags([\"unsloth\",])\\n+\\n+    if \"commit_message\" in arguments:\\n+        commit_message = arguments[\"commit_message\"]\\n+        if commit_message is not None:\\n+            if not commit_message.endswith(\" \"): commit_message += \" \"\\n+            if \"Unsloth\" not in commit_message:\\n+                commit_message += \"(Trained with Unsloth)\"\\n+        else:\\n+            commit_message = \"Upload model trained with Unsloth\"\\n+        arguments[\"commit_message\"] = commit_message\\n+\\n+    if \"commit_description\" in arguments:\\n+        commit_description = arguments[\"commit_description\"]\\n+        if commit_description is not None:\\n+            if not commit_description.endswith(\" \"): commit_description += \" \"\\n+            if \"Unsloth\" not in commit_description:\\n+                commit_description += \"(Trained with Unsloth 2x faster)\"\\n+        else:\\n+            commit_description = \"Upload model trained with Unsloth 2x faster\"\\n+        arguments[\"commit_description\"] = commit_description\\n+\\n+    # Update model tag\\n+    if hasattr(self, \"config\"):\\n+        _ = upload_to_huggingface(\\n+            self, arguments[\"repo_id\"], arguments[\"token\"],\\n+            \"finetuned\", \"trl\", file_location = None,\\n+            old_username = None, private = arguments[\"private\"],\\n+        )\\n+    pass\\n+\\n     try:\\n-        return self._original_push_to_hub(**arguments)\\n+        self.original_push_to_hub(**arguments)\\n     except:\\n         del arguments[\"tags\"]\\n-        return self._original_push_to_hub(**arguments)\\n+        self.original_push_to_hub(**arguments)\\n+    pass\\n+\\n+    if hasattr(self, \"config\"):\\n+        print(\"Saved model to https://huggingface.co/\" + arguments[\"repo_id\"])\\n     pass\\n     \\'\\'\\'\\n     exec(push_to_hub_text, globals())\\n@@ -1089,12 +1220,12 @@ def patch_saving_functions(model):\\n     original_model = model\\n     while True:\\n \\n-        if not hasattr(original_model, \"_original_push_to_hub\"):\\n-            original_model._original_push_to_hub = original_model.push_to_hub\\n+        if original_model.push_to_hub.__name__ != \"unsloth_push_to_hub\":\\n+            original_model.original_push_to_hub = original_model.push_to_hub\\n             original_model.push_to_hub = types.MethodType(unsloth_push_to_hub, original_model)\\n-\\n             if hasattr(original_model, \"add_model_tags\"):\\n                 original_model.add_model_tags([\"unsloth\",])\\n+            pass\\n         pass\\n \\n         if hasattr(original_model, \"model\"): original_model = original_model.model\\n',\n",
       " '@@ -10,7 +10,7 @@\\n <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n \\n-### Finetune Mistral, Llama 2-5x faster with 70% less memory!\\n+### Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory!\\n \\n ![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n \\n@@ -22,28 +22,30 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n \\n | Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n+| **Gemma 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 58% less |\\n | **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\\n | **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |\\n-| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n | **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |\\n | **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |\\n | **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\\\* | 62% less |\\n+| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\\n \\n - This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\\n-- Colab provides a free GPU sometimes. Kaggle has 30 hrs free per week on a 12 hr running cap.\\n-- \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Use Colab as Kaggle takes 10 mins to install.\\n+- \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\\n \\n ## 🦥 Unsloth.ai News\\n-- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO.\\n-- 📣 [TinyLlama 1.1b](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing) on 3T tokens now works.\\n-- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face, and we\\'re in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).\\n-- 📣 Now supports **Llama, Yi, Mistral, CodeLlama, Qwen (llamafied), Deepseek** and their derived models (**Open Hermes** etc). Llama 7, 13, 70b; CodeLlama 7, 13, 34, 70b; Yi 6, 34b are all supported!\\n-- 📣 **Download models 4x faster** from 🤗Hugging Face! Eg: `unsloth/mistral-7b-bnb-4bit` See our [HF collection](https://huggingface.co/collections/unsloth/load-4bit-models-4x-faster-659042e3a41c3cbad582e734) for more!\\n+- 📣 [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) on 6T tokens now works. And [Gemma 2b notebook](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)\\n+- 📣 Added [conversational notebooks](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) and [raw text notebooks](https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing)\\n+- 📣 [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models\\n+- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO\\n+- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face and are in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)\\n+- 📣 [Download models 4x faster](https://huggingface.co/collections/unsloth/)  from 🤗Hugging Face. Eg: `unsloth/mistral-7b-bnb-4bit`\\n \\n ## 🔗 Links and Resources\\n | Type                            | Links                               |\\n | ------------------------------- | --------------------------------------- |\\n+| 📚 **Wiki & FAQ**              | [Read Our Wiki](https://github.com/unslothai/unsloth/wiki) |\\n | 📜 **Documentation**              | [Read The Doc](https://github.com/unslothai/unsloth/tree/main#-documentation) |\\n | 💾 **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#installation-instructions)|\\n | <img height=\"14\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\\n@@ -113,8 +115,8 @@ pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \\\\\\n ```bash\\n pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install \"unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu118_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu121_ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118-ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-ampere] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n 3. For Pytorch 2.1.1: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -122,10 +124,10 @@ pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n ```\\n ```bash\\n-pip install \"unsloth[cu118_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu121_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu118_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118-torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118-ampere-torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-ampere-torch211] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n 4. For Pytorch 2.2.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -133,10 +135,10 @@ pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\\\\n   --index-url https://download.pytorch.org/whl/cu121\\n ```\\n ```bash\\n-pip install \"unsloth[cu118_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu121_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu118_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install \"unsloth[cu121_ampere_torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118-torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\\n ```\\n 5. If you get errors, try the below first, then go back to step 1:\\n ```bash\\n',\n",
       " '@@ -33,7 +33,7 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-    \"transformers>=4.37.0\",\\n+    \"transformers>=4.38.0\",\\n     \"datasets\",\\n     \"sentencepiece\",\\n     \"accelerate>=0.26.1\",\\n',\n",
       " '@@ -217,6 +217,35 @@ alpaca_eos_token = \"eos_token\"\\n CHAT_TEMPLATES[\"alpaca\"] = (alpaca_template, alpaca_eos_token,)\\n \\n \\n+# https://huggingface.co/google/gemma-7b-it\\n+# Notice we must use |trim for lstrip and rstrip. <start_of_turn> maps to 106.\\n+# <end_of_turn> maps to 107. user and model are normal 1 word tokens.\\n+gemma_template = \\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n+            \"{{\\'<start_of_turn>user\\\\n\\' + message[\\'content\\'] | trim + \\'<end_of_turn>\\\\n\\'}}\"\\\\\\n+        \"{% elif message[\\'role\\'] == \\'assistant\\' %}\"\\\\\\n+            \"{{\\'<start_of_turn>model\\\\n\\' + message[\\'content\\'] | trim + \\'<end_of_turn>\\\\n\\' }}\"\\\\\\n+        \"{% else %}\"\\\\\\n+            \"{{ \\'<start_of_turn>system\\\\n\\' + message[\\'content\\'] | trim + \\'<end_of_turn>\\\\n\\' }}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'<start_of_turn>model\\\\n\\' }}\"\\\\\\n+    \"{% endif %}\"\\n+gemma_eos_token = \"<end_of_turn>\"\\n+CHAT_TEMPLATES[\"gemma\"] = (gemma_template, gemma_eos_token,)\\n+\\n+\\n+# Gemma with ChatML instead\\n+gemma_chatml_template = chatml_template\\n+gemma_chatml_eos_token = (\\n+    {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"},\\n+    \"<|im_end|>\",\\n+)\\n+CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -229,7 +258,7 @@ def get_chat_template(\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n-    if type(chat_template) in (list, tuple):\\n+    if type(chat_template) in (list, tuple,):\\n         chat_template, stop_word = chat_template\\n         assert(type(chat_template) is str)\\n         assert(type(stop_word) is str)\\n@@ -238,7 +267,38 @@ def get_chat_template(\\n \\n         chat_template, stop_word = CHAT_TEMPLATES[chat_template]\\n \\n-        if stop_word != \"eos_token\":\\n+        if type(stop_word) in (list, tuple,):\\n+            token_mapping, stop_word = stop_word\\n+            assert(type(token_mapping) is dict)\\n+        else:\\n+            token_mapping = None\\n+\\n+        assert(type(stop_word) is str)\\n+\\n+        # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n+        # For Gemma :)\\n+        if token_mapping is not None:\\n+\\n+            string_vocab = tokenizer._tokenizer.to_str()\\n+\\n+            for old_token, new_token in token_mapping.items():\\n+                old_count = string_vocab.count(f\\'\"{old_token}\"\\')\\n+                new_count = string_vocab.count(f\\'\"{new_token}\"\\')\\n+                if new_count != 0:\\n+                    print(f\"{new_token} is already a token. Skipping.\")\\n+                elif old_count == 0:\\n+                    raise RuntimeError(f\"{old_token} was not part of the tokenizer!\")\\n+                else:\\n+                    string_vocab = string_vocab.replace(f\\'\"{old_token}\"\\', f\\'\"{new_token}\"\\')\\n+                pass\\n+            pass\\n+\\n+            logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n+            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n+            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+        elif stop_word != \"eos_token\":\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n \\n             # Replaces the old EOS token with a new one.\\n@@ -252,6 +312,7 @@ def get_chat_template(\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n         pass\\n+\\n     else:\\n         raise TypeError(\\n             f\"Unsloth: `chat_template` must be a tuple of (your_template, eos_token,) or one of\\\\n\"\\\\\\n@@ -318,6 +379,7 @@ def test_chat_templates():\\n         {\"role\": \"user\", \"content\": \"  No it\\'s 100% 5! \"},\\n     ]\\n \\n+    # Zephyr\\n     from transformers import AutoTokenizer\\n     template = zephyr_template\\n     correct_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\\n@@ -326,6 +388,7 @@ def test_chat_templates():\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n \\n+    # Chatml\\n     template = chatml_template\\n     correct_tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\\n     correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n@@ -333,6 +396,7 @@ def test_chat_templates():\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n \\n+    # Mistral\\n     template = mistral_template\\n     correct_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\\n     correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n@@ -340,6 +404,7 @@ def test_chat_templates():\\n     our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n \\n+    # Llama\\n     template = llama_template\\n     correct_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-2-7b-chat\")\\n     correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n@@ -347,6 +412,7 @@ def test_chat_templates():\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n \\n+    # Vicuna\\n     try:\\n         from fastchat.conversation import get_conv_template\\n     except:\\n@@ -381,4 +447,11 @@ def test_chat_templates():\\n     our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n     # We add </s> ourselves\\n     assert(correct_prompt == our_prompt.replace(\"</s>\", \"\"))\\n+\\n+    # Gemma\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-7b-it\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = gemma_template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n+    assert(our_prompt == correct_prompt)\\n pass\\n',\n",
       " '@@ -16,9 +16,11 @@ from .cross_entropy_loss import fast_cross_entropy_loss\\n from .rms_layernorm import fast_rms_layernorm\\n from .rope_embedding import fast_rope_embedding, inplace_rope_embedding\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n+from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n-\\tapply_lora_mlp,\\n+\\tapply_lora_mlp_swiglu,\\n+\\tapply_lora_mlp_geglu,\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n',\n",
       " '@@ -20,12 +20,14 @@ from transformers.models.llama.modeling_llama import logger\\n \\n \\n @triton.jit\\n-def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n-                           loss_ptr,\\n-                           lse_ptr,\\n-                           labels_ptr,\\n-                           n_cols,\\n-                           BLOCK_SIZE: tl.constexpr,):\\n+def _cross_entropy_forward(\\n+    logits_ptr, logits_row_stride,\\n+    loss_ptr,\\n+    logsumexp_ptr,\\n+    labels_ptr,\\n+    VOCAB_SIZE : tl.constexpr,\\n+    BLOCK_SIZE : tl.constexpr,\\n+):\\n     \"\"\"\\n         Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]\\n         Pi = exp(xi) / sum(exp(xi))\\n@@ -34,40 +36,114 @@ def _cross_entropy_forward(logits_ptr, logits_row_stride,\\n              = y * (log[sum(exp(x))] - x)\\n         If y == 0: CE_i = 0\\n         If y == 1: CE_i = logsumexp - x\\n+\\n+        logsumexp is also stable\\n+        Take    y =         log[sum(exp(x))]\\n+           exp(y) =             sum(exp(x))\\n+           exp(y) =             sum(exp(x - c)*exp(c)) Since e^(x-c)*e^c = e^x\\n+           exp(y) =      exp(c)*sum(exp(x - c))\\n+               y  = log(exp(c)*sum(exp(x - c)))\\n+               y  = c + log[sum(exp(x - c))]\\n+        This means we can set c = max(x) to make sure\\n+        exp(x - c) always is exp(x - max(x)).\\n+        This ensures exp(x - max(x))\\'s maximum is 1 as exp(0) = 1.\\n     \"\"\"\\n     row_idx = tl.program_id(0)\\n-    logits_ptr += row_idx * logits_row_stride\\n-    loss_ptr   += row_idx\\n-    lse_ptr    += row_idx\\n-    labels_ptr += row_idx\\n+    logits_ptr    += row_idx * logits_row_stride.to(tl.int64)\\n+    loss_ptr      += row_idx\\n+    logsumexp_ptr += row_idx\\n+    labels_ptr    += row_idx\\n \\n     col_offsets = tl.arange(0, BLOCK_SIZE)\\n-    mask = col_offsets < n_cols\\n+    mask = col_offsets < VOCAB_SIZE\\n \\n-    # TODO: Fixup int32 locations to int64\\n     label_idx = tl.load(labels_ptr).to(tl.int32)\\n     logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(\"inf\")).to(tl.float32)\\n-    max_logits = tl.max(logits, 0)\\n-    # Maximum stops overflow\\n-    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\\n-    tl.store(lse_ptr, lse)\\n+    c = tl.max(logits, 0)\\n+    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\\n \\n     if label_idx != -100:\\n-        logits_label = tl.load(logits_ptr + label_idx).to(tl.float32)\\n-        loss = lse - logits_label\\n+        x = tl.load(logits_ptr + label_idx).to(tl.float32)\\n+        loss = logsumexp - x\\n     else:\\n         loss = 0.0\\n+    tl.store(logsumexp_ptr, logsumexp)\\n     tl.store(loss_ptr, loss)\\n pass\\n \\n \\n @triton.jit\\n-def _cross_entropy_backward(logits_ptr, logits_row_stride,\\n-                            dloss_ptr,   dloss_row_stride,\\n-                            lse_ptr,\\n-                            labels_ptr,\\n-                            n_cols,\\n-                            BLOCK_SIZE: tl.constexpr,):\\n+def _chunked_cross_entropy_forward(\\n+    logits_ptr, logits_row_stride,\\n+    loss_ptr,\\n+    logsumexp_ptr,\\n+    labels_ptr,\\n+    VOCAB_SIZE : tl.constexpr,\\n+    N_CHUNKS   : tl.constexpr,\\n+    BLOCK_SIZE : tl.constexpr,\\n+):\\n+    \"\"\"\\n+        256K vocab divided in 4 chunks\\n+\\n+        |-65536-| |-65536-| |-65536-| |-65536-|\\n+        |-------| |-------| |-------| |-------|\\n+        |-------| |-------| |-------| |-------|\\n+\\n+        If y == 0: CE_i = 0\\n+        If y == 1: CE_i = logsumexp - x\\n+\\n+        Notice we can do logsumexp for each chunk and then\\n+        logsumexp[chunk_sum(logsumexp)] == logsumexp\\n+\\n+        chunk_sum = log[chunk_sum(logsumexp)]\\n+                  = log[exp(logsumexp(a)) + ... + exp(logsumexp(z))]\\n+                  = log[exp(log[sum(exp(a))]) + ... + exp(log[sum(exp(z))])]\\n+                  = log[sum(exp(a)) + ... + sum(exp(z))]\\n+                  = logsumexp(x)\\n+\\n+        This means we can perform a logsumexp for each chunk, then do a\\n+        final logsumexp reduction!\\n+\\n+        Ie do: logsumexp(chunked_logsumexp) - x\\n+    \"\"\"\\n+    row_idx   = tl.program_id(0)\\n+    chunk_idx = tl.program_id(1)\\n+    logits_ptr    += row_idx * logits_row_stride.to(tl.int64)\\n+    loss_ptr      += row_idx\\n+    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\\n+    labels_ptr    += row_idx\\n+\\n+    col_offsets = chunk_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = col_offsets < VOCAB_SIZE\\n+\\n+    label_idx = tl.load(labels_ptr).to(tl.int32)\\n+    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(\"inf\")).to(tl.float32)\\n+    c = tl.max(logits, 0)\\n+    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\\n+\\n+    if chunk_idx == 0:\\n+        # logsumexp(chunked_logsumexp) - x\\n+        # Do the -x separately\\n+        if label_idx != -100:\\n+            x = tl.load(logits_ptr + label_idx).to(tl.float32)\\n+            loss = -1.0 * x\\n+        else:\\n+            loss = 0.0\\n+        tl.store(loss_ptr, loss)\\n+    pass\\n+    tl.store(logsumexp_ptr, logsumexp)\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _cross_entropy_backward(\\n+    logits_ptr, logits_row_stride,\\n+    dloss_ptr,   dloss_row_stride,\\n+    logsumexp_ptr,\\n+    labels_ptr,\\n+    VOCAB_SIZE : tl.constexpr,\\n+    BLOCK_SIZE : tl.constexpr,\\n+):\\n     \"\"\"\\n         CE_i = -y log(P) = y * (log[sum(exp(x))] - x)\\n         dC/dx = d/dx (y * log[sum(exp(x))] - x * y)\\n@@ -83,47 +159,80 @@ def _cross_entropy_backward(logits_ptr, logits_row_stride,\\n         If y == 1 and x == label: dC/dlabel = exp[x - logsumexp] - 1\\n         If y == 1 and x != label: dC/dx     = exp[x - logsumexp]\\n     \"\"\"\\n-    row_idx = tl.program_id(0)\\n-    logits_ptr += row_idx * logits_row_stride\\n+    row_idx   = tl.program_id(0)\\n+    block_idx = tl.program_id(1)\\n+\\n+    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\\n     dloss_ptr  += row_idx *  dloss_row_stride\\n-    col_offsets = tl.arange(0, BLOCK_SIZE)\\n-    mask = col_offsets < n_cols\\n-    # TODO: Fixup int32 locations to int64\\n+    col_offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = col_offsets < VOCAB_SIZE\\n     label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\\n \\n     if label_idx != -100:\\n         dloss = tl.load(dloss_ptr)\\n     else:\\n         dloss = 0.0\\n-    logits = tl.load(logits_ptr + col_offsets, mask = mask, other = 0).to(tl.float32)\\n-    lse = tl.load(lse_ptr + row_idx)\\n-    probs = tl.exp(logits - lse)\\n \\n-    probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\\n-    tl.store(logits_ptr + col_offsets, dloss * probs, mask = mask)\\n+    x = tl.load(logits_ptr + col_offsets, mask = mask, other = -float(\"inf\")).to(tl.float32)\\n+    logsumexp = tl.load(logsumexp_ptr + row_idx)\\n+    y = tl.exp(x - logsumexp)\\n+    y = tl.where(\\n+        col_offsets == label_idx,\\n+        y - 1.0, # exp(x - logsumexp) - 1\\n+        y,       # exp(x - logsumexp)\\n+    )\\n+\\n+    # If y == 0: dC/dx = 0 ==> we already masked it to be = 0, so dloss = 0.\\n+    tl.store(logits_ptr + col_offsets, dloss * y, mask = mask)\\n pass\\n \\n \\n+MAX_FUSED_SIZE = 65536 # 2**16\\n+\\n class Fast_CrossEntropyLoss(torch.autograd.Function):\\n     @staticmethod\\n     def forward(ctx, logits, labels):\\n-        n_rows, n_cols = logits.shape\\n-        BLOCK_SIZE, num_warps = calculate_settings(n_cols)\\n-        losses    = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n-        logsumexp = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n-\\n-        _cross_entropy_forward[(n_rows,)](\\n-            logits, logits.stride(0),\\n-            losses,\\n-            logsumexp,\\n-            labels,\\n-            n_cols,\\n-            BLOCK_SIZE = BLOCK_SIZE,\\n-            num_warps  = num_warps,\\n-        )\\n+        n_rows, vocab_size = logits.shape\\n+\\n+        div, mod = divmod(vocab_size, MAX_FUSED_SIZE)\\n+        n_chunks = div + (mod != 0)\\n+        losses = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n+\\n+        if n_chunks == 1:\\n+            # For small vocabs <= 65336 like Llama, Mistral\\n+            BLOCK_SIZE, num_warps = calculate_settings(vocab_size)\\n+            logsumexp = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n+\\n+            _cross_entropy_forward[(n_rows,)](\\n+                logits, logits.stride(0),\\n+                losses,\\n+                logsumexp,\\n+                labels,\\n+                VOCAB_SIZE = vocab_size,\\n+                BLOCK_SIZE = BLOCK_SIZE,\\n+                num_warps  = num_warps,\\n+            )\\n+        else:\\n+            # For large vocabs > 65336 like Gemma 256K\\n+            logsumexp = torch.empty((n_rows, n_chunks,), dtype = torch.float32, device = \"cuda\")\\n+\\n+            _chunked_cross_entropy_forward[(n_rows, n_chunks,)](\\n+                logits, logits.stride(0),\\n+                losses,\\n+                logsumexp,\\n+                labels,\\n+                VOCAB_SIZE = vocab_size,\\n+                N_CHUNKS   = n_chunks,\\n+                BLOCK_SIZE = MAX_FUSED_SIZE,\\n+                num_warps  = 32,\\n+            )\\n+            # logsumexp(chunked_logsumexp) - x\\n+            # Do the -x separately\\n+            logsumexp = torch.logsumexp(logsumexp, dim = 1) # Row sum\\n+            losses += logsumexp\\n+            losses.masked_fill_(labels == -100, 0) # Don\\'t forget to mask padding out!\\n+        pass\\n \\n-        ctx.BLOCK_SIZE = BLOCK_SIZE\\n-        ctx.num_warps = num_warps\\n         ctx.save_for_backward(logits, logsumexp, labels)\\n         return losses\\n     pass\\n@@ -131,23 +240,26 @@ class Fast_CrossEntropyLoss(torch.autograd.Function):\\n     @staticmethod\\n     def backward(ctx, dlosses):\\n         logits, logsumexp, labels = ctx.saved_tensors\\n-        n_rows, n_cols = logits.shape\\n+        n_rows, vocab_size = logits.shape\\n \\n-        _cross_entropy_backward[(n_rows,)](\\n+        BLOCK_SIZE = 4096\\n+        div, mod = divmod(vocab_size, BLOCK_SIZE)\\n+        n_blocks = div + (mod != 0)\\n+\\n+        _cross_entropy_backward[(n_rows, n_blocks,)](\\n             logits,   logits.stride(0),\\n             dlosses, dlosses.stride(0),\\n             logsumexp,\\n             labels,\\n-            n_cols,\\n-            BLOCK_SIZE = ctx.BLOCK_SIZE,\\n-            num_warps  = ctx.num_warps,\\n+            VOCAB_SIZE = vocab_size,\\n+            BLOCK_SIZE = BLOCK_SIZE,\\n+            num_warps  = 8,\\n         )\\n         return logits, None, None,\\n     pass\\n pass\\n \\n \\n-slow_cross_entropy_loss = torch.nn.functional.cross_entropy\\n def fast_cross_entropy_loss(logits, labels):\\n     \"\"\"\\n     Arguments:\\n@@ -159,25 +271,10 @@ def fast_cross_entropy_loss(logits, labels):\\n     batch, seq_len, d = logits.shape\\n     assert(labels.shape == (batch, seq_len))\\n \\n-    # Prelim support Qwen, Deepseek other large vocab sizes > 2^16\\n-    if d > MAX_FUSED_SIZE:\\n-        logger.warning_once(\\n-            f\"Unsloth: Vocab size of {d} exceeds the max CUDA blocksize of {MAX_FUSED_SIZE}.\\\\n\"\\\\\\n-            \"For now, Unsloth will use Pytorch\\'s CrossEntropyLoss, which will entail a\\\\n\"\\\\\\n-            \"25% increase in memory usage and be slower. Make an issue on \\\\n\"\\\\\\n-            \"Unsloth\\'s Github page if you want a faster and more memory efficient kernel!\"\\n-        )\\n-        loss = slow_cross_entropy_loss(\\n-            logits.float().view(batch*seq_len, d), # Must cast to float32 for numerical stability\\n-            labels.view(-1),\\n-        )\\n-        return loss\\n-    else:\\n-        loss = Fast_CrossEntropyLoss.apply(\\n-            logits.view(batch*seq_len, d),\\n-            labels.view(-1),\\n-        )\\n-        n_items = torch.count_nonzero(labels != -100)\\n-        return loss.sum() / n_items\\n-    pass\\n+    loss = Fast_CrossEntropyLoss.apply(\\n+        logits.view(batch*seq_len, d),\\n+        labels.view(-1),\\n+    )\\n+    n_items = torch.count_nonzero(labels != -100)\\n+    return loss.sum() / n_items\\n pass\\n',\n",
       " '@@ -14,7 +14,6 @@\\n \\n import torch\\n from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters\\n-from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n \\n \\n def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n@@ -85,20 +84,20 @@ class LoRA_MLP(torch.autograd.Function):\\n     def forward(ctx, X : torch.Tensor,\\n                 gateW, gateW_quant, gateA, gateB, gateS,\\n                   upW,   upW_quant, upA,   upB,   upS,\\n-                downW, downW_quant, downA, downB, downS):\\n+                downW, downW_quant, downA, downB, downS,\\n+                _forward_function, _backward_function,):\\n         dtype = X.dtype\\n \\n         e = matmul_lora(X, gateW, gateW_quant, gateA, gateB, gateS)\\n         g = matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\\n-        # f = torch.nn.functional.silu(e)\\n-        # h = f * g\\n-        h = swiglu_fg_kernel(e, g)\\n+        h = _forward_function(e, g)\\n         i = matmul_lora(h, downW, downW_quant, downA, downB, downS)\\n \\n         ctx.custom_saved_tensors = (\\n             gateW, gateW_quant, gateS,\\n             upW, upW_quant, upS,\\n             downW, downW_quant, downS,\\n+            _backward_function,\\n         )\\n         ctx.save_for_backward(gateA, gateB, upA, upB, downA, downB,\\n                               X, e, g)\\n@@ -109,8 +108,8 @@ class LoRA_MLP(torch.autograd.Function):\\n     @staticmethod\\n     @torch.cuda.amp.custom_bwd\\n     def backward(ctx, dY : torch.Tensor):\\n-        gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, = \\\\\\n-            ctx.custom_saved_tensors\\n+        gateW, gateW_quant, gateS, upW, upW_quant, upS, downW, downW_quant, downS, \\\\\\n+            _backward_function = ctx.custom_saved_tensors\\n         gateA, gateB, upA, upB, downA, downB, \\\\\\n             X, e, g = ctx.saved_tensors\\n \\n@@ -125,14 +124,7 @@ class LoRA_MLP(torch.autograd.Function):\\n         dtype = X.dtype\\n \\n         DW = matmul_lora(dY, downW.t(), downW_quant, downB, downA, downS)\\n-        # e = e.float()\\n-        # se = 1.0 / (1.0 + torch.exp(-e))\\n-        # f = (se * e).to(dtype)\\n-        # h = f * g\\n-        # df = DW * f\\n-        # dg = DW * g\\n-        # de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\\n-        DW, e, g = swiglu_DWf_DW_dfg_kernel(DW, e, g)\\n+        DW, e, g = _backward_function(DW, e, g)\\n         h, df, de = DW, e, g\\n \\n         # Down projection LoRA weights\\n@@ -155,7 +147,6 @@ class LoRA_MLP(torch.autograd.Function):\\n \\n         # dX  = matmul_lora(df, upW.t(), upW_quant, upB, upA, upS)\\n         # dX += matmul_lora(de, gateW.t(), gateW_quant, gateB, gateA, gateS)\\n-\\n         upW = fast_dequantize(upW.t(), upW_quant)\\n         dX = torch.matmul(df, upW.t(), out = X)\\n         del upW\\n@@ -172,24 +163,36 @@ class LoRA_MLP(torch.autograd.Function):\\n         return dX.view(batch, seq_len, hd), \\\\\\n             None, None, d_gateA.t(), d_gateB.t(), None, \\\\\\n             None, None,   d_upA.t(),   d_upB.t(), None, \\\\\\n-            None, None, d_downA.t(), d_downB.t(), None,\\n+            None, None, d_downA.t(), d_downB.t(), None, \\\\\\n+            None, None, # _backward and _forward\\n     pass\\n pass\\n \\n \\n-def apply_lora_mlp(self, X):\\n-    # gate = self.gate_proj(X)\\n-    # up   = self.  up_proj(X)\\n-    # h = torch.nn.functional.silu(gate) * up\\n-    # down = self.down_proj(h)\\n-    # return down\\n+from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n+def apply_lora_mlp_swiglu(self, X):\\n+    gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n+    upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n+    downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n+    out = LoRA_MLP.apply(X,\\n+                         gateW, gateW_quant, gateA, gateB, gateS,\\n+                         upW,     upW_quant, upA,   upB,   upS,\\n+                         downW, downW_quant, downA, downB, downS,\\n+                         swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel,)\\n+    return out\\n+pass\\n+\\n+\\n+from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n+def apply_lora_mlp_geglu(self, X):\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n     out = LoRA_MLP.apply(X,\\n                          gateW, gateW_quant, gateA, gateB, gateS,\\n                          upW,     upW_quant, upA,   upB,   upS,\\n-                         downW, downW_quant, downA, downB, downS)\\n+                         downW, downW_quant, downA, downB, downS,\\n+                         geglu_forward_kernel, geglu_backward_kernel,)\\n     return out\\n pass\\n \\n',\n",
       " '@@ -0,0 +1,104 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+import triton\\n+import triton.language as tl\\n+import torch\\n+from .utils import calculate_settings\\n+\\n+\\n+@triton.jit\\n+def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    # f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\\n+    # h = f * up\\n+    e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    f_row = 0.5 * e_row * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n+    h_row = f_row * g_row\\n+\\n+    # Store h\\n+    tl.store(h + offsets, h_row, mask = mask)\\n+pass\\n+\\n+\\n+def geglu_forward_kernel(gate, up):\\n+    batch, seq_len, hd = gate.shape\\n+    n_elements = gate.numel()\\n+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    return out\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\\n+    h = f * up\\n+\\n+    df/de (with help of Wolfram :)\\n+    df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)\\n+\\n+    Reuse via\\n+    f =        1/2 * (1 + erf(1/sqrt(2) * e)) * e\\n+    \"\"\"\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    # Break e_row away for re-use\\n+    # f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\\n+    f_partial_row = 0.5 * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\\n+    f_row = f_partial_row * e_row\\n+    \\n+    f_row = f_row.to(DW_row.dtype)\\n+    # h = f * g\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+\\n+    # df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)\\n+    t = 0.3989422804014327 # 1/sqrt(2*pi)\\n+    df_de = f_partial_row + t * e_row * tl.exp(-0.5 * e_row * e_row)\\n+\\n+    de_row = dg_row.to(tl.float32) * df_de\\n+    de_row = de_row.to(DW_row.dtype)\\n+\\n+    # Store derivatives in buffers\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n+pass\\n+\\n+\\n+def geglu_backward_kernel(DW, e, g):\\n+    batch_seq_len, hd = e.shape\\n+    n_elements = e.numel()\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    return DW, e, g\\n+pass\\n',\n",
       " '@@ -44,7 +44,7 @@ def _rms_layernorm_forward(\\n     W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)\\n \\n     row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n-    inv_var = 1.0 / tl.sqrt(row_var + eps)\\n+    inv_var = tl.math.rsqrt(row_var + eps)\\n     tl.store(r, inv_var)\\n     normed = X_row * inv_var\\n     normed = normed.to(W_row.dtype) # Exact copy from HF\\n',\n",
       " '@@ -0,0 +1,282 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from .llama import *\\n+from ._utils import __version__\\n+\\n+from transformers.models.gemma.modeling_gemma import (\\n+    GemmaAttention,\\n+    GemmaDecoderLayer,\\n+    GemmaModel,\\n+    GemmaForCausalLM,\\n+    GemmaRotaryEmbedding,\\n+    apply_rotary_pos_emb,\\n+    repeat_kv,\\n+)\\n+from transformers.modeling_attn_mask_utils import (\\n+    _prepare_4d_causal_attention_mask_for_sdpa,\\n+)\\n+# For Pytorch 2.1.1\\n+try:\\n+    from transformers.models.gemma.modeling_gemma import (\\n+        GemmaSdpaAttention,\\n+        GemmaFlashAttention2,\\n+    )\\n+except:\\n+    GemmaSdpaAttention   = GemmaAttention\\n+    GemmaFlashAttention2 = GemmaAttention\\n+pass\\n+\\n+\\n+def fast_geglu_inference(self, X):\\n+    # gate = self.gate_proj(X)\\n+    # up   = self.up_proj(X)\\n+    bsz, _, hd = X.shape\\n+    mlp_size = self.config.intermediate_size\\n+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+\\n+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = torch.nn.functional.gelu(gate)\\n+    gate *= up\\n+\\n+    # X = self.down_proj(gate)\\n+    down = fast_linear_forward(self.down_proj, gate, out = up[:,:,:hd])\\n+    return down\\n+pass\\n+\\n+\\n+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n+def GemmaDecoderLayer_fast_forward(\\n+    self,\\n+    hidden_states:        torch.Tensor,\\n+    causal_mask:          Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+    attention_mask:       Optional[torch.Tensor] = None,\\n+    position_ids:         Optional[torch.LongTensor] = None,\\n+    past_key_value:       Optional[Tuple[torch.Tensor]] = None,\\n+    output_attentions:    Optional[bool] = False,\\n+    use_cache:            Optional[bool] = False,\\n+    padding_mask:         Optional[torch.LongTensor] = None,\\n+    *args, **kwargs,\\n+):\\n+    if False:#past_key_value is not None:\\n+        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+            self.self_attn,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+            do_prefill = do_prefill,\\n+        )\\n+        hidden_states += residual\\n+\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_geglu_inference(self.mlp, hidden_states)\\n+        hidden_states += residual\\n+    else:\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        # hidden_states = self.input_layernorm(hidden_states)\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n+        )\\n+        hidden_states = residual + hidden_states\\n+\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        # hidden_states = self.post_attention_layernorm(hidden_states)\\n+        hidden_states = self.mlp(hidden_states)\\n+        hidden_states = residual + hidden_states\\n+    pass\\n+\\n+    outputs = (hidden_states,)\\n+\\n+    if output_attentions:\\n+        outputs += (self_attn_weights,)\\n+\\n+    if use_cache:\\n+        outputs += (present_key_value,)\\n+\\n+    return outputs\\n+pass\\n+\\n+\\n+from math import sqrt as math_sqrt\\n+\\n+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n+@torch.inference_mode\\n+def GemmaModel_fast_forward_inference(\\n+    self,\\n+    input_ids,\\n+    past_key_values,\\n+):\\n+    # Fix out of bounds tokenization\\n+    input_ids = input_ids[:,:self.max_seq_length]\\n+\\n+    hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states *= math_sqrt(self.config.hidden_size)\\n+\\n+    next_decoder_cache = []\\n+    for idx, decoder_layer in enumerate(self.layers):\\n+        # Self Attention\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+            decoder_layer.self_attn,\\n+            hidden_states,\\n+            past_key_values[idx],\\n+            None,\\n+        )\\n+        hidden_states += residual\\n+\\n+        # Fully Connected\\n+        residual = hidden_states\\n+        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n+        hidden_states += residual\\n+\\n+        next_decoder_cache.append(present_key_value)\\n+    pass\\n+    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+\\n+    return BaseModelOutputWithPast(\\n+        last_hidden_state = hidden_states,\\n+        past_key_values   = next_decoder_cache,\\n+        hidden_states     = [],\\n+        attentions        = [],\\n+    )\\n+pass\\n+\\n+\\n+class FastGemmaModel(FastLlamaModel):\\n+\\n+    @staticmethod\\n+    def pre_patch():\\n+        GemmaAttention      .forward = LlamaAttention_fast_forward\\n+        GemmaSdpaAttention  .forward = LlamaAttention_fast_forward\\n+        GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n+        GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n+        GemmaModel          .forward = LlamaModel_fast_forward\\n+        GemmaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n+        # Solves https://github.com/unslothai/unsloth/issues/168\\n+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.\\n+        # https://github.com/huggingface/transformers/pull/27931\\n+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n+        import transformers.models.gemma.modeling_gemma\\n+        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = LlamaRotaryEmbedding\\n+        return\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def post_patch(model):\\n+        # Patch model for Gemma\\n+        layers = model.model.layers\\n+\\n+        # Torch.compile fails on embedding matrix??\\n+        # Workaround randomnly fixes it for torch versions < 2.2\\n+        model.model.embed_tokens = torch.nn.Embedding.from_pretrained(model.model.embed_tokens.weight)\\n+        model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        # We also do this for the lm_head\\n+        lm_head = torch.nn.Linear(1, 1, bias = None)\\n+        del lm_head.weight\\n+        lm_head.weight = model.lm_head.weight\\n+        lm_head.in_features  = lm_head.weight.shape[1]\\n+        lm_head.out_features = lm_head.weight.shape[0]\\n+        model.lm_head = lm_head\\n+\\n+        # Gemma has tied weights! This means lm_head == embed_tokens\\n+        if model.model.embed_tokens.weight.data_ptr() != model.lm_head.weight.data_ptr():\\n+            lm_head = torch.nn.Linear(1, 1, bias = None)\\n+            del lm_head.weight\\n+            lm_head.weight = model.model.embed_tokens.weight\\n+            lm_head.in_features  = lm_head.weight.shape[1]\\n+            lm_head.out_features = lm_head.weight.shape[0]\\n+            model.lm_head = lm_head\\n+        pass\\n+\\n+        # Also patch all dtypes - BnB seems to not allocate the correct type?\\n+        # BnB default dtype seems to be float16!\\n+        correct_dtype = lm_head.weight.dtype\\n+\\n+        for name, module in model.named_modules():\\n+            if isinstance(module, (Bnb_Linear4bit, Peft_Linear4bit)):\\n+                weight = module.weight\\n+                quant_state = weight.quant_state\\n+\\n+                if type(quant_state) is list:\\n+                    # BnB seems to have float16 as default!\\n+                    module.weight.quant_state[2] = correct_dtype # Cast to correct dtype\\n+                else:\\n+                    # https://github.com/TimDettmers/bitsandbytes/pull/763/files\\n+                    quant_state.dtype = correct_dtype\\n+                pass\\n+            pass\\n+            # Downcast RoPE embedding to correct data type\\n+            if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n+                and (module.cos_cached.dtype != correct_dtype):\\n+\\n+                module.cos_cached = module.cos_cached.to(correct_dtype)\\n+                module.sin_cached = module.sin_cached.to(correct_dtype)\\n+                pass\\n+            pass\\n+        pass\\n+\\n+        # Add 1 to weight\\n+        # return output * (1 + self.weight)\\n+        # https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py#L89\\n+        from transformers.models.gemma.modeling_gemma import GemmaRMSNorm\\n+\\n+        # Freeze all parameters except LoRA\\n+        # We do this first since += 1 seems to not be liked by requires_grad = True\\n+        for name, param in model.named_parameters():\\n+            if \".lora_A.\" in name or \".lora_B.\" in name:\\n+                param.requires_grad_(True)\\n+            else:\\n+                param.requires_grad_(False)\\n+        pass\\n+\\n+        # Patch RMS Layernorm\\n+        for name, module in model.named_modules():\\n+            if isinstance(module, GemmaRMSNorm):\\n+                module.weight += 1.0 # return output * (1 + self.weight)\\n+                if not hasattr(module, \"variance_epsilon\"):\\n+                    module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n+        pass\\n+\\n+        # Clear deleted GPU items\\n+        import gc\\n+        for _ in range(3):\\n+            gc.collect()\\n+            torch.cuda.empty_cache()\\n+        return model\\n+    pass\\n+pass\\n',\n",
       " '@@ -119,6 +119,7 @@ def LlamaAttention_fast_forward_inference(\\n     n_groups   = self.num_key_value_groups\\n     n_kv_heads = self.num_key_value_heads\\n     head_dim   = self.head_dim\\n+    attention_size = n_heads*head_dim\\n     # assert(n_kv_heads * n_groups == n_heads)\\n     seq_len = K1.shape[-2]\\n     kv_seq_len = seq_len + 1\\n@@ -131,7 +132,7 @@ def LlamaAttention_fast_forward_inference(\\n         self.paged_attention_V = self.paged_attention[:,1]\\n         self.paged_attention_K[:seq_len] = K1.permute(2, 0, 1, 3)\\n         self.paged_attention_V[:seq_len] = V1.permute(2, 0, 1, 3)\\n-        self.temp_QA = torch.empty((2, bsz, 1, hd), dtype = dtype, device = \"cuda\")\\n+        self.temp_QA = torch.empty((2, bsz, 1, attention_size), dtype = dtype, device = \"cuda\")\\n         self.temp_KV = torch.empty((2, bsz, 1, n_kv_heads*head_dim), dtype = dtype, device = \"cuda\")\\n         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = \"cuda\")\\n@@ -201,13 +202,13 @@ def LlamaAttention_fast_forward_inference(\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n-    A = A.reshape(bsz, 1, self.hidden_size)\\n-    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1])\\n+    A = A.reshape(bsz, 1, attention_size)\\n+    A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])\\n     return A, (Kn, Vn)\\n pass\\n \\n \\n-def fast_mlp_inference(self, X):\\n+def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n@@ -339,7 +340,7 @@ def LlamaAttention_fast_forward(\\n         # Go back to (batch_size, seq_len, n_heads, head_dim)\\n         A = A.transpose(1, 2).contiguous()\\n     pass\\n-    attn_output = A.reshape(bsz, q_len, self.hidden_size)\\n+    attn_output = A.reshape(bsz, q_len, n_heads*head_dim)\\n     attn_output = self.apply_o(self, attn_output)\\n     attn_weights = None\\n     return attn_output, attn_weights, past_key_value\\n@@ -390,7 +391,7 @@ def LlamaDecoderLayer_fast_forward(\\n         # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n-        hidden_states = fast_mlp_inference(self.mlp, hidden_states)\\n+        hidden_states = fast_swiglu_inference(self.mlp, hidden_states)\\n         hidden_states += residual\\n     else:\\n         residual = hidden_states\\n@@ -507,6 +508,14 @@ def LlamaModel_fast_forward(\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n+    # Mormalized from Gemma\\n+    if self.config.model_type == \"gemma\":\\n+        inputs_requires_grad = inputs_embeds.requires_grad\\n+        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+    pass\\n+\\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n     if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n@@ -646,7 +655,7 @@ def LlamaModel_fast_forward_inference(\\n         # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n-        hidden_states = fast_mlp_inference(decoder_layer.mlp, hidden_states)\\n+        hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)\\n         hidden_states += residual\\n \\n         next_decoder_cache.append(present_key_value)\\n@@ -812,7 +821,7 @@ class LlamaRotaryEmbedding(torch.nn.Module):\\n         self.register_buffer(\"sin_cached\", emb.sin().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\\n     pass\\n \\n-    def forward(self, x, seq_len=None):\\n+    def forward(self, x, position_ids=None, seq_len=None):\\n         # x: [bs, num_attention_heads, seq_len, head_size]\\n         if seq_len > self.max_seq_len_cached:\\n             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\\n@@ -886,20 +895,22 @@ class FastLlamaModel:\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        model_patcher  = None,\\n         **kwargs,\\n     ):\\n+        if model_patcher is None: model_patcher = FastLlamaModel\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n \\n         statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast Llama patching release {__version__}\\\\n\"\\\\\\n+           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n            f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n-        FastLlamaModel.pre_patch()\\n+        model_patcher.pre_patch()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n@@ -955,7 +966,7 @@ class FastLlamaModel:\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = FastLlamaModel.post_patch(model)\\n+        model = model_patcher.post_patch(model)\\n \\n         # Patch up QKV / O and MLP\\n         for idx, layer in enumerate(model.model.layers):\\n@@ -1159,6 +1170,14 @@ class FastLlamaModel:\\n                     quant_state.dtype = correct_dtype\\n                 pass\\n             pass\\n+            # Downcast RoPE embedding to correct data type\\n+            if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n+                and (module.cos_cached.dtype != correct_dtype):\\n+                \\n+                module.cos_cached = module.cos_cached.to(correct_dtype)\\n+                module.sin_cached = module.sin_cached.to(correct_dtype)\\n+                pass\\n+            pass\\n         pass\\n \\n         # Clear deleted GPU items\\n@@ -1309,6 +1328,16 @@ class FastLlamaModel:\\n             )\\n         pass\\n \\n+        # Get activation function\\n+        model_type = model.config.model_type\\n+\\n+        if   model_type == \"llama\":   apply_lora_mlp = apply_lora_mlp_swiglu\\n+        elif model_type == \"mistral\": apply_lora_mlp = apply_lora_mlp_swiglu\\n+        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu\\n+        else:\\n+            raise NotImplementedError(f\"Unsloth: {model_type} is not yet implemented!\")\\n+        pass\\n+\\n         model = prepare_model_for_kbit_training(\\n             model,\\n             use_gradient_checkpointing = use_gradient_checkpointing,\\n',\n",
       " '@@ -24,6 +24,9 @@ from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n major, minor = transformers_version.split(\".\")[:2]\\n major, minor = int(major), int(minor)\\n SUPPORTS_FOURBIT = (major > 4) or (major == 4 and minor >= 37)\\n+SUPPORTS_GEMMA   = (major > 4) or (major == 4 and minor >= 38)\\n+if SUPPORTS_GEMMA:\\n+    from .gemma import FastGemmaModel\\n del major, minor\\n \\n \\n@@ -99,6 +102,15 @@ class FastLanguageModel(FastLlamaModel):\\n \\n         if   model_type == \"llama\":   dispatch_model = FastLlamaModel\\n         elif model_type == \"mistral\": dispatch_model = FastMistralModel\\n+        elif model_type == \"gemma\":\\n+            if not SUPPORTS_GEMMA:\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Your transformers version of {transformers_version} does not support Gemma.\\\\n\"\\\\\\n+                    f\"The minimum required version is 4.38.\\\\n\"\\\\\\n+                    f\\'Try `pip install --upgrade \"transformers>=4.38\"`\\\\n\\'\\\\\\n+                    f\"to obtain the latest transformers build, then restart this session.\"\\\\\\n+                )\\n+            dispatch_model = FastGemmaModel\\n         else:\\n             raise NotImplementedError(\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n@@ -115,6 +127,7 @@ class FastLanguageModel(FastLlamaModel):\\n             device_map     = device_map,\\n             rope_scaling   = rope_scaling,\\n             fix_tokenizer  = fix_tokenizer,\\n+            model_patcher  = dispatch_model,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -74,6 +74,22 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/solar-10.7b-bnb-4bit\" : (\\n         \"upstage/SOLAR-10.7B-v1.0\",\\n     ),\\n+    \"unsloth/gemma-7b-bnb-4bit\" : (\\n+        \"unsloth/gemma-7b\",\\n+        \"google/gemma-7b\",\\n+    ),\\n+    \"unsloth/gemma-2b-bnb-4bit\" : (\\n+        \"unsloth/gemma-2b\",\\n+        \"google/gemma-2b\",\\n+    ),\\n+    \"unsloth/gemma-7b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-7b-it\",\\n+        \"google/gemma-7b-it\",\\n+    ),\\n+    \"unsloth/gemma-2b-bnb-4bit\" : (\\n+        \"unsloth/gemma-2b-it\",\\n+        \"google/gemma-2b-it\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -293,8 +293,10 @@ class FastMistralModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None, # Mistral does not support RoPE scaling\\n         fix_tokenizer  = True,\\n+        model_patcher  = None,\\n         **kwargs,\\n     ):\\n+        if model_patcher is None: model_patcher = FastMistralModel\\n         # Mistral does NOT support RoPE Scaling!\\n         if rope_scaling is not None:\\n             logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n@@ -305,13 +307,13 @@ class FastMistralModel(FastLlamaModel):\\n         max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n \\n         statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast Mistral patching release {__version__}\\\\n\"\\\\\\n+           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n            f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n            f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n            f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\\'\\n+           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n-        FastMistralModel.pre_patch()\\n+        model_patcher.pre_patch()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n@@ -360,7 +362,7 @@ class FastMistralModel(FastLlamaModel):\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = FastMistralModel.post_patch(model)\\n+        model = model_patcher.post_patch(model)\\n \\n         # Patch up QKV / O and MLP\\n         for idx, layer in enumerate(model.model.layers):\\n',\n",
       " '@@ -369,6 +369,7 @@ def unsloth_save_model(\\n \\n     # Switch to our fast saving modules if it\\'s a slow PC!\\n     n_cpus = psutil.cpu_count(logical = False)\\n+    if n_cpus is None: n_cpus = 1\\n \\n     if safe_serialization is None:\\n         safe_serialization = True\\n@@ -669,7 +670,9 @@ def save_to_gguf(\\n         pass\\n     pass\\n \\n-    n_cpus = psutil.cpu_count()*2\\n+    n_cpus = psutil.cpu_count()\\n+    if n_cpus is None: n_cpus = 1\\n+    n_cpus *= 2\\n     # Concurrency from https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model\\n     \\n     final_location = f\"./{model_directory}-unsloth.{first_conversion.upper()}.gguf\"\\n',\n",
       " '@@ -221,6 +221,7 @@ CHAT_TEMPLATES[\"alpaca\"] = (alpaca_template, alpaca_eos_token,)\\n # Notice we must use |trim for lstrip and rstrip. <start_of_turn> maps to 106.\\n # <end_of_turn> maps to 107. user and model are normal 1 word tokens.\\n gemma_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n     \"{% for message in messages %}\"\\\\\\n         \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n             \"{{\\'<start_of_turn>user\\\\n\\' + message[\\'content\\'] | trim + \\'<end_of_turn>\\\\n\\'}}\"\\\\\\n@@ -238,7 +239,7 @@ CHAT_TEMPLATES[\"gemma\"] = (gemma_template, gemma_eos_token,)\\n \\n \\n # Gemma with ChatML instead\\n-gemma_chatml_template = chatml_template\\n+gemma_chatml_template = \"{{ bos_token }}\" + chatml_template\\n gemma_chatml_eos_token = (\\n     {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"},\\n     \"<|im_end|>\",\\n',\n",
       " '@@ -240,61 +240,30 @@ pass\\n \\n # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??\\n # For mixed precision, we need it to be in float32 not float16.\\n-def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,\\n-    use_rslora = False):\\n-    # This code works for linear layers, override for other layer types\\n-    if r <= 0:\\n-        raise ValueError(f\"`r` should be a positive integer value but the value passed is {r}\")\\n-\\n-    self.r[adapter_name] = r\\n-    self.lora_alpha[adapter_name] = lora_alpha\\n-    if lora_dropout > 0.0:\\n-        lora_dropout_layer = torch.nn.Dropout(p=lora_dropout)\\n-    else:\\n-        lora_dropout_layer = torch.nn.Identity()\\n-\\n-    self.lora_dropout.update(torch.nn.ModuleDict({adapter_name: lora_dropout_layer}))\\n-    # Actual trainable parameters\\n-    self.lora_A[adapter_name] = torch.nn.Linear(self.in_features, r, bias=False)\\n-    self.lora_B[adapter_name] = torch.nn.Linear(r, self.out_features, bias=False)\\n-    if use_rslora:\\n-        self.scaling[adapter_name] = lora_alpha / math.sqrt(r)\\n-    else:\\n-        self.scaling[adapter_name] = lora_alpha / r\\n-\\n-    if init_lora_weights == \"loftq\":\\n-        # We manually check for PEFT\\n-        if not hasattr(self, \"loftq_init\"):\\n-            import peft\\n-            raise RuntimeError(\\n-                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n-                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n-            )\\n-        pass\\n-        self.loftq_init(adapter_name)\\n-\\n-    elif init_lora_weights:\\n-        self.reset_lora_parameters(adapter_name, init_lora_weights)\\n+from peft.tuners.lora.layer import LoraLayer\\n+import inspect, re\\n+try:\\n+    source = inspect.getsource(LoraLayer.update_layer)\\n+    text = \"if weight is not None:\\\\n\"\\n+    start = source.find(text) + len(text)\\n+    end = source.find(\"self.to(weight.device)\", start)\\n+    spaces = re.findall(r\"^([ ]{1,})break\", source, flags = re.MULTILINE)[0]\\n+    source = source.replace(source[start : end], spaces)\\n+    spaces = len(re.match(r\"[\\\\s]{1,}\", source).group(0))\\n+    lines = source.split(\"\\\\n\")\\n+    source = \"\\\\n\".join(x[spaces:] for x in lines)\\n+    source = re.sub(\"([^\\\\.])nn\\\\.\", r\"\\\\1torch.nn.\", source)\\n+    source = source.replace(\"def update_layer\", \"def LoraLayer_update_layer\")\\n+    exec(source, globals())\\n \\n-    # check weight and qweight (for GPTQ)\\n-    for weight_name in (\"weight\", \"qweight\"):\\n-        weight = getattr(self.get_base_layer(), weight_name, None)\\n-        if weight is not None:\\n-            # [INCORRECT code]\\n-            # \\n-            # the layer is already completely initialized, this is an update\\n-            # if weight.dtype.is_floating_point or weight.dtype.is_complex:\\n-            #     self.to(weight.device, dtype=weight.dtype)\\n-            # else:\\n-            #     self.to(weight.device)\\n-            self.to(weight.device, non_blocking = True)\\n-            break\\n-    self.set_adapter(self.active_adapters)\\n+    # Fix up incorrect downcasting of LoRA weights\\n+    from peft.tuners.lora.layer import LoraLayer\\n+    LoraLayer.update_layer = LoraLayer_update_layer\\n+    from peft.tuners.lora import LoraLayer\\n+    LoraLayer.update_layer = LoraLayer_update_layer\\n+except:\\n+    logger.warning_once(\\n+        \"Unsloth unsuccessfully patched LoraLayer.update_layer. Please file a bug report.\\\\n\"\\\\\\n+        \"Luckily, your training run will still work in the meantime!\"\\n+    )\\n pass\\n-\\n-# Fix up incorrect downcasting of LoRA weights\\n-from peft.tuners.lora.layer import LoraLayer\\n-LoraLayer.update_layer = LoraLayer_update_layer\\n-from peft.tuners.lora import LoraLayer\\n-LoraLayer.update_layer = LoraLayer_update_layer\\n',\n",
       " '@@ -221,6 +221,7 @@ CHAT_TEMPLATES[\"alpaca\"] = (alpaca_template, alpaca_eos_token,)\\n # Notice we must use |trim for lstrip and rstrip. <start_of_turn> maps to 106.\\n # <end_of_turn> maps to 107. user and model are normal 1 word tokens.\\n gemma_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n     \"{% for message in messages %}\"\\\\\\n         \"{% if message[\\'role\\'] == \\'user\\' %}\"\\\\\\n             \"{{\\'<start_of_turn>user\\\\n\\' + message[\\'content\\'] | trim + \\'<end_of_turn>\\\\n\\'}}\"\\\\\\n@@ -238,7 +239,7 @@ CHAT_TEMPLATES[\"gemma\"] = (gemma_template, gemma_eos_token,)\\n \\n \\n # Gemma with ChatML instead\\n-gemma_chatml_template = chatml_template\\n+gemma_chatml_template = \"{{ bos_token }}\" + chatml_template\\n gemma_chatml_eos_token = (\\n     {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"},\\n     \"<|im_end|>\",\\n',\n",
       " '@@ -240,61 +240,30 @@ pass\\n \\n # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??\\n # For mixed precision, we need it to be in float32 not float16.\\n-def LoraLayer_update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,\\n-    use_rslora = False):\\n-    # This code works for linear layers, override for other layer types\\n-    if r <= 0:\\n-        raise ValueError(f\"`r` should be a positive integer value but the value passed is {r}\")\\n-\\n-    self.r[adapter_name] = r\\n-    self.lora_alpha[adapter_name] = lora_alpha\\n-    if lora_dropout > 0.0:\\n-        lora_dropout_layer = torch.nn.Dropout(p=lora_dropout)\\n-    else:\\n-        lora_dropout_layer = torch.nn.Identity()\\n-\\n-    self.lora_dropout.update(torch.nn.ModuleDict({adapter_name: lora_dropout_layer}))\\n-    # Actual trainable parameters\\n-    self.lora_A[adapter_name] = torch.nn.Linear(self.in_features, r, bias=False)\\n-    self.lora_B[adapter_name] = torch.nn.Linear(r, self.out_features, bias=False)\\n-    if use_rslora:\\n-        self.scaling[adapter_name] = lora_alpha / math.sqrt(r)\\n-    else:\\n-        self.scaling[adapter_name] = lora_alpha / r\\n-\\n-    if init_lora_weights == \"loftq\":\\n-        # We manually check for PEFT\\n-        if not hasattr(self, \"loftq_init\"):\\n-            import peft\\n-            raise RuntimeError(\\n-                f\"Unsloth: Your PEFT version of {peft.__version__} does not support LoftQ init.\\\\n\"\\\\\\n-                \"Please install PEFT 0.7.2 or higher.\\\\n\"\\\\\\n-                \"You can also install from source: `pip install git+https://github.com/huggingface/peft.git\"\\n-            )\\n-        pass\\n-        self.loftq_init(adapter_name)\\n-\\n-    elif init_lora_weights:\\n-        self.reset_lora_parameters(adapter_name, init_lora_weights)\\n+from peft.tuners.lora.layer import LoraLayer\\n+import inspect, re\\n+try:\\n+    source = inspect.getsource(LoraLayer.update_layer)\\n+    text = \"if weight is not None:\\\\n\"\\n+    start = source.find(text) + len(text)\\n+    end = source.find(\"self.to(weight.device)\", start)\\n+    spaces = re.findall(r\"^([ ]{1,})break\", source, flags = re.MULTILINE)[0]\\n+    source = source.replace(source[start : end], spaces)\\n+    spaces = len(re.match(r\"[\\\\s]{1,}\", source).group(0))\\n+    lines = source.split(\"\\\\n\")\\n+    source = \"\\\\n\".join(x[spaces:] for x in lines)\\n+    source = re.sub(\"([^\\\\.])nn\\\\.\", r\"\\\\1torch.nn.\", source)\\n+    source = source.replace(\"def update_layer\", \"def LoraLayer_update_layer\")\\n+    exec(source, globals())\\n \\n-    # check weight and qweight (for GPTQ)\\n-    for weight_name in (\"weight\", \"qweight\"):\\n-        weight = getattr(self.get_base_layer(), weight_name, None)\\n-        if weight is not None:\\n-            # [INCORRECT code]\\n-            # \\n-            # the layer is already completely initialized, this is an update\\n-            # if weight.dtype.is_floating_point or weight.dtype.is_complex:\\n-            #     self.to(weight.device, dtype=weight.dtype)\\n-            # else:\\n-            #     self.to(weight.device)\\n-            self.to(weight.device, non_blocking = True)\\n-            break\\n-    self.set_adapter(self.active_adapters)\\n+    # Fix up incorrect downcasting of LoRA weights\\n+    from peft.tuners.lora.layer import LoraLayer\\n+    LoraLayer.update_layer = LoraLayer_update_layer\\n+    from peft.tuners.lora import LoraLayer\\n+    LoraLayer.update_layer = LoraLayer_update_layer\\n+except:\\n+    logger.warning_once(\\n+        \"Unsloth unsuccessfully patched LoraLayer.update_layer. Please file a bug report.\\\\n\"\\\\\\n+        \"Luckily, your training run will still work in the meantime!\"\\n+    )\\n pass\\n-\\n-# Fix up incorrect downcasting of LoRA weights\\n-from peft.tuners.lora.layer import LoraLayer\\n-LoraLayer.update_layer = LoraLayer_update_layer\\n-from peft.tuners.lora import LoraLayer\\n-LoraLayer.update_layer = LoraLayer_update_layer\\n',\n",
       " '@@ -1216,6 +1216,8 @@ class FastLlamaModel:\\n             )\\n         pass\\n \\n+        if loftq_config is None: loftq_config = {}\\n+\\n         import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n',\n",
       " '@@ -1216,6 +1216,8 @@ class FastLlamaModel:\\n             )\\n         pass\\n \\n+        if loftq_config is None: loftq_config = {}\\n+\\n         import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n',\n",
       " '@@ -33,8 +33,8 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-    \"transformers>=4.38.0\",\\n-    \"datasets\",\\n+    \"transformers>=4.38.2\",\\n+    \"datasets>=2.16.0\",\\n     \"sentencepiece\",\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n@@ -64,6 +64,16 @@ cu121onlytorch211 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n ]\\n+cu118onlytorch212 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+cu121onlytorch212 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n cu118onlytorch220 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n',\n",
       " '@@ -16,11 +16,17 @@ from .cross_entropy_loss import fast_cross_entropy_loss\\n from .rms_layernorm import fast_rms_layernorm\\n from .rope_embedding import fast_rope_embedding, inplace_rope_embedding\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n-from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n+from .geglu import (\\n+\\tgeglu_exact_forward_kernel,\\n+\\tgeglu_exact_backward_kernel,\\n+\\tgeglu_approx_forward_kernel,\\n+\\tgeglu_approx_backward_kernel,\\n+)\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n \\tapply_lora_mlp_swiglu,\\n-\\tapply_lora_mlp_geglu,\\n+\\tapply_lora_mlp_geglu_exact,\\n+\\tapply_lora_mlp_geglu_approx,\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n',\n",
       " '@@ -183,8 +183,8 @@ def apply_lora_mlp_swiglu(self, X):\\n pass\\n \\n \\n-from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n-def apply_lora_mlp_geglu(self, X):\\n+from .geglu import geglu_exact_forward_kernel, geglu_exact_backward_kernel\\n+def apply_lora_mlp_geglu_exact(self, X):\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n@@ -192,7 +192,21 @@ def apply_lora_mlp_geglu(self, X):\\n                          gateW, gateW_quant, gateA, gateB, gateS,\\n                          upW,     upW_quant, upA,   upB,   upS,\\n                          downW, downW_quant, downA, downB, downS,\\n-                         geglu_forward_kernel, geglu_backward_kernel,)\\n+                         geglu_exact_forward_kernel, geglu_exact_backward_kernel,)\\n+    return out\\n+pass\\n+\\n+\\n+from .geglu import geglu_approx_forward_kernel, geglu_approx_backward_kernel\\n+def apply_lora_mlp_geglu_approx(self, X):\\n+    gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n+    upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n+    downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n+    out = LoRA_MLP.apply(X,\\n+                         gateW, gateW_quant, gateA, gateB, gateS,\\n+                         upW,     upW_quant, upA,   upB,   upS,\\n+                         downW, downW_quant, downA, downB, downS,\\n+                         geglu_approx_forward_kernel, geglu_approx_backward_kernel,)\\n     return out\\n pass\\n \\n',\n",
       " '@@ -19,7 +19,7 @@ from .utils import calculate_settings\\n \\n \\n @triton.jit\\n-def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+def _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     block_idx = tl.program_id(0)\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n@@ -38,18 +38,18 @@ def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n pass\\n \\n \\n-def geglu_forward_kernel(gate, up):\\n+def geglu_exact_forward_kernel(gate, up):\\n     batch, seq_len, hd = gate.shape\\n     n_elements = gate.numel()\\n     out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n     grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n-    _forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n     return out\\n pass\\n \\n \\n @triton.jit\\n-def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+def _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     \"\"\"\\n     f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\\n     h = f * up\\n@@ -95,10 +95,109 @@ def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n pass\\n \\n \\n-def geglu_backward_kernel(DW, e, g):\\n+def geglu_exact_backward_kernel(DW, e, g):\\n     batch_seq_len, hd = e.shape\\n     n_elements = e.numel()\\n     grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n-    _backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    return DW, e, g\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\\n+    # h = f * up\\n+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)\\n+    \\n+    e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    f_row = 0.5 * e_row * (\\n+        tl.math.tanh(s * e_row * (1.0 + 0.044715 * e_row * e_row)) \\\\\\n+        + 1.0\\n+    )\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n+    h_row = f_row * g_row\\n+\\n+    # Store h\\n+    tl.store(h + offsets, h_row, mask = mask)\\n+pass\\n+\\n+\\n+def geglu_approx_forward_kernel(gate, up):\\n+    batch, seq_len, hd = gate.shape\\n+    n_elements = gate.numel()\\n+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    return out\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\\n+    h = f * up\\n+\\n+    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\\n+    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\\n+            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] * \\\\\\n+                           ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\\n+\\n+    Notice sech^2(x) = 1 - tanh^2(x)\\n+    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\\n+\\n+    See https://www.desmos.com/calculator/nqprfoni6x\\n+    \"\"\"\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    # See https://www.desmos.com/calculator/nqprfoni6x\\n+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)\\n+    a = s * e_row # a = sqrt(2 / pi) * x\\n+    b = a * 0.044715 * e_row * e_row # b = a * 0.044715 * x^2\\n+    T = 1.0 + tl.math.tanh(a + b)\\n+    T2 = 0.5 * T\\n+    # Q = 0.5 * -T * (T - 2.0) * (a + 3.0 * b)\\n+    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b) \\n+    df_de = T2 + Q2 # 1/2 * (T + Q)\\n+\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))\\n+    f_row = T2 * e_row\\n+    f_row = f_row.to(DW_row.dtype)\\n+    # h = f * g\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+\\n+    de_row = dg_row.to(tl.float32) * df_de\\n+    de_row = de_row.to(DW_row.dtype)\\n+\\n+    # Store derivatives in buffers\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n+pass\\n+\\n+\\n+def geglu_approx_backward_kernel(DW, e, g):\\n+    batch_seq_len, hd = e.shape\\n+    n_elements = e.numel()\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n     return DW, e, g\\n pass\\n',\n",
       " '@@ -25,7 +25,7 @@ from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n \\n-__version__ = \"2024.2\"\\n+__version__ = \"2024.3\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n',\n",
       " '@@ -12,11 +12,16 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from transformers.utils.notebook import (\\n-    IntervalStrategy,\\n-    NotebookTrainingTracker,\\n-    NotebookProgressCallback,\\n-)\\n+try:\\n+    from transformers.utils.notebook import (\\n+        IntervalStrategy,\\n+        NotebookTrainingTracker,\\n+        NotebookProgressCallback,\\n+    )\\n+    HAS_NOTEBOOK = True\\n+except:\\n+    HAS_NOTEBOOK = False\\n+pass\\n \\n DPOTrainer_metrics = [\\n     \"rewards/chosen\",\\n@@ -101,13 +106,15 @@ pass\\n \\n \\n def PatchDPOTrainer():\\n-    from transformers.trainer import is_in_notebook\\n-    if is_in_notebook():\\n-        # Patch DPO notebook printing\\n-        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n-        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n-        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n-        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    if HAS_NOTEBOOK:\\n+        from transformers.trainer import is_in_notebook\\n+        if is_in_notebook():\\n+            # Patch DPO notebook printing\\n+            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n+            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n+            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+        pass\\n     pass\\n pass\\n \\n',\n",
       " '@@ -48,7 +48,7 @@ def fast_geglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.gelu(gate)\\n+    gate = torch.nn.functional.gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -70,7 +70,7 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if False:#past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n@@ -267,6 +267,9 @@ class FastGemmaModel(FastLlamaModel):\\n         # Patch RMS Layernorm\\n         for name, module in model.named_modules():\\n             if isinstance(module, GemmaRMSNorm):\\n+                # Must be in float32\\n+                # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n+                module = module.to(torch.float32)\\n                 module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n',\n",
       " '@@ -511,7 +511,12 @@ def LlamaModel_fast_forward(\\n     # Mormalized from Gemma\\n     if self.config.model_type == \"gemma\":\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        if not inputs_embeds.is_leaf:\\n+            inputs_embeds = inputs_embeds.detach()\\n+            inputs_requires_grad = True\\n+        elif inputs_requires_grad:\\n+            inputs_embeds.requires_grad_(False)\\n+        pass\\n         inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n@@ -522,7 +527,12 @@ def LlamaModel_fast_forward(\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        if not inputs_embeds.is_leaf:\\n+            inputs_embeds = inputs_embeds.detach()\\n+            inputs_requires_grad = True\\n+        elif inputs_requires_grad:\\n+            inputs_embeds.requires_grad_(False)\\n+        pass\\n         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n@@ -1335,7 +1345,7 @@ class FastLlamaModel:\\n \\n         if   model_type == \"llama\":   apply_lora_mlp = apply_lora_mlp_swiglu\\n         elif model_type == \"mistral\": apply_lora_mlp = apply_lora_mlp_swiglu\\n-        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu\\n+        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu_approx\\n         else:\\n             raise NotImplementedError(f\"Unsloth: {model_type} is not yet implemented!\")\\n         pass\\n',\n",
       " '@@ -33,8 +33,8 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-    \"transformers>=4.38.0\",\\n-    \"datasets\",\\n+    \"transformers>=4.38.2\",\\n+    \"datasets>=2.16.0\",\\n     \"sentencepiece\",\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n@@ -64,6 +64,16 @@ cu121onlytorch211 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n ]\\n+cu118onlytorch212 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+cu121onlytorch212 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n cu118onlytorch220 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n',\n",
       " '@@ -16,11 +16,17 @@ from .cross_entropy_loss import fast_cross_entropy_loss\\n from .rms_layernorm import fast_rms_layernorm\\n from .rope_embedding import fast_rope_embedding, inplace_rope_embedding\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n-from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n+from .geglu import (\\n+\\tgeglu_exact_forward_kernel,\\n+\\tgeglu_exact_backward_kernel,\\n+\\tgeglu_approx_forward_kernel,\\n+\\tgeglu_approx_backward_kernel,\\n+)\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n \\tapply_lora_mlp_swiglu,\\n-\\tapply_lora_mlp_geglu,\\n+\\tapply_lora_mlp_geglu_exact,\\n+\\tapply_lora_mlp_geglu_approx,\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n',\n",
       " '@@ -183,8 +183,8 @@ def apply_lora_mlp_swiglu(self, X):\\n pass\\n \\n \\n-from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n-def apply_lora_mlp_geglu(self, X):\\n+from .geglu import geglu_exact_forward_kernel, geglu_exact_backward_kernel\\n+def apply_lora_mlp_geglu_exact(self, X):\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n@@ -192,7 +192,21 @@ def apply_lora_mlp_geglu(self, X):\\n                          gateW, gateW_quant, gateA, gateB, gateS,\\n                          upW,     upW_quant, upA,   upB,   upS,\\n                          downW, downW_quant, downA, downB, downS,\\n-                         geglu_forward_kernel, geglu_backward_kernel,)\\n+                         geglu_exact_forward_kernel, geglu_exact_backward_kernel,)\\n+    return out\\n+pass\\n+\\n+\\n+from .geglu import geglu_approx_forward_kernel, geglu_approx_backward_kernel\\n+def apply_lora_mlp_geglu_approx(self, X):\\n+    gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n+    upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n+    downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n+    out = LoRA_MLP.apply(X,\\n+                         gateW, gateW_quant, gateA, gateB, gateS,\\n+                         upW,     upW_quant, upA,   upB,   upS,\\n+                         downW, downW_quant, downA, downB, downS,\\n+                         geglu_approx_forward_kernel, geglu_approx_backward_kernel,)\\n     return out\\n pass\\n \\n',\n",
       " '@@ -19,7 +19,7 @@ from .utils import calculate_settings\\n \\n \\n @triton.jit\\n-def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+def _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     block_idx = tl.program_id(0)\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n@@ -38,18 +38,18 @@ def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n pass\\n \\n \\n-def geglu_forward_kernel(gate, up):\\n+def geglu_exact_forward_kernel(gate, up):\\n     batch, seq_len, hd = gate.shape\\n     n_elements = gate.numel()\\n     out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n     grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n-    _forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n     return out\\n pass\\n \\n \\n @triton.jit\\n-def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+def _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     \"\"\"\\n     f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\\n     h = f * up\\n@@ -95,10 +95,109 @@ def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n pass\\n \\n \\n-def geglu_backward_kernel(DW, e, g):\\n+def geglu_exact_backward_kernel(DW, e, g):\\n     batch_seq_len, hd = e.shape\\n     n_elements = e.numel()\\n     grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n-    _backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    return DW, e, g\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\\n+    # h = f * up\\n+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)\\n+    \\n+    e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    f_row = 0.5 * e_row * (\\n+        tl.math.tanh(s * e_row * (1.0 + 0.044715 * e_row * e_row)) \\\\\\n+        + 1.0\\n+    )\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n+    h_row = f_row * g_row\\n+\\n+    # Store h\\n+    tl.store(h + offsets, h_row, mask = mask)\\n+pass\\n+\\n+\\n+def geglu_approx_forward_kernel(gate, up):\\n+    batch, seq_len, hd = gate.shape\\n+    n_elements = gate.numel()\\n+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    return out\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\\n+    h = f * up\\n+\\n+    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\\n+    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\\n+            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] * \\\\\\n+                           ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\\n+\\n+    Notice sech^2(x) = 1 - tanh^2(x)\\n+    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\\n+\\n+    See https://www.desmos.com/calculator/nqprfoni6x\\n+    \"\"\"\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    # See https://www.desmos.com/calculator/nqprfoni6x\\n+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)\\n+    a = s * e_row # a = sqrt(2 / pi) * x\\n+    b = a * 0.044715 * e_row * e_row # b = a * 0.044715 * x^2\\n+    T = 1.0 + tl.math.tanh(a + b)\\n+    T2 = 0.5 * T\\n+    # Q = 0.5 * -T * (T - 2.0) * (a + 3.0 * b)\\n+    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b) \\n+    df_de = T2 + Q2 # 1/2 * (T + Q)\\n+\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))\\n+    f_row = T2 * e_row\\n+    f_row = f_row.to(DW_row.dtype)\\n+    # h = f * g\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+\\n+    de_row = dg_row.to(tl.float32) * df_de\\n+    de_row = de_row.to(DW_row.dtype)\\n+\\n+    # Store derivatives in buffers\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n+pass\\n+\\n+\\n+def geglu_approx_backward_kernel(DW, e, g):\\n+    batch_seq_len, hd = e.shape\\n+    n_elements = e.numel()\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n     return DW, e, g\\n pass\\n',\n",
       " '@@ -25,7 +25,7 @@ from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n \\n-__version__ = \"2024.2\"\\n+__version__ = \"2024.3\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n',\n",
       " '@@ -12,11 +12,16 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from transformers.utils.notebook import (\\n-    IntervalStrategy,\\n-    NotebookTrainingTracker,\\n-    NotebookProgressCallback,\\n-)\\n+try:\\n+    from transformers.utils.notebook import (\\n+        IntervalStrategy,\\n+        NotebookTrainingTracker,\\n+        NotebookProgressCallback,\\n+    )\\n+    HAS_NOTEBOOK = True\\n+except:\\n+    HAS_NOTEBOOK = False\\n+pass\\n \\n DPOTrainer_metrics = [\\n     \"rewards/chosen\",\\n@@ -101,13 +106,15 @@ pass\\n \\n \\n def PatchDPOTrainer():\\n-    from transformers.trainer import is_in_notebook\\n-    if is_in_notebook():\\n-        # Patch DPO notebook printing\\n-        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n-        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n-        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n-        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    if HAS_NOTEBOOK:\\n+        from transformers.trainer import is_in_notebook\\n+        if is_in_notebook():\\n+            # Patch DPO notebook printing\\n+            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n+            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n+            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+        pass\\n     pass\\n pass\\n \\n',\n",
       " '@@ -48,7 +48,7 @@ def fast_geglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.gelu(gate)\\n+    gate = torch.nn.functional.gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -70,7 +70,7 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if False:#past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n@@ -267,6 +267,9 @@ class FastGemmaModel(FastLlamaModel):\\n         # Patch RMS Layernorm\\n         for name, module in model.named_modules():\\n             if isinstance(module, GemmaRMSNorm):\\n+                # Must be in float32\\n+                # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n+                module = module.to(torch.float32)\\n                 module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n',\n",
       " '@@ -511,7 +511,12 @@ def LlamaModel_fast_forward(\\n     # Mormalized from Gemma\\n     if self.config.model_type == \"gemma\":\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        if not inputs_embeds.is_leaf:\\n+            inputs_embeds = inputs_embeds.detach()\\n+            inputs_requires_grad = True\\n+        elif inputs_requires_grad:\\n+            inputs_embeds.requires_grad_(False)\\n+        pass\\n         inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n@@ -522,7 +527,12 @@ def LlamaModel_fast_forward(\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        if not inputs_embeds.is_leaf:\\n+            inputs_embeds = inputs_embeds.detach()\\n+            inputs_requires_grad = True\\n+        elif inputs_requires_grad:\\n+            inputs_embeds.requires_grad_(False)\\n+        pass\\n         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n@@ -1335,7 +1345,7 @@ class FastLlamaModel:\\n \\n         if   model_type == \"llama\":   apply_lora_mlp = apply_lora_mlp_swiglu\\n         elif model_type == \"mistral\": apply_lora_mlp = apply_lora_mlp_swiglu\\n-        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu\\n+        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu_approx\\n         else:\\n             raise NotImplementedError(f\"Unsloth: {model_type} is not yet implemented!\")\\n         pass\\n',\n",
       " '@@ -33,8 +33,8 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n-    \"transformers>=4.38.0\",\\n-    \"datasets\",\\n+    \"transformers>=4.38.2\",\\n+    \"datasets>=2.16.0\",\\n     \"sentencepiece\",\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n@@ -64,6 +64,16 @@ cu121onlytorch211 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n ]\\n+cu118onlytorch212 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.23.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+cu121onlytorch212 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n cu118onlytorch220 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.24%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n',\n",
       " '@@ -16,11 +16,17 @@ from .cross_entropy_loss import fast_cross_entropy_loss\\n from .rms_layernorm import fast_rms_layernorm\\n from .rope_embedding import fast_rope_embedding, inplace_rope_embedding\\n from .swiglu import swiglu_fg_kernel, swiglu_DWf_DW_dfg_kernel\\n-from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n+from .geglu import (\\n+\\tgeglu_exact_forward_kernel,\\n+\\tgeglu_exact_backward_kernel,\\n+\\tgeglu_approx_forward_kernel,\\n+\\tgeglu_approx_backward_kernel,\\n+)\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n \\tapply_lora_mlp_swiglu,\\n-\\tapply_lora_mlp_geglu,\\n+\\tapply_lora_mlp_geglu_exact,\\n+\\tapply_lora_mlp_geglu_approx,\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n',\n",
       " '@@ -183,8 +183,8 @@ def apply_lora_mlp_swiglu(self, X):\\n pass\\n \\n \\n-from .geglu import geglu_forward_kernel, geglu_backward_kernel\\n-def apply_lora_mlp_geglu(self, X):\\n+from .geglu import geglu_exact_forward_kernel, geglu_exact_backward_kernel\\n+def apply_lora_mlp_geglu_exact(self, X):\\n     gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n     upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n     downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n@@ -192,7 +192,21 @@ def apply_lora_mlp_geglu(self, X):\\n                          gateW, gateW_quant, gateA, gateB, gateS,\\n                          upW,     upW_quant, upA,   upB,   upS,\\n                          downW, downW_quant, downA, downB, downS,\\n-                         geglu_forward_kernel, geglu_backward_kernel,)\\n+                         geglu_exact_forward_kernel, geglu_exact_backward_kernel,)\\n+    return out\\n+pass\\n+\\n+\\n+from .geglu import geglu_approx_forward_kernel, geglu_approx_backward_kernel\\n+def apply_lora_mlp_geglu_approx(self, X):\\n+    gateW, gateW_quant, gateA, gateB, gateS = get_lora_parameters(self.gate_proj)\\n+    upW,     upW_quant,   upA,   upB,   upS = get_lora_parameters(self.  up_proj)\\n+    downW, downW_quant, downA, downB, downS = get_lora_parameters(self.down_proj)\\n+    out = LoRA_MLP.apply(X,\\n+                         gateW, gateW_quant, gateA, gateB, gateS,\\n+                         upW,     upW_quant, upA,   upB,   upS,\\n+                         downW, downW_quant, downA, downB, downS,\\n+                         geglu_approx_forward_kernel, geglu_approx_backward_kernel,)\\n     return out\\n pass\\n \\n',\n",
       " '@@ -19,7 +19,7 @@ from .utils import calculate_settings\\n \\n \\n @triton.jit\\n-def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+def _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     block_idx = tl.program_id(0)\\n     offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n     mask = offsets < n_elements\\n@@ -38,18 +38,18 @@ def _forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n pass\\n \\n \\n-def geglu_forward_kernel(gate, up):\\n+def geglu_exact_forward_kernel(gate, up):\\n     batch, seq_len, hd = gate.shape\\n     n_elements = gate.numel()\\n     out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n     grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n-    _forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    _exact_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n     return out\\n pass\\n \\n \\n @triton.jit\\n-def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+def _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n     \"\"\"\\n     f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\\n     h = f * up\\n@@ -95,10 +95,109 @@ def _backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n pass\\n \\n \\n-def geglu_backward_kernel(DW, e, g):\\n+def geglu_exact_backward_kernel(DW, e, g):\\n     batch_seq_len, hd = e.shape\\n     n_elements = e.numel()\\n     grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n-    _backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    _exact_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n+    return DW, e, g\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\\n+    # h = f * up\\n+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)\\n+    \\n+    e_row = tl.load(e + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row = tl.load(g + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    f_row = 0.5 * e_row * (\\n+        tl.math.tanh(s * e_row * (1.0 + 0.044715 * e_row * e_row)) \\\\\\n+        + 1.0\\n+    )\\n+    f_row = f_row.to(g_row.dtype) # Exact copy from HF\\n+    h_row = f_row * g_row\\n+\\n+    # Store h\\n+    tl.store(h + offsets, h_row, mask = mask)\\n+pass\\n+\\n+\\n+def geglu_approx_forward_kernel(gate, up):\\n+    batch, seq_len, hd = gate.shape\\n+    n_elements = gate.numel()\\n+    out = torch.empty((batch, seq_len, hd), dtype = gate.dtype, device = \"cuda\")\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _approx_forward_kernel[grid](gate, up, out, n_elements, BLOCK_SIZE = 1024,)\\n+    return out\\n+pass\\n+\\n+\\n+@triton.jit\\n+def _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE : tl.constexpr,):\\n+    \"\"\"\\n+    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\\n+    h = f * up\\n+\\n+    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\\n+    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\\n+            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] * \\\\\\n+                           ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\\n+\\n+    Notice sech^2(x) = 1 - tanh^2(x)\\n+    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\\n+\\n+    See https://www.desmos.com/calculator/nqprfoni6x\\n+    \"\"\"\\n+    block_idx = tl.program_id(0)\\n+    offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\\n+    mask = offsets < n_elements\\n+\\n+    DW_row = tl.load(DW + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+    e_row  = tl.load(e  + offsets, mask = mask, other = 0).to(tl.float32)\\n+    g_row  = tl.load(g  + offsets, mask = mask, other = 0)#.to(tl.float32)\\n+\\n+    # See https://www.desmos.com/calculator/nqprfoni6x\\n+    s = 0.7978845608028654 # math.sqrt(2 / math.pi)\\n+    a = s * e_row # a = sqrt(2 / pi) * x\\n+    b = a * 0.044715 * e_row * e_row # b = a * 0.044715 * x^2\\n+    T = 1.0 + tl.math.tanh(a + b)\\n+    T2 = 0.5 * T\\n+    # Q = 0.5 * -T * (T - 2.0) * (a + 3.0 * b)\\n+    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b) \\n+    df_de = T2 + Q2 # 1/2 * (T + Q)\\n+\\n+    # f = 1/2 * e * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ))\\n+    f_row = T2 * e_row\\n+    f_row = f_row.to(DW_row.dtype)\\n+    # h = f * g\\n+    h_row  =  f_row * g_row\\n+    # df = DW * f\\n+    df_row = DW_row * f_row\\n+    # dg = DW * g\\n+    dg_row = DW_row * g_row\\n+\\n+    de_row = dg_row.to(tl.float32) * df_de\\n+    de_row = de_row.to(DW_row.dtype)\\n+\\n+    # Store derivatives in buffers\\n+    tl.store(DW + offsets, h_row,  mask = mask) # h  = f * g\\n+    tl.store(e  + offsets, df_row, mask = mask) # df = DW * f\\n+    tl.store(g  + offsets, de_row, mask = mask) # de\\n+pass\\n+\\n+\\n+def geglu_approx_backward_kernel(DW, e, g):\\n+    batch_seq_len, hd = e.shape\\n+    n_elements = e.numel()\\n+    grid = lambda meta: (triton.cdiv(n_elements, meta[\\'BLOCK_SIZE\\']),)\\n+    _approx_backward_kernel[grid](DW, e, g, n_elements, BLOCK_SIZE = 1024,)\\n     return DW, e, g\\n pass\\n',\n",
       " '@@ -25,7 +25,7 @@ from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n \\n-__version__ = \"2024.2\"\\n+__version__ = \"2024.3\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n',\n",
       " '@@ -12,11 +12,16 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from transformers.utils.notebook import (\\n-    IntervalStrategy,\\n-    NotebookTrainingTracker,\\n-    NotebookProgressCallback,\\n-)\\n+try:\\n+    from transformers.utils.notebook import (\\n+        IntervalStrategy,\\n+        NotebookTrainingTracker,\\n+        NotebookProgressCallback,\\n+    )\\n+    HAS_NOTEBOOK = True\\n+except:\\n+    HAS_NOTEBOOK = False\\n+pass\\n \\n DPOTrainer_metrics = [\\n     \"rewards/chosen\",\\n@@ -101,13 +106,15 @@ pass\\n \\n \\n def PatchDPOTrainer():\\n-    from transformers.trainer import is_in_notebook\\n-    if is_in_notebook():\\n-        # Patch DPO notebook printing\\n-        NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n-        from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n-        DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n-        DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+    if HAS_NOTEBOOK:\\n+        from transformers.trainer import is_in_notebook\\n+        if is_in_notebook():\\n+            # Patch DPO notebook printing\\n+            NotebookTrainingTracker.write_line = NotebookTrainingTracker_write_line\\n+            from transformers.trainer import DEFAULT_PROGRESS_CALLBACK\\n+            DEFAULT_PROGRESS_CALLBACK.on_train_begin = NotebookProgressCallback_on_train_begin\\n+            DEFAULT_PROGRESS_CALLBACK.on_log         = NotebookProgressCallback_on_log\\n+        pass\\n     pass\\n pass\\n \\n',\n",
       " '@@ -48,7 +48,7 @@ def fast_geglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.gelu(gate)\\n+    gate = torch.nn.functional.gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -70,7 +70,7 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if False:#past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n@@ -267,6 +267,9 @@ class FastGemmaModel(FastLlamaModel):\\n         # Patch RMS Layernorm\\n         for name, module in model.named_modules():\\n             if isinstance(module, GemmaRMSNorm):\\n+                # Must be in float32\\n+                # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n+                module = module.to(torch.float32)\\n                 module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n',\n",
       " '@@ -511,7 +511,12 @@ def LlamaModel_fast_forward(\\n     # Mormalized from Gemma\\n     if self.config.model_type == \"gemma\":\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        if not inputs_embeds.is_leaf:\\n+            inputs_embeds = inputs_embeds.detach()\\n+            inputs_requires_grad = True\\n+        elif inputs_requires_grad:\\n+            inputs_embeds.requires_grad_(False)\\n+        pass\\n         inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n@@ -522,7 +527,12 @@ def LlamaModel_fast_forward(\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(False)\\n+        if not inputs_embeds.is_leaf:\\n+            inputs_embeds = inputs_embeds.detach()\\n+            inputs_requires_grad = True\\n+        elif inputs_requires_grad:\\n+            inputs_embeds.requires_grad_(False)\\n+        pass\\n         inputs_embeds *= attention_mask.unsqueeze(0).transpose(0, 1).transpose(1, 2)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n@@ -1335,7 +1345,7 @@ class FastLlamaModel:\\n \\n         if   model_type == \"llama\":   apply_lora_mlp = apply_lora_mlp_swiglu\\n         elif model_type == \"mistral\": apply_lora_mlp = apply_lora_mlp_swiglu\\n-        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu\\n+        elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu_approx\\n         else:\\n             raise NotImplementedError(f\"Unsloth: {model_type} is not yet implemented!\")\\n         pass\\n',\n",
       " '@@ -173,6 +173,94 @@ def GemmaModel_fast_forward_inference(\\n pass\\n \\n \\n+def GemmaForCausalLM_fast_forward(\\n+    self,\\n+    input_ids: torch.LongTensor = None,\\n+    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+    attention_mask: Optional[torch.Tensor] = None,\\n+    position_ids: Optional[torch.LongTensor] = None,\\n+    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n+    inputs_embeds: Optional[torch.FloatTensor] = None,\\n+    labels: Optional[torch.LongTensor] = None,\\n+    use_cache: Optional[bool] = None,\\n+    output_attentions: Optional[bool] = None,\\n+    output_hidden_states: Optional[bool] = None,\\n+    return_dict: Optional[bool] = None,\\n+    *args, **kwargs,\\n+) -> Union[Tuple, CausalLMOutputWithPast]:\\n+\\n+    if causal_mask is None and past_key_values is None:\\n+        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+\\n+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+    output_hidden_states = (\\n+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+    )\\n+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n+\\n+    if past_key_values is not None and \\\\\\n+        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        outputs = GemmaModel_fast_forward_inference(\\n+            self.model,\\n+            input_ids,\\n+            past_key_values,\\n+        )\\n+    else:\\n+        outputs = self.model(\\n+            input_ids=input_ids,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_values=past_key_values,\\n+            inputs_embeds=inputs_embeds,\\n+            use_cache=use_cache,\\n+            output_attentions=output_attentions,\\n+            output_hidden_states=output_hidden_states,\\n+            return_dict=return_dict,\\n+        )\\n+    pass\\n+\\n+    hidden_states = outputs[0]\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n+\\n+    loss = None\\n+    if labels is not None:\\n+        shift_logits = logits\\n+        if not hasattr(self, \"extra_ignored_labels\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/10\\n+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+        pass\\n+        \\n+        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n+        loss = fast_cross_entropy_loss(\\n+            logits = shift_logits,\\n+            labels = shift_labels,\\n+        )\\n+    pass\\n+\\n+    if not return_dict:\\n+        output = (logits,) + outputs[1:]\\n+        return (loss,) + output if loss is not None else output\\n+\\n+    return CausalLMOutputWithPast(\\n+        loss=loss,\\n+        logits=logits,\\n+        past_key_values=outputs.past_key_values,\\n+        hidden_states=outputs.hidden_states,\\n+        attentions=outputs.attentions,\\n+    )\\n+pass\\n+\\n+\\n class FastGemmaModel(FastLlamaModel):\\n \\n     @staticmethod\\n@@ -182,7 +270,7 @@ class FastGemmaModel(FastLlamaModel):\\n         GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n         GemmaModel          .forward = LlamaModel_fast_forward\\n-        GemmaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n',\n",
       " '@@ -173,6 +173,94 @@ def GemmaModel_fast_forward_inference(\\n pass\\n \\n \\n+def GemmaForCausalLM_fast_forward(\\n+    self,\\n+    input_ids: torch.LongTensor = None,\\n+    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+    attention_mask: Optional[torch.Tensor] = None,\\n+    position_ids: Optional[torch.LongTensor] = None,\\n+    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n+    inputs_embeds: Optional[torch.FloatTensor] = None,\\n+    labels: Optional[torch.LongTensor] = None,\\n+    use_cache: Optional[bool] = None,\\n+    output_attentions: Optional[bool] = None,\\n+    output_hidden_states: Optional[bool] = None,\\n+    return_dict: Optional[bool] = None,\\n+    *args, **kwargs,\\n+) -> Union[Tuple, CausalLMOutputWithPast]:\\n+\\n+    if causal_mask is None and past_key_values is None:\\n+        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+\\n+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+    output_hidden_states = (\\n+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+    )\\n+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n+\\n+    if past_key_values is not None and \\\\\\n+        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        outputs = GemmaModel_fast_forward_inference(\\n+            self.model,\\n+            input_ids,\\n+            past_key_values,\\n+        )\\n+    else:\\n+        outputs = self.model(\\n+            input_ids=input_ids,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_values=past_key_values,\\n+            inputs_embeds=inputs_embeds,\\n+            use_cache=use_cache,\\n+            output_attentions=output_attentions,\\n+            output_hidden_states=output_hidden_states,\\n+            return_dict=return_dict,\\n+        )\\n+    pass\\n+\\n+    hidden_states = outputs[0]\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n+\\n+    loss = None\\n+    if labels is not None:\\n+        shift_logits = logits\\n+        if not hasattr(self, \"extra_ignored_labels\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/10\\n+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+        pass\\n+        \\n+        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n+        loss = fast_cross_entropy_loss(\\n+            logits = shift_logits,\\n+            labels = shift_labels,\\n+        )\\n+    pass\\n+\\n+    if not return_dict:\\n+        output = (logits,) + outputs[1:]\\n+        return (loss,) + output if loss is not None else output\\n+\\n+    return CausalLMOutputWithPast(\\n+        loss=loss,\\n+        logits=logits,\\n+        past_key_values=outputs.past_key_values,\\n+        hidden_states=outputs.hidden_states,\\n+        attentions=outputs.attentions,\\n+    )\\n+pass\\n+\\n+\\n class FastGemmaModel(FastLlamaModel):\\n \\n     @staticmethod\\n@@ -182,7 +270,7 @@ class FastGemmaModel(FastLlamaModel):\\n         GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n         GemmaModel          .forward = LlamaModel_fast_forward\\n-        GemmaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n',\n",
       " '@@ -173,6 +173,94 @@ def GemmaModel_fast_forward_inference(\\n pass\\n \\n \\n+def GemmaForCausalLM_fast_forward(\\n+    self,\\n+    input_ids: torch.LongTensor = None,\\n+    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+    attention_mask: Optional[torch.Tensor] = None,\\n+    position_ids: Optional[torch.LongTensor] = None,\\n+    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n+    inputs_embeds: Optional[torch.FloatTensor] = None,\\n+    labels: Optional[torch.LongTensor] = None,\\n+    use_cache: Optional[bool] = None,\\n+    output_attentions: Optional[bool] = None,\\n+    output_hidden_states: Optional[bool] = None,\\n+    return_dict: Optional[bool] = None,\\n+    *args, **kwargs,\\n+) -> Union[Tuple, CausalLMOutputWithPast]:\\n+\\n+    if causal_mask is None and past_key_values is None:\\n+        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+\\n+    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+    output_hidden_states = (\\n+        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+    )\\n+    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+    self.model._has_no_labels = labels is None\\n+\\n+    if past_key_values is not None and \\\\\\n+        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        outputs = GemmaModel_fast_forward_inference(\\n+            self.model,\\n+            input_ids,\\n+            past_key_values,\\n+        )\\n+    else:\\n+        outputs = self.model(\\n+            input_ids=input_ids,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_values=past_key_values,\\n+            inputs_embeds=inputs_embeds,\\n+            use_cache=use_cache,\\n+            output_attentions=output_attentions,\\n+            output_hidden_states=output_hidden_states,\\n+            return_dict=return_dict,\\n+        )\\n+    pass\\n+\\n+    hidden_states = outputs[0]\\n+    bsz, q_len, hd = hidden_states.shape\\n+    if bsz == 1 and q_len == 1:\\n+        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        logits = logits.unsqueeze(0).unsqueeze(0)\\n+    else:\\n+        logits = self.lm_head(hidden_states)\\n+    pass\\n+\\n+    loss = None\\n+    if labels is not None:\\n+        shift_logits = logits\\n+        if not hasattr(self, \"extra_ignored_labels\"):\\n+            # Fixes https://github.com/unslothai/unsloth/issues/10\\n+            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+        pass\\n+        \\n+        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n+        loss = fast_cross_entropy_loss(\\n+            logits = shift_logits,\\n+            labels = shift_labels,\\n+        )\\n+    pass\\n+\\n+    if not return_dict:\\n+        output = (logits,) + outputs[1:]\\n+        return (loss,) + output if loss is not None else output\\n+\\n+    return CausalLMOutputWithPast(\\n+        loss=loss,\\n+        logits=logits,\\n+        past_key_values=outputs.past_key_values,\\n+        hidden_states=outputs.hidden_states,\\n+        attentions=outputs.attentions,\\n+    )\\n+pass\\n+\\n+\\n class FastGemmaModel(FastLlamaModel):\\n \\n     @staticmethod\\n@@ -182,7 +270,7 @@ class FastGemmaModel(FastLlamaModel):\\n         GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n         GemmaModel          .forward = LlamaModel_fast_forward\\n-        GemmaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n',\n",
       " '@@ -357,7 +357,8 @@ class FastGemmaModel(FastLlamaModel):\\n             if isinstance(module, GemmaRMSNorm):\\n                 # Must be in float32\\n                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n-                module = module.to(torch.float32)\\n+                # module = module.to(torch.float32)\\n+                # Don\\'t convert to float32 since error analysis shows it makes it worse!!\\n                 module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n',\n",
       " '@@ -357,7 +357,8 @@ class FastGemmaModel(FastLlamaModel):\\n             if isinstance(module, GemmaRMSNorm):\\n                 # Must be in float32\\n                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n-                module = module.to(torch.float32)\\n+                # module = module.to(torch.float32)\\n+                # Don\\'t convert to float32 since error analysis shows it makes it worse!!\\n                 module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n',\n",
       " '@@ -357,7 +357,8 @@ class FastGemmaModel(FastLlamaModel):\\n             if isinstance(module, GemmaRMSNorm):\\n                 # Must be in float32\\n                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n-                module = module.to(torch.float32)\\n+                # module = module.to(torch.float32)\\n+                # Don\\'t convert to float32 since error analysis shows it makes it worse!!\\n                 module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n',\n",
       " '@@ -43,6 +43,7 @@ huggingface = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"triton\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -104,6 +105,16 @@ cu121-torch211 = [\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n+cu118-torch212 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118onlytorch212]\",\\n+]\\n+cu121-torch212 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121onlytorch212]\",\\n+]\\n cu118-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n',\n",
       " '@@ -99,10 +99,14 @@ except:\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n-        raise ImportError(\"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n-                          \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n-                          \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n-                          \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\")\\n+        warnings.warn(\\n+            \"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n+            \"Try running `python -m bitsandbytes` then `python -m xformers.info`\\\\n\"\\\\\\n+            \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n+            \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n+            \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\\\\n\"\\\\\\n+            \"Unsloth will still run for now, but maybe it might crash - let\\'s hope it works!\"\\n+        )\\n pass\\n \\n from .models import *\\n',\n",
       " '@@ -257,6 +257,10 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n+\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n',\n",
       " '@@ -53,6 +53,7 @@ def _rms_layernorm_forward(\\n pass\\n \\n \\n+@triton.heuristics({\"GEMMA\": lambda args: args[\"GEMMA\"],})\\n @triton.jit\\n def _rms_layernorm_backward(\\n     dY, dY_row_stride,\\n@@ -61,6 +62,7 @@ def _rms_layernorm_backward(\\n     r,   r_row_stride,\\n     dW, dW_row_stride,\\n     n_cols, eps,\\n+    GEMMA      : tl.constexpr,\\n     BLOCK_SIZE : tl.constexpr,\\n ):\\n     \"\"\"\\n@@ -84,16 +86,51 @@ def _rms_layernorm_backward(\\n     inv_var = tl.load(r).to(tl.float32)\\n     normed = X_row * inv_var\\n \\n-    dY_W = dY_row * W_row\\n+    if GEMMA: dY_W = dY_row * (W_row + 1.0)\\n+    else:     dY_W = dY_row * W_row\\n+\\n     rowsum_dY_normed = tl.sum(dY_W * normed, axis = 0)\\n     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)\\n     tl.store(dY + col_offsets, output, mask = mask)\\n pass\\n \\n \\n+@triton.jit\\n+def _gemma_rms_layernorm_forward(\\n+    Y, Y_row_stride,\\n+    X, X_row_stride,\\n+    W, W_row_stride,\\n+    r, r_row_stride,\\n+    n_cols, eps,\\n+    BLOCK_SIZE : tl.constexpr,\\n+):\\n+    # Copies https://github.com/google-deepmind/gemma/blob/main/gemma/layers.py#L31\\n+    # and https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L33\\n+    # exactly. Essentially all in float32!\\n+    row_idx = tl.program_id(0)\\n+    col_offsets = tl.arange(0, BLOCK_SIZE)\\n+    mask = col_offsets < n_cols\\n+\\n+    Y += row_idx * Y_row_stride\\n+    X += row_idx * X_row_stride\\n+    r += row_idx * r_row_stride\\n+\\n+    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+\\n+    row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps) # Must be 1/sqrt to match Deepmind\\'s impl\\n+    tl.store(r, inv_var)\\n+    normed = X_row * inv_var\\n+    output = normed * (W_row + 1.0)\\n+\\n+    tl.store(Y + col_offsets, output, mask = mask)\\n+pass\\n+\\n+\\n class Fast_RMS_Layernorm(torch.autograd.Function):\\n     @staticmethod\\n-    def forward(ctx, X, W, eps):\\n+    def forward(ctx, X, W, eps, gemma = False):\\n         shape = X.shape\\n         dim = shape[-1]\\n         X = X.view(-1, dim)\\n@@ -103,7 +140,8 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n         Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = \"cuda\")\\n         r = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n \\n-        _rms_layernorm_forward[(n_rows,)](\\n+        fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward\\n+        fx[(n_rows,)](\\n             Y, Y.stride(0),\\n             X, X.stride(0),\\n             W, W.stride(0),\\n@@ -115,6 +153,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n         ctx.eps = eps\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.GEMMA = gemma\\n         ctx.save_for_backward(X, W, r)\\n         return Y.view(*shape)\\n     pass\\n@@ -135,18 +174,19 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n             r,  r .stride(0),\\n             dW, dW.stride(0),\\n             n_cols, ctx.eps,\\n+            GEMMA      = ctx.GEMMA,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n         )\\n         dX = dY.view(*shape)\\n-        return dX, None, None\\n+        return dX, None, None, None\\n     pass\\n pass\\n \\n \\n-def fast_rms_layernorm(layernorm, X):\\n+def fast_rms_layernorm(layernorm, X, gemma = False):\\n     W   = layernorm.weight\\n     eps = layernorm.variance_epsilon\\n-    out = Fast_RMS_Layernorm.apply(X, W, eps)\\n+    out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)\\n     return out\\n pass\\n',\n",
       " '@@ -39,24 +39,28 @@ def _rope_embedding(\\n     half_head_dim = head_dim // 2\\n     mask = col_offsets < half_head_dim\\n \\n-    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n-    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-                   half_head_dim*1 + col_offsets, mask = mask, other = 0)\\n     sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\\\\n                    half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n     cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\\\\n                    half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n \\n+    # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n+    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n+                   half_head_dim*0 + col_offsets, mask = mask, other = 0).to(sin1.dtype)\\n+    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n+                   half_head_dim*1 + col_offsets, mask = mask, other = 0).to(sin1.dtype)\\n+\\n     if BACKWARD_PASS:\\n         # See our blog post for more info.\\n         sin1 = -sin1\\n     pass\\n \\n     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-             half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)\\n+             half_head_dim*0 + col_offsets,\\n+             Q1*cos1 - Q2*sin1, mask = mask)\\n     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-             half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)\\n+             half_head_dim*1 + col_offsets,\\n+             Q2*cos1 + Q1*sin1, mask = mask)\\n pass\\n \\n \\n',\n",
       " '@@ -39,6 +39,7 @@ except:\\n pass\\n \\n \\n+torch_nn_functional_gelu = torch.nn.functional.gelu\\n def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n@@ -48,7 +49,7 @@ def fast_geglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.gelu(gate, approximate = \"tanh\")\\n+    gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -57,6 +58,18 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+    out_weight[:] = self.weight\\n+    out_weight += 1.0\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -72,10 +85,11 @@ def GemmaDecoderLayer_fast_forward(\\n ):\\n     if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             self.self_attn,\\n             hidden_states,\\n@@ -87,12 +101,12 @@ def GemmaDecoderLayer_fast_forward(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(self.mlp, hidden_states)\\n         hidden_states += residual\\n     else:\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n         # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n@@ -108,7 +122,7 @@ def GemmaDecoderLayer_fast_forward(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n         # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n@@ -137,15 +151,18 @@ def GemmaModel_fast_forward_inference(\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n+    out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n-    hidden_states *= math_sqrt(self.config.hidden_size)\\n+    # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n+    # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n+    hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n@@ -156,13 +173,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n         hidden_states += residual\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n@@ -173,91 +190,54 @@ def GemmaModel_fast_forward_inference(\\n pass\\n \\n \\n-def GemmaForCausalLM_fast_forward(\\n-    self,\\n-    input_ids: torch.LongTensor = None,\\n-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n-    attention_mask: Optional[torch.Tensor] = None,\\n-    position_ids: Optional[torch.LongTensor] = None,\\n-    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n-    inputs_embeds: Optional[torch.FloatTensor] = None,\\n-    labels: Optional[torch.LongTensor] = None,\\n-    use_cache: Optional[bool] = None,\\n-    output_attentions: Optional[bool] = None,\\n-    output_hidden_states: Optional[bool] = None,\\n-    return_dict: Optional[bool] = None,\\n-    *args, **kwargs,\\n-) -> Union[Tuple, CausalLMOutputWithPast]:\\n-\\n-    if causal_mask is None and past_key_values is None:\\n-        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-    output_hidden_states = (\\n-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-    )\\n-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+# Follows line by line https://github.com/google-deepmind/gemma/blob/main/gemma/positional_embeddings.py#L45\\n+# Formulates cos and sin differently from Llama!\\n+class GemmaFixedRotaryEmbedding(torch.nn.Module):\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\\n+        super().__init__()\\n+        self.dim = dim\\n+        self.max_position_embeddings = max_position_embeddings\\n+        self.base = base\\n+\\n+        # Build here to make `torch.jit.trace` work.\\n+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())\\n+    pass\\n \\n-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-    self.model._has_no_labels = labels is None\\n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and\\n+        # in FP32. They are applied (multiplied) in FP32 as well.\\n+        self.max_seq_len_cached = seq_len\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n-        outputs = GemmaModel_fast_forward_inference(\\n-            self.model,\\n-            input_ids,\\n-            past_key_values,\\n-        )\\n-    else:\\n-        outputs = self.model(\\n-            input_ids=input_ids,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_values=past_key_values,\\n-            inputs_embeds=inputs_embeds,\\n-            use_cache=use_cache,\\n-            output_attentions=output_attentions,\\n-            output_hidden_states=output_hidden_states,\\n-            return_dict=return_dict,\\n+        # The difference is we do division explicity instead of t * (1/x) ie we do t/x.\\n+        freq_exponents = (2.0 / self.dim) * (\\n+            torch.arange(self.dim // 2, dtype = torch.int64, device = \"cpu\").float()\\n         )\\n+        timescale = self.base**freq_exponents\\n+        positions = torch.arange(self.max_seq_len_cached, device = \"cpu\", dtype = torch.int64).float()\\n+        radians_new = positions[..., None] / timescale[None, None, :]\\n+        radians_new = radians_new.squeeze(0)\\n+\\n+        emb = torch.cat((radians_new, radians_new), dim = -1)\\n+        # We must do RoPE in float32!\\n+        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)\\n+        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)\\n+        self.register_buffer(\"cos_cached\", cos, persistent = False)\\n+        self.register_buffer(\"sin_cached\", sin, persistent = False)\\n     pass\\n \\n-    hidden_states = outputs[0]\\n-    bsz, q_len, hd = hidden_states.shape\\n-    if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n-        logits = logits.unsqueeze(0).unsqueeze(0)\\n-    else:\\n-        logits = self.lm_head(hidden_states)\\n-    pass\\n+    def forward(self, x, position_ids=None, seq_len=None):\\n+        # x: [bs, num_attention_heads, seq_len, head_size]\\n+        if seq_len > self.max_seq_len_cached:\\n+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\\n \\n-    loss = None\\n-    if labels is not None:\\n-        shift_logits = logits\\n-        if not hasattr(self, \"extra_ignored_labels\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/10\\n-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n-        pass\\n-        \\n-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-        loss = fast_cross_entropy_loss(\\n-            logits = shift_logits,\\n-            labels = shift_labels,\\n+        return (\\n+            self.cos_cached[:seq_len].to(dtype=x.dtype),\\n+            self.sin_cached[:seq_len].to(dtype=x.dtype),\\n         )\\n     pass\\n-\\n-    if not return_dict:\\n-        output = (logits,) + outputs[1:]\\n-        return (loss,) + output if loss is not None else output\\n-\\n-    return CausalLMOutputWithPast(\\n-        loss=loss,\\n-        logits=logits,\\n-        past_key_values=outputs.past_key_values,\\n-        hidden_states=outputs.hidden_states,\\n-        attentions=outputs.attentions,\\n-    )\\n pass\\n \\n \\n@@ -270,7 +250,7 @@ class FastGemmaModel(FastLlamaModel):\\n         GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n         GemmaModel          .forward = LlamaModel_fast_forward\\n-        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward\\n+        GemmaForCausalLM    .forward = CausalLM_fast_forward(GemmaModel_fast_forward_inference)\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n@@ -278,7 +258,7 @@ class FastGemmaModel(FastLlamaModel):\\n         # https://github.com/huggingface/transformers/pull/27931\\n         # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n         import transformers.models.gemma.modeling_gemma\\n-        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = LlamaRotaryEmbedding\\n+        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = GemmaFixedRotaryEmbedding\\n         return\\n     pass\\n \\n@@ -329,13 +309,14 @@ class FastGemmaModel(FastLlamaModel):\\n                 pass\\n             pass\\n             # Downcast RoPE embedding to correct data type\\n-            if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n-                and (module.cos_cached.dtype != correct_dtype):\\n-\\n-                module.cos_cached = module.cos_cached.to(correct_dtype)\\n-                module.sin_cached = module.sin_cached.to(correct_dtype)\\n-                pass\\n-            pass\\n+            # RoPE must be done in float32 for Gemma\\n+            # if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n+            #     and (module.cos_cached.dtype != correct_dtype):\\n+\\n+            #     module.cos_cached = module.cos_cached.to(correct_dtype)\\n+            #     module.sin_cached = module.sin_cached.to(correct_dtype)\\n+            #     pass\\n+            # pass\\n         pass\\n \\n         # Add 1 to weight\\n@@ -358,8 +339,8 @@ class FastGemmaModel(FastLlamaModel):\\n                 # Must be in float32\\n                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n                 # module = module.to(torch.float32)\\n-                # Don\\'t convert to float32 since error analysis shows it makes it worse!!\\n-                module.weight += 1.0 # return output * (1 + self.weight)\\n+                # Leave + 1 to Triton kernel itself\\n+                # module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n         pass\\n',\n",
       " '@@ -208,6 +208,7 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_nn_functional_silu = torch.nn.functional.silu\\n def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n@@ -217,7 +218,7 @@ def fast_swiglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.silu(gate, inplace = True)\\n+    gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -509,7 +510,8 @@ def LlamaModel_fast_forward(\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n     # Mormalized from Gemma\\n-    if self.config.model_type == \"gemma\":\\n+    IS_GEMMA = self.config.model_type == \"gemma\"\\n+    if IS_GEMMA:\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n         if not inputs_embeds.is_leaf:\\n             inputs_embeds = inputs_embeds.detach()\\n@@ -517,7 +519,12 @@ def LlamaModel_fast_forward(\\n         elif inputs_requires_grad:\\n             inputs_embeds.requires_grad_(False)\\n         pass\\n-        inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        # Match Gemma exactly by casting to bfloat16 / float16\\n+        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n+        # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n+        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n \\n@@ -619,7 +626,7 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n     \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -681,91 +688,94 @@ def LlamaModel_fast_forward_inference(\\n pass\\n \\n \\n-def LlamaForCausalLM_fast_forward(\\n-    self,\\n-    input_ids: torch.LongTensor = None,\\n-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n-    attention_mask: Optional[torch.Tensor] = None,\\n-    position_ids: Optional[torch.LongTensor] = None,\\n-    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n-    inputs_embeds: Optional[torch.FloatTensor] = None,\\n-    labels: Optional[torch.LongTensor] = None,\\n-    use_cache: Optional[bool] = None,\\n-    output_attentions: Optional[bool] = None,\\n-    output_hidden_states: Optional[bool] = None,\\n-    return_dict: Optional[bool] = None,\\n-    *args, **kwargs,\\n-) -> Union[Tuple, CausalLMOutputWithPast]:\\n+def CausalLM_fast_forward(fast_forward_inference):\\n+    def _CausalLM_fast_forward(\\n+        self,\\n+        input_ids: torch.LongTensor = None,\\n+        causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+        attention_mask: Optional[torch.Tensor] = None,\\n+        position_ids: Optional[torch.LongTensor] = None,\\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\\n+        labels: Optional[torch.LongTensor] = None,\\n+        use_cache: Optional[bool] = None,\\n+        output_attentions: Optional[bool] = None,\\n+        output_hidden_states: Optional[bool] = None,\\n+        return_dict: Optional[bool] = None,\\n+        *args, **kwargs,\\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\\n+\\n+        if causal_mask is None and past_key_values is None:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+\\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+        output_hidden_states = (\\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+        )\\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n-    if causal_mask is None and past_key_values is None:\\n-        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+        self.model._has_no_labels = labels is None\\n \\n-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-    output_hidden_states = (\\n-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-    )\\n-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+        if past_key_values is not None and \\\\\\n+            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+            outputs = fast_forward_inference(\\n+                self.model,\\n+                input_ids,\\n+                past_key_values,\\n+            )\\n+        else:\\n+            outputs = self.model(\\n+                input_ids=input_ids,\\n+                causal_mask=causal_mask,\\n+                attention_mask=attention_mask,\\n+                position_ids=position_ids,\\n+                past_key_values=past_key_values,\\n+                inputs_embeds=inputs_embeds,\\n+                use_cache=use_cache,\\n+                output_attentions=output_attentions,\\n+                output_hidden_states=output_hidden_states,\\n+                return_dict=return_dict,\\n+            )\\n+        pass\\n \\n-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-    self.model._has_no_labels = labels is None\\n+        hidden_states = outputs[0]\\n+        bsz, q_len, hd = hidden_states.shape\\n+        if bsz == 1 and q_len == 1:\\n+            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            logits = logits.unsqueeze(0).unsqueeze(0)\\n+        else:\\n+            logits = self.lm_head(hidden_states)\\n+        pass\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n-        outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n-            input_ids,\\n-            past_key_values,\\n-        )\\n-    else:\\n-        outputs = self.model(\\n-            input_ids=input_ids,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_values=past_key_values,\\n-            inputs_embeds=inputs_embeds,\\n-            use_cache=use_cache,\\n-            output_attentions=output_attentions,\\n-            output_hidden_states=output_hidden_states,\\n-            return_dict=return_dict,\\n-        )\\n-    pass\\n+        loss = None\\n+        if labels is not None:\\n+            shift_logits = logits\\n+            if not hasattr(self, \"extra_ignored_labels\"):\\n+                # Fixes https://github.com/unslothai/unsloth/issues/10\\n+                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+            pass\\n+            \\n+            shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n+            loss = fast_cross_entropy_loss(\\n+                logits = shift_logits,\\n+                labels = shift_labels,\\n+            )\\n+        pass\\n \\n-    hidden_states = outputs[0]\\n-    bsz, q_len, hd = hidden_states.shape\\n-    if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n-        logits = logits.unsqueeze(0).unsqueeze(0)\\n-    else:\\n-        logits = self.lm_head(hidden_states)\\n-    pass\\n+        if not return_dict:\\n+            output = (logits,) + outputs[1:]\\n+            return (loss,) + output if loss is not None else output\\n \\n-    loss = None\\n-    if labels is not None:\\n-        shift_logits = logits\\n-        if not hasattr(self, \"extra_ignored_labels\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/10\\n-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n-        pass\\n-        \\n-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-        loss = fast_cross_entropy_loss(\\n-            logits = shift_logits,\\n-            labels = shift_labels,\\n+        return CausalLMOutputWithPast(\\n+            loss=loss,\\n+            logits=logits,\\n+            past_key_values=outputs.past_key_values,\\n+            hidden_states=outputs.hidden_states,\\n+            attentions=outputs.attentions,\\n         )\\n     pass\\n-\\n-    if not return_dict:\\n-        output = (logits,) + outputs[1:]\\n-        return (loss,) + output if loss is not None else output\\n-\\n-    return CausalLMOutputWithPast(\\n-        loss=loss,\\n-        logits=logits,\\n-        past_key_values=outputs.past_key_values,\\n-        hidden_states=outputs.hidden_states,\\n-        attentions=outputs.attentions,\\n-    )\\n+    return _CausalLM_fast_forward\\n pass\\n \\n \\n@@ -880,7 +890,7 @@ class FastLlamaModel:\\n         LlamaFlashAttention2.forward = LlamaAttention_fast_forward\\n         LlamaDecoderLayer   .forward = LlamaDecoderLayer_fast_forward\\n         LlamaModel          .forward = LlamaModel_fast_forward\\n-        LlamaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        LlamaForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n \\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n',\n",
       " \"@@ -369,6 +369,7 @@ def unsloth_save_model(\\n \\n     # Switch to our fast saving modules if it's a slow PC!\\n     n_cpus = psutil.cpu_count(logical = False)\\n+    if n_cpus is None: n_cpus = psutil.cpu_count()\\n     if n_cpus is None: n_cpus = 1\\n \\n     if safe_serialization is None:\\n\",\n",
       " '@@ -43,6 +43,7 @@ huggingface = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"triton\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -104,6 +105,16 @@ cu121-torch211 = [\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n+cu118-torch212 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118onlytorch212]\",\\n+]\\n+cu121-torch212 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121onlytorch212]\",\\n+]\\n cu118-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n',\n",
       " '@@ -99,10 +99,14 @@ except:\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n-        raise ImportError(\"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n-                          \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n-                          \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n-                          \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\")\\n+        warnings.warn(\\n+            \"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n+            \"Try running `python -m bitsandbytes` then `python -m xformers.info`\\\\n\"\\\\\\n+            \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n+            \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n+            \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\\\\n\"\\\\\\n+            \"Unsloth will still run for now, but maybe it might crash - let\\'s hope it works!\"\\n+        )\\n pass\\n \\n from .models import *\\n',\n",
       " '@@ -257,6 +257,10 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n+\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n',\n",
       " '@@ -53,6 +53,7 @@ def _rms_layernorm_forward(\\n pass\\n \\n \\n+@triton.heuristics({\"GEMMA\": lambda args: args[\"GEMMA\"],})\\n @triton.jit\\n def _rms_layernorm_backward(\\n     dY, dY_row_stride,\\n@@ -61,6 +62,7 @@ def _rms_layernorm_backward(\\n     r,   r_row_stride,\\n     dW, dW_row_stride,\\n     n_cols, eps,\\n+    GEMMA      : tl.constexpr,\\n     BLOCK_SIZE : tl.constexpr,\\n ):\\n     \"\"\"\\n@@ -84,16 +86,51 @@ def _rms_layernorm_backward(\\n     inv_var = tl.load(r).to(tl.float32)\\n     normed = X_row * inv_var\\n \\n-    dY_W = dY_row * W_row\\n+    if GEMMA: dY_W = dY_row * (W_row + 1.0)\\n+    else:     dY_W = dY_row * W_row\\n+\\n     rowsum_dY_normed = tl.sum(dY_W * normed, axis = 0)\\n     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)\\n     tl.store(dY + col_offsets, output, mask = mask)\\n pass\\n \\n \\n+@triton.jit\\n+def _gemma_rms_layernorm_forward(\\n+    Y, Y_row_stride,\\n+    X, X_row_stride,\\n+    W, W_row_stride,\\n+    r, r_row_stride,\\n+    n_cols, eps,\\n+    BLOCK_SIZE : tl.constexpr,\\n+):\\n+    # Copies https://github.com/google-deepmind/gemma/blob/main/gemma/layers.py#L31\\n+    # and https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L33\\n+    # exactly. Essentially all in float32!\\n+    row_idx = tl.program_id(0)\\n+    col_offsets = tl.arange(0, BLOCK_SIZE)\\n+    mask = col_offsets < n_cols\\n+\\n+    Y += row_idx * Y_row_stride\\n+    X += row_idx * X_row_stride\\n+    r += row_idx * r_row_stride\\n+\\n+    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+\\n+    row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps) # Must be 1/sqrt to match Deepmind\\'s impl\\n+    tl.store(r, inv_var)\\n+    normed = X_row * inv_var\\n+    output = normed * (W_row + 1.0)\\n+\\n+    tl.store(Y + col_offsets, output, mask = mask)\\n+pass\\n+\\n+\\n class Fast_RMS_Layernorm(torch.autograd.Function):\\n     @staticmethod\\n-    def forward(ctx, X, W, eps):\\n+    def forward(ctx, X, W, eps, gemma = False):\\n         shape = X.shape\\n         dim = shape[-1]\\n         X = X.view(-1, dim)\\n@@ -103,7 +140,8 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n         Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = \"cuda\")\\n         r = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n \\n-        _rms_layernorm_forward[(n_rows,)](\\n+        fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward\\n+        fx[(n_rows,)](\\n             Y, Y.stride(0),\\n             X, X.stride(0),\\n             W, W.stride(0),\\n@@ -115,6 +153,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n         ctx.eps = eps\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.GEMMA = gemma\\n         ctx.save_for_backward(X, W, r)\\n         return Y.view(*shape)\\n     pass\\n@@ -135,18 +174,19 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n             r,  r .stride(0),\\n             dW, dW.stride(0),\\n             n_cols, ctx.eps,\\n+            GEMMA      = ctx.GEMMA,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n         )\\n         dX = dY.view(*shape)\\n-        return dX, None, None\\n+        return dX, None, None, None\\n     pass\\n pass\\n \\n \\n-def fast_rms_layernorm(layernorm, X):\\n+def fast_rms_layernorm(layernorm, X, gemma = False):\\n     W   = layernorm.weight\\n     eps = layernorm.variance_epsilon\\n-    out = Fast_RMS_Layernorm.apply(X, W, eps)\\n+    out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)\\n     return out\\n pass\\n',\n",
       " '@@ -39,24 +39,28 @@ def _rope_embedding(\\n     half_head_dim = head_dim // 2\\n     mask = col_offsets < half_head_dim\\n \\n-    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n-    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-                   half_head_dim*1 + col_offsets, mask = mask, other = 0)\\n     sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\\\\n                    half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n     cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\\\\n                    half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n \\n+    # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n+    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n+                   half_head_dim*0 + col_offsets, mask = mask, other = 0).to(sin1.dtype)\\n+    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n+                   half_head_dim*1 + col_offsets, mask = mask, other = 0).to(sin1.dtype)\\n+\\n     if BACKWARD_PASS:\\n         # See our blog post for more info.\\n         sin1 = -sin1\\n     pass\\n \\n     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-             half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)\\n+             half_head_dim*0 + col_offsets,\\n+             Q1*cos1 - Q2*sin1, mask = mask)\\n     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-             half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)\\n+             half_head_dim*1 + col_offsets,\\n+             Q2*cos1 + Q1*sin1, mask = mask)\\n pass\\n \\n \\n',\n",
       " '@@ -39,6 +39,7 @@ except:\\n pass\\n \\n \\n+torch_nn_functional_gelu = torch.nn.functional.gelu\\n def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n@@ -48,7 +49,7 @@ def fast_geglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.gelu(gate, approximate = \"tanh\")\\n+    gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -57,6 +58,18 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+    out_weight[:] = self.weight\\n+    out_weight += 1.0\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -72,10 +85,11 @@ def GemmaDecoderLayer_fast_forward(\\n ):\\n     if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             self.self_attn,\\n             hidden_states,\\n@@ -87,12 +101,12 @@ def GemmaDecoderLayer_fast_forward(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(self.mlp, hidden_states)\\n         hidden_states += residual\\n     else:\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n         # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n@@ -108,7 +122,7 @@ def GemmaDecoderLayer_fast_forward(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n         # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n@@ -137,15 +151,18 @@ def GemmaModel_fast_forward_inference(\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n+    out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n-    hidden_states *= math_sqrt(self.config.hidden_size)\\n+    # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n+    # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n+    hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n@@ -156,13 +173,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n         hidden_states += residual\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n@@ -173,91 +190,54 @@ def GemmaModel_fast_forward_inference(\\n pass\\n \\n \\n-def GemmaForCausalLM_fast_forward(\\n-    self,\\n-    input_ids: torch.LongTensor = None,\\n-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n-    attention_mask: Optional[torch.Tensor] = None,\\n-    position_ids: Optional[torch.LongTensor] = None,\\n-    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n-    inputs_embeds: Optional[torch.FloatTensor] = None,\\n-    labels: Optional[torch.LongTensor] = None,\\n-    use_cache: Optional[bool] = None,\\n-    output_attentions: Optional[bool] = None,\\n-    output_hidden_states: Optional[bool] = None,\\n-    return_dict: Optional[bool] = None,\\n-    *args, **kwargs,\\n-) -> Union[Tuple, CausalLMOutputWithPast]:\\n-\\n-    if causal_mask is None and past_key_values is None:\\n-        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-    output_hidden_states = (\\n-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-    )\\n-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+# Follows line by line https://github.com/google-deepmind/gemma/blob/main/gemma/positional_embeddings.py#L45\\n+# Formulates cos and sin differently from Llama!\\n+class GemmaFixedRotaryEmbedding(torch.nn.Module):\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\\n+        super().__init__()\\n+        self.dim = dim\\n+        self.max_position_embeddings = max_position_embeddings\\n+        self.base = base\\n+\\n+        # Build here to make `torch.jit.trace` work.\\n+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())\\n+    pass\\n \\n-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-    self.model._has_no_labels = labels is None\\n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and\\n+        # in FP32. They are applied (multiplied) in FP32 as well.\\n+        self.max_seq_len_cached = seq_len\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n-        outputs = GemmaModel_fast_forward_inference(\\n-            self.model,\\n-            input_ids,\\n-            past_key_values,\\n-        )\\n-    else:\\n-        outputs = self.model(\\n-            input_ids=input_ids,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_values=past_key_values,\\n-            inputs_embeds=inputs_embeds,\\n-            use_cache=use_cache,\\n-            output_attentions=output_attentions,\\n-            output_hidden_states=output_hidden_states,\\n-            return_dict=return_dict,\\n+        # The difference is we do division explicity instead of t * (1/x) ie we do t/x.\\n+        freq_exponents = (2.0 / self.dim) * (\\n+            torch.arange(self.dim // 2, dtype = torch.int64, device = \"cpu\").float()\\n         )\\n+        timescale = self.base**freq_exponents\\n+        positions = torch.arange(self.max_seq_len_cached, device = \"cpu\", dtype = torch.int64).float()\\n+        radians_new = positions[..., None] / timescale[None, None, :]\\n+        radians_new = radians_new.squeeze(0)\\n+\\n+        emb = torch.cat((radians_new, radians_new), dim = -1)\\n+        # We must do RoPE in float32!\\n+        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)\\n+        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)\\n+        self.register_buffer(\"cos_cached\", cos, persistent = False)\\n+        self.register_buffer(\"sin_cached\", sin, persistent = False)\\n     pass\\n \\n-    hidden_states = outputs[0]\\n-    bsz, q_len, hd = hidden_states.shape\\n-    if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n-        logits = logits.unsqueeze(0).unsqueeze(0)\\n-    else:\\n-        logits = self.lm_head(hidden_states)\\n-    pass\\n+    def forward(self, x, position_ids=None, seq_len=None):\\n+        # x: [bs, num_attention_heads, seq_len, head_size]\\n+        if seq_len > self.max_seq_len_cached:\\n+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\\n \\n-    loss = None\\n-    if labels is not None:\\n-        shift_logits = logits\\n-        if not hasattr(self, \"extra_ignored_labels\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/10\\n-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n-        pass\\n-        \\n-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-        loss = fast_cross_entropy_loss(\\n-            logits = shift_logits,\\n-            labels = shift_labels,\\n+        return (\\n+            self.cos_cached[:seq_len].to(dtype=x.dtype),\\n+            self.sin_cached[:seq_len].to(dtype=x.dtype),\\n         )\\n     pass\\n-\\n-    if not return_dict:\\n-        output = (logits,) + outputs[1:]\\n-        return (loss,) + output if loss is not None else output\\n-\\n-    return CausalLMOutputWithPast(\\n-        loss=loss,\\n-        logits=logits,\\n-        past_key_values=outputs.past_key_values,\\n-        hidden_states=outputs.hidden_states,\\n-        attentions=outputs.attentions,\\n-    )\\n pass\\n \\n \\n@@ -270,7 +250,7 @@ class FastGemmaModel(FastLlamaModel):\\n         GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n         GemmaModel          .forward = LlamaModel_fast_forward\\n-        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward\\n+        GemmaForCausalLM    .forward = CausalLM_fast_forward(GemmaModel_fast_forward_inference)\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n@@ -278,7 +258,7 @@ class FastGemmaModel(FastLlamaModel):\\n         # https://github.com/huggingface/transformers/pull/27931\\n         # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n         import transformers.models.gemma.modeling_gemma\\n-        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = LlamaRotaryEmbedding\\n+        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = GemmaFixedRotaryEmbedding\\n         return\\n     pass\\n \\n@@ -329,13 +309,14 @@ class FastGemmaModel(FastLlamaModel):\\n                 pass\\n             pass\\n             # Downcast RoPE embedding to correct data type\\n-            if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n-                and (module.cos_cached.dtype != correct_dtype):\\n-\\n-                module.cos_cached = module.cos_cached.to(correct_dtype)\\n-                module.sin_cached = module.sin_cached.to(correct_dtype)\\n-                pass\\n-            pass\\n+            # RoPE must be done in float32 for Gemma\\n+            # if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n+            #     and (module.cos_cached.dtype != correct_dtype):\\n+\\n+            #     module.cos_cached = module.cos_cached.to(correct_dtype)\\n+            #     module.sin_cached = module.sin_cached.to(correct_dtype)\\n+            #     pass\\n+            # pass\\n         pass\\n \\n         # Add 1 to weight\\n@@ -358,8 +339,8 @@ class FastGemmaModel(FastLlamaModel):\\n                 # Must be in float32\\n                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n                 # module = module.to(torch.float32)\\n-                # Don\\'t convert to float32 since error analysis shows it makes it worse!!\\n-                module.weight += 1.0 # return output * (1 + self.weight)\\n+                # Leave + 1 to Triton kernel itself\\n+                # module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n         pass\\n',\n",
       " '@@ -208,6 +208,7 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_nn_functional_silu = torch.nn.functional.silu\\n def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n@@ -217,7 +218,7 @@ def fast_swiglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.silu(gate, inplace = True)\\n+    gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -509,7 +510,8 @@ def LlamaModel_fast_forward(\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n     # Mormalized from Gemma\\n-    if self.config.model_type == \"gemma\":\\n+    IS_GEMMA = self.config.model_type == \"gemma\"\\n+    if IS_GEMMA:\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n         if not inputs_embeds.is_leaf:\\n             inputs_embeds = inputs_embeds.detach()\\n@@ -517,7 +519,12 @@ def LlamaModel_fast_forward(\\n         elif inputs_requires_grad:\\n             inputs_embeds.requires_grad_(False)\\n         pass\\n-        inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        # Match Gemma exactly by casting to bfloat16 / float16\\n+        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n+        # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n+        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n \\n@@ -619,7 +626,7 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n     \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -681,91 +688,94 @@ def LlamaModel_fast_forward_inference(\\n pass\\n \\n \\n-def LlamaForCausalLM_fast_forward(\\n-    self,\\n-    input_ids: torch.LongTensor = None,\\n-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n-    attention_mask: Optional[torch.Tensor] = None,\\n-    position_ids: Optional[torch.LongTensor] = None,\\n-    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n-    inputs_embeds: Optional[torch.FloatTensor] = None,\\n-    labels: Optional[torch.LongTensor] = None,\\n-    use_cache: Optional[bool] = None,\\n-    output_attentions: Optional[bool] = None,\\n-    output_hidden_states: Optional[bool] = None,\\n-    return_dict: Optional[bool] = None,\\n-    *args, **kwargs,\\n-) -> Union[Tuple, CausalLMOutputWithPast]:\\n+def CausalLM_fast_forward(fast_forward_inference):\\n+    def _CausalLM_fast_forward(\\n+        self,\\n+        input_ids: torch.LongTensor = None,\\n+        causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+        attention_mask: Optional[torch.Tensor] = None,\\n+        position_ids: Optional[torch.LongTensor] = None,\\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\\n+        labels: Optional[torch.LongTensor] = None,\\n+        use_cache: Optional[bool] = None,\\n+        output_attentions: Optional[bool] = None,\\n+        output_hidden_states: Optional[bool] = None,\\n+        return_dict: Optional[bool] = None,\\n+        *args, **kwargs,\\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\\n+\\n+        if causal_mask is None and past_key_values is None:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+\\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+        output_hidden_states = (\\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+        )\\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n-    if causal_mask is None and past_key_values is None:\\n-        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+        self.model._has_no_labels = labels is None\\n \\n-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-    output_hidden_states = (\\n-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-    )\\n-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+        if past_key_values is not None and \\\\\\n+            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+            outputs = fast_forward_inference(\\n+                self.model,\\n+                input_ids,\\n+                past_key_values,\\n+            )\\n+        else:\\n+            outputs = self.model(\\n+                input_ids=input_ids,\\n+                causal_mask=causal_mask,\\n+                attention_mask=attention_mask,\\n+                position_ids=position_ids,\\n+                past_key_values=past_key_values,\\n+                inputs_embeds=inputs_embeds,\\n+                use_cache=use_cache,\\n+                output_attentions=output_attentions,\\n+                output_hidden_states=output_hidden_states,\\n+                return_dict=return_dict,\\n+            )\\n+        pass\\n \\n-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-    self.model._has_no_labels = labels is None\\n+        hidden_states = outputs[0]\\n+        bsz, q_len, hd = hidden_states.shape\\n+        if bsz == 1 and q_len == 1:\\n+            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            logits = logits.unsqueeze(0).unsqueeze(0)\\n+        else:\\n+            logits = self.lm_head(hidden_states)\\n+        pass\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n-        outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n-            input_ids,\\n-            past_key_values,\\n-        )\\n-    else:\\n-        outputs = self.model(\\n-            input_ids=input_ids,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_values=past_key_values,\\n-            inputs_embeds=inputs_embeds,\\n-            use_cache=use_cache,\\n-            output_attentions=output_attentions,\\n-            output_hidden_states=output_hidden_states,\\n-            return_dict=return_dict,\\n-        )\\n-    pass\\n+        loss = None\\n+        if labels is not None:\\n+            shift_logits = logits\\n+            if not hasattr(self, \"extra_ignored_labels\"):\\n+                # Fixes https://github.com/unslothai/unsloth/issues/10\\n+                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+            pass\\n+            \\n+            shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n+            loss = fast_cross_entropy_loss(\\n+                logits = shift_logits,\\n+                labels = shift_labels,\\n+            )\\n+        pass\\n \\n-    hidden_states = outputs[0]\\n-    bsz, q_len, hd = hidden_states.shape\\n-    if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n-        logits = logits.unsqueeze(0).unsqueeze(0)\\n-    else:\\n-        logits = self.lm_head(hidden_states)\\n-    pass\\n+        if not return_dict:\\n+            output = (logits,) + outputs[1:]\\n+            return (loss,) + output if loss is not None else output\\n \\n-    loss = None\\n-    if labels is not None:\\n-        shift_logits = logits\\n-        if not hasattr(self, \"extra_ignored_labels\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/10\\n-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n-        pass\\n-        \\n-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-        loss = fast_cross_entropy_loss(\\n-            logits = shift_logits,\\n-            labels = shift_labels,\\n+        return CausalLMOutputWithPast(\\n+            loss=loss,\\n+            logits=logits,\\n+            past_key_values=outputs.past_key_values,\\n+            hidden_states=outputs.hidden_states,\\n+            attentions=outputs.attentions,\\n         )\\n     pass\\n-\\n-    if not return_dict:\\n-        output = (logits,) + outputs[1:]\\n-        return (loss,) + output if loss is not None else output\\n-\\n-    return CausalLMOutputWithPast(\\n-        loss=loss,\\n-        logits=logits,\\n-        past_key_values=outputs.past_key_values,\\n-        hidden_states=outputs.hidden_states,\\n-        attentions=outputs.attentions,\\n-    )\\n+    return _CausalLM_fast_forward\\n pass\\n \\n \\n@@ -880,7 +890,7 @@ class FastLlamaModel:\\n         LlamaFlashAttention2.forward = LlamaAttention_fast_forward\\n         LlamaDecoderLayer   .forward = LlamaDecoderLayer_fast_forward\\n         LlamaModel          .forward = LlamaModel_fast_forward\\n-        LlamaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        LlamaForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n \\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n',\n",
       " \"@@ -369,6 +369,7 @@ def unsloth_save_model(\\n \\n     # Switch to our fast saving modules if it's a slow PC!\\n     n_cpus = psutil.cpu_count(logical = False)\\n+    if n_cpus is None: n_cpus = psutil.cpu_count()\\n     if n_cpus is None: n_cpus = 1\\n \\n     if safe_serialization is None:\\n\",\n",
       " '@@ -43,6 +43,7 @@ huggingface = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"triton\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -104,6 +105,16 @@ cu121-torch211 = [\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch211]\",\\n ]\\n+cu118-torch212 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118onlytorch212]\",\\n+]\\n+cu121-torch212 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121onlytorch212]\",\\n+]\\n cu118-torch220 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n',\n",
       " '@@ -99,10 +99,14 @@ except:\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n-        raise ImportError(\"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n-                          \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n-                          \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n-                          \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\")\\n+        warnings.warn(\\n+            \"Unsloth: CUDA is not linked properly.\\\\n\"\\\\\\n+            \"Try running `python -m bitsandbytes` then `python -m xformers.info`\\\\n\"\\\\\\n+            \"We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\\'t work.\\\\n\"\\\\\\n+            \"You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\\\\n\"\\\\\\n+            \"Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.\\\\n\"\\\\\\n+            \"Unsloth will still run for now, but maybe it might crash - let\\'s hope it works!\"\\n+        )\\n pass\\n \\n from .models import *\\n',\n",
       " '@@ -257,6 +257,10 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n+\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n',\n",
       " '@@ -53,6 +53,7 @@ def _rms_layernorm_forward(\\n pass\\n \\n \\n+@triton.heuristics({\"GEMMA\": lambda args: args[\"GEMMA\"],})\\n @triton.jit\\n def _rms_layernorm_backward(\\n     dY, dY_row_stride,\\n@@ -61,6 +62,7 @@ def _rms_layernorm_backward(\\n     r,   r_row_stride,\\n     dW, dW_row_stride,\\n     n_cols, eps,\\n+    GEMMA      : tl.constexpr,\\n     BLOCK_SIZE : tl.constexpr,\\n ):\\n     \"\"\"\\n@@ -84,16 +86,51 @@ def _rms_layernorm_backward(\\n     inv_var = tl.load(r).to(tl.float32)\\n     normed = X_row * inv_var\\n \\n-    dY_W = dY_row * W_row\\n+    if GEMMA: dY_W = dY_row * (W_row + 1.0)\\n+    else:     dY_W = dY_row * W_row\\n+\\n     rowsum_dY_normed = tl.sum(dY_W * normed, axis = 0)\\n     output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)\\n     tl.store(dY + col_offsets, output, mask = mask)\\n pass\\n \\n \\n+@triton.jit\\n+def _gemma_rms_layernorm_forward(\\n+    Y, Y_row_stride,\\n+    X, X_row_stride,\\n+    W, W_row_stride,\\n+    r, r_row_stride,\\n+    n_cols, eps,\\n+    BLOCK_SIZE : tl.constexpr,\\n+):\\n+    # Copies https://github.com/google-deepmind/gemma/blob/main/gemma/layers.py#L31\\n+    # and https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L33\\n+    # exactly. Essentially all in float32!\\n+    row_idx = tl.program_id(0)\\n+    col_offsets = tl.arange(0, BLOCK_SIZE)\\n+    mask = col_offsets < n_cols\\n+\\n+    Y += row_idx * Y_row_stride\\n+    X += row_idx * X_row_stride\\n+    r += row_idx * r_row_stride\\n+\\n+    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\\n+\\n+    row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\\n+    inv_var = 1.0 / tl.sqrt(row_var + eps) # Must be 1/sqrt to match Deepmind\\'s impl\\n+    tl.store(r, inv_var)\\n+    normed = X_row * inv_var\\n+    output = normed * (W_row + 1.0)\\n+\\n+    tl.store(Y + col_offsets, output, mask = mask)\\n+pass\\n+\\n+\\n class Fast_RMS_Layernorm(torch.autograd.Function):\\n     @staticmethod\\n-    def forward(ctx, X, W, eps):\\n+    def forward(ctx, X, W, eps, gemma = False):\\n         shape = X.shape\\n         dim = shape[-1]\\n         X = X.view(-1, dim)\\n@@ -103,7 +140,8 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n         Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = \"cuda\")\\n         r = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\\n \\n-        _rms_layernorm_forward[(n_rows,)](\\n+        fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward\\n+        fx[(n_rows,)](\\n             Y, Y.stride(0),\\n             X, X.stride(0),\\n             W, W.stride(0),\\n@@ -115,6 +153,7 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n         ctx.eps = eps\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.GEMMA = gemma\\n         ctx.save_for_backward(X, W, r)\\n         return Y.view(*shape)\\n     pass\\n@@ -135,18 +174,19 @@ class Fast_RMS_Layernorm(torch.autograd.Function):\\n             r,  r .stride(0),\\n             dW, dW.stride(0),\\n             n_cols, ctx.eps,\\n+            GEMMA      = ctx.GEMMA,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n         )\\n         dX = dY.view(*shape)\\n-        return dX, None, None\\n+        return dX, None, None, None\\n     pass\\n pass\\n \\n \\n-def fast_rms_layernorm(layernorm, X):\\n+def fast_rms_layernorm(layernorm, X, gemma = False):\\n     W   = layernorm.weight\\n     eps = layernorm.variance_epsilon\\n-    out = Fast_RMS_Layernorm.apply(X, W, eps)\\n+    out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)\\n     return out\\n pass\\n',\n",
       " '@@ -39,24 +39,28 @@ def _rope_embedding(\\n     half_head_dim = head_dim // 2\\n     mask = col_offsets < half_head_dim\\n \\n-    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n-    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-                   half_head_dim*1 + col_offsets, mask = mask, other = 0)\\n     sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\\\\n                    half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n     cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\\\\n                    half_head_dim*0 + col_offsets, mask = mask, other = 0)\\n \\n+    # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n+    Q1   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n+                   half_head_dim*0 + col_offsets, mask = mask, other = 0).to(sin1.dtype)\\n+    Q2   = tl.load(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n+                   half_head_dim*1 + col_offsets, mask = mask, other = 0).to(sin1.dtype)\\n+\\n     if BACKWARD_PASS:\\n         # See our blog post for more info.\\n         sin1 = -sin1\\n     pass\\n \\n     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-             half_head_dim*0 + col_offsets, Q1*cos1 - Q2*sin1, mask = mask)\\n+             half_head_dim*0 + col_offsets,\\n+             Q1*cos1 - Q2*sin1, mask = mask)\\n     tl.store(Q + row_position*Q_row_stride + head_position*head_dim + \\\\\\n-             half_head_dim*1 + col_offsets, Q2*cos1 + Q1*sin1, mask = mask)\\n+             half_head_dim*1 + col_offsets,\\n+             Q2*cos1 + Q1*sin1, mask = mask)\\n pass\\n \\n \\n',\n",
       " '@@ -39,6 +39,7 @@ except:\\n pass\\n \\n \\n+torch_nn_functional_gelu = torch.nn.functional.gelu\\n def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n@@ -48,7 +49,7 @@ def fast_geglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.gelu(gate, approximate = \"tanh\")\\n+    gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -57,6 +58,18 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+    out_weight[:] = self.weight\\n+    out_weight += 1.0\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -72,10 +85,11 @@ def GemmaDecoderLayer_fast_forward(\\n ):\\n     if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+        out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             self.self_attn,\\n             hidden_states,\\n@@ -87,12 +101,12 @@ def GemmaDecoderLayer_fast_forward(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(self.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(self.mlp, hidden_states)\\n         hidden_states += residual\\n     else:\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n         # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n@@ -108,7 +122,7 @@ def GemmaDecoderLayer_fast_forward(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n         # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n@@ -137,15 +151,18 @@ def GemmaModel_fast_forward_inference(\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n+    out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n-    hidden_states *= math_sqrt(self.config.hidden_size)\\n+    # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n+    # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n+    hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n         # Self Attention\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n@@ -156,13 +173,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         # Fully Connected\\n         residual = hidden_states\\n-        hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n+        hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n         hidden_states += residual\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n@@ -173,91 +190,54 @@ def GemmaModel_fast_forward_inference(\\n pass\\n \\n \\n-def GemmaForCausalLM_fast_forward(\\n-    self,\\n-    input_ids: torch.LongTensor = None,\\n-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n-    attention_mask: Optional[torch.Tensor] = None,\\n-    position_ids: Optional[torch.LongTensor] = None,\\n-    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n-    inputs_embeds: Optional[torch.FloatTensor] = None,\\n-    labels: Optional[torch.LongTensor] = None,\\n-    use_cache: Optional[bool] = None,\\n-    output_attentions: Optional[bool] = None,\\n-    output_hidden_states: Optional[bool] = None,\\n-    return_dict: Optional[bool] = None,\\n-    *args, **kwargs,\\n-) -> Union[Tuple, CausalLMOutputWithPast]:\\n-\\n-    if causal_mask is None and past_key_values is None:\\n-        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-    output_hidden_states = (\\n-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-    )\\n-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+# Follows line by line https://github.com/google-deepmind/gemma/blob/main/gemma/positional_embeddings.py#L45\\n+# Formulates cos and sin differently from Llama!\\n+class GemmaFixedRotaryEmbedding(torch.nn.Module):\\n+    # Fixes https://github.com/huggingface/transformers/pull/28837\\n+    # https://github.com/microsoft/DeepSpeed/issues/4932\\n+    # The precision of RoPE buffers is not correct, so we cast to int64.\\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\\n+        super().__init__()\\n+        self.dim = dim\\n+        self.max_position_embeddings = max_position_embeddings\\n+        self.base = base\\n+\\n+        # Build here to make `torch.jit.trace` work.\\n+        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())\\n+    pass\\n \\n-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-    self.model._has_no_labels = labels is None\\n+    def _set_cos_sin_cache(self, seq_len, device, dtype):\\n+        # Note: on the original Llama codebase, these tensors are created on the target device (and not on CPU) and\\n+        # in FP32. They are applied (multiplied) in FP32 as well.\\n+        self.max_seq_len_cached = seq_len\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n-        outputs = GemmaModel_fast_forward_inference(\\n-            self.model,\\n-            input_ids,\\n-            past_key_values,\\n-        )\\n-    else:\\n-        outputs = self.model(\\n-            input_ids=input_ids,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_values=past_key_values,\\n-            inputs_embeds=inputs_embeds,\\n-            use_cache=use_cache,\\n-            output_attentions=output_attentions,\\n-            output_hidden_states=output_hidden_states,\\n-            return_dict=return_dict,\\n+        # The difference is we do division explicity instead of t * (1/x) ie we do t/x.\\n+        freq_exponents = (2.0 / self.dim) * (\\n+            torch.arange(self.dim // 2, dtype = torch.int64, device = \"cpu\").float()\\n         )\\n+        timescale = self.base**freq_exponents\\n+        positions = torch.arange(self.max_seq_len_cached, device = \"cpu\", dtype = torch.int64).float()\\n+        radians_new = positions[..., None] / timescale[None, None, :]\\n+        radians_new = radians_new.squeeze(0)\\n+\\n+        emb = torch.cat((radians_new, radians_new), dim = -1)\\n+        # We must do RoPE in float32!\\n+        cos = emb.cos().to(device = device, non_blocking = True)#, dtype = dtype)\\n+        sin = emb.sin().to(device = device, non_blocking = True)#, dtype = dtype)\\n+        self.register_buffer(\"cos_cached\", cos, persistent = False)\\n+        self.register_buffer(\"sin_cached\", sin, persistent = False)\\n     pass\\n \\n-    hidden_states = outputs[0]\\n-    bsz, q_len, hd = hidden_states.shape\\n-    if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n-        logits = logits.unsqueeze(0).unsqueeze(0)\\n-    else:\\n-        logits = self.lm_head(hidden_states)\\n-    pass\\n+    def forward(self, x, position_ids=None, seq_len=None):\\n+        # x: [bs, num_attention_heads, seq_len, head_size]\\n+        if seq_len > self.max_seq_len_cached:\\n+            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\\n \\n-    loss = None\\n-    if labels is not None:\\n-        shift_logits = logits\\n-        if not hasattr(self, \"extra_ignored_labels\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/10\\n-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n-        pass\\n-        \\n-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-        loss = fast_cross_entropy_loss(\\n-            logits = shift_logits,\\n-            labels = shift_labels,\\n+        return (\\n+            self.cos_cached[:seq_len].to(dtype=x.dtype),\\n+            self.sin_cached[:seq_len].to(dtype=x.dtype),\\n         )\\n     pass\\n-\\n-    if not return_dict:\\n-        output = (logits,) + outputs[1:]\\n-        return (loss,) + output if loss is not None else output\\n-\\n-    return CausalLMOutputWithPast(\\n-        loss=loss,\\n-        logits=logits,\\n-        past_key_values=outputs.past_key_values,\\n-        hidden_states=outputs.hidden_states,\\n-        attentions=outputs.attentions,\\n-    )\\n pass\\n \\n \\n@@ -270,7 +250,7 @@ class FastGemmaModel(FastLlamaModel):\\n         GemmaFlashAttention2.forward = LlamaAttention_fast_forward\\n         GemmaDecoderLayer   .forward = GemmaDecoderLayer_fast_forward\\n         GemmaModel          .forward = LlamaModel_fast_forward\\n-        GemmaForCausalLM    .forward = GemmaForCausalLM_fast_forward\\n+        GemmaForCausalLM    .forward = CausalLM_fast_forward(GemmaModel_fast_forward_inference)\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n         # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n@@ -278,7 +258,7 @@ class FastGemmaModel(FastLlamaModel):\\n         # https://github.com/huggingface/transformers/pull/27931\\n         # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n         import transformers.models.gemma.modeling_gemma\\n-        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = LlamaRotaryEmbedding\\n+        transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding = GemmaFixedRotaryEmbedding\\n         return\\n     pass\\n \\n@@ -329,13 +309,14 @@ class FastGemmaModel(FastLlamaModel):\\n                 pass\\n             pass\\n             # Downcast RoPE embedding to correct data type\\n-            if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n-                and (module.cos_cached.dtype != correct_dtype):\\n-\\n-                module.cos_cached = module.cos_cached.to(correct_dtype)\\n-                module.sin_cached = module.sin_cached.to(correct_dtype)\\n-                pass\\n-            pass\\n+            # RoPE must be done in float32 for Gemma\\n+            # if (name.endswith(\"rotary_emb\") or hasattr(module, \"cos_cached\")) \\\\\\n+            #     and (module.cos_cached.dtype != correct_dtype):\\n+\\n+            #     module.cos_cached = module.cos_cached.to(correct_dtype)\\n+            #     module.sin_cached = module.sin_cached.to(correct_dtype)\\n+            #     pass\\n+            # pass\\n         pass\\n \\n         # Add 1 to weight\\n@@ -358,8 +339,8 @@ class FastGemmaModel(FastLlamaModel):\\n                 # Must be in float32\\n                 # https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/rms_normalization.py#L36\\n                 # module = module.to(torch.float32)\\n-                # Don\\'t convert to float32 since error analysis shows it makes it worse!!\\n-                module.weight += 1.0 # return output * (1 + self.weight)\\n+                # Leave + 1 to Triton kernel itself\\n+                # module.weight += 1.0 # return output * (1 + self.weight)\\n                 if not hasattr(module, \"variance_epsilon\"):\\n                     module.variance_epsilon = module.eps # Gemma doesn\\'t use variance_epsilon\\n         pass\\n',\n",
       " '@@ -208,6 +208,7 @@ def LlamaAttention_fast_forward_inference(\\n pass\\n \\n \\n+torch_nn_functional_silu = torch.nn.functional.silu\\n def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n@@ -217,7 +218,7 @@ def fast_swiglu_inference(self, X):\\n \\n     gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n     up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n-    gate = torch.nn.functional.silu(gate, inplace = True)\\n+    gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n     # X = self.down_proj(gate)\\n@@ -509,7 +510,8 @@ def LlamaModel_fast_forward(\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n     # Mormalized from Gemma\\n-    if self.config.model_type == \"gemma\":\\n+    IS_GEMMA = self.config.model_type == \"gemma\"\\n+    if IS_GEMMA:\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n         if not inputs_embeds.is_leaf:\\n             inputs_embeds = inputs_embeds.detach()\\n@@ -517,7 +519,12 @@ def LlamaModel_fast_forward(\\n         elif inputs_requires_grad:\\n             inputs_embeds.requires_grad_(False)\\n         pass\\n-        inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        # Match Gemma exactly by casting to bfloat16 / float16\\n+        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+        # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n+        # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n+        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n     pass\\n \\n@@ -619,7 +626,7 @@ def LlamaModel_fast_forward(\\n             all_self_attns += (layer_outputs[1],)\\n     pass\\n     \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n     # add hidden states from the last decoder layer\\n     if output_hidden_states:\\n@@ -681,91 +688,94 @@ def LlamaModel_fast_forward_inference(\\n pass\\n \\n \\n-def LlamaForCausalLM_fast_forward(\\n-    self,\\n-    input_ids: torch.LongTensor = None,\\n-    causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n-    attention_mask: Optional[torch.Tensor] = None,\\n-    position_ids: Optional[torch.LongTensor] = None,\\n-    past_key_values: Optional[List[torch.FloatTensor]] = None,\\n-    inputs_embeds: Optional[torch.FloatTensor] = None,\\n-    labels: Optional[torch.LongTensor] = None,\\n-    use_cache: Optional[bool] = None,\\n-    output_attentions: Optional[bool] = None,\\n-    output_hidden_states: Optional[bool] = None,\\n-    return_dict: Optional[bool] = None,\\n-    *args, **kwargs,\\n-) -> Union[Tuple, CausalLMOutputWithPast]:\\n+def CausalLM_fast_forward(fast_forward_inference):\\n+    def _CausalLM_fast_forward(\\n+        self,\\n+        input_ids: torch.LongTensor = None,\\n+        causal_mask: Optional[xformers.attn_bias.BlockDiagonalCausalMask] = None,\\n+        attention_mask: Optional[torch.Tensor] = None,\\n+        position_ids: Optional[torch.LongTensor] = None,\\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\\n+        labels: Optional[torch.LongTensor] = None,\\n+        use_cache: Optional[bool] = None,\\n+        output_attentions: Optional[bool] = None,\\n+        output_hidden_states: Optional[bool] = None,\\n+        return_dict: Optional[bool] = None,\\n+        *args, **kwargs,\\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\\n+\\n+        if causal_mask is None and past_key_values is None:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+\\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+        output_hidden_states = (\\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+        )\\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n \\n-    if causal_mask is None and past_key_values is None:\\n-        causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+        self.model._has_no_labels = labels is None\\n \\n-    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-    output_hidden_states = (\\n-        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-    )\\n-    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+        if past_key_values is not None and \\\\\\n+            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+            outputs = fast_forward_inference(\\n+                self.model,\\n+                input_ids,\\n+                past_key_values,\\n+            )\\n+        else:\\n+            outputs = self.model(\\n+                input_ids=input_ids,\\n+                causal_mask=causal_mask,\\n+                attention_mask=attention_mask,\\n+                position_ids=position_ids,\\n+                past_key_values=past_key_values,\\n+                inputs_embeds=inputs_embeds,\\n+                use_cache=use_cache,\\n+                output_attentions=output_attentions,\\n+                output_hidden_states=output_hidden_states,\\n+                return_dict=return_dict,\\n+            )\\n+        pass\\n \\n-    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-    self.model._has_no_labels = labels is None\\n+        hidden_states = outputs[0]\\n+        bsz, q_len, hd = hidden_states.shape\\n+        if bsz == 1 and q_len == 1:\\n+            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            logits = logits.unsqueeze(0).unsqueeze(0)\\n+        else:\\n+            logits = self.lm_head(hidden_states)\\n+        pass\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n-        outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n-            input_ids,\\n-            past_key_values,\\n-        )\\n-    else:\\n-        outputs = self.model(\\n-            input_ids=input_ids,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_values=past_key_values,\\n-            inputs_embeds=inputs_embeds,\\n-            use_cache=use_cache,\\n-            output_attentions=output_attentions,\\n-            output_hidden_states=output_hidden_states,\\n-            return_dict=return_dict,\\n-        )\\n-    pass\\n+        loss = None\\n+        if labels is not None:\\n+            shift_logits = logits\\n+            if not hasattr(self, \"extra_ignored_labels\"):\\n+                # Fixes https://github.com/unslothai/unsloth/issues/10\\n+                self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n+            pass\\n+            \\n+            shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n+            loss = fast_cross_entropy_loss(\\n+                logits = shift_logits,\\n+                labels = shift_labels,\\n+            )\\n+        pass\\n \\n-    hidden_states = outputs[0]\\n-    bsz, q_len, hd = hidden_states.shape\\n-    if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n-        logits = logits.unsqueeze(0).unsqueeze(0)\\n-    else:\\n-        logits = self.lm_head(hidden_states)\\n-    pass\\n+        if not return_dict:\\n+            output = (logits,) + outputs[1:]\\n+            return (loss,) + output if loss is not None else output\\n \\n-    loss = None\\n-    if labels is not None:\\n-        shift_logits = logits\\n-        if not hasattr(self, \"extra_ignored_labels\"):\\n-            # Fixes https://github.com/unslothai/unsloth/issues/10\\n-            self.extra_ignored_labels = torch.full((self.max_seq_length, 1), -100, device = \"cuda\")\\n-        pass\\n-        \\n-        shift_labels = torch.hstack((labels[..., 1:], self.extra_ignored_labels[:labels.shape[0]]))\\n-        loss = fast_cross_entropy_loss(\\n-            logits = shift_logits,\\n-            labels = shift_labels,\\n+        return CausalLMOutputWithPast(\\n+            loss=loss,\\n+            logits=logits,\\n+            past_key_values=outputs.past_key_values,\\n+            hidden_states=outputs.hidden_states,\\n+            attentions=outputs.attentions,\\n         )\\n     pass\\n-\\n-    if not return_dict:\\n-        output = (logits,) + outputs[1:]\\n-        return (loss,) + output if loss is not None else output\\n-\\n-    return CausalLMOutputWithPast(\\n-        loss=loss,\\n-        logits=logits,\\n-        past_key_values=outputs.past_key_values,\\n-        hidden_states=outputs.hidden_states,\\n-        attentions=outputs.attentions,\\n-    )\\n+    return _CausalLM_fast_forward\\n pass\\n \\n \\n@@ -880,7 +890,7 @@ class FastLlamaModel:\\n         LlamaFlashAttention2.forward = LlamaAttention_fast_forward\\n         LlamaDecoderLayer   .forward = LlamaDecoderLayer_fast_forward\\n         LlamaModel          .forward = LlamaModel_fast_forward\\n-        LlamaForCausalLM    .forward = LlamaForCausalLM_fast_forward\\n+        LlamaForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)\\n         PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n \\n         # Solves https://github.com/unslothai/unsloth/issues/168\\n',\n",
       " \"@@ -369,6 +369,7 @@ def unsloth_save_model(\\n \\n     # Switch to our fast saving modules if it's a slow PC!\\n     n_cpus = psutil.cpu_count(logical = False)\\n+    if n_cpus is None: n_cpus = psutil.cpu_count()\\n     if n_cpus is None: n_cpus = 1\\n \\n     if safe_serialization is None:\\n\",\n",
       " '@@ -257,9 +257,9 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n-    pass\\n+    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+    #     chat_template = \"gemma_chatml\"\\n+    # pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -298,8 +298,12 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            if not stop_word in token_mapping.values():\\n+                # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n+                logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n+                string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            pass\\n+\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n \\n',\n",
       " '@@ -916,6 +916,7 @@ class FastLlamaModel:\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -978,13 +979,16 @@ class FastLlamaModel:\\n             max_position_embeddings = max_position_embeddings,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-\\n+        \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -18,7 +18,7 @@ from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n from peft import PeftConfig, PeftModel\\n from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n-\\n+import os\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -118,6 +118,16 @@ class FastLanguageModel(FastLlamaModel):\\n             )\\n         pass\\n \\n+        # Check if this is local model since the tokenizer gets overwritten\\n+        if  os.path.exists(os.path.join(old_model_name, \"tokenizer_config.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"tokenizer.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"special_tokens_map.json\")):\\n+\\n+            tokenizer_name = old_model_name\\n+        else:\\n+            tokenizer_name = None\\n+        pass\\n+\\n         model, tokenizer = dispatch_model.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n@@ -128,6 +138,7 @@ class FastLanguageModel(FastLlamaModel):\\n             rope_scaling   = rope_scaling,\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n+            tokenizer_name = tokenizer_name,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -294,6 +294,7 @@ class FastMistralModel(FastLlamaModel):\\n         rope_scaling   = None, # Mistral does not support RoPE scaling\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -354,8 +355,11 @@ class FastMistralModel(FastLlamaModel):\\n             # rope_scaling      = rope_scaling,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n',\n",
       " '@@ -91,11 +91,13 @@ def _merge_lora(layer, name):\\n         else:\\n             dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n+        # W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n             W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -257,9 +257,9 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n-    pass\\n+    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+    #     chat_template = \"gemma_chatml\"\\n+    # pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -298,8 +298,12 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            if not stop_word in token_mapping.values():\\n+                # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n+                logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n+                string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            pass\\n+\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n \\n',\n",
       " '@@ -916,6 +916,7 @@ class FastLlamaModel:\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -978,13 +979,16 @@ class FastLlamaModel:\\n             max_position_embeddings = max_position_embeddings,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-\\n+        \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -18,7 +18,7 @@ from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n from peft import PeftConfig, PeftModel\\n from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n-\\n+import os\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -118,6 +118,16 @@ class FastLanguageModel(FastLlamaModel):\\n             )\\n         pass\\n \\n+        # Check if this is local model since the tokenizer gets overwritten\\n+        if  os.path.exists(os.path.join(old_model_name, \"tokenizer_config.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"tokenizer.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"special_tokens_map.json\")):\\n+\\n+            tokenizer_name = old_model_name\\n+        else:\\n+            tokenizer_name = None\\n+        pass\\n+\\n         model, tokenizer = dispatch_model.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n@@ -128,6 +138,7 @@ class FastLanguageModel(FastLlamaModel):\\n             rope_scaling   = rope_scaling,\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n+            tokenizer_name = tokenizer_name,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -294,6 +294,7 @@ class FastMistralModel(FastLlamaModel):\\n         rope_scaling   = None, # Mistral does not support RoPE scaling\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -354,8 +355,11 @@ class FastMistralModel(FastLlamaModel):\\n             # rope_scaling      = rope_scaling,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n',\n",
       " '@@ -91,11 +91,13 @@ def _merge_lora(layer, name):\\n         else:\\n             dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n+        # W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n             W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -257,9 +257,9 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n-    pass\\n+    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+    #     chat_template = \"gemma_chatml\"\\n+    # pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -298,8 +298,12 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            if not stop_word in token_mapping.values():\\n+                # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n+                logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n+                string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            pass\\n+\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n \\n',\n",
       " '@@ -916,6 +916,7 @@ class FastLlamaModel:\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -978,13 +979,16 @@ class FastLlamaModel:\\n             max_position_embeddings = max_position_embeddings,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-\\n+        \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -18,7 +18,7 @@ from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n from peft import PeftConfig, PeftModel\\n from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n-\\n+import os\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -118,6 +118,16 @@ class FastLanguageModel(FastLlamaModel):\\n             )\\n         pass\\n \\n+        # Check if this is local model since the tokenizer gets overwritten\\n+        if  os.path.exists(os.path.join(old_model_name, \"tokenizer_config.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"tokenizer.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"special_tokens_map.json\")):\\n+\\n+            tokenizer_name = old_model_name\\n+        else:\\n+            tokenizer_name = None\\n+        pass\\n+\\n         model, tokenizer = dispatch_model.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n@@ -128,6 +138,7 @@ class FastLanguageModel(FastLlamaModel):\\n             rope_scaling   = rope_scaling,\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n+            tokenizer_name = tokenizer_name,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -294,6 +294,7 @@ class FastMistralModel(FastLlamaModel):\\n         rope_scaling   = None, # Mistral does not support RoPE scaling\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -354,8 +355,11 @@ class FastMistralModel(FastLlamaModel):\\n             # rope_scaling      = rope_scaling,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n',\n",
       " '@@ -91,11 +91,13 @@ def _merge_lora(layer, name):\\n         else:\\n             dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n+        # W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n             W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -257,9 +257,9 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n-    pass\\n+    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+    #     chat_template = \"gemma_chatml\"\\n+    # pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -298,8 +298,12 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            if not stop_word in token_mapping.values():\\n+                # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n+                logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n+                string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            pass\\n+\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n             tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n \\n',\n",
       " '@@ -916,6 +916,7 @@ class FastLlamaModel:\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -978,13 +979,16 @@ class FastLlamaModel:\\n             max_position_embeddings = max_position_embeddings,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-\\n+        \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -18,7 +18,7 @@ from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n from peft import PeftConfig, PeftModel\\n from .mapper import INT_TO_FLOAT_MAPPER, FLOAT_TO_INT_MAPPER\\n-\\n+import os\\n \\n # https://github.com/huggingface/transformers/pull/26037 allows 4 bit loading!\\n major, minor = transformers_version.split(\".\")[:2]\\n@@ -118,6 +118,16 @@ class FastLanguageModel(FastLlamaModel):\\n             )\\n         pass\\n \\n+        # Check if this is local model since the tokenizer gets overwritten\\n+        if  os.path.exists(os.path.join(old_model_name, \"tokenizer_config.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"tokenizer.json\")) and \\\\\\n+            os.path.exists(os.path.join(old_model_name, \"special_tokens_map.json\")):\\n+\\n+            tokenizer_name = old_model_name\\n+        else:\\n+            tokenizer_name = None\\n+        pass\\n+\\n         model, tokenizer = dispatch_model.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n@@ -128,6 +138,7 @@ class FastLanguageModel(FastLlamaModel):\\n             rope_scaling   = rope_scaling,\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n+            tokenizer_name = tokenizer_name,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -294,6 +294,7 @@ class FastMistralModel(FastLlamaModel):\\n         rope_scaling   = None, # Mistral does not support RoPE scaling\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n+        tokenizer_name = None,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -354,8 +355,11 @@ class FastMistralModel(FastLlamaModel):\\n             # rope_scaling      = rope_scaling,\\n             **kwargs,\\n         )\\n+\\n+        # Counteract saved tokenizers\\n+        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n-            model_name,\\n+            tokenizer_name,\\n             model_max_length = max_position_embeddings,\\n             padding_side     = \"right\",\\n             token            = token,\\n',\n",
       " '@@ -91,11 +91,13 @@ def _merge_lora(layer, name):\\n         else:\\n             dtype = W.dtype\\n         W = W.to(torch.float32).t()\\n+        # W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n             W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -705,26 +705,24 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if causal_mask is None and past_key_values is None:\\n-            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-        output_hidden_states = (\\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-        )\\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n-\\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-        self.model._has_no_labels = labels is None\\n-\\n-        if past_key_values is not None and \\\\\\n-            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n             )\\n         else:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+    \\n+            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+            output_hidden_states = (\\n+                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+            )\\n+            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+            self.model._has_no_labels = labels is None\\n+\\n             outputs = self.model(\\n                 input_ids=input_ids,\\n                 causal_mask=causal_mask,\\n@@ -988,7 +986,7 @@ class FastLlamaModel:\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-        \\n+\\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -90,14 +90,14 @@ def _merge_lora(layer, name):\\n             W = fast_dequantize(W, quant_state)\\n         else:\\n             dtype = W.dtype\\n-        W = W.to(torch.float32).t()\\n-        # W = W.t()\\n+        # W = W.to(torch.float32).t()\\n+        W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n-            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n-            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n+            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -705,26 +705,24 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if causal_mask is None and past_key_values is None:\\n-            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-        output_hidden_states = (\\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-        )\\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n-\\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-        self.model._has_no_labels = labels is None\\n-\\n-        if past_key_values is not None and \\\\\\n-            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n             )\\n         else:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+    \\n+            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+            output_hidden_states = (\\n+                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+            )\\n+            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+            self.model._has_no_labels = labels is None\\n+\\n             outputs = self.model(\\n                 input_ids=input_ids,\\n                 causal_mask=causal_mask,\\n@@ -988,7 +986,7 @@ class FastLlamaModel:\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-        \\n+\\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -90,14 +90,14 @@ def _merge_lora(layer, name):\\n             W = fast_dequantize(W, quant_state)\\n         else:\\n             dtype = W.dtype\\n-        W = W.to(torch.float32).t()\\n-        # W = W.t()\\n+        # W = W.to(torch.float32).t()\\n+        W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n-            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n-            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n+            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -705,26 +705,24 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if causal_mask is None and past_key_values is None:\\n-            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-        output_hidden_states = (\\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-        )\\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n-\\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-        self.model._has_no_labels = labels is None\\n-\\n-        if past_key_values is not None and \\\\\\n-            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n             )\\n         else:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+    \\n+            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+            output_hidden_states = (\\n+                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+            )\\n+            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+            self.model._has_no_labels = labels is None\\n+\\n             outputs = self.model(\\n                 input_ids=input_ids,\\n                 causal_mask=causal_mask,\\n@@ -988,7 +986,7 @@ class FastLlamaModel:\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-        \\n+\\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -90,14 +90,14 @@ def _merge_lora(layer, name):\\n             W = fast_dequantize(W, quant_state)\\n         else:\\n             dtype = W.dtype\\n-        W = W.to(torch.float32).t()\\n-        # W = W.t()\\n+        # W = W.to(torch.float32).t()\\n+        W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n-            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n-            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n+            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -705,26 +705,24 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if causal_mask is None and past_key_values is None:\\n-            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n-\\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n-        output_hidden_states = (\\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n-        )\\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n-\\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n-        self.model._has_no_labels = labels is None\\n-\\n-        if past_key_values is not None and \\\\\\n-            hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n             )\\n         else:\\n+            causal_mask = xformers.attn_bias.LowerTriangularMask()\\n+    \\n+            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\\n+            output_hidden_states = (\\n+                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\\n+            )\\n+            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n+\\n+            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n+            self.model._has_no_labels = labels is None\\n+\\n             outputs = self.model(\\n                 input_ids=input_ids,\\n                 causal_mask=causal_mask,\\n@@ -988,7 +986,7 @@ class FastLlamaModel:\\n             padding_side     = \"right\",\\n             token            = token,\\n         )\\n-        \\n+\\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n         model = model_patcher.post_patch(model)\\n \\n',\n",
       " '@@ -90,14 +90,14 @@ def _merge_lora(layer, name):\\n             W = fast_dequantize(W, quant_state)\\n         else:\\n             dtype = W.dtype\\n-        W = W.to(torch.float32).t()\\n-        # W = W.t()\\n+        # W = W.to(torch.float32).t()\\n+        W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n-            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n-            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n+            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n',\n",
       " '@@ -253,13 +253,15 @@ def get_chat_template(\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    old_tokenizer = tokenizer\\n+\\n     if map_eos_token is False:\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-    #     chat_template = \"gemma_chatml\"\\n-    # pass\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -340,6 +342,17 @@ def get_chat_template(\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n \\n+    # Also fix up other tokens\\n+    old_pad_token = getattr(old_tokenizer, \"pad_token\", None)\\n+    old_bos_token = getattr(old_tokenizer, \"bos_token\", None)\\n+    old_unk_token = getattr(old_tokenizer, \"unk_token\", None)\\n+    new_pad_token = getattr(tokenizer,     \"pad_token\", None)\\n+    new_bos_token = getattr(tokenizer,     \"bos_token\", None)\\n+    new_unk_token = getattr(tokenizer,     \"unk_token\", None)\\n+    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token\\n+    if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n+    if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n+\\n     #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n \\n     return tokenizer#, stopping_criteria\\n',\n",
       " '@@ -95,7 +95,7 @@ def prepare_model_for_kbit_training(\\n \\n     # Freeze all parameters except LoRA\\n     for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n             param.requires_grad_(True)\\n         else:\\n             param.requires_grad_(False)\\n',\n",
       " '@@ -511,26 +511,36 @@ def LlamaModel_fast_forward(\\n \\n     # Mormalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n+    train_embed_tokens = self.embed_tokens.weight.requires_grad\\n+\\n     if IS_GEMMA:\\n-        inputs_requires_grad = inputs_embeds.requires_grad\\n-        if not inputs_embeds.is_leaf:\\n-            inputs_embeds = inputs_embeds.detach()\\n-            inputs_requires_grad = True\\n-        elif inputs_requires_grad:\\n-            inputs_embeds.requires_grad_(False)\\n-        pass\\n         # Match Gemma exactly by casting to bfloat16 / float16\\n         # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n         # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n-        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n-        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        normalizer = torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+\\n+        if train_embed_tokens:\\n+            # Careful we must not do an inplace op!\\n+            inputs_embeds = inputs_embeds * normalizer\\n+        else:\\n+            inputs_requires_grad = inputs_embeds.requires_grad\\n+            if not inputs_embeds.is_leaf:\\n+                inputs_embeds = inputs_embeds.detach()\\n+                inputs_requires_grad = True\\n+            elif inputs_requires_grad:\\n+                inputs_embeds.requires_grad_(False)\\n+            pass\\n+            inputs_embeds *= normalizer\\n+            # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+            if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        pass\\n     pass\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None) and \\\\\\n+        (not train_embed_tokens):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -1226,6 +1236,7 @@ class FastLlamaModel:\\n         random_state        = 3407,\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n+        modules_to_save     = None,\\n         init_lora_weights   = True,\\n         loftq_config        = {},\\n         **kwargs,\\n@@ -1312,15 +1323,45 @@ class FastLlamaModel:\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        train_lm_head = False\\n+        train_embed_tokens = False\\n+        final_modules = []\\n         for module in target_modules:\\n-            assert(module in accepted_modules)\\n+            if module == \"lm_head\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_lm_head = True\\n+\\n+            elif module == \"embed_tokens\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_embed_tokens = True\\n+\\n+            else:\\n+                assert(module in accepted_modules)\\n+                final_modules.append(module)\\n+        pass\\n+\\n+        # Check modules_to_save\\n+        if modules_to_save is not None:\\n+            for module in modules_to_save:\\n+                if module == \"lm_head\":\\n+                    train_lm_head = True\\n+                elif module == \"embed_tokens\":\\n+                    train_embed_tokens = True\\n+            pass\\n         pass\\n \\n         # Get LoRA\\n         arguments = dict(\\n             r                   = r,\\n             lora_alpha          = lora_alpha,\\n-            target_modules      = target_modules,\\n+            target_modules      = final_modules,\\n             lora_dropout        = lora_dropout,\\n             bias                = bias,\\n             task_type           = TaskType.CAUSAL_LM,\\n@@ -1328,6 +1369,7 @@ class FastLlamaModel:\\n             init_lora_weights   = init_lora_weights,\\n             loftq_config        = loftq_config,\\n             use_rslora          = use_rslora,\\n+            modules_to_save     = modules_to_save,\\n             **kwargs,\\n         )\\n         if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n@@ -1337,6 +1379,14 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+\\n+        # Now patch lm_head and embed_tokens\\n+        if train_embed_tokens:\\n+            model.model.model.embed_tokens.requires_grad_(True)\\n+        if train_lm_head:\\n+            model.model.lm_head.requires_grad_(True)\\n+        pass\\n+\\n         return model\\n     pass\\n \\n@@ -1427,9 +1477,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    (gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None and \\\\\\n-                    (  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None and \\\\\\n-                    (down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None:\\n+                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n+                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n+                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n+                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1448,9 +1501,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    (q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None and \\\\\\n-                    (k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None and \\\\\\n-                    (v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None:\\n+                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n+                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n+                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n+                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1464,7 +1520,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    (o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None:\\n+                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n+                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -203,7 +203,11 @@ def unsloth_save_model(\\n \\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n-        model = model.merge_and_unload()\\n+\\n+        # Counteract no LoRA adapters!\\n+        if hasattr(model, \"merge_and_unload\"):\\n+            model = model.merge_and_unload()\\n+        pass\\n         print(\"Done.\")\\n     pass\\n \\n@@ -573,6 +577,21 @@ def install_llama_cpp_old(version = -10):\\n     latest = releases[-1]\\n     version = releases[version].split(\" \")[0]\\n \\n+    # Check if the llama.cpp exists\\n+    if os.path.exists(\"llama.cpp\"):\\n+        print(\\n+            \"**[WARNING]** You have a llama.cpp old directory which is broken.\\\\n\"\\\\\\n+            \"Unsloth will DELETE the broken directory and install a new one.\\\\n\"\\\\\\n+            \"Press CTRL + C / cancel this if this is wrong. We shall wait 10 seconds.\\\\n\"\\n+        )\\n+        import time\\n+        for i in range(10):\\n+            print(f\"**[WARNING]** Deleting llama.cpp directory... {10-i} seconds left.\")\\n+            time.sleep(1)\\n+        import shutil\\n+        shutil.rmtree(\"llama.cpp\")\\n+    pass\\n+\\n     # Clone a specific commit\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n',\n",
       " '@@ -253,13 +253,15 @@ def get_chat_template(\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    old_tokenizer = tokenizer\\n+\\n     if map_eos_token is False:\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-    #     chat_template = \"gemma_chatml\"\\n-    # pass\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -340,6 +342,17 @@ def get_chat_template(\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n \\n+    # Also fix up other tokens\\n+    old_pad_token = getattr(old_tokenizer, \"pad_token\", None)\\n+    old_bos_token = getattr(old_tokenizer, \"bos_token\", None)\\n+    old_unk_token = getattr(old_tokenizer, \"unk_token\", None)\\n+    new_pad_token = getattr(tokenizer,     \"pad_token\", None)\\n+    new_bos_token = getattr(tokenizer,     \"bos_token\", None)\\n+    new_unk_token = getattr(tokenizer,     \"unk_token\", None)\\n+    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token\\n+    if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n+    if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n+\\n     #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n \\n     return tokenizer#, stopping_criteria\\n',\n",
       " '@@ -95,7 +95,7 @@ def prepare_model_for_kbit_training(\\n \\n     # Freeze all parameters except LoRA\\n     for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n             param.requires_grad_(True)\\n         else:\\n             param.requires_grad_(False)\\n',\n",
       " '@@ -511,26 +511,36 @@ def LlamaModel_fast_forward(\\n \\n     # Mormalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n+    train_embed_tokens = self.embed_tokens.weight.requires_grad\\n+\\n     if IS_GEMMA:\\n-        inputs_requires_grad = inputs_embeds.requires_grad\\n-        if not inputs_embeds.is_leaf:\\n-            inputs_embeds = inputs_embeds.detach()\\n-            inputs_requires_grad = True\\n-        elif inputs_requires_grad:\\n-            inputs_embeds.requires_grad_(False)\\n-        pass\\n         # Match Gemma exactly by casting to bfloat16 / float16\\n         # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n         # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n-        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n-        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        normalizer = torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+\\n+        if train_embed_tokens:\\n+            # Careful we must not do an inplace op!\\n+            inputs_embeds = inputs_embeds * normalizer\\n+        else:\\n+            inputs_requires_grad = inputs_embeds.requires_grad\\n+            if not inputs_embeds.is_leaf:\\n+                inputs_embeds = inputs_embeds.detach()\\n+                inputs_requires_grad = True\\n+            elif inputs_requires_grad:\\n+                inputs_embeds.requires_grad_(False)\\n+            pass\\n+            inputs_embeds *= normalizer\\n+            # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+            if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        pass\\n     pass\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None) and \\\\\\n+        (not train_embed_tokens):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -1226,6 +1236,7 @@ class FastLlamaModel:\\n         random_state        = 3407,\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n+        modules_to_save     = None,\\n         init_lora_weights   = True,\\n         loftq_config        = {},\\n         **kwargs,\\n@@ -1312,15 +1323,45 @@ class FastLlamaModel:\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        train_lm_head = False\\n+        train_embed_tokens = False\\n+        final_modules = []\\n         for module in target_modules:\\n-            assert(module in accepted_modules)\\n+            if module == \"lm_head\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_lm_head = True\\n+\\n+            elif module == \"embed_tokens\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_embed_tokens = True\\n+\\n+            else:\\n+                assert(module in accepted_modules)\\n+                final_modules.append(module)\\n+        pass\\n+\\n+        # Check modules_to_save\\n+        if modules_to_save is not None:\\n+            for module in modules_to_save:\\n+                if module == \"lm_head\":\\n+                    train_lm_head = True\\n+                elif module == \"embed_tokens\":\\n+                    train_embed_tokens = True\\n+            pass\\n         pass\\n \\n         # Get LoRA\\n         arguments = dict(\\n             r                   = r,\\n             lora_alpha          = lora_alpha,\\n-            target_modules      = target_modules,\\n+            target_modules      = final_modules,\\n             lora_dropout        = lora_dropout,\\n             bias                = bias,\\n             task_type           = TaskType.CAUSAL_LM,\\n@@ -1328,6 +1369,7 @@ class FastLlamaModel:\\n             init_lora_weights   = init_lora_weights,\\n             loftq_config        = loftq_config,\\n             use_rslora          = use_rslora,\\n+            modules_to_save     = modules_to_save,\\n             **kwargs,\\n         )\\n         if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n@@ -1337,6 +1379,14 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+\\n+        # Now patch lm_head and embed_tokens\\n+        if train_embed_tokens:\\n+            model.model.model.embed_tokens.requires_grad_(True)\\n+        if train_lm_head:\\n+            model.model.lm_head.requires_grad_(True)\\n+        pass\\n+\\n         return model\\n     pass\\n \\n@@ -1427,9 +1477,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    (gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None and \\\\\\n-                    (  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None and \\\\\\n-                    (down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None:\\n+                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n+                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n+                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n+                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1448,9 +1501,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    (q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None and \\\\\\n-                    (k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None and \\\\\\n-                    (v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None:\\n+                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n+                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n+                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n+                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1464,7 +1520,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    (o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None:\\n+                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n+                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -203,7 +203,11 @@ def unsloth_save_model(\\n \\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n-        model = model.merge_and_unload()\\n+\\n+        # Counteract no LoRA adapters!\\n+        if hasattr(model, \"merge_and_unload\"):\\n+            model = model.merge_and_unload()\\n+        pass\\n         print(\"Done.\")\\n     pass\\n \\n@@ -573,6 +577,21 @@ def install_llama_cpp_old(version = -10):\\n     latest = releases[-1]\\n     version = releases[version].split(\" \")[0]\\n \\n+    # Check if the llama.cpp exists\\n+    if os.path.exists(\"llama.cpp\"):\\n+        print(\\n+            \"**[WARNING]** You have a llama.cpp old directory which is broken.\\\\n\"\\\\\\n+            \"Unsloth will DELETE the broken directory and install a new one.\\\\n\"\\\\\\n+            \"Press CTRL + C / cancel this if this is wrong. We shall wait 10 seconds.\\\\n\"\\n+        )\\n+        import time\\n+        for i in range(10):\\n+            print(f\"**[WARNING]** Deleting llama.cpp directory... {10-i} seconds left.\")\\n+            time.sleep(1)\\n+        import shutil\\n+        shutil.rmtree(\"llama.cpp\")\\n+    pass\\n+\\n     # Clone a specific commit\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n',\n",
       " '@@ -253,13 +253,15 @@ def get_chat_template(\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    old_tokenizer = tokenizer\\n+\\n     if map_eos_token is False:\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-    #     chat_template = \"gemma_chatml\"\\n-    # pass\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -340,6 +342,17 @@ def get_chat_template(\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n \\n+    # Also fix up other tokens\\n+    old_pad_token = getattr(old_tokenizer, \"pad_token\", None)\\n+    old_bos_token = getattr(old_tokenizer, \"bos_token\", None)\\n+    old_unk_token = getattr(old_tokenizer, \"unk_token\", None)\\n+    new_pad_token = getattr(tokenizer,     \"pad_token\", None)\\n+    new_bos_token = getattr(tokenizer,     \"bos_token\", None)\\n+    new_unk_token = getattr(tokenizer,     \"unk_token\", None)\\n+    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token\\n+    if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n+    if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n+\\n     #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n \\n     return tokenizer#, stopping_criteria\\n',\n",
       " '@@ -95,7 +95,7 @@ def prepare_model_for_kbit_training(\\n \\n     # Freeze all parameters except LoRA\\n     for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n             param.requires_grad_(True)\\n         else:\\n             param.requires_grad_(False)\\n',\n",
       " '@@ -511,26 +511,36 @@ def LlamaModel_fast_forward(\\n \\n     # Mormalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n+    train_embed_tokens = self.embed_tokens.weight.requires_grad\\n+\\n     if IS_GEMMA:\\n-        inputs_requires_grad = inputs_embeds.requires_grad\\n-        if not inputs_embeds.is_leaf:\\n-            inputs_embeds = inputs_embeds.detach()\\n-            inputs_requires_grad = True\\n-        elif inputs_requires_grad:\\n-            inputs_embeds.requires_grad_(False)\\n-        pass\\n         # Match Gemma exactly by casting to bfloat16 / float16\\n         # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n         # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n-        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n-        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        normalizer = torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+\\n+        if train_embed_tokens:\\n+            # Careful we must not do an inplace op!\\n+            inputs_embeds = inputs_embeds * normalizer\\n+        else:\\n+            inputs_requires_grad = inputs_embeds.requires_grad\\n+            if not inputs_embeds.is_leaf:\\n+                inputs_embeds = inputs_embeds.detach()\\n+                inputs_requires_grad = True\\n+            elif inputs_requires_grad:\\n+                inputs_embeds.requires_grad_(False)\\n+            pass\\n+            inputs_embeds *= normalizer\\n+            # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+            if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        pass\\n     pass\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None) and \\\\\\n+        (not train_embed_tokens):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -1226,6 +1236,7 @@ class FastLlamaModel:\\n         random_state        = 3407,\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n+        modules_to_save     = None,\\n         init_lora_weights   = True,\\n         loftq_config        = {},\\n         **kwargs,\\n@@ -1312,15 +1323,45 @@ class FastLlamaModel:\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        train_lm_head = False\\n+        train_embed_tokens = False\\n+        final_modules = []\\n         for module in target_modules:\\n-            assert(module in accepted_modules)\\n+            if module == \"lm_head\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_lm_head = True\\n+\\n+            elif module == \"embed_tokens\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_embed_tokens = True\\n+\\n+            else:\\n+                assert(module in accepted_modules)\\n+                final_modules.append(module)\\n+        pass\\n+\\n+        # Check modules_to_save\\n+        if modules_to_save is not None:\\n+            for module in modules_to_save:\\n+                if module == \"lm_head\":\\n+                    train_lm_head = True\\n+                elif module == \"embed_tokens\":\\n+                    train_embed_tokens = True\\n+            pass\\n         pass\\n \\n         # Get LoRA\\n         arguments = dict(\\n             r                   = r,\\n             lora_alpha          = lora_alpha,\\n-            target_modules      = target_modules,\\n+            target_modules      = final_modules,\\n             lora_dropout        = lora_dropout,\\n             bias                = bias,\\n             task_type           = TaskType.CAUSAL_LM,\\n@@ -1328,6 +1369,7 @@ class FastLlamaModel:\\n             init_lora_weights   = init_lora_weights,\\n             loftq_config        = loftq_config,\\n             use_rslora          = use_rslora,\\n+            modules_to_save     = modules_to_save,\\n             **kwargs,\\n         )\\n         if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n@@ -1337,6 +1379,14 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+\\n+        # Now patch lm_head and embed_tokens\\n+        if train_embed_tokens:\\n+            model.model.model.embed_tokens.requires_grad_(True)\\n+        if train_lm_head:\\n+            model.model.lm_head.requires_grad_(True)\\n+        pass\\n+\\n         return model\\n     pass\\n \\n@@ -1427,9 +1477,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    (gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None and \\\\\\n-                    (  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None and \\\\\\n-                    (down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None:\\n+                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n+                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n+                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n+                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1448,9 +1501,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    (q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None and \\\\\\n-                    (k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None and \\\\\\n-                    (v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None:\\n+                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n+                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n+                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n+                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1464,7 +1520,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    (o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None:\\n+                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n+                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -203,7 +203,11 @@ def unsloth_save_model(\\n \\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n-        model = model.merge_and_unload()\\n+\\n+        # Counteract no LoRA adapters!\\n+        if hasattr(model, \"merge_and_unload\"):\\n+            model = model.merge_and_unload()\\n+        pass\\n         print(\"Done.\")\\n     pass\\n \\n@@ -573,6 +577,21 @@ def install_llama_cpp_old(version = -10):\\n     latest = releases[-1]\\n     version = releases[version].split(\" \")[0]\\n \\n+    # Check if the llama.cpp exists\\n+    if os.path.exists(\"llama.cpp\"):\\n+        print(\\n+            \"**[WARNING]** You have a llama.cpp old directory which is broken.\\\\n\"\\\\\\n+            \"Unsloth will DELETE the broken directory and install a new one.\\\\n\"\\\\\\n+            \"Press CTRL + C / cancel this if this is wrong. We shall wait 10 seconds.\\\\n\"\\n+        )\\n+        import time\\n+        for i in range(10):\\n+            print(f\"**[WARNING]** Deleting llama.cpp directory... {10-i} seconds left.\")\\n+            time.sleep(1)\\n+        import shutil\\n+        shutil.rmtree(\"llama.cpp\")\\n+    pass\\n+\\n     # Clone a specific commit\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n',\n",
       " '@@ -253,13 +253,15 @@ def get_chat_template(\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    old_tokenizer = tokenizer\\n+\\n     if map_eos_token is False:\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    # if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-    #     chat_template = \"gemma_chatml\"\\n-    # pass\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n+        chat_template = \"gemma_chatml\"\\n+    pass\\n \\n     old_padding_side = tokenizer.padding_side\\n \\n@@ -340,6 +342,17 @@ def get_chat_template(\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n \\n+    # Also fix up other tokens\\n+    old_pad_token = getattr(old_tokenizer, \"pad_token\", None)\\n+    old_bos_token = getattr(old_tokenizer, \"bos_token\", None)\\n+    old_unk_token = getattr(old_tokenizer, \"unk_token\", None)\\n+    new_pad_token = getattr(tokenizer,     \"pad_token\", None)\\n+    new_bos_token = getattr(tokenizer,     \"bos_token\", None)\\n+    new_unk_token = getattr(tokenizer,     \"unk_token\", None)\\n+    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token\\n+    if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n+    if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n+\\n     #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n \\n     return tokenizer#, stopping_criteria\\n',\n",
       " '@@ -95,7 +95,7 @@ def prepare_model_for_kbit_training(\\n \\n     # Freeze all parameters except LoRA\\n     for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name:\\n+        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n             param.requires_grad_(True)\\n         else:\\n             param.requires_grad_(False)\\n',\n",
       " '@@ -511,26 +511,36 @@ def LlamaModel_fast_forward(\\n \\n     # Mormalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n+    train_embed_tokens = self.embed_tokens.weight.requires_grad\\n+\\n     if IS_GEMMA:\\n-        inputs_requires_grad = inputs_embeds.requires_grad\\n-        if not inputs_embeds.is_leaf:\\n-            inputs_embeds = inputs_embeds.detach()\\n-            inputs_requires_grad = True\\n-        elif inputs_requires_grad:\\n-            inputs_embeds.requires_grad_(False)\\n-        pass\\n         # Match Gemma exactly by casting to bfloat16 / float16\\n         # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n         # Ie 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n         # &  2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n-        inputs_embeds *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n-        # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n-        if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        normalizer = torch.tensor(math_sqrt(self.config.hidden_size), dtype = inputs_embeds.dtype)\\n+\\n+        if train_embed_tokens:\\n+            # Careful we must not do an inplace op!\\n+            inputs_embeds = inputs_embeds * normalizer\\n+        else:\\n+            inputs_requires_grad = inputs_embeds.requires_grad\\n+            if not inputs_embeds.is_leaf:\\n+                inputs_embeds = inputs_embeds.detach()\\n+                inputs_requires_grad = True\\n+            elif inputs_requires_grad:\\n+                inputs_embeds.requires_grad_(False)\\n+            pass\\n+            inputs_embeds *= normalizer\\n+            # inputs_embeds *= math_sqrt(self.config.hidden_size)\\n+            if inputs_requires_grad: inputs_embeds.requires_grad_(True)\\n+        pass\\n     pass\\n \\n     # Fix up attention mask by setting elements to 0\\n     # Specifically for DPO\\n-    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None):\\n+    if self._has_no_labels and (attention_mask is not None) and (past_key_values is None) and \\\\\\n+        (not train_embed_tokens):\\n         # Careful for inference the attention_mask is size (1, kv_seq_len)\\n         # Whilst the input_embeds is size (1, 1, 4096)\\n         inputs_requires_grad = inputs_embeds.requires_grad\\n@@ -1226,6 +1236,7 @@ class FastLlamaModel:\\n         random_state        = 3407,\\n         max_seq_length      = 2048, # not used anymore\\n         use_rslora          = False,\\n+        modules_to_save     = None,\\n         init_lora_weights   = True,\\n         loftq_config        = {},\\n         **kwargs,\\n@@ -1312,15 +1323,45 @@ class FastLlamaModel:\\n         accepted_modules = frozenset((\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n+\\n+        train_lm_head = False\\n+        train_embed_tokens = False\\n+        final_modules = []\\n         for module in target_modules:\\n-            assert(module in accepted_modules)\\n+            if module == \"lm_head\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_lm_head = True\\n+\\n+            elif module == \"embed_tokens\":\\n+                logger.warning_once(\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n+                    \"We shall do it for you!\"\\n+                )\\n+                train_embed_tokens = True\\n+\\n+            else:\\n+                assert(module in accepted_modules)\\n+                final_modules.append(module)\\n+        pass\\n+\\n+        # Check modules_to_save\\n+        if modules_to_save is not None:\\n+            for module in modules_to_save:\\n+                if module == \"lm_head\":\\n+                    train_lm_head = True\\n+                elif module == \"embed_tokens\":\\n+                    train_embed_tokens = True\\n+            pass\\n         pass\\n \\n         # Get LoRA\\n         arguments = dict(\\n             r                   = r,\\n             lora_alpha          = lora_alpha,\\n-            target_modules      = target_modules,\\n+            target_modules      = final_modules,\\n             lora_dropout        = lora_dropout,\\n             bias                = bias,\\n             task_type           = TaskType.CAUSAL_LM,\\n@@ -1328,6 +1369,7 @@ class FastLlamaModel:\\n             init_lora_weights   = init_lora_weights,\\n             loftq_config        = loftq_config,\\n             use_rslora          = use_rslora,\\n+            modules_to_save     = modules_to_save,\\n             **kwargs,\\n         )\\n         if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n@@ -1337,6 +1379,14 @@ class FastLlamaModel:\\n         model = _get_peft_model(model, lora_config)\\n \\n         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n+\\n+        # Now patch lm_head and embed_tokens\\n+        if train_embed_tokens:\\n+            model.model.model.embed_tokens.requires_grad_(True)\\n+        if train_lm_head:\\n+            model.model.lm_head.requires_grad_(True)\\n+        pass\\n+\\n         return model\\n     pass\\n \\n@@ -1427,9 +1477,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    (gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None and \\\\\\n-                    (  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None and \\\\\\n-                    (down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None:\\n+                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n+                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n+                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n+                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1448,9 +1501,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    (q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None and \\\\\\n-                    (k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None and \\\\\\n-                    (v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None:\\n+                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n+                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n+                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n+                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n+                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1464,7 +1520,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    (o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None:\\n+                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n+                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -203,7 +203,11 @@ def unsloth_save_model(\\n \\n         print(\"Unsloth: Merging 4bit and LoRA weights to 4bit...\")\\n         print(\"This might take 5 minutes...\")\\n-        model = model.merge_and_unload()\\n+\\n+        # Counteract no LoRA adapters!\\n+        if hasattr(model, \"merge_and_unload\"):\\n+            model = model.merge_and_unload()\\n+        pass\\n         print(\"Done.\")\\n     pass\\n \\n@@ -573,6 +577,21 @@ def install_llama_cpp_old(version = -10):\\n     latest = releases[-1]\\n     version = releases[version].split(\" \")[0]\\n \\n+    # Check if the llama.cpp exists\\n+    if os.path.exists(\"llama.cpp\"):\\n+        print(\\n+            \"**[WARNING]** You have a llama.cpp old directory which is broken.\\\\n\"\\\\\\n+            \"Unsloth will DELETE the broken directory and install a new one.\\\\n\"\\\\\\n+            \"Press CTRL + C / cancel this if this is wrong. We shall wait 10 seconds.\\\\n\"\\n+        )\\n+        import time\\n+        for i in range(10):\\n+            print(f\"**[WARNING]** Deleting llama.cpp directory... {10-i} seconds left.\")\\n+            time.sleep(1)\\n+        import shutil\\n+        shutil.rmtree(\"llama.cpp\")\\n+    pass\\n+\\n     # Clone a specific commit\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n',\n",
       " '@@ -59,7 +59,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n - Works on **Linux** and **Windows** via WSL.\\n - Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for up to **30x faster training**!\\n - If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n \\n \\n',\n",
       " '@@ -33,17 +33,17 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n+    \"tyro\",\\n     \"transformers>=4.38.2\",\\n     \"datasets>=2.16.0\",\\n     \"sentencepiece\",\\n-    \"accelerate>=0.26.1\",\\n-    \"trl>=0.7.9\",\\n-    \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n-    \"triton\",\\n+    \"accelerate>=0.26.1\",\\n+    \"trl>=0.7.9\",\\n+    \"peft>=0.7.1\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -128,17 +128,12 @@ cu121-torch220 = [\\n kaggle = [\\n     \"unsloth[huggingface]\",\\n ]\\n-conda = [\\n+kaggle-new = [\\n     \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n ]\\n-colab = [\\n-    \"unsloth[cu121]\",\\n-]\\n-colab-ampere = [\\n-    \"unsloth[cu121]\",\\n-    \"packaging\",\\n-    \"ninja\",\\n-    \"flash-attn\",\\n+conda = [\\n+    \"unsloth[huggingface]\",\\n ]\\n colab-torch211 = [\\n     \"unsloth[huggingface]\",\\n@@ -166,6 +161,29 @@ colab-ampere-torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n+colab-new = [\\n+    \"tyro\",\\n+    \"transformers>=4.38.2\",\\n+    \"datasets>=2.16.0\",\\n+    \"sentencepiece\",\\n+    \"tqdm\",\\n+    \"psutil\",\\n+    \"wheel>=0.42.0\",\\n+    \"numpy\",\\n+]\\n+colab-no-deps = [\\n+    \"accelerate>=0.26.1\",\\n+    \"trl>=0.7.9\",\\n+    \"peft>=0.7.1\",\\n+    \"xformers\",\\n+    \"bitsandbytes\",\\n+]\\n+colab-ampere = [\\n+    \"unsloth[colab-ampere-torch220]\",\\n+    \"packaging\",\\n+    \"ninja\",\\n+    \"flash-attn\",\\n+]\\n cu118-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n',\n",
       " '@@ -259,8 +259,10 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n+    IS_GEMMA = False\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n+        if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n+        IS_GEMMA = True\\n     pass\\n \\n     old_padding_side = tokenizer.padding_side\\n@@ -338,6 +340,12 @@ def get_chat_template(\\n         .replace(\"\\'user\\'\",      \"\\'\" + mapping[\"user\"]      + \"\\'\")\\\\\\n         .replace(\"\\'assistant\\'\", \"\\'\" + mapping[\"assistant\"] + \"\\'\")\\n \\n+    # Careful on Gemma\\n+    # bos_token is a must or else losses become too high\\n+    if IS_GEMMA and not chat_template.startswith(\"{{ bos_token }}\"):\\n+        chat_template = \"{{ bos_token }}\" + chat_template\\n+    pass\\n+\\n     _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n',\n",
       " '@@ -154,6 +154,7 @@ def GemmaModel_fast_forward_inference(\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n',\n",
       " '@@ -509,7 +509,10 @@ def LlamaModel_fast_forward(\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Mormalized from Gemma\\n+    # Downcast to the correct dtype ie float32 to float16\\n+    inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n+\\n+    # Normalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n     train_embed_tokens = self.embed_tokens.weight.requires_grad\\n \\n@@ -665,6 +668,7 @@ def LlamaModel_fast_forward_inference(\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = hidden_states.to(self.config.torch_dtype)\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -1334,6 +1338,7 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1341,6 +1346,7 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1477,12 +1483,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n-                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n-                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n-                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(gate_proj, \"base_layer\", gate_proj).bias is None) and \\\\\\n+                    (getattr(  up_proj, \"base_layer\",   up_proj).bias is None) and \\\\\\n+                    (getattr(down_proj, \"base_layer\", down_proj).bias is None) and \\\\\\n+                    (getattr(gate_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(  up_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(down_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1501,12 +1507,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n-                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n-                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n-                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(q_proj, \"base_layer\", q_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"base_layer\", k_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"base_layer\", v_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(k_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(v_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1520,8 +1526,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n-                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(o_proj, \"base_layer\", o_proj).bias is None) and \\\\\\n+                    (getattr(o_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -632,6 +632,7 @@ pass\\n \\n \\n def save_to_gguf(\\n+    model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -639,10 +640,18 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    # Careful convert.py is only for Llama / Mistral based archs\\n+    use_fast_convert = False\\n+    if   model_type == \"llama\":   use_fast_convert = True\\n+    elif model_type == \"mistral\": use_fast_convert = True\\n+    pass\\n+    logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n+\\n     if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n     elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n     elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    pass\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -692,6 +701,12 @@ def save_to_gguf(\\n         pass\\n     pass\\n \\n+    # Non llama/mistral needs can only use f32 or f16\\n+    if not use_fast_convert and (first_conversion != \"f16\" or first_conversion != \"f32\"):\\n+        logger.warning_once(\"Unsloth: We must use f16 for non Llama and Mistral models.\")\\n+        first_conversion = \"f16\"\\n+    pass\\n+\\n     n_cpus = psutil.cpu_count()\\n     if n_cpus is None: n_cpus = 1\\n     n_cpus *= 2\\n@@ -703,9 +718,15 @@ def save_to_gguf(\\n           f\"The output location will be {final_location}\\\\n\"\\\\\\n           \"This will take 3 minutes...\")\\n \\n-    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-        f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n-        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+    if use_fast_convert:\\n+        command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+            f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n+            f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+    else:\\n+        command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n+            f\"--outfile {final_location} \"\\\\\\n+            f\"--outtype {first_conversion}\"\\n+    pass\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n@@ -1054,7 +1075,8 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n+    model_type = self.config.model_type\\n+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1154,7 +1176,8 @@ def unsloth_push_to_hub_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n+    model_type = self.config.model_type\\n+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -59,7 +59,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n - Works on **Linux** and **Windows** via WSL.\\n - Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for up to **30x faster training**!\\n - If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n \\n \\n',\n",
       " '@@ -33,17 +33,17 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n+    \"tyro\",\\n     \"transformers>=4.38.2\",\\n     \"datasets>=2.16.0\",\\n     \"sentencepiece\",\\n-    \"accelerate>=0.26.1\",\\n-    \"trl>=0.7.9\",\\n-    \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n-    \"triton\",\\n+    \"accelerate>=0.26.1\",\\n+    \"trl>=0.7.9\",\\n+    \"peft>=0.7.1\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -128,17 +128,12 @@ cu121-torch220 = [\\n kaggle = [\\n     \"unsloth[huggingface]\",\\n ]\\n-conda = [\\n+kaggle-new = [\\n     \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n ]\\n-colab = [\\n-    \"unsloth[cu121]\",\\n-]\\n-colab-ampere = [\\n-    \"unsloth[cu121]\",\\n-    \"packaging\",\\n-    \"ninja\",\\n-    \"flash-attn\",\\n+conda = [\\n+    \"unsloth[huggingface]\",\\n ]\\n colab-torch211 = [\\n     \"unsloth[huggingface]\",\\n@@ -166,6 +161,29 @@ colab-ampere-torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n+colab-new = [\\n+    \"tyro\",\\n+    \"transformers>=4.38.2\",\\n+    \"datasets>=2.16.0\",\\n+    \"sentencepiece\",\\n+    \"tqdm\",\\n+    \"psutil\",\\n+    \"wheel>=0.42.0\",\\n+    \"numpy\",\\n+]\\n+colab-no-deps = [\\n+    \"accelerate>=0.26.1\",\\n+    \"trl>=0.7.9\",\\n+    \"peft>=0.7.1\",\\n+    \"xformers\",\\n+    \"bitsandbytes\",\\n+]\\n+colab-ampere = [\\n+    \"unsloth[colab-ampere-torch220]\",\\n+    \"packaging\",\\n+    \"ninja\",\\n+    \"flash-attn\",\\n+]\\n cu118-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n',\n",
       " '@@ -259,8 +259,10 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n+    IS_GEMMA = False\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n+        if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n+        IS_GEMMA = True\\n     pass\\n \\n     old_padding_side = tokenizer.padding_side\\n@@ -338,6 +340,12 @@ def get_chat_template(\\n         .replace(\"\\'user\\'\",      \"\\'\" + mapping[\"user\"]      + \"\\'\")\\\\\\n         .replace(\"\\'assistant\\'\", \"\\'\" + mapping[\"assistant\"] + \"\\'\")\\n \\n+    # Careful on Gemma\\n+    # bos_token is a must or else losses become too high\\n+    if IS_GEMMA and not chat_template.startswith(\"{{ bos_token }}\"):\\n+        chat_template = \"{{ bos_token }}\" + chat_template\\n+    pass\\n+\\n     _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n',\n",
       " '@@ -154,6 +154,7 @@ def GemmaModel_fast_forward_inference(\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n',\n",
       " '@@ -509,7 +509,10 @@ def LlamaModel_fast_forward(\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Mormalized from Gemma\\n+    # Downcast to the correct dtype ie float32 to float16\\n+    inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n+\\n+    # Normalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n     train_embed_tokens = self.embed_tokens.weight.requires_grad\\n \\n@@ -665,6 +668,7 @@ def LlamaModel_fast_forward_inference(\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = hidden_states.to(self.config.torch_dtype)\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -1334,6 +1338,7 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1341,6 +1346,7 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1477,12 +1483,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n-                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n-                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n-                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(gate_proj, \"base_layer\", gate_proj).bias is None) and \\\\\\n+                    (getattr(  up_proj, \"base_layer\",   up_proj).bias is None) and \\\\\\n+                    (getattr(down_proj, \"base_layer\", down_proj).bias is None) and \\\\\\n+                    (getattr(gate_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(  up_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(down_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1501,12 +1507,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n-                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n-                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n-                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(q_proj, \"base_layer\", q_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"base_layer\", k_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"base_layer\", v_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(k_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(v_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1520,8 +1526,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n-                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(o_proj, \"base_layer\", o_proj).bias is None) and \\\\\\n+                    (getattr(o_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -632,6 +632,7 @@ pass\\n \\n \\n def save_to_gguf(\\n+    model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -639,10 +640,18 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    # Careful convert.py is only for Llama / Mistral based archs\\n+    use_fast_convert = False\\n+    if   model_type == \"llama\":   use_fast_convert = True\\n+    elif model_type == \"mistral\": use_fast_convert = True\\n+    pass\\n+    logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n+\\n     if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n     elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n     elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    pass\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -692,6 +701,12 @@ def save_to_gguf(\\n         pass\\n     pass\\n \\n+    # Non llama/mistral needs can only use f32 or f16\\n+    if not use_fast_convert and (first_conversion != \"f16\" or first_conversion != \"f32\"):\\n+        logger.warning_once(\"Unsloth: We must use f16 for non Llama and Mistral models.\")\\n+        first_conversion = \"f16\"\\n+    pass\\n+\\n     n_cpus = psutil.cpu_count()\\n     if n_cpus is None: n_cpus = 1\\n     n_cpus *= 2\\n@@ -703,9 +718,15 @@ def save_to_gguf(\\n           f\"The output location will be {final_location}\\\\n\"\\\\\\n           \"This will take 3 minutes...\")\\n \\n-    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-        f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n-        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+    if use_fast_convert:\\n+        command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+            f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n+            f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+    else:\\n+        command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n+            f\"--outfile {final_location} \"\\\\\\n+            f\"--outtype {first_conversion}\"\\n+    pass\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n@@ -1054,7 +1075,8 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n+    model_type = self.config.model_type\\n+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1154,7 +1176,8 @@ def unsloth_push_to_hub_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n+    model_type = self.config.model_type\\n+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -59,7 +59,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\\n - Works on **Linux** and **Windows** via WSL.\\n - Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).\\n-- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for **30x faster training**!\\n+- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for up to **30x faster training**!\\n - If you trained a model with 🦥Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" height=\"50\" align=\"center\" />\\n \\n \\n',\n",
       " '@@ -33,17 +33,17 @@ exclude = [\"images*\"]\\n \\n [project.optional-dependencies]\\n huggingface = [\\n+    \"tyro\",\\n     \"transformers>=4.38.2\",\\n     \"datasets>=2.16.0\",\\n     \"sentencepiece\",\\n-    \"accelerate>=0.26.1\",\\n-    \"trl>=0.7.9\",\\n-    \"peft>=0.7.1\",\\n     \"tqdm\",\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n-    \"triton\",\\n+    \"accelerate>=0.26.1\",\\n+    \"trl>=0.7.9\",\\n+    \"peft>=0.7.1\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -128,17 +128,12 @@ cu121-torch220 = [\\n kaggle = [\\n     \"unsloth[huggingface]\",\\n ]\\n-conda = [\\n+kaggle-new = [\\n     \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n ]\\n-colab = [\\n-    \"unsloth[cu121]\",\\n-]\\n-colab-ampere = [\\n-    \"unsloth[cu121]\",\\n-    \"packaging\",\\n-    \"ninja\",\\n-    \"flash-attn\",\\n+conda = [\\n+    \"unsloth[huggingface]\",\\n ]\\n colab-torch211 = [\\n     \"unsloth[huggingface]\",\\n@@ -166,6 +161,29 @@ colab-ampere-torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n+colab-new = [\\n+    \"tyro\",\\n+    \"transformers>=4.38.2\",\\n+    \"datasets>=2.16.0\",\\n+    \"sentencepiece\",\\n+    \"tqdm\",\\n+    \"psutil\",\\n+    \"wheel>=0.42.0\",\\n+    \"numpy\",\\n+]\\n+colab-no-deps = [\\n+    \"accelerate>=0.26.1\",\\n+    \"trl>=0.7.9\",\\n+    \"peft>=0.7.1\",\\n+    \"xformers\",\\n+    \"bitsandbytes\",\\n+]\\n+colab-ampere = [\\n+    \"unsloth[colab-ampere-torch220]\",\\n+    \"packaging\",\\n+    \"ninja\",\\n+    \"flash-attn\",\\n+]\\n cu118-ampere = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n',\n",
       " '@@ -259,8 +259,10 @@ def get_chat_template(\\n         assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n     pass\\n \\n-    if tokenizer.__class__.__name__.startswith(\"Gemma\") and chat_template == \"chatml\":\\n-        chat_template = \"gemma_chatml\"\\n+    IS_GEMMA = False\\n+    if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n+        if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n+        IS_GEMMA = True\\n     pass\\n \\n     old_padding_side = tokenizer.padding_side\\n@@ -338,6 +340,12 @@ def get_chat_template(\\n         .replace(\"\\'user\\'\",      \"\\'\" + mapping[\"user\"]      + \"\\'\")\\\\\\n         .replace(\"\\'assistant\\'\", \"\\'\" + mapping[\"assistant\"] + \"\\'\")\\n \\n+    # Careful on Gemma\\n+    # bos_token is a must or else losses become too high\\n+    if IS_GEMMA and not chat_template.startswith(\"{{ bos_token }}\"):\\n+        chat_template = \"{{ bos_token }}\" + chat_template\\n+    pass\\n+\\n     _, tokenizer = patch_tokenizer(model = None, tokenizer = tokenizer)\\n     tokenizer.padding_side  = old_padding_side\\n     tokenizer.chat_template = chat_template\\n',\n",
       " '@@ -154,6 +154,7 @@ def GemmaModel_fast_forward_inference(\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n',\n",
       " '@@ -509,7 +509,10 @@ def LlamaModel_fast_forward(\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Mormalized from Gemma\\n+    # Downcast to the correct dtype ie float32 to float16\\n+    inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n+\\n+    # Normalized from Gemma\\n     IS_GEMMA = self.config.model_type == \"gemma\"\\n     train_embed_tokens = self.embed_tokens.weight.requires_grad\\n \\n@@ -665,6 +668,7 @@ def LlamaModel_fast_forward_inference(\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = hidden_states.to(self.config.torch_dtype)\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -1334,6 +1338,7 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1341,6 +1346,7 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1477,12 +1483,12 @@ class FastLlamaModel:\\n                 if  hasattr(gate_proj, \"lora_A\") and \\\\\\n                     hasattr(  up_proj, \"lora_A\") and \\\\\\n                     hasattr(down_proj, \"lora_A\") and \\\\\\n-                    ((gate_proj.base_layer if hasattr(gate_proj, \"base_layer\") else gate_proj).bias is None) and \\\\\\n-                    ((  up_proj.base_layer if hasattr(  up_proj, \"base_layer\") else   up_proj).bias is None) and \\\\\\n-                    ((down_proj.base_layer if hasattr(down_proj, \"base_layer\") else down_proj).bias is None) and \\\\\\n-                    ((gate_proj.lora_magnitude_vector if hasattr(gate_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((  up_proj.lora_magnitude_vector if hasattr(  up_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((down_proj.lora_magnitude_vector if hasattr(down_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(gate_proj, \"base_layer\", gate_proj).bias is None) and \\\\\\n+                    (getattr(  up_proj, \"base_layer\",   up_proj).bias is None) and \\\\\\n+                    (getattr(down_proj, \"base_layer\", down_proj).bias is None) and \\\\\\n+                    (getattr(gate_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(  up_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(down_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     # https://stackoverflow.com/questions/50599045/python-replacing-a-function-within-a-class-of-a-module\\n                     layer.mlp.forward = types.MethodType(apply_lora_mlp, layer.mlp)\\n@@ -1501,12 +1507,12 @@ class FastLlamaModel:\\n                 if  hasattr(q_proj, \"lora_A\") and \\\\\\n                     hasattr(k_proj, \"lora_A\") and \\\\\\n                     hasattr(v_proj, \"lora_A\") and \\\\\\n-                    ((q_proj.base_layer if hasattr(q_proj, \"base_layer\") else q_proj).bias is None) and \\\\\\n-                    ((k_proj.base_layer if hasattr(k_proj, \"base_layer\") else k_proj).bias is None) and \\\\\\n-                    ((v_proj.base_layer if hasattr(v_proj, \"base_layer\") else v_proj).bias is None) and \\\\\\n-                    ((q_proj.lora_magnitude_vector if hasattr(q_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((k_proj.lora_magnitude_vector if hasattr(k_proj, \"lora_magnitude_vector\") else None) is None) and \\\\\\n-                    ((v_proj.lora_magnitude_vector if hasattr(v_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(q_proj, \"base_layer\", q_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"base_layer\", k_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"base_layer\", v_proj).bias is None) and \\\\\\n+                    (getattr(q_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(k_proj, \"lora_magnitude_vector\", None) is None) and \\\\\\n+                    (getattr(v_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     layer.self_attn.apply_qkv = apply_lora_qkv\\n                     n_qkv += 1\\n@@ -1520,8 +1526,8 @@ class FastLlamaModel:\\n                 # O attention patching\\n                 o_proj = layer.self_attn.o_proj\\n                 if hasattr(o_proj, \"lora_A\") and \\\\\\n-                    ((o_proj.base_layer if hasattr(o_proj, \"base_layer\") else o_proj).bias is None) and \\\\\\n-                    ((o_proj.lora_magnitude_vector if hasattr(o_proj, \"lora_magnitude_vector\") else None) is None):\\n+                    (getattr(o_proj, \"base_layer\", o_proj).bias is None) and \\\\\\n+                    (getattr(o_proj, \"lora_magnitude_vector\", None) is None):\\n \\n                     layer.self_attn.apply_o = apply_lora_o\\n                     n_o += 1\\n',\n",
       " '@@ -632,6 +632,7 @@ pass\\n \\n \\n def save_to_gguf(\\n+    model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -639,10 +640,18 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    # Careful convert.py is only for Llama / Mistral based archs\\n+    use_fast_convert = False\\n+    if   model_type == \"llama\":   use_fast_convert = True\\n+    elif model_type == \"mistral\": use_fast_convert = True\\n+    pass\\n+    logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n+\\n     if   quantization_method == \"not_quantized\":  quantization_method = \"f16\"\\n     elif quantization_method == \"fast_quantized\": quantization_method = \"q8_0\"\\n     elif quantization_method == \"quantized\":      quantization_method = \"q4_k_m\"\\n     elif quantization_method is None:             quantization_method = \"q8_0\"\\n+    pass\\n \\n     if quantization_method not in ALLOWED_QUANTS.keys():\\n         error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\\\n\"\\n@@ -692,6 +701,12 @@ def save_to_gguf(\\n         pass\\n     pass\\n \\n+    # Non llama/mistral needs can only use f32 or f16\\n+    if not use_fast_convert and (first_conversion != \"f16\" or first_conversion != \"f32\"):\\n+        logger.warning_once(\"Unsloth: We must use f16 for non Llama and Mistral models.\")\\n+        first_conversion = \"f16\"\\n+    pass\\n+\\n     n_cpus = psutil.cpu_count()\\n     if n_cpus is None: n_cpus = 1\\n     n_cpus *= 2\\n@@ -703,9 +718,15 @@ def save_to_gguf(\\n           f\"The output location will be {final_location}\\\\n\"\\\\\\n           \"This will take 3 minutes...\")\\n \\n-    command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-        f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n-        f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+    if use_fast_convert:\\n+        command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n+            f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n+            f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+    else:\\n+        command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n+            f\"--outfile {final_location} \"\\\\\\n+            f\"--outtype {first_conversion}\"\\n+    pass\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n@@ -1054,7 +1075,8 @@ def unsloth_save_pretrained_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n+    model_type = self.config.model_type\\n+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1154,7 +1176,8 @@ def unsloth_push_to_hub_gguf(\\n     for _ in range(3):\\n         gc.collect()\\n \\n-    file_location = save_to_gguf(new_save_directory, quantization_method, first_conversion, makefile)\\n+    model_type = self.config.model_type\\n+    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -15,12 +15,15 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n+    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n from .models._utils import patch_tokenizer\\n+import os\\n+import shutil\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -239,14 +242,75 @@ CHAT_TEMPLATES[\"gemma\"] = (gemma_template, gemma_eos_token,)\\n \\n \\n # Gemma with ChatML instead\\n+# We find using <eos> is still more appropriate!\\n gemma_chatml_template = \"{{ bos_token }}\" + chatml_template\\n gemma_chatml_eos_token = (\\n-    {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"},\\n+    {\"<start_of_turn>\" : \"<|im_start|>\", \"<eos>\" : \"<|im_end|>\"},\\n     \"<|im_end|>\",\\n )\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    try:\\n+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n+    except:\\n+        if not os.path.exists(temporary_location):\\n+            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n+            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            shutil.rmtree(temporary_location)\\n+        pass\\n+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n+    pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -290,11 +354,13 @@ def get_chat_template(\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n+            skipped = 0\\n             for old_token, new_token in token_mapping.items():\\n                 old_count = string_vocab.count(f\\'\"{old_token}\"\\')\\n                 new_count = string_vocab.count(f\\'\"{new_token}\"\\')\\n                 if new_count != 0:\\n                     print(f\"{new_token} is already a token. Skipping.\")\\n+                    skipped += 1\\n                 elif old_count == 0:\\n                     raise RuntimeError(f\"{old_token} was not part of the tokenizer!\")\\n                 else:\\n@@ -308,8 +374,14 @@ def get_chat_template(\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n             pass\\n \\n-            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            if skipped != len(token_mapping):\\n+                new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n+                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+                tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n+            else:\\n+                pass\\n \\n         elif stop_word != \"eos_token\":\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n',\n",
       " '@@ -15,12 +15,15 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n+    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n from .models._utils import patch_tokenizer\\n+import os\\n+import shutil\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -239,14 +242,75 @@ CHAT_TEMPLATES[\"gemma\"] = (gemma_template, gemma_eos_token,)\\n \\n \\n # Gemma with ChatML instead\\n+# We find using <eos> is still more appropriate!\\n gemma_chatml_template = \"{{ bos_token }}\" + chatml_template\\n gemma_chatml_eos_token = (\\n-    {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"},\\n+    {\"<start_of_turn>\" : \"<|im_start|>\", \"<eos>\" : \"<|im_end|>\"},\\n     \"<|im_end|>\",\\n )\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    try:\\n+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n+    except:\\n+        if not os.path.exists(temporary_location):\\n+            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n+            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            shutil.rmtree(temporary_location)\\n+        pass\\n+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n+    pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -290,11 +354,13 @@ def get_chat_template(\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n+            skipped = 0\\n             for old_token, new_token in token_mapping.items():\\n                 old_count = string_vocab.count(f\\'\"{old_token}\"\\')\\n                 new_count = string_vocab.count(f\\'\"{new_token}\"\\')\\n                 if new_count != 0:\\n                     print(f\"{new_token} is already a token. Skipping.\")\\n+                    skipped += 1\\n                 elif old_count == 0:\\n                     raise RuntimeError(f\"{old_token} was not part of the tokenizer!\")\\n                 else:\\n@@ -308,8 +374,14 @@ def get_chat_template(\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n             pass\\n \\n-            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            if skipped != len(token_mapping):\\n+                new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n+                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+                tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n+            else:\\n+                pass\\n \\n         elif stop_word != \"eos_token\":\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n',\n",
       " '@@ -15,12 +15,15 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n+    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n from .models._utils import patch_tokenizer\\n+import os\\n+import shutil\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -239,14 +242,75 @@ CHAT_TEMPLATES[\"gemma\"] = (gemma_template, gemma_eos_token,)\\n \\n \\n # Gemma with ChatML instead\\n+# We find using <eos> is still more appropriate!\\n gemma_chatml_template = \"{{ bos_token }}\" + chatml_template\\n gemma_chatml_eos_token = (\\n-    {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"},\\n+    {\"<start_of_turn>\" : \"<|im_start|>\", \"<eos>\" : \"<|im_end|>\"},\\n     \"<|im_end|>\",\\n )\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    try:\\n+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n+    except:\\n+        if not os.path.exists(temporary_location):\\n+            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n+            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            shutil.rmtree(temporary_location)\\n+        pass\\n+        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n+    pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -290,11 +354,13 @@ def get_chat_template(\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n+            skipped = 0\\n             for old_token, new_token in token_mapping.items():\\n                 old_count = string_vocab.count(f\\'\"{old_token}\"\\')\\n                 new_count = string_vocab.count(f\\'\"{new_token}\"\\')\\n                 if new_count != 0:\\n                     print(f\"{new_token} is already a token. Skipping.\")\\n+                    skipped += 1\\n                 elif old_count == 0:\\n                     raise RuntimeError(f\"{old_token} was not part of the tokenizer!\")\\n                 else:\\n@@ -308,8 +374,14 @@ def get_chat_template(\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n             pass\\n \\n-            new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            if skipped != len(token_mapping):\\n+                new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n+                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+                tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n+            else:\\n+                pass\\n \\n         elif stop_word != \"eos_token\":\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n',\n",
       " '@@ -91,13 +91,11 @@ Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA\\n conda create --name unsloth_env python=3.10\\n conda activate unsloth_env\\n \\n-conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n \\n-conda install xformers -c xformers\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install bitsandbytes\\n-\\n-pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps trl peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -144,6 +142,22 @@ pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/u\\n ```bash\\n pip install --upgrade pip\\n ```\\n+6. For Pytorch 2.2.1:\\n+```bash\\n+# RTX 3090, 4090 Ampere GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\\n+\\n+# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+```\\n+7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n+```bash\\n+nvcc\\n+python -m xformers.info\\n+python -m bitsandbytes\\n+```\\n \\n ## 📜 Documentation\\n - Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n',\n",
       " '@@ -18,9 +18,9 @@ import importlib\\n # Currently only supports 1 GPU, or else seg faults will occur.\\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n-    # check if there are multiple cuda devices set in env\\n+    # Check if there are multiple cuda devices set in env\\n     if not devices.isdigit():\\n-        first_id = devices.split(\\',\\')[0]\\n+        first_id = devices.split(\",\")[0]\\n         warnings.warn(\\n             f\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is currently {devices} \\\\n\"\\\\\\n             \"Multiple CUDA devices detected but we require a single device.\\\\n\"\\\\\\n@@ -33,20 +33,29 @@ else:\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n \\n+# Reduce VRAM usage by reducing fragmentation\\n+os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\\n+\\n try:\\n     import torch\\n except:\\n     raise ImportError(\"Pytorch is not installed. Go to https://pytorch.org/.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n-# We support torch 2.1 and 2.1.1\\n+# We support Pytorch 2\\n # Fixes https://github.com/unslothai/unsloth/issues/38\\n torch_version = torch.__version__.split(\".\")\\n major_torch, minor_torch = torch_version[0], torch_version[1]\\n major_torch, minor_torch = int(major_torch), int(minor_torch)\\n-if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n-    raise ImportError(\"Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n+if (major_torch < 2):\\n+    raise ImportError(\"Unsloth only supports Pytorch 2 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n+elif (major_torch == 2) and (minor_torch < 2):\\n+    # Disable expandable_segments\\n+    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n+    # Must reimport Pytorch!\\n+    importlib.reload(torch)\\n+pass\\n \\n \\n # Try loading bitsandbytes and triton\\n',\n",
       " '@@ -17,6 +17,7 @@ import triton.language as tl\\n import torch\\n from .utils import calculate_settings\\n \\n+ROPE_GROUP_SIZE = 4\\n \\n @triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\\n @triton.jit\\n@@ -24,9 +25,11 @@ def _rope_embedding(\\n     Q,     Q_row_stride,\\n     cos, cos_row_stride,\\n     sin, sin_row_stride,\\n-    seqlen, head_dim, group_size, n_heads,\\n-    BACKWARD_PASS: tl.constexpr,\\n-    BLOCK_SIZE : tl.constexpr,\\n+    seqlen,\\n+    head_dim      : tl.constexpr,\\n+    n_heads       : tl.constexpr,\\n+    BACKWARD_PASS : tl.constexpr,\\n+    BLOCK_SIZE    : tl.constexpr,\\n ):\\n     \"\"\"\\n         Calculates the RoPE Embedding quickly\\n@@ -49,16 +52,18 @@ def _rope_embedding(\\n         sin1 = -sin1\\n     pass\\n \\n-    head_start = group_head_position * group_size\\n-    head_end = tl.math.min((head_start + group_size), n_heads)\\n+    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\\n+    head_start = group_head_position * ROPE_GROUP_SIZE\\n+    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\\n \\n-    for i in range(head_start, head_end):\\n-        offs_q1 = row_position * Q_row_stride + i * head_dim + col_offsets\\n-        offs_q2 = row_position * Q_row_stride + i * head_dim + col_offsets + half_head_dim\\n+    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\\n+    for k in range(head_start, head_end):\\n+        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\\n+        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\\n \\n         # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n-        Q1   = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n-        Q2   = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n+        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n+        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n \\n         tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\\n         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\\n@@ -78,21 +83,24 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         # [TODO] Changing blocksize to head_dim//2 seems to have\\n         # some concurrency / un-deterministic issues.\\n         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n+        \\n+        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n+        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\\n+        n_groups = div + (mod != 0)\\n \\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, n_groups, )](\\n               Q,   Q.stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len,\\n+            head_dim, n_heads,\\n             BACKWARD_PASS = False,\\n             BLOCK_SIZE = BLOCK_SIZE,\\n             num_warps  = num_warps,\\n         )\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.n_groups = n_groups\\n         ctx.cos = cos\\n         ctx.sin = sin\\n         return Q.view(batch, seq_len, n_heads, head_dim)\\n@@ -108,15 +116,11 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         cos = ctx.cos\\n         sin = ctx.sin\\n \\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n-\\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, ctx.n_groups, )](\\n             dY,  dY .stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len, head_dim, n_heads,\\n             BACKWARD_PASS = True,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n',\n",
       " '@@ -18,6 +18,8 @@ import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"transformers\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning, module = \"accelerate\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -593,16 +593,17 @@ def install_llama_cpp_old(version = -10):\\n     pass\\n \\n     # Clone a specific commit\\n+    # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n         f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n     # Check if successful\\n@@ -625,12 +626,55 @@ def install_llama_cpp_blocking():\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n pass\\n \\n \\n+def _fix_gemma_gguf():\\n+    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n+        text = file.read()\\n+    pass\\n+\\n+    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n+    if gemma_start == -1: return\\n+\\n+    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n+    if gemma_end == -1: return\\n+\\n+    gemma_text = text[gemma_start : gemma_end]\\n+    bad_text = \\\\\\n+b\"\"\"         data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    good_text = \\\\\\n+b\"\"\"         # if f32 desired, convert any float16 to float32\\n+            if self.ftype == 0 and data_dtype == np.float16:\\n+                data = data.astype(np.float32)\\n+\\n+            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n+            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n+                data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    find_bad = gemma_text.find(bad_text)\\n+    if find_bad == -1: return\\n+\\n+    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n+    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n+\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n+        file.write(text)\\n+    pass\\n+pass\\n+\\n+\\n def save_to_gguf(\\n     model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n@@ -686,7 +730,10 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+        print(f\"Unsloth: llama.cpp error code = {error}.\")\\n+        install_llama_cpp_old(-10)\\n+    pass\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -723,6 +770,9 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n+        # Need to fix convert-hf-to-gguf.py for some models!\\n+        _fix_gemma_gguf()\\n+\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -730,7 +780,7 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n-            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+            print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n     pass\\n@@ -760,7 +810,7 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stderr:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n         pass\\n',\n",
       " '@@ -91,13 +91,11 @@ Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA\\n conda create --name unsloth_env python=3.10\\n conda activate unsloth_env\\n \\n-conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n \\n-conda install xformers -c xformers\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install bitsandbytes\\n-\\n-pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps trl peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -144,6 +142,22 @@ pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/u\\n ```bash\\n pip install --upgrade pip\\n ```\\n+6. For Pytorch 2.2.1:\\n+```bash\\n+# RTX 3090, 4090 Ampere GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\\n+\\n+# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+```\\n+7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n+```bash\\n+nvcc\\n+python -m xformers.info\\n+python -m bitsandbytes\\n+```\\n \\n ## 📜 Documentation\\n - Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n',\n",
       " '@@ -18,9 +18,9 @@ import importlib\\n # Currently only supports 1 GPU, or else seg faults will occur.\\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n-    # check if there are multiple cuda devices set in env\\n+    # Check if there are multiple cuda devices set in env\\n     if not devices.isdigit():\\n-        first_id = devices.split(\\',\\')[0]\\n+        first_id = devices.split(\",\")[0]\\n         warnings.warn(\\n             f\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is currently {devices} \\\\n\"\\\\\\n             \"Multiple CUDA devices detected but we require a single device.\\\\n\"\\\\\\n@@ -33,20 +33,29 @@ else:\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n \\n+# Reduce VRAM usage by reducing fragmentation\\n+os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\\n+\\n try:\\n     import torch\\n except:\\n     raise ImportError(\"Pytorch is not installed. Go to https://pytorch.org/.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n-# We support torch 2.1 and 2.1.1\\n+# We support Pytorch 2\\n # Fixes https://github.com/unslothai/unsloth/issues/38\\n torch_version = torch.__version__.split(\".\")\\n major_torch, minor_torch = torch_version[0], torch_version[1]\\n major_torch, minor_torch = int(major_torch), int(minor_torch)\\n-if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n-    raise ImportError(\"Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n+if (major_torch < 2):\\n+    raise ImportError(\"Unsloth only supports Pytorch 2 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n+elif (major_torch == 2) and (minor_torch < 2):\\n+    # Disable expandable_segments\\n+    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n+    # Must reimport Pytorch!\\n+    importlib.reload(torch)\\n+pass\\n \\n \\n # Try loading bitsandbytes and triton\\n',\n",
       " '@@ -17,6 +17,7 @@ import triton.language as tl\\n import torch\\n from .utils import calculate_settings\\n \\n+ROPE_GROUP_SIZE = 4\\n \\n @triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\\n @triton.jit\\n@@ -24,9 +25,11 @@ def _rope_embedding(\\n     Q,     Q_row_stride,\\n     cos, cos_row_stride,\\n     sin, sin_row_stride,\\n-    seqlen, head_dim, group_size, n_heads,\\n-    BACKWARD_PASS: tl.constexpr,\\n-    BLOCK_SIZE : tl.constexpr,\\n+    seqlen,\\n+    head_dim      : tl.constexpr,\\n+    n_heads       : tl.constexpr,\\n+    BACKWARD_PASS : tl.constexpr,\\n+    BLOCK_SIZE    : tl.constexpr,\\n ):\\n     \"\"\"\\n         Calculates the RoPE Embedding quickly\\n@@ -49,16 +52,18 @@ def _rope_embedding(\\n         sin1 = -sin1\\n     pass\\n \\n-    head_start = group_head_position * group_size\\n-    head_end = tl.math.min((head_start + group_size), n_heads)\\n+    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\\n+    head_start = group_head_position * ROPE_GROUP_SIZE\\n+    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\\n \\n-    for i in range(head_start, head_end):\\n-        offs_q1 = row_position * Q_row_stride + i * head_dim + col_offsets\\n-        offs_q2 = row_position * Q_row_stride + i * head_dim + col_offsets + half_head_dim\\n+    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\\n+    for k in range(head_start, head_end):\\n+        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\\n+        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\\n \\n         # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n-        Q1   = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n-        Q2   = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n+        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n+        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n \\n         tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\\n         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\\n@@ -78,21 +83,24 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         # [TODO] Changing blocksize to head_dim//2 seems to have\\n         # some concurrency / un-deterministic issues.\\n         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n+        \\n+        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n+        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\\n+        n_groups = div + (mod != 0)\\n \\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, n_groups, )](\\n               Q,   Q.stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len,\\n+            head_dim, n_heads,\\n             BACKWARD_PASS = False,\\n             BLOCK_SIZE = BLOCK_SIZE,\\n             num_warps  = num_warps,\\n         )\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.n_groups = n_groups\\n         ctx.cos = cos\\n         ctx.sin = sin\\n         return Q.view(batch, seq_len, n_heads, head_dim)\\n@@ -108,15 +116,11 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         cos = ctx.cos\\n         sin = ctx.sin\\n \\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n-\\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, ctx.n_groups, )](\\n             dY,  dY .stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len, head_dim, n_heads,\\n             BACKWARD_PASS = True,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n',\n",
       " '@@ -18,6 +18,8 @@ import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"transformers\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning, module = \"accelerate\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -593,16 +593,17 @@ def install_llama_cpp_old(version = -10):\\n     pass\\n \\n     # Clone a specific commit\\n+    # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n         f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n     # Check if successful\\n@@ -625,12 +626,55 @@ def install_llama_cpp_blocking():\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n pass\\n \\n \\n+def _fix_gemma_gguf():\\n+    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n+        text = file.read()\\n+    pass\\n+\\n+    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n+    if gemma_start == -1: return\\n+\\n+    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n+    if gemma_end == -1: return\\n+\\n+    gemma_text = text[gemma_start : gemma_end]\\n+    bad_text = \\\\\\n+b\"\"\"         data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    good_text = \\\\\\n+b\"\"\"         # if f32 desired, convert any float16 to float32\\n+            if self.ftype == 0 and data_dtype == np.float16:\\n+                data = data.astype(np.float32)\\n+\\n+            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n+            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n+                data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    find_bad = gemma_text.find(bad_text)\\n+    if find_bad == -1: return\\n+\\n+    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n+    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n+\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n+        file.write(text)\\n+    pass\\n+pass\\n+\\n+\\n def save_to_gguf(\\n     model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n@@ -686,7 +730,10 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+        print(f\"Unsloth: llama.cpp error code = {error}.\")\\n+        install_llama_cpp_old(-10)\\n+    pass\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -723,6 +770,9 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n+        # Need to fix convert-hf-to-gguf.py for some models!\\n+        _fix_gemma_gguf()\\n+\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -730,7 +780,7 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n-            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+            print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n     pass\\n@@ -760,7 +810,7 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stderr:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n         pass\\n',\n",
       " '@@ -91,13 +91,11 @@ Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA\\n conda create --name unsloth_env python=3.10\\n conda activate unsloth_env\\n \\n-conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n \\n-conda install xformers -c xformers\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install bitsandbytes\\n-\\n-pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps trl peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -144,6 +142,22 @@ pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/u\\n ```bash\\n pip install --upgrade pip\\n ```\\n+6. For Pytorch 2.2.1:\\n+```bash\\n+# RTX 3090, 4090 Ampere GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\\n+\\n+# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+```\\n+7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n+```bash\\n+nvcc\\n+python -m xformers.info\\n+python -m bitsandbytes\\n+```\\n \\n ## 📜 Documentation\\n - Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n',\n",
       " '@@ -18,9 +18,9 @@ import importlib\\n # Currently only supports 1 GPU, or else seg faults will occur.\\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n-    # check if there are multiple cuda devices set in env\\n+    # Check if there are multiple cuda devices set in env\\n     if not devices.isdigit():\\n-        first_id = devices.split(\\',\\')[0]\\n+        first_id = devices.split(\",\")[0]\\n         warnings.warn(\\n             f\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is currently {devices} \\\\n\"\\\\\\n             \"Multiple CUDA devices detected but we require a single device.\\\\n\"\\\\\\n@@ -33,20 +33,29 @@ else:\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n \\n+# Reduce VRAM usage by reducing fragmentation\\n+os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\\n+\\n try:\\n     import torch\\n except:\\n     raise ImportError(\"Pytorch is not installed. Go to https://pytorch.org/.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n-# We support torch 2.1 and 2.1.1\\n+# We support Pytorch 2\\n # Fixes https://github.com/unslothai/unsloth/issues/38\\n torch_version = torch.__version__.split(\".\")\\n major_torch, minor_torch = torch_version[0], torch_version[1]\\n major_torch, minor_torch = int(major_torch), int(minor_torch)\\n-if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n-    raise ImportError(\"Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n+if (major_torch < 2):\\n+    raise ImportError(\"Unsloth only supports Pytorch 2 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n+elif (major_torch == 2) and (minor_torch < 2):\\n+    # Disable expandable_segments\\n+    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n+    # Must reimport Pytorch!\\n+    importlib.reload(torch)\\n+pass\\n \\n \\n # Try loading bitsandbytes and triton\\n',\n",
       " '@@ -17,6 +17,7 @@ import triton.language as tl\\n import torch\\n from .utils import calculate_settings\\n \\n+ROPE_GROUP_SIZE = 4\\n \\n @triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\\n @triton.jit\\n@@ -24,9 +25,11 @@ def _rope_embedding(\\n     Q,     Q_row_stride,\\n     cos, cos_row_stride,\\n     sin, sin_row_stride,\\n-    seqlen, head_dim, group_size, n_heads,\\n-    BACKWARD_PASS: tl.constexpr,\\n-    BLOCK_SIZE : tl.constexpr,\\n+    seqlen,\\n+    head_dim      : tl.constexpr,\\n+    n_heads       : tl.constexpr,\\n+    BACKWARD_PASS : tl.constexpr,\\n+    BLOCK_SIZE    : tl.constexpr,\\n ):\\n     \"\"\"\\n         Calculates the RoPE Embedding quickly\\n@@ -49,16 +52,18 @@ def _rope_embedding(\\n         sin1 = -sin1\\n     pass\\n \\n-    head_start = group_head_position * group_size\\n-    head_end = tl.math.min((head_start + group_size), n_heads)\\n+    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\\n+    head_start = group_head_position * ROPE_GROUP_SIZE\\n+    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\\n \\n-    for i in range(head_start, head_end):\\n-        offs_q1 = row_position * Q_row_stride + i * head_dim + col_offsets\\n-        offs_q2 = row_position * Q_row_stride + i * head_dim + col_offsets + half_head_dim\\n+    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\\n+    for k in range(head_start, head_end):\\n+        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\\n+        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\\n \\n         # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n-        Q1   = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n-        Q2   = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n+        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n+        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n \\n         tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\\n         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\\n@@ -78,21 +83,24 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         # [TODO] Changing blocksize to head_dim//2 seems to have\\n         # some concurrency / un-deterministic issues.\\n         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n+        \\n+        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n+        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\\n+        n_groups = div + (mod != 0)\\n \\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, n_groups, )](\\n               Q,   Q.stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len,\\n+            head_dim, n_heads,\\n             BACKWARD_PASS = False,\\n             BLOCK_SIZE = BLOCK_SIZE,\\n             num_warps  = num_warps,\\n         )\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.n_groups = n_groups\\n         ctx.cos = cos\\n         ctx.sin = sin\\n         return Q.view(batch, seq_len, n_heads, head_dim)\\n@@ -108,15 +116,11 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         cos = ctx.cos\\n         sin = ctx.sin\\n \\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n-\\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, ctx.n_groups, )](\\n             dY,  dY .stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len, head_dim, n_heads,\\n             BACKWARD_PASS = True,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n',\n",
       " '@@ -18,6 +18,8 @@ import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"transformers\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning, module = \"accelerate\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -593,16 +593,17 @@ def install_llama_cpp_old(version = -10):\\n     pass\\n \\n     # Clone a specific commit\\n+    # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n         f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n     # Check if successful\\n@@ -625,12 +626,55 @@ def install_llama_cpp_blocking():\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n pass\\n \\n \\n+def _fix_gemma_gguf():\\n+    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n+        text = file.read()\\n+    pass\\n+\\n+    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n+    if gemma_start == -1: return\\n+\\n+    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n+    if gemma_end == -1: return\\n+\\n+    gemma_text = text[gemma_start : gemma_end]\\n+    bad_text = \\\\\\n+b\"\"\"         data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    good_text = \\\\\\n+b\"\"\"         # if f32 desired, convert any float16 to float32\\n+            if self.ftype == 0 and data_dtype == np.float16:\\n+                data = data.astype(np.float32)\\n+\\n+            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n+            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n+                data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    find_bad = gemma_text.find(bad_text)\\n+    if find_bad == -1: return\\n+\\n+    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n+    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n+\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n+        file.write(text)\\n+    pass\\n+pass\\n+\\n+\\n def save_to_gguf(\\n     model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n@@ -686,7 +730,10 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+        print(f\"Unsloth: llama.cpp error code = {error}.\")\\n+        install_llama_cpp_old(-10)\\n+    pass\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -723,6 +770,9 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n+        # Need to fix convert-hf-to-gguf.py for some models!\\n+        _fix_gemma_gguf()\\n+\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -730,7 +780,7 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n-            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+            print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n     pass\\n@@ -760,7 +810,7 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stderr:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n         pass\\n',\n",
       " '@@ -91,13 +91,11 @@ Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA\\n conda create --name unsloth_env python=3.10\\n conda activate unsloth_env\\n \\n-conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n \\n-conda install xformers -c xformers\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install bitsandbytes\\n-\\n-pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps trl peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -144,6 +142,22 @@ pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/u\\n ```bash\\n pip install --upgrade pip\\n ```\\n+6. For Pytorch 2.2.1:\\n+```bash\\n+# RTX 3090, 4090 Ampere GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\\n+\\n+# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+```\\n+7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n+```bash\\n+nvcc\\n+python -m xformers.info\\n+python -m bitsandbytes\\n+```\\n \\n ## 📜 Documentation\\n - Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n',\n",
       " '@@ -18,9 +18,9 @@ import importlib\\n # Currently only supports 1 GPU, or else seg faults will occur.\\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n-    # check if there are multiple cuda devices set in env\\n+    # Check if there are multiple cuda devices set in env\\n     if not devices.isdigit():\\n-        first_id = devices.split(\\',\\')[0]\\n+        first_id = devices.split(\",\")[0]\\n         warnings.warn(\\n             f\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is currently {devices} \\\\n\"\\\\\\n             \"Multiple CUDA devices detected but we require a single device.\\\\n\"\\\\\\n@@ -33,20 +33,29 @@ else:\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n \\n+# Reduce VRAM usage by reducing fragmentation\\n+os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\\n+\\n try:\\n     import torch\\n except:\\n     raise ImportError(\"Pytorch is not installed. Go to https://pytorch.org/.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n-# We support torch 2.1 and 2.1.1\\n+# We support Pytorch 2\\n # Fixes https://github.com/unslothai/unsloth/issues/38\\n torch_version = torch.__version__.split(\".\")\\n major_torch, minor_torch = torch_version[0], torch_version[1]\\n major_torch, minor_torch = int(major_torch), int(minor_torch)\\n-if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n-    raise ImportError(\"Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n+if (major_torch < 2):\\n+    raise ImportError(\"Unsloth only supports Pytorch 2 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n+elif (major_torch == 2) and (minor_torch < 2):\\n+    # Disable expandable_segments\\n+    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n+    # Must reimport Pytorch!\\n+    importlib.reload(torch)\\n+pass\\n \\n \\n # Try loading bitsandbytes and triton\\n',\n",
       " '@@ -17,6 +17,7 @@ import triton.language as tl\\n import torch\\n from .utils import calculate_settings\\n \\n+ROPE_GROUP_SIZE = 4\\n \\n @triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\\n @triton.jit\\n@@ -24,9 +25,11 @@ def _rope_embedding(\\n     Q,     Q_row_stride,\\n     cos, cos_row_stride,\\n     sin, sin_row_stride,\\n-    seqlen, head_dim, group_size, n_heads,\\n-    BACKWARD_PASS: tl.constexpr,\\n-    BLOCK_SIZE : tl.constexpr,\\n+    seqlen,\\n+    head_dim      : tl.constexpr,\\n+    n_heads       : tl.constexpr,\\n+    BACKWARD_PASS : tl.constexpr,\\n+    BLOCK_SIZE    : tl.constexpr,\\n ):\\n     \"\"\"\\n         Calculates the RoPE Embedding quickly\\n@@ -49,16 +52,18 @@ def _rope_embedding(\\n         sin1 = -sin1\\n     pass\\n \\n-    head_start = group_head_position * group_size\\n-    head_end = tl.math.min((head_start + group_size), n_heads)\\n+    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\\n+    head_start = group_head_position * ROPE_GROUP_SIZE\\n+    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\\n \\n-    for i in range(head_start, head_end):\\n-        offs_q1 = row_position * Q_row_stride + i * head_dim + col_offsets\\n-        offs_q2 = row_position * Q_row_stride + i * head_dim + col_offsets + half_head_dim\\n+    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\\n+    for k in range(head_start, head_end):\\n+        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\\n+        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\\n \\n         # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n-        Q1   = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n-        Q2   = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n+        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n+        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n \\n         tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\\n         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\\n@@ -78,21 +83,24 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         # [TODO] Changing blocksize to head_dim//2 seems to have\\n         # some concurrency / un-deterministic issues.\\n         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n+        \\n+        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n+        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\\n+        n_groups = div + (mod != 0)\\n \\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, n_groups, )](\\n               Q,   Q.stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len,\\n+            head_dim, n_heads,\\n             BACKWARD_PASS = False,\\n             BLOCK_SIZE = BLOCK_SIZE,\\n             num_warps  = num_warps,\\n         )\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.n_groups = n_groups\\n         ctx.cos = cos\\n         ctx.sin = sin\\n         return Q.view(batch, seq_len, n_heads, head_dim)\\n@@ -108,15 +116,11 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         cos = ctx.cos\\n         sin = ctx.sin\\n \\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n-\\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, ctx.n_groups, )](\\n             dY,  dY .stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len, head_dim, n_heads,\\n             BACKWARD_PASS = True,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n',\n",
       " '@@ -18,6 +18,8 @@ import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"transformers\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning, module = \"accelerate\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -593,16 +593,17 @@ def install_llama_cpp_old(version = -10):\\n     pass\\n \\n     # Clone a specific commit\\n+    # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n         f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n     # Check if successful\\n@@ -625,12 +626,55 @@ def install_llama_cpp_blocking():\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n pass\\n \\n \\n+def _fix_gemma_gguf():\\n+    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n+        text = file.read()\\n+    pass\\n+\\n+    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n+    if gemma_start == -1: return\\n+\\n+    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n+    if gemma_end == -1: return\\n+\\n+    gemma_text = text[gemma_start : gemma_end]\\n+    bad_text = \\\\\\n+b\"\"\"         data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    good_text = \\\\\\n+b\"\"\"         # if f32 desired, convert any float16 to float32\\n+            if self.ftype == 0 and data_dtype == np.float16:\\n+                data = data.astype(np.float32)\\n+\\n+            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n+            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n+                data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    find_bad = gemma_text.find(bad_text)\\n+    if find_bad == -1: return\\n+\\n+    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n+    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n+\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n+        file.write(text)\\n+    pass\\n+pass\\n+\\n+\\n def save_to_gguf(\\n     model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n@@ -686,7 +730,10 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+        print(f\"Unsloth: llama.cpp error code = {error}.\")\\n+        install_llama_cpp_old(-10)\\n+    pass\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -723,6 +770,9 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n+        # Need to fix convert-hf-to-gguf.py for some models!\\n+        _fix_gemma_gguf()\\n+\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -730,7 +780,7 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n-            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+            print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n     pass\\n@@ -760,7 +810,7 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stderr:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n         pass\\n',\n",
       " '@@ -91,13 +91,11 @@ Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA\\n conda create --name unsloth_env python=3.10\\n conda activate unsloth_env\\n \\n-conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia\\n+conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n \\n-conda install xformers -c xformers\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install bitsandbytes\\n-\\n-pip install \"unsloth[conda] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps trl peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -144,6 +142,22 @@ pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/u\\n ```bash\\n pip install --upgrade pip\\n ```\\n+6. For Pytorch 2.2.1:\\n+```bash\\n+# RTX 3090, 4090 Ampere GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\\n+\\n+# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n+pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+```\\n+7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n+```bash\\n+nvcc\\n+python -m xformers.info\\n+python -m bitsandbytes\\n+```\\n \\n ## 📜 Documentation\\n - Go to our [Wiki page](https://github.com/unslothai/unsloth/wiki) for saving to GGUF, checkpointing, evaluation and more!\\n',\n",
       " '@@ -18,9 +18,9 @@ import importlib\\n # Currently only supports 1 GPU, or else seg faults will occur.\\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n-    # check if there are multiple cuda devices set in env\\n+    # Check if there are multiple cuda devices set in env\\n     if not devices.isdigit():\\n-        first_id = devices.split(\\',\\')[0]\\n+        first_id = devices.split(\",\")[0]\\n         warnings.warn(\\n             f\"Unsloth: \\'CUDA_VISIBLE_DEVICES\\' is currently {devices} \\\\n\"\\\\\\n             \"Multiple CUDA devices detected but we require a single device.\\\\n\"\\\\\\n@@ -33,20 +33,29 @@ else:\\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n pass\\n \\n+# Reduce VRAM usage by reducing fragmentation\\n+os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\\n+\\n try:\\n     import torch\\n except:\\n     raise ImportError(\"Pytorch is not installed. Go to https://pytorch.org/.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n \\n-# We support torch 2.1 and 2.1.1\\n+# We support Pytorch 2\\n # Fixes https://github.com/unslothai/unsloth/issues/38\\n torch_version = torch.__version__.split(\".\")\\n major_torch, minor_torch = torch_version[0], torch_version[1]\\n major_torch, minor_torch = int(major_torch), int(minor_torch)\\n-if (major_torch != 2):# or (major_torch == 2 and minor_torch < 1):\\n-    raise ImportError(\"Unsloth only supports Pytorch 2.1 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n+if (major_torch < 2):\\n+    raise ImportError(\"Unsloth only supports Pytorch 2 for now. Please update your Pytorch to 2.1.\\\\n\"\\\\\\n                       \"We have some installation instructions on our Github page.\")\\n+elif (major_torch == 2) and (minor_torch < 2):\\n+    # Disable expandable_segments\\n+    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n+    # Must reimport Pytorch!\\n+    importlib.reload(torch)\\n+pass\\n \\n \\n # Try loading bitsandbytes and triton\\n',\n",
       " '@@ -17,6 +17,7 @@ import triton.language as tl\\n import torch\\n from .utils import calculate_settings\\n \\n+ROPE_GROUP_SIZE = 4\\n \\n @triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\\n @triton.jit\\n@@ -24,9 +25,11 @@ def _rope_embedding(\\n     Q,     Q_row_stride,\\n     cos, cos_row_stride,\\n     sin, sin_row_stride,\\n-    seqlen, head_dim, group_size, n_heads,\\n-    BACKWARD_PASS: tl.constexpr,\\n-    BLOCK_SIZE : tl.constexpr,\\n+    seqlen,\\n+    head_dim      : tl.constexpr,\\n+    n_heads       : tl.constexpr,\\n+    BACKWARD_PASS : tl.constexpr,\\n+    BLOCK_SIZE    : tl.constexpr,\\n ):\\n     \"\"\"\\n         Calculates the RoPE Embedding quickly\\n@@ -49,16 +52,18 @@ def _rope_embedding(\\n         sin1 = -sin1\\n     pass\\n \\n-    head_start = group_head_position * group_size\\n-    head_end = tl.math.min((head_start + group_size), n_heads)\\n+    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\\n+    head_start = group_head_position * ROPE_GROUP_SIZE\\n+    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\\n \\n-    for i in range(head_start, head_end):\\n-        offs_q1 = row_position * Q_row_stride + i * head_dim + col_offsets\\n-        offs_q2 = row_position * Q_row_stride + i * head_dim + col_offsets + half_head_dim\\n+    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\\n+    for k in range(head_start, head_end):\\n+        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\\n+        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\\n \\n         # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\\n-        Q1   = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n-        Q2   = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n+        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\\n+        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\\n \\n         tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\\n         tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\\n@@ -78,21 +83,24 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         # [TODO] Changing blocksize to head_dim//2 seems to have\\n         # some concurrency / un-deterministic issues.\\n         BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n+        \\n+        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n+        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\\n+        n_groups = div + (mod != 0)\\n \\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, n_groups, )](\\n               Q,   Q.stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len,\\n+            head_dim, n_heads,\\n             BACKWARD_PASS = False,\\n             BLOCK_SIZE = BLOCK_SIZE,\\n             num_warps  = num_warps,\\n         )\\n         ctx.BLOCK_SIZE = BLOCK_SIZE\\n         ctx.num_warps  = num_warps\\n+        ctx.n_groups = n_groups\\n         ctx.cos = cos\\n         ctx.sin = sin\\n         return Q.view(batch, seq_len, n_heads, head_dim)\\n@@ -108,15 +116,11 @@ class Fast_RoPE_Embedding(torch.autograd.Function):\\n         cos = ctx.cos\\n         sin = ctx.sin\\n \\n-        group_size = 4 # 4 or 8, too large group_size can hurt performance.\\n-        n_groups = triton.cdiv(n_heads, group_size)\\n-\\n-        grid = (n_rows, n_groups, )\\n-        _rope_embedding[grid](\\n+        _rope_embedding[(n_rows, ctx.n_groups, )](\\n             dY,  dY .stride(0),\\n             cos, cos.stride(0),\\n             sin, sin.stride(0),\\n-            seq_len, head_dim, group_size, n_heads,\\n+            seq_len, head_dim, n_heads,\\n             BACKWARD_PASS = True,\\n             BLOCK_SIZE = ctx.BLOCK_SIZE,\\n             num_warps  = ctx.num_warps,\\n',\n",
       " '@@ -18,6 +18,8 @@ import warnings\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"transformers\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning, module = \"accelerate\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n',\n",
       " '@@ -593,16 +593,17 @@ def install_llama_cpp_old(version = -10):\\n     pass\\n \\n     # Clone a specific commit\\n+    # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n         f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n     # Check if successful\\n@@ -625,12 +626,55 @@ def install_llama_cpp_blocking():\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n pass\\n \\n \\n+def _fix_gemma_gguf():\\n+    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n+        text = file.read()\\n+    pass\\n+\\n+    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n+    if gemma_start == -1: return\\n+\\n+    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n+    if gemma_end == -1: return\\n+\\n+    gemma_text = text[gemma_start : gemma_end]\\n+    bad_text = \\\\\\n+b\"\"\"         data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    good_text = \\\\\\n+b\"\"\"         # if f32 desired, convert any float16 to float32\\n+            if self.ftype == 0 and data_dtype == np.float16:\\n+                data = data.astype(np.float32)\\n+\\n+            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n+            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n+                data = data.astype(np.float32)\\n+\\n+            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n+            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n+                data = data.astype(np.float16)\"\"\"\\n+    find_bad = gemma_text.find(bad_text)\\n+    if find_bad == -1: return\\n+\\n+    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n+    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n+\\n+    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n+        file.write(text)\\n+    pass\\n+pass\\n+\\n+\\n def save_to_gguf(\\n     model_type           : str,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n@@ -686,7 +730,10 @@ def save_to_gguf(\\n         install_llama_cpp_blocking()\\n     pass\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"): install_llama_cpp_old(-10)\\n+    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+        print(f\"Unsloth: llama.cpp error code = {error}.\")\\n+        install_llama_cpp_old(-10)\\n+    pass\\n \\n     if   quantization_method == \"f32\":  first_conversion = \"f32\"\\n     elif quantization_method == \"f16\":  first_conversion = \"f16\"\\n@@ -723,6 +770,9 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n+        # Need to fix convert-hf-to-gguf.py for some models!\\n+        _fix_gemma_gguf()\\n+\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -730,7 +780,7 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n-            print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+            print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n     pass\\n@@ -760,7 +810,7 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stderr:\\n-                print(line.decode(\"utf-8\"), flush = True, end = \"\")\\n+                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n         pass\\n',\n",
       " '@@ -178,6 +178,9 @@ colab-no-deps = [\\n     \"xformers\",\\n     \"bitsandbytes\",\\n ]\\n+colab = [\\n+    \"unsloth[cu121]\",\\n+]\\n colab-ampere = [\\n     \"unsloth[colab-ampere-torch220]\",\\n     \"packaging\",\\n',\n",
       " '@@ -684,6 +684,9 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    if quantization_method.startswith(\"iq2\"):\\n+        raise RuntimeError(\"Unsloth: Currently iq2 type quantizations aren\\'t supported yet - sorry!\")\\n+\\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n     if   model_type == \"llama\":   use_fast_convert = True\\n@@ -743,8 +746,11 @@ def save_to_gguf(\\n         if   first_conversion == \"f32\" : pass\\n         elif first_conversion == \"f16\" : pass\\n         elif first_conversion == \"q8_0\":\\n-            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n-            first_conversion = \"f16\"\\n+            logger.warning_once(\\n+                \"Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, \"\\\\\\n+                \"but saves disk space!\"\\n+            )\\n+            # first_conversion = \"f16\"\\n         pass\\n     pass\\n \\n',\n",
       " '@@ -178,6 +178,9 @@ colab-no-deps = [\\n     \"xformers\",\\n     \"bitsandbytes\",\\n ]\\n+colab = [\\n+    \"unsloth[cu121]\",\\n+]\\n colab-ampere = [\\n     \"unsloth[colab-ampere-torch220]\",\\n     \"packaging\",\\n',\n",
       " '@@ -684,6 +684,9 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    if quantization_method.startswith(\"iq2\"):\\n+        raise RuntimeError(\"Unsloth: Currently iq2 type quantizations aren\\'t supported yet - sorry!\")\\n+\\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n     if   model_type == \"llama\":   use_fast_convert = True\\n@@ -743,8 +746,11 @@ def save_to_gguf(\\n         if   first_conversion == \"f32\" : pass\\n         elif first_conversion == \"f16\" : pass\\n         elif first_conversion == \"q8_0\":\\n-            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n-            first_conversion = \"f16\"\\n+            logger.warning_once(\\n+                \"Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, \"\\\\\\n+                \"but saves disk space!\"\\n+            )\\n+            # first_conversion = \"f16\"\\n         pass\\n     pass\\n \\n',\n",
       " '@@ -178,6 +178,9 @@ colab-no-deps = [\\n     \"xformers\",\\n     \"bitsandbytes\",\\n ]\\n+colab = [\\n+    \"unsloth[cu121]\",\\n+]\\n colab-ampere = [\\n     \"unsloth[colab-ampere-torch220]\",\\n     \"packaging\",\\n',\n",
       " '@@ -684,6 +684,9 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    if quantization_method.startswith(\"iq2\"):\\n+        raise RuntimeError(\"Unsloth: Currently iq2 type quantizations aren\\'t supported yet - sorry!\")\\n+\\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n     if   model_type == \"llama\":   use_fast_convert = True\\n@@ -743,8 +746,11 @@ def save_to_gguf(\\n         if   first_conversion == \"f32\" : pass\\n         elif first_conversion == \"f16\" : pass\\n         elif first_conversion == \"q8_0\":\\n-            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n-            first_conversion = \"f16\"\\n+            logger.warning_once(\\n+                \"Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, \"\\\\\\n+                \"but saves disk space!\"\\n+            )\\n+            # first_conversion = \"f16\"\\n         pass\\n     pass\\n \\n',\n",
       " '@@ -178,6 +178,9 @@ colab-no-deps = [\\n     \"xformers\",\\n     \"bitsandbytes\",\\n ]\\n+colab = [\\n+    \"unsloth[cu121]\",\\n+]\\n colab-ampere = [\\n     \"unsloth[colab-ampere-torch220]\",\\n     \"packaging\",\\n',\n",
       " '@@ -684,6 +684,9 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    if quantization_method.startswith(\"iq2\"):\\n+        raise RuntimeError(\"Unsloth: Currently iq2 type quantizations aren\\'t supported yet - sorry!\")\\n+\\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n     if   model_type == \"llama\":   use_fast_convert = True\\n@@ -743,8 +746,11 @@ def save_to_gguf(\\n         if   first_conversion == \"f32\" : pass\\n         elif first_conversion == \"f16\" : pass\\n         elif first_conversion == \"q8_0\":\\n-            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n-            first_conversion = \"f16\"\\n+            logger.warning_once(\\n+                \"Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, \"\\\\\\n+                \"but saves disk space!\"\\n+            )\\n+            # first_conversion = \"f16\"\\n         pass\\n     pass\\n \\n',\n",
       " '@@ -178,6 +178,9 @@ colab-no-deps = [\\n     \"xformers\",\\n     \"bitsandbytes\",\\n ]\\n+colab = [\\n+    \"unsloth[cu121]\",\\n+]\\n colab-ampere = [\\n     \"unsloth[colab-ampere-torch220]\",\\n     \"packaging\",\\n',\n",
       " '@@ -684,6 +684,9 @@ def save_to_gguf(\\n ):\\n     from transformers.models.llama.modeling_llama import logger\\n \\n+    if quantization_method.startswith(\"iq2\"):\\n+        raise RuntimeError(\"Unsloth: Currently iq2 type quantizations aren\\'t supported yet - sorry!\")\\n+\\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n     if   model_type == \"llama\":   use_fast_convert = True\\n@@ -743,8 +746,11 @@ def save_to_gguf(\\n         if   first_conversion == \"f32\" : pass\\n         elif first_conversion == \"f16\" : pass\\n         elif first_conversion == \"q8_0\":\\n-            logger.warning_once(\"Unsloth: We must use f16 for quantization first.\")\\n-            first_conversion = \"f16\"\\n+            logger.warning_once(\\n+                \"Unsloth: Using q8_0 for the `first_conversion` will lose a bit of accuracy, \"\\\\\\n+                \"but saves disk space!\"\\n+            )\\n+            # first_conversion = \"f16\"\\n         pass\\n     pass\\n \\n',\n",
       " '@@ -53,8 +53,6 @@ if (major_torch < 2):\\n elif (major_torch == 2) and (minor_torch < 2):\\n     # Disable expandable_segments\\n     del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n-    # Must reimport Pytorch!\\n-    importlib.reload(torch)\\n pass\\n \\n \\n',\n",
       " '@@ -53,8 +53,6 @@ if (major_torch < 2):\\n elif (major_torch == 2) and (minor_torch < 2):\\n     # Disable expandable_segments\\n     del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n-    # Must reimport Pytorch!\\n-    importlib.reload(torch)\\n pass\\n \\n \\n',\n",
       " '@@ -53,8 +53,6 @@ if (major_torch < 2):\\n elif (major_torch == 2) and (minor_torch < 2):\\n     # Disable expandable_segments\\n     del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n-    # Must reimport Pytorch!\\n-    importlib.reload(torch)\\n pass\\n \\n \\n',\n",
       " '@@ -53,8 +53,6 @@ if (major_torch < 2):\\n elif (major_torch == 2) and (minor_torch < 2):\\n     # Disable expandable_segments\\n     del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n-    # Must reimport Pytorch!\\n-    importlib.reload(torch)\\n pass\\n \\n \\n',\n",
       " '@@ -53,8 +53,6 @@ if (major_torch < 2):\\n elif (major_torch == 2) and (minor_torch < 2):\\n     # Disable expandable_segments\\n     del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n-    # Must reimport Pytorch!\\n-    importlib.reload(torch)\\n pass\\n \\n \\n',\n",
       " '@@ -53,8 +53,6 @@ if (major_torch < 2):\\n elif (major_torch == 2) and (minor_torch < 2):\\n     # Disable expandable_segments\\n     del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\\n-    # Must reimport Pytorch!\\n-    importlib.reload(torch)\\n pass\\n \\n \\n',\n",
       " '@@ -505,11 +505,10 @@ def LlamaModel_fast_forward(\\n             position_ids = position_ids.repeat((batch_size, 1))\\n     pass\\n \\n-    # embed positions\\n+    # Embed positions\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Downcast to the correct dtype ie float32 to float16\\n     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n \\n     # Normalized from Gemma\\n@@ -759,6 +758,7 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         else:\\n             logits = self.lm_head(hidden_states)\\n         pass\\n+        logits = logits.to(self.config.torch_dtype)\\n \\n         loss = None\\n         if labels is not None:\\n@@ -929,6 +929,7 @@ class FastLlamaModel:\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -989,6 +990,7 @@ class FastLlamaModel:\\n             token                   = token,\\n             rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -996,9 +998,10 @@ class FastLlamaModel:\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -1338,7 +1341,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n-                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1346,7 +1348,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n-                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1388,9 +1389,17 @@ class FastLlamaModel:\\n \\n         # Now patch lm_head and embed_tokens\\n         if train_embed_tokens:\\n-            model.model.model.embed_tokens.requires_grad_(True)\\n+            print(\"Unsloth: Casting embed_tokens to float32\")\\n+            assert(hasattr(model.model.model.embed_tokens, \"modules_to_save\"))\\n+            model.model.model.embed_tokens.modules_to_save.default.to(torch.float32)\\n+            model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)\\n+        pass\\n+\\n         if train_lm_head:\\n-            model.model.lm_head.requires_grad_(True)\\n+            print(\"Unsloth: Casting lm_head to float32\")\\n+            assert(hasattr(model.model.lm_head, \"modules_to_save\"))\\n+            model.model.lm_head.modules_to_save.default.to(torch.float32)\\n+            model.model.lm_head.modules_to_save.default.requires_grad_(True)\\n         pass\\n \\n         return model\\n',\n",
       " '@@ -74,6 +74,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        trust_remote_code = False,\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n@@ -139,6 +140,7 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n             tokenizer_name = tokenizer_name,\\n+            trust_remote_code = trust_remote_code,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -230,6 +230,7 @@ def MistralForCausalLM_fast_forward(\\n     else:\\n         logits = self.lm_head(hidden_states)\\n     pass\\n+    logits = logits.to(self.config.torch_dtype)\\n \\n     loss = None\\n     if labels is not None:\\n@@ -295,6 +296,7 @@ class FastMistralModel(FastLlamaModel):\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -353,6 +355,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token               = token,\\n             # rope_scaling      = rope_scaling,\\n+            trust_remote_code   = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -360,9 +363,10 @@ class FastMistralModel(FastLlamaModel):\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n',\n",
       " '@@ -505,11 +505,10 @@ def LlamaModel_fast_forward(\\n             position_ids = position_ids.repeat((batch_size, 1))\\n     pass\\n \\n-    # embed positions\\n+    # Embed positions\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Downcast to the correct dtype ie float32 to float16\\n     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n \\n     # Normalized from Gemma\\n@@ -759,6 +758,7 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         else:\\n             logits = self.lm_head(hidden_states)\\n         pass\\n+        logits = logits.to(self.config.torch_dtype)\\n \\n         loss = None\\n         if labels is not None:\\n@@ -929,6 +929,7 @@ class FastLlamaModel:\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -989,6 +990,7 @@ class FastLlamaModel:\\n             token                   = token,\\n             rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -996,9 +998,10 @@ class FastLlamaModel:\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -1338,7 +1341,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n-                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1346,7 +1348,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n-                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1388,9 +1389,17 @@ class FastLlamaModel:\\n \\n         # Now patch lm_head and embed_tokens\\n         if train_embed_tokens:\\n-            model.model.model.embed_tokens.requires_grad_(True)\\n+            print(\"Unsloth: Casting embed_tokens to float32\")\\n+            assert(hasattr(model.model.model.embed_tokens, \"modules_to_save\"))\\n+            model.model.model.embed_tokens.modules_to_save.default.to(torch.float32)\\n+            model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)\\n+        pass\\n+\\n         if train_lm_head:\\n-            model.model.lm_head.requires_grad_(True)\\n+            print(\"Unsloth: Casting lm_head to float32\")\\n+            assert(hasattr(model.model.lm_head, \"modules_to_save\"))\\n+            model.model.lm_head.modules_to_save.default.to(torch.float32)\\n+            model.model.lm_head.modules_to_save.default.requires_grad_(True)\\n         pass\\n \\n         return model\\n',\n",
       " '@@ -74,6 +74,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        trust_remote_code = False,\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n@@ -139,6 +140,7 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n             tokenizer_name = tokenizer_name,\\n+            trust_remote_code = trust_remote_code,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -230,6 +230,7 @@ def MistralForCausalLM_fast_forward(\\n     else:\\n         logits = self.lm_head(hidden_states)\\n     pass\\n+    logits = logits.to(self.config.torch_dtype)\\n \\n     loss = None\\n     if labels is not None:\\n@@ -295,6 +296,7 @@ class FastMistralModel(FastLlamaModel):\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -353,6 +355,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token               = token,\\n             # rope_scaling      = rope_scaling,\\n+            trust_remote_code   = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -360,9 +363,10 @@ class FastMistralModel(FastLlamaModel):\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n',\n",
       " '@@ -505,11 +505,10 @@ def LlamaModel_fast_forward(\\n             position_ids = position_ids.repeat((batch_size, 1))\\n     pass\\n \\n-    # embed positions\\n+    # Embed positions\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Downcast to the correct dtype ie float32 to float16\\n     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n \\n     # Normalized from Gemma\\n@@ -759,6 +758,7 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         else:\\n             logits = self.lm_head(hidden_states)\\n         pass\\n+        logits = logits.to(self.config.torch_dtype)\\n \\n         loss = None\\n         if labels is not None:\\n@@ -929,6 +929,7 @@ class FastLlamaModel:\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -989,6 +990,7 @@ class FastLlamaModel:\\n             token                   = token,\\n             rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -996,9 +998,10 @@ class FastLlamaModel:\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -1338,7 +1341,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n-                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1346,7 +1348,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n-                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1388,9 +1389,17 @@ class FastLlamaModel:\\n \\n         # Now patch lm_head and embed_tokens\\n         if train_embed_tokens:\\n-            model.model.model.embed_tokens.requires_grad_(True)\\n+            print(\"Unsloth: Casting embed_tokens to float32\")\\n+            assert(hasattr(model.model.model.embed_tokens, \"modules_to_save\"))\\n+            model.model.model.embed_tokens.modules_to_save.default.to(torch.float32)\\n+            model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)\\n+        pass\\n+\\n         if train_lm_head:\\n-            model.model.lm_head.requires_grad_(True)\\n+            print(\"Unsloth: Casting lm_head to float32\")\\n+            assert(hasattr(model.model.lm_head, \"modules_to_save\"))\\n+            model.model.lm_head.modules_to_save.default.to(torch.float32)\\n+            model.model.lm_head.modules_to_save.default.requires_grad_(True)\\n         pass\\n \\n         return model\\n',\n",
       " '@@ -74,6 +74,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        trust_remote_code = False,\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n@@ -139,6 +140,7 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n             tokenizer_name = tokenizer_name,\\n+            trust_remote_code = trust_remote_code,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -230,6 +230,7 @@ def MistralForCausalLM_fast_forward(\\n     else:\\n         logits = self.lm_head(hidden_states)\\n     pass\\n+    logits = logits.to(self.config.torch_dtype)\\n \\n     loss = None\\n     if labels is not None:\\n@@ -295,6 +296,7 @@ class FastMistralModel(FastLlamaModel):\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -353,6 +355,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token               = token,\\n             # rope_scaling      = rope_scaling,\\n+            trust_remote_code   = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -360,9 +363,10 @@ class FastMistralModel(FastLlamaModel):\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n',\n",
       " '@@ -505,11 +505,10 @@ def LlamaModel_fast_forward(\\n             position_ids = position_ids.repeat((batch_size, 1))\\n     pass\\n \\n-    # embed positions\\n+    # Embed positions\\n     if inputs_embeds is None:\\n         inputs_embeds = self.embed_tokens(input_ids)\\n \\n-    # Downcast to the correct dtype ie float32 to float16\\n     inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\\n \\n     # Normalized from Gemma\\n@@ -759,6 +758,7 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         else:\\n             logits = self.lm_head(hidden_states)\\n         pass\\n+        logits = logits.to(self.config.torch_dtype)\\n \\n         loss = None\\n         if labels is not None:\\n@@ -929,6 +929,7 @@ class FastLlamaModel:\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n@@ -989,6 +990,7 @@ class FastLlamaModel:\\n             token                   = token,\\n             rope_scaling            = rope_scaling,\\n             max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -996,9 +998,10 @@ class FastLlamaModel:\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n@@ -1338,7 +1341,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n-                model.model.embed_tokens.to(torch.float32, non_blocking = True)\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1346,7 +1348,6 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n-                model.lm_head.to(torch.float32, non_blocking = True)\\n \\n             else:\\n                 assert(module in accepted_modules)\\n@@ -1388,9 +1389,17 @@ class FastLlamaModel:\\n \\n         # Now patch lm_head and embed_tokens\\n         if train_embed_tokens:\\n-            model.model.model.embed_tokens.requires_grad_(True)\\n+            print(\"Unsloth: Casting embed_tokens to float32\")\\n+            assert(hasattr(model.model.model.embed_tokens, \"modules_to_save\"))\\n+            model.model.model.embed_tokens.modules_to_save.default.to(torch.float32)\\n+            model.model.model.embed_tokens.modules_to_save.default.requires_grad_(True)\\n+        pass\\n+\\n         if train_lm_head:\\n-            model.model.lm_head.requires_grad_(True)\\n+            print(\"Unsloth: Casting lm_head to float32\")\\n+            assert(hasattr(model.model.lm_head, \"modules_to_save\"))\\n+            model.model.lm_head.modules_to_save.default.to(torch.float32)\\n+            model.model.lm_head.modules_to_save.default.requires_grad_(True)\\n         pass\\n \\n         return model\\n',\n",
       " '@@ -74,6 +74,7 @@ class FastLanguageModel(FastLlamaModel):\\n         device_map     = \"sequential\",\\n         rope_scaling   = None,\\n         fix_tokenizer  = True,\\n+        trust_remote_code = False,\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n@@ -139,6 +140,7 @@ class FastLanguageModel(FastLlamaModel):\\n             fix_tokenizer  = fix_tokenizer,\\n             model_patcher  = dispatch_model,\\n             tokenizer_name = tokenizer_name,\\n+            trust_remote_code = trust_remote_code,\\n             *args, **kwargs,\\n         )\\n \\n',\n",
       " '@@ -230,6 +230,7 @@ def MistralForCausalLM_fast_forward(\\n     else:\\n         logits = self.lm_head(hidden_states)\\n     pass\\n+    logits = logits.to(self.config.torch_dtype)\\n \\n     loss = None\\n     if labels is not None:\\n@@ -295,6 +296,7 @@ class FastMistralModel(FastLlamaModel):\\n         fix_tokenizer  = True,\\n         model_patcher  = None,\\n         tokenizer_name = None,\\n+        trust_remote_code = False,\\n         **kwargs,\\n     ):\\n         if model_patcher is None: model_patcher = FastMistralModel\\n@@ -353,6 +355,7 @@ class FastMistralModel(FastLlamaModel):\\n             quantization_config = bnb_config,\\n             token               = token,\\n             # rope_scaling      = rope_scaling,\\n+            trust_remote_code   = trust_remote_code,\\n             **kwargs,\\n         )\\n \\n@@ -360,9 +363,10 @@ class FastMistralModel(FastLlamaModel):\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n         tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = max_position_embeddings,\\n-            padding_side     = \"right\",\\n-            token            = token,\\n+            model_max_length  = max_position_embeddings,\\n+            padding_side      = \"right\",\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n         )\\n \\n         model, tokenizer = patch_tokenizer(model, tokenizer)\\n',\n",
       " '@@ -143,7 +143,7 @@ pass\\n from math import sqrt as math_sqrt\\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n',\n",
       " '@@ -657,7 +657,7 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n@@ -753,7 +753,8 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n         if bsz == 1 and q_len == 1:\\n-            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            lm_head = self.lm_head.weight\\n+            logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n             logits = self.lm_head(hidden_states)\\n@@ -893,6 +894,16 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n pass\\n \\n \\n+def _wrap_fast_inference(generate, device_type, dtype):\\n+    # Wraps inference with bfloat16 / float16\\n+    @torch.inference_mode\\n+    def _fast_generate(*args, **kwargs):\\n+        with torch.autocast(device_type = device_type, dtype = dtype):\\n+            return generate(*args, **kwargs)\\n+    return _fast_generate\\n+pass\\n+\\n+\\n class FastLlamaModel:\\n \\n     @staticmethod\\n@@ -1581,6 +1592,15 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = False\\n             internal_model.training = False\\n         pass\\n+\\n+        # Also check if lm_head / embeddings are trained\\n+        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        device_type = lm_head.device.type\\n+        dtype = model.config.torch_dtype\\n+\\n+        # Wrap model.generate\\n+        model._unwrapped_old_generate = model.generate\\n+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)\\n     pass\\n \\n \\n@@ -1601,5 +1621,14 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = use_gradient_checkpointing\\n             internal_model.training = True\\n         pass\\n+\\n+        # Also revert model.generate\\n+        if hasattr(model, \"_unwrapped_old_generate\"):\\n+            model.generate = model._unwrapped_old_generate\\n+            del model._unwrapped_old_generate\\n+        pass\\n     pass\\n pass\\n+\\n+\\n+\\n',\n",
       " '@@ -225,7 +225,8 @@ def MistralForCausalLM_fast_forward(\\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n     if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        lm_head = self.lm_head.weight\\n+        logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n         logits = self.lm_head(hidden_states)\\n',\n",
       " '@@ -32,6 +32,8 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n+# Check Kaggle\\n+IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n \\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n@@ -66,9 +68,9 @@ ALLOWED_QUANTS = \\\\\\n     \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n     \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n     \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n-    \"iq2_xxs\" : \"2.06 bpw quantization\",\\n-    \"iq2_xs\"  : \"2.31 bpw quantization\",\\n-    \"iq3_xxs\" : \"3.06 bpw quantization\",\\n+    # \"iq2_xxs\" : \"2.06 bpw quantization\", # Not supported sadly\\n+    # \"iq2_xs\"  : \"2.31 bpw quantization\",\\n+    # \"iq3_xxs\" : \"3.06 bpw quantization\",\\n     \"q3_k_xs\" : \"3-bit extra small quantization\",\\n }\\n \\n@@ -79,6 +81,27 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def _free_cached_model(model):\\n+    from huggingface_hub import scan_cache_dir\\n+    cached_repos = list(scan_cache_dir().repos)\\n+\\n+    # Go through every cached repo, and delete the one that matches the model we want to save.\\n+    # Can save 4GB of disk space - useful for Kaggle systems.\\n+    for cached_repo in cached_repos:\\n+        if cached_repo.repo_id == model.config._name_or_path:\\n+            remove_cache_commit = list(cached_repo.revisions)[0].commit_hash\\n+            delete_strategy = scan_cache_dir().delete_revisions(remove_cache_commit,)\\n+\\n+            logger.warning_once(\\n+                \"Unsloth: Will remove a cached repo with size \" + \\\\\\n+                delete_strategy.expected_freed_size_str,\\n+            )\\n+\\n+            delete_strategy.execute()\\n+        pass\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n \\n@@ -153,6 +176,19 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -411,6 +447,16 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n+    # Check if Kaggle, since only 20GB of Disk space allowed.\\n+    if IS_A_KAGGLE_ENVIRONMENT:\\n+        # We free up 4GB of space\\n+        logger.warning_once(\\n+            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+        )\\n+        _free_cached_model(internal_model)\\n+    pass\\n+\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n@@ -480,12 +526,35 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n+    # First check if we\\'re pushing to an organization!\\n+    save_directory = save_pretrained_settings[\"save_directory\"]\\n+\\n+    if save_pretrained_settings[\"push_to_hub\"]:\\n+        new_save_directory, new_username = _determine_username(save_directory, username, token)\\n+\\n+        if token is not None:\\n+            from huggingface_hub import whoami\\n+            actual_username = whoami(token = token)[\"name\"]\\n+        else:\\n+            actual_username = username\\n+    pass\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # We upload everything at the end!\\n+        tokenizer_save_settings[\"push_to_hub\"] = False\\n+        tokenizer_save_settings[\"save_directory\"] = new_save_directory\\n+    pass\\n+\\n+    # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n+    pass\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n \\n@@ -502,7 +571,35 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # Pushing to organization!\\n+        # Sadly .save_pretrained doesn\\'t work :(\\n+        # We first save it via .save_pretrained, then upload manually!\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_pretrained_settings[\"push_to_hub\"] = False\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+        # Now manually go through each file and upload them manually!\\n+        filenames = os.listdir(new_save_directory)\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n+\\n+        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        hf_api.upload_folder(\\n+            folder_path = new_save_directory,\\n+            path_in_repo = \".\",\\n+            repo_id = new_save_directory,\\n+            repo_type = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n+            ignore_patterns = \"*.md\",\\n+        )\\n+    else:\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+    pass\\n \\n     # Revert config back\\n     original_model = model\\n@@ -616,13 +713,16 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking():\\n+def install_llama_cpp_blocking(use_cuda = True):\\n+    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n+\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -954,17 +1054,8 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/\\n [<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n \"\"\"\\n \\n-def upload_to_huggingface(\\n-    model,\\n-    save_directory,\\n-    token,\\n-    method,\\n-    extra = \"\",\\n-    file_location = None,\\n-    old_username = None,\\n-    private = None,\\n-):\\n-    # Check for username\\n+\\n+def _determine_username(save_directory, old_username, token):\\n     username = \"\"\\n     save_directory = save_directory.lstrip(\"./\")\\n     if \"/\" not in save_directory:\\n@@ -980,6 +1071,21 @@ def upload_to_huggingface(\\n     else:\\n         username = save_directory.split(\"/\")[0]\\n     pass\\n+    return save_directory, username\\n+pass\\n+\\n+\\n+def upload_to_huggingface(\\n+    model,\\n+    save_directory,\\n+    token,\\n+    method,\\n+    extra = \"\",\\n+    file_location = None,\\n+    old_username = None,\\n+    private = None,\\n+):\\n+    save_directory, username = _determine_username(save_directory, old_username, token)\\n \\n     from huggingface_hub import create_repo\\n     try:\\n@@ -1107,18 +1213,15 @@ def unsloth_save_pretrained_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n@@ -1126,6 +1229,28 @@ def unsloth_save_pretrained_gguf(\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n@@ -1208,25 +1333,44 @@ def unsloth_push_to_hub_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n-            makefile = install_llama_cpp_make_non_blocking()\\n+            makefile  = install_llama_cpp_make_non_blocking()\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n',\n",
       " '@@ -143,7 +143,7 @@ pass\\n from math import sqrt as math_sqrt\\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n',\n",
       " '@@ -657,7 +657,7 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n@@ -753,7 +753,8 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n         if bsz == 1 and q_len == 1:\\n-            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            lm_head = self.lm_head.weight\\n+            logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n             logits = self.lm_head(hidden_states)\\n@@ -893,6 +894,16 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n pass\\n \\n \\n+def _wrap_fast_inference(generate, device_type, dtype):\\n+    # Wraps inference with bfloat16 / float16\\n+    @torch.inference_mode\\n+    def _fast_generate(*args, **kwargs):\\n+        with torch.autocast(device_type = device_type, dtype = dtype):\\n+            return generate(*args, **kwargs)\\n+    return _fast_generate\\n+pass\\n+\\n+\\n class FastLlamaModel:\\n \\n     @staticmethod\\n@@ -1581,6 +1592,15 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = False\\n             internal_model.training = False\\n         pass\\n+\\n+        # Also check if lm_head / embeddings are trained\\n+        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        device_type = lm_head.device.type\\n+        dtype = model.config.torch_dtype\\n+\\n+        # Wrap model.generate\\n+        model._unwrapped_old_generate = model.generate\\n+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)\\n     pass\\n \\n \\n@@ -1601,5 +1621,14 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = use_gradient_checkpointing\\n             internal_model.training = True\\n         pass\\n+\\n+        # Also revert model.generate\\n+        if hasattr(model, \"_unwrapped_old_generate\"):\\n+            model.generate = model._unwrapped_old_generate\\n+            del model._unwrapped_old_generate\\n+        pass\\n     pass\\n pass\\n+\\n+\\n+\\n',\n",
       " '@@ -225,7 +225,8 @@ def MistralForCausalLM_fast_forward(\\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n     if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        lm_head = self.lm_head.weight\\n+        logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n         logits = self.lm_head(hidden_states)\\n',\n",
       " '@@ -32,6 +32,8 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n+# Check Kaggle\\n+IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n \\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n@@ -66,9 +68,9 @@ ALLOWED_QUANTS = \\\\\\n     \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n     \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n     \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n-    \"iq2_xxs\" : \"2.06 bpw quantization\",\\n-    \"iq2_xs\"  : \"2.31 bpw quantization\",\\n-    \"iq3_xxs\" : \"3.06 bpw quantization\",\\n+    # \"iq2_xxs\" : \"2.06 bpw quantization\", # Not supported sadly\\n+    # \"iq2_xs\"  : \"2.31 bpw quantization\",\\n+    # \"iq3_xxs\" : \"3.06 bpw quantization\",\\n     \"q3_k_xs\" : \"3-bit extra small quantization\",\\n }\\n \\n@@ -79,6 +81,27 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def _free_cached_model(model):\\n+    from huggingface_hub import scan_cache_dir\\n+    cached_repos = list(scan_cache_dir().repos)\\n+\\n+    # Go through every cached repo, and delete the one that matches the model we want to save.\\n+    # Can save 4GB of disk space - useful for Kaggle systems.\\n+    for cached_repo in cached_repos:\\n+        if cached_repo.repo_id == model.config._name_or_path:\\n+            remove_cache_commit = list(cached_repo.revisions)[0].commit_hash\\n+            delete_strategy = scan_cache_dir().delete_revisions(remove_cache_commit,)\\n+\\n+            logger.warning_once(\\n+                \"Unsloth: Will remove a cached repo with size \" + \\\\\\n+                delete_strategy.expected_freed_size_str,\\n+            )\\n+\\n+            delete_strategy.execute()\\n+        pass\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n \\n@@ -153,6 +176,19 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -411,6 +447,16 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n+    # Check if Kaggle, since only 20GB of Disk space allowed.\\n+    if IS_A_KAGGLE_ENVIRONMENT:\\n+        # We free up 4GB of space\\n+        logger.warning_once(\\n+            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+        )\\n+        _free_cached_model(internal_model)\\n+    pass\\n+\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n@@ -480,12 +526,35 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n+    # First check if we\\'re pushing to an organization!\\n+    save_directory = save_pretrained_settings[\"save_directory\"]\\n+\\n+    if save_pretrained_settings[\"push_to_hub\"]:\\n+        new_save_directory, new_username = _determine_username(save_directory, username, token)\\n+\\n+        if token is not None:\\n+            from huggingface_hub import whoami\\n+            actual_username = whoami(token = token)[\"name\"]\\n+        else:\\n+            actual_username = username\\n+    pass\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # We upload everything at the end!\\n+        tokenizer_save_settings[\"push_to_hub\"] = False\\n+        tokenizer_save_settings[\"save_directory\"] = new_save_directory\\n+    pass\\n+\\n+    # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n+    pass\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n \\n@@ -502,7 +571,35 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # Pushing to organization!\\n+        # Sadly .save_pretrained doesn\\'t work :(\\n+        # We first save it via .save_pretrained, then upload manually!\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_pretrained_settings[\"push_to_hub\"] = False\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+        # Now manually go through each file and upload them manually!\\n+        filenames = os.listdir(new_save_directory)\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n+\\n+        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        hf_api.upload_folder(\\n+            folder_path = new_save_directory,\\n+            path_in_repo = \".\",\\n+            repo_id = new_save_directory,\\n+            repo_type = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n+            ignore_patterns = \"*.md\",\\n+        )\\n+    else:\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+    pass\\n \\n     # Revert config back\\n     original_model = model\\n@@ -616,13 +713,16 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking():\\n+def install_llama_cpp_blocking(use_cuda = True):\\n+    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n+\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -954,17 +1054,8 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/\\n [<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n \"\"\"\\n \\n-def upload_to_huggingface(\\n-    model,\\n-    save_directory,\\n-    token,\\n-    method,\\n-    extra = \"\",\\n-    file_location = None,\\n-    old_username = None,\\n-    private = None,\\n-):\\n-    # Check for username\\n+\\n+def _determine_username(save_directory, old_username, token):\\n     username = \"\"\\n     save_directory = save_directory.lstrip(\"./\")\\n     if \"/\" not in save_directory:\\n@@ -980,6 +1071,21 @@ def upload_to_huggingface(\\n     else:\\n         username = save_directory.split(\"/\")[0]\\n     pass\\n+    return save_directory, username\\n+pass\\n+\\n+\\n+def upload_to_huggingface(\\n+    model,\\n+    save_directory,\\n+    token,\\n+    method,\\n+    extra = \"\",\\n+    file_location = None,\\n+    old_username = None,\\n+    private = None,\\n+):\\n+    save_directory, username = _determine_username(save_directory, old_username, token)\\n \\n     from huggingface_hub import create_repo\\n     try:\\n@@ -1107,18 +1213,15 @@ def unsloth_save_pretrained_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n@@ -1126,6 +1229,28 @@ def unsloth_save_pretrained_gguf(\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n@@ -1208,25 +1333,44 @@ def unsloth_push_to_hub_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n-            makefile = install_llama_cpp_make_non_blocking()\\n+            makefile  = install_llama_cpp_make_non_blocking()\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n',\n",
       " '@@ -143,7 +143,7 @@ pass\\n from math import sqrt as math_sqrt\\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n',\n",
       " '@@ -657,7 +657,7 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n@@ -753,7 +753,8 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n         if bsz == 1 and q_len == 1:\\n-            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            lm_head = self.lm_head.weight\\n+            logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n             logits = self.lm_head(hidden_states)\\n@@ -893,6 +894,16 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n pass\\n \\n \\n+def _wrap_fast_inference(generate, device_type, dtype):\\n+    # Wraps inference with bfloat16 / float16\\n+    @torch.inference_mode\\n+    def _fast_generate(*args, **kwargs):\\n+        with torch.autocast(device_type = device_type, dtype = dtype):\\n+            return generate(*args, **kwargs)\\n+    return _fast_generate\\n+pass\\n+\\n+\\n class FastLlamaModel:\\n \\n     @staticmethod\\n@@ -1581,6 +1592,15 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = False\\n             internal_model.training = False\\n         pass\\n+\\n+        # Also check if lm_head / embeddings are trained\\n+        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        device_type = lm_head.device.type\\n+        dtype = model.config.torch_dtype\\n+\\n+        # Wrap model.generate\\n+        model._unwrapped_old_generate = model.generate\\n+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)\\n     pass\\n \\n \\n@@ -1601,5 +1621,14 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = use_gradient_checkpointing\\n             internal_model.training = True\\n         pass\\n+\\n+        # Also revert model.generate\\n+        if hasattr(model, \"_unwrapped_old_generate\"):\\n+            model.generate = model._unwrapped_old_generate\\n+            del model._unwrapped_old_generate\\n+        pass\\n     pass\\n pass\\n+\\n+\\n+\\n',\n",
       " '@@ -225,7 +225,8 @@ def MistralForCausalLM_fast_forward(\\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n     if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        lm_head = self.lm_head.weight\\n+        logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n         logits = self.lm_head(hidden_states)\\n',\n",
       " '@@ -32,6 +32,8 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n+# Check Kaggle\\n+IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n \\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n@@ -66,9 +68,9 @@ ALLOWED_QUANTS = \\\\\\n     \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n     \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n     \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n-    \"iq2_xxs\" : \"2.06 bpw quantization\",\\n-    \"iq2_xs\"  : \"2.31 bpw quantization\",\\n-    \"iq3_xxs\" : \"3.06 bpw quantization\",\\n+    # \"iq2_xxs\" : \"2.06 bpw quantization\", # Not supported sadly\\n+    # \"iq2_xs\"  : \"2.31 bpw quantization\",\\n+    # \"iq3_xxs\" : \"3.06 bpw quantization\",\\n     \"q3_k_xs\" : \"3-bit extra small quantization\",\\n }\\n \\n@@ -79,6 +81,27 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def _free_cached_model(model):\\n+    from huggingface_hub import scan_cache_dir\\n+    cached_repos = list(scan_cache_dir().repos)\\n+\\n+    # Go through every cached repo, and delete the one that matches the model we want to save.\\n+    # Can save 4GB of disk space - useful for Kaggle systems.\\n+    for cached_repo in cached_repos:\\n+        if cached_repo.repo_id == model.config._name_or_path:\\n+            remove_cache_commit = list(cached_repo.revisions)[0].commit_hash\\n+            delete_strategy = scan_cache_dir().delete_revisions(remove_cache_commit,)\\n+\\n+            logger.warning_once(\\n+                \"Unsloth: Will remove a cached repo with size \" + \\\\\\n+                delete_strategy.expected_freed_size_str,\\n+            )\\n+\\n+            delete_strategy.execute()\\n+        pass\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n \\n@@ -153,6 +176,19 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -411,6 +447,16 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n+    # Check if Kaggle, since only 20GB of Disk space allowed.\\n+    if IS_A_KAGGLE_ENVIRONMENT:\\n+        # We free up 4GB of space\\n+        logger.warning_once(\\n+            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+        )\\n+        _free_cached_model(internal_model)\\n+    pass\\n+\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n@@ -480,12 +526,35 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n+    # First check if we\\'re pushing to an organization!\\n+    save_directory = save_pretrained_settings[\"save_directory\"]\\n+\\n+    if save_pretrained_settings[\"push_to_hub\"]:\\n+        new_save_directory, new_username = _determine_username(save_directory, username, token)\\n+\\n+        if token is not None:\\n+            from huggingface_hub import whoami\\n+            actual_username = whoami(token = token)[\"name\"]\\n+        else:\\n+            actual_username = username\\n+    pass\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # We upload everything at the end!\\n+        tokenizer_save_settings[\"push_to_hub\"] = False\\n+        tokenizer_save_settings[\"save_directory\"] = new_save_directory\\n+    pass\\n+\\n+    # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n+    pass\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n \\n@@ -502,7 +571,35 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # Pushing to organization!\\n+        # Sadly .save_pretrained doesn\\'t work :(\\n+        # We first save it via .save_pretrained, then upload manually!\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_pretrained_settings[\"push_to_hub\"] = False\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+        # Now manually go through each file and upload them manually!\\n+        filenames = os.listdir(new_save_directory)\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n+\\n+        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        hf_api.upload_folder(\\n+            folder_path = new_save_directory,\\n+            path_in_repo = \".\",\\n+            repo_id = new_save_directory,\\n+            repo_type = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n+            ignore_patterns = \"*.md\",\\n+        )\\n+    else:\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+    pass\\n \\n     # Revert config back\\n     original_model = model\\n@@ -616,13 +713,16 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking():\\n+def install_llama_cpp_blocking(use_cuda = True):\\n+    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n+\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -954,17 +1054,8 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/\\n [<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n \"\"\"\\n \\n-def upload_to_huggingface(\\n-    model,\\n-    save_directory,\\n-    token,\\n-    method,\\n-    extra = \"\",\\n-    file_location = None,\\n-    old_username = None,\\n-    private = None,\\n-):\\n-    # Check for username\\n+\\n+def _determine_username(save_directory, old_username, token):\\n     username = \"\"\\n     save_directory = save_directory.lstrip(\"./\")\\n     if \"/\" not in save_directory:\\n@@ -980,6 +1071,21 @@ def upload_to_huggingface(\\n     else:\\n         username = save_directory.split(\"/\")[0]\\n     pass\\n+    return save_directory, username\\n+pass\\n+\\n+\\n+def upload_to_huggingface(\\n+    model,\\n+    save_directory,\\n+    token,\\n+    method,\\n+    extra = \"\",\\n+    file_location = None,\\n+    old_username = None,\\n+    private = None,\\n+):\\n+    save_directory, username = _determine_username(save_directory, old_username, token)\\n \\n     from huggingface_hub import create_repo\\n     try:\\n@@ -1107,18 +1213,15 @@ def unsloth_save_pretrained_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n@@ -1126,6 +1229,28 @@ def unsloth_save_pretrained_gguf(\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n@@ -1208,25 +1333,44 @@ def unsloth_push_to_hub_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n-            makefile = install_llama_cpp_make_non_blocking()\\n+            makefile  = install_llama_cpp_make_non_blocking()\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n',\n",
       " '@@ -143,7 +143,7 @@ pass\\n from math import sqrt as math_sqrt\\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n',\n",
       " '@@ -657,7 +657,7 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-@torch.inference_mode\\n+# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n@@ -753,7 +753,8 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n         if bsz == 1 and q_len == 1:\\n-            logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+            lm_head = self.lm_head.weight\\n+            logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n             logits = self.lm_head(hidden_states)\\n@@ -893,6 +894,16 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n pass\\n \\n \\n+def _wrap_fast_inference(generate, device_type, dtype):\\n+    # Wraps inference with bfloat16 / float16\\n+    @torch.inference_mode\\n+    def _fast_generate(*args, **kwargs):\\n+        with torch.autocast(device_type = device_type, dtype = dtype):\\n+            return generate(*args, **kwargs)\\n+    return _fast_generate\\n+pass\\n+\\n+\\n class FastLlamaModel:\\n \\n     @staticmethod\\n@@ -1581,6 +1592,15 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = False\\n             internal_model.training = False\\n         pass\\n+\\n+        # Also check if lm_head / embeddings are trained\\n+        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        device_type = lm_head.device.type\\n+        dtype = model.config.torch_dtype\\n+\\n+        # Wrap model.generate\\n+        model._unwrapped_old_generate = model.generate\\n+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)\\n     pass\\n \\n \\n@@ -1601,5 +1621,14 @@ class FastLlamaModel:\\n             internal_model.gradient_checkpointing = use_gradient_checkpointing\\n             internal_model.training = True\\n         pass\\n+\\n+        # Also revert model.generate\\n+        if hasattr(model, \"_unwrapped_old_generate\"):\\n+            model.generate = model._unwrapped_old_generate\\n+            del model._unwrapped_old_generate\\n+        pass\\n     pass\\n pass\\n+\\n+\\n+\\n',\n",
       " '@@ -225,7 +225,8 @@ def MistralForCausalLM_fast_forward(\\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n     if bsz == 1 and q_len == 1:\\n-        logits = torch.mv(self.lm_head.weight, hidden_states.ravel())\\n+        lm_head = self.lm_head.weight\\n+        logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n         logits = self.lm_head(hidden_states)\\n',\n",
       " '@@ -32,6 +32,8 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n+# Check Kaggle\\n+IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n \\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n@@ -66,9 +68,9 @@ ALLOWED_QUANTS = \\\\\\n     \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\\n     \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\\n     \"q6_k\"    : \"Uses Q8_K for all tensors\",\\n-    \"iq2_xxs\" : \"2.06 bpw quantization\",\\n-    \"iq2_xs\"  : \"2.31 bpw quantization\",\\n-    \"iq3_xxs\" : \"3.06 bpw quantization\",\\n+    # \"iq2_xxs\" : \"2.06 bpw quantization\", # Not supported sadly\\n+    # \"iq2_xs\"  : \"2.31 bpw quantization\",\\n+    # \"iq3_xxs\" : \"3.06 bpw quantization\",\\n     \"q3_k_xs\" : \"3-bit extra small quantization\",\\n }\\n \\n@@ -79,6 +81,27 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def _free_cached_model(model):\\n+    from huggingface_hub import scan_cache_dir\\n+    cached_repos = list(scan_cache_dir().repos)\\n+\\n+    # Go through every cached repo, and delete the one that matches the model we want to save.\\n+    # Can save 4GB of disk space - useful for Kaggle systems.\\n+    for cached_repo in cached_repos:\\n+        if cached_repo.repo_id == model.config._name_or_path:\\n+            remove_cache_commit = list(cached_repo.revisions)[0].commit_hash\\n+            delete_strategy = scan_cache_dir().delete_revisions(remove_cache_commit,)\\n+\\n+            logger.warning_once(\\n+                \"Unsloth: Will remove a cached repo with size \" + \\\\\\n+                delete_strategy.expected_freed_size_str,\\n+            )\\n+\\n+            delete_strategy.execute()\\n+        pass\\n+    pass\\n+pass\\n+\\n \\n def _merge_lora(layer, name):\\n \\n@@ -153,6 +176,19 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -411,6 +447,16 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n+    # Check if Kaggle, since only 20GB of Disk space allowed.\\n+    if IS_A_KAGGLE_ENVIRONMENT:\\n+        # We free up 4GB of space\\n+        logger.warning_once(\\n+            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+        )\\n+        _free_cached_model(internal_model)\\n+    pass\\n+\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n@@ -480,12 +526,35 @@ def unsloth_save_model(\\n         )\\n     pass\\n \\n+    # First check if we\\'re pushing to an organization!\\n+    save_directory = save_pretrained_settings[\"save_directory\"]\\n+\\n+    if save_pretrained_settings[\"push_to_hub\"]:\\n+        new_save_directory, new_username = _determine_username(save_directory, username, token)\\n+\\n+        if token is not None:\\n+            from huggingface_hub import whoami\\n+            actual_username = whoami(token = token)[\"name\"]\\n+        else:\\n+            actual_username = username\\n+    pass\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # We upload everything at the end!\\n+        tokenizer_save_settings[\"push_to_hub\"] = False\\n+        tokenizer_save_settings[\"save_directory\"] = new_save_directory\\n+    pass\\n+\\n+    # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n         print(\" Done.\")\\n     else:\\n         print()\\n+    pass\\n \\n     print(\"Unsloth: Saving model... This might take 5 minutes for Llama-7b...\")\\n \\n@@ -502,7 +571,35 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-    internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+    # Check if pushing to an organization\\n+    if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n+        print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n+        # Pushing to organization!\\n+        # Sadly .save_pretrained doesn\\'t work :(\\n+        # We first save it via .save_pretrained, then upload manually!\\n+        save_pretrained_settings[\"save_directory\"] = new_save_directory\\n+        save_pretrained_settings[\"push_to_hub\"] = False\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+\\n+        # Now manually go through each file and upload them manually!\\n+        filenames = os.listdir(new_save_directory)\\n+\\n+        from huggingface_hub import HfApi\\n+        hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n+\\n+        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        hf_api.upload_folder(\\n+            folder_path = new_save_directory,\\n+            path_in_repo = \".\",\\n+            repo_id = new_save_directory,\\n+            repo_type = \"model\",\\n+            commit_message  = \"(Trained with Unsloth)\",\\n+            ignore_patterns = \"*.md\",\\n+        )\\n+    else:\\n+        internal_model.save_pretrained(**save_pretrained_settings)\\n+    pass\\n \\n     # Revert config back\\n     original_model = model\\n@@ -616,13 +713,16 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking():\\n+def install_llama_cpp_blocking(use_cuda = True):\\n+    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j{psutil.cpu_count()*2}\",\\n+        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n+\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n@@ -954,17 +1054,8 @@ This {model_type} model was trained 2x faster with [Unsloth](https://github.com/\\n [<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n \"\"\"\\n \\n-def upload_to_huggingface(\\n-    model,\\n-    save_directory,\\n-    token,\\n-    method,\\n-    extra = \"\",\\n-    file_location = None,\\n-    old_username = None,\\n-    private = None,\\n-):\\n-    # Check for username\\n+\\n+def _determine_username(save_directory, old_username, token):\\n     username = \"\"\\n     save_directory = save_directory.lstrip(\"./\")\\n     if \"/\" not in save_directory:\\n@@ -980,6 +1071,21 @@ def upload_to_huggingface(\\n     else:\\n         username = save_directory.split(\"/\")[0]\\n     pass\\n+    return save_directory, username\\n+pass\\n+\\n+\\n+def upload_to_huggingface(\\n+    model,\\n+    save_directory,\\n+    token,\\n+    method,\\n+    extra = \"\",\\n+    file_location = None,\\n+    old_username = None,\\n+    private = None,\\n+):\\n+    save_directory, username = _determine_username(save_directory, old_username, token)\\n \\n     from huggingface_hub import create_repo\\n     try:\\n@@ -1107,18 +1213,15 @@ def unsloth_save_pretrained_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n@@ -1126,6 +1229,28 @@ def unsloth_save_pretrained_gguf(\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n@@ -1208,25 +1333,44 @@ def unsloth_push_to_hub_gguf(\\n \\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n-        git_clone = install_llama_cpp_clone_non_blocking()\\n-        python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n-        git_clone.wait()\\n-        makefile  = install_llama_cpp_make_non_blocking()\\n-        new_save_directory, old_username = unsloth_save_model(**arguments)\\n-        python_install.wait()\\n-    else:\\n-        try:\\n+\\n+        if IS_A_KAGGLE_ENVIRONMENT:\\n+            # Kaggle is weird - no blocking installs, and no CUDA?\\n+            python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda = False)\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             makefile = None\\n-        except:\\n-            # Retry by recloning llama.cpp\\n+        else:\\n             git_clone = install_llama_cpp_clone_non_blocking()\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             git_clone.wait()\\n-            makefile = install_llama_cpp_make_non_blocking()\\n+            makefile  = install_llama_cpp_make_non_blocking()\\n             new_save_directory, old_username = unsloth_save_model(**arguments)\\n             python_install.wait()\\n         pass\\n+    else:\\n+        try:\\n+            new_save_directory, old_username = unsloth_save_model(**arguments)\\n+            makefile = None\\n+        except:\\n+            # Retry by recloning llama.cpp\\n+            if IS_A_KAGGLE_ENVIRONMENT:\\n+                # Kaggle is weird - no blocking installs, and no CUDA?\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                python_install.wait()\\n+                install_llama_cpp_blocking(use_cuda = False)\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                makefile = None\\n+            else:\\n+                git_clone = install_llama_cpp_clone_non_blocking()\\n+                python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n+                git_clone.wait()\\n+                makefile  = install_llama_cpp_make_non_blocking()\\n+                new_save_directory, old_username = unsloth_save_model(**arguments)\\n+                python_install.wait()\\n+            pass\\n+        pass\\n     pass\\n \\n     for _ in range(3):\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers import AutoTokenizer\\n from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n+import numpy as np\\n \\n __version__ = \"2024.3\"\\n \\n@@ -269,3 +270,88 @@ except:\\n         \"Luckily, your training run will still work in the meantime!\"\\n     )\\n pass\\n+\\n+\\n+def _calculate_n_gradient_checkpoints(\\n+    n_layers : int,\\n+    method   : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if method is None: method = \"sqrt\"\\n+\\n+    if method == \"sqrt\":\\n+        n_checkpoints = int(n_layers**0.5)\\n+    elif type(method) is int and method > 0:\\n+        n_checkpoints = int(np.ceil(n_layers / method))\\n+    else:\\n+        raise ValueError(\"method must be \\'sqrt\\' or an int >0 and <= n_layers.\")\\n+\\n+    size = n_layers // n_checkpoints\\n+    sizes = np.full(n_checkpoints, size, dtype = int)\\n+    leftovers = n_layers % n_checkpoints\\n+    # We append leftovers from the right\\n+    for k in range(leftovers):\\n+        sizes[n_checkpoints-1-k] += 1\\n+    boundaries = np.hstack((0, np.cumsum(sizes)))\\n+    boundaries = boundaries.tolist()\\n+    return boundaries\\n+pass\\n+\\n+\\n+def calculate_n_gradient_checkpoints(\\n+    n_layers              : int,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if layers_per_checkpoint is None or layers_per_checkpoint == 1:\\n+        return None\\n+\\n+    boundaries = _calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+\\n+    assert(boundaries[0] == 0 and boundaries[-1] == n_layers)\\n+    assert(min(boundaries) == 0 and max(boundaries) == n_layers)\\n+    assert(np.diff(boundaries).min() >= 0)\\n+    return boundaries\\n+pass\\n+\\n+\\n+def prepare_n_gradient_checkpoints(\\n+    model                 : Any,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+    use_reentrant         : Optional[bool] = True,\\n+) -> None:\\n+    \"\"\"\\n+    Calculates where to place the gradient checkpoints given n_layers.\\n+\\n+    Args:\\n+        model: Any LlamaModel with layers.\\n+        layers_per_checkpoint (`Union[str, int]`, *optional*):\\n+            Can either be `sqrt` or an integer for how many layers per checkpoint you want.\\n+            The more, the less memory usage, but can be slower. Default is `sqrt`.\\n+            Choose 1 for Pytorch gradient checkpointing. 2 to wrap 2 layers in 1 module etc.\\n+        use_reentrant (`bool`, *optional*):\\n+            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354\\n+            Optimal gradient checkpointing algorithm `use_reentrant=False` which will\\n+            be the default in future Pytorch versions doesn\\'t seem to work??\\n+    \"\"\"\\n+    _model = None\\n+    if hasattr(model, \"layers\"):\\n+        _model = model\\n+    elif hasattr(model, \"model\"):\\n+        if hasattr(model.model, \"layers\"):\\n+            _model = model.model\\n+    if _model is None:\\n+        raise TypeError(\"`model` or `model.model` does not have attribute `layers`. Are you sure this is a model?\")\\n+    pass\\n+\\n+    if use_reentrant is False:\\n+        use_reentrant = True\\n+    pass\\n+\\n+    n_layers = len(_model.layers)\\n+    boundaries = calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+    _model._gradient_checkpointing_boundaries    = boundaries\\n+    _model._gradient_checkpointing_use_reentrant = use_reentrant\\n+pass\\n',\n",
       " '@@ -593,6 +593,13 @@ def LlamaModel_fast_forward(\\n     all_self_attns = () if output_attentions else None\\n     next_decoder_cache = () if use_cache else None\\n \\n+    # Gradient checkpointing methods (ie sqrt)\\n+    if hasattr(self, \"_gradient_checkpointing_boundaries\"):\\n+        boundaries = self._gradient_checkpointing_boundaries\\n+    else:\\n+        boundaries = None\\n+    pass\\n+    \\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n',\n",
       " '@@ -24,6 +24,7 @@ from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n import subprocess\\n import psutil\\n+import re\\n \\n __all__ = [\\n     \"print_quantization_methods\",\\n@@ -176,19 +177,6 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n-    # First check for a token!\\n-    if push_to_hub:\\n-        from huggingface_hub import whoami\\n-        try: \\n-            username = whoami(token = token)[\"name\"]\\n-        except:\\n-            raise RuntimeError(\\n-                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n-                \"Go to https://huggingface.co/settings/tokens\"\\n-            )\\n-        pass\\n-    pass\\n-\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -215,7 +203,19 @@ def unsloth_save_model(\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n-    import re\\n+\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n \\n     assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n \\n@@ -588,7 +588,7 @@ def unsloth_save_model(\\n         from huggingface_hub import HfApi\\n         hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n \\n-        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        print(\"Unsloth: Uploading all files... Please wait...\")\\n         hf_api.upload_folder(\\n             folder_path = new_save_directory,\\n             path_in_repo = \".\",\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers import AutoTokenizer\\n from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n+import numpy as np\\n \\n __version__ = \"2024.3\"\\n \\n@@ -269,3 +270,88 @@ except:\\n         \"Luckily, your training run will still work in the meantime!\"\\n     )\\n pass\\n+\\n+\\n+def _calculate_n_gradient_checkpoints(\\n+    n_layers : int,\\n+    method   : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if method is None: method = \"sqrt\"\\n+\\n+    if method == \"sqrt\":\\n+        n_checkpoints = int(n_layers**0.5)\\n+    elif type(method) is int and method > 0:\\n+        n_checkpoints = int(np.ceil(n_layers / method))\\n+    else:\\n+        raise ValueError(\"method must be \\'sqrt\\' or an int >0 and <= n_layers.\")\\n+\\n+    size = n_layers // n_checkpoints\\n+    sizes = np.full(n_checkpoints, size, dtype = int)\\n+    leftovers = n_layers % n_checkpoints\\n+    # We append leftovers from the right\\n+    for k in range(leftovers):\\n+        sizes[n_checkpoints-1-k] += 1\\n+    boundaries = np.hstack((0, np.cumsum(sizes)))\\n+    boundaries = boundaries.tolist()\\n+    return boundaries\\n+pass\\n+\\n+\\n+def calculate_n_gradient_checkpoints(\\n+    n_layers              : int,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if layers_per_checkpoint is None or layers_per_checkpoint == 1:\\n+        return None\\n+\\n+    boundaries = _calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+\\n+    assert(boundaries[0] == 0 and boundaries[-1] == n_layers)\\n+    assert(min(boundaries) == 0 and max(boundaries) == n_layers)\\n+    assert(np.diff(boundaries).min() >= 0)\\n+    return boundaries\\n+pass\\n+\\n+\\n+def prepare_n_gradient_checkpoints(\\n+    model                 : Any,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+    use_reentrant         : Optional[bool] = True,\\n+) -> None:\\n+    \"\"\"\\n+    Calculates where to place the gradient checkpoints given n_layers.\\n+\\n+    Args:\\n+        model: Any LlamaModel with layers.\\n+        layers_per_checkpoint (`Union[str, int]`, *optional*):\\n+            Can either be `sqrt` or an integer for how many layers per checkpoint you want.\\n+            The more, the less memory usage, but can be slower. Default is `sqrt`.\\n+            Choose 1 for Pytorch gradient checkpointing. 2 to wrap 2 layers in 1 module etc.\\n+        use_reentrant (`bool`, *optional*):\\n+            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354\\n+            Optimal gradient checkpointing algorithm `use_reentrant=False` which will\\n+            be the default in future Pytorch versions doesn\\'t seem to work??\\n+    \"\"\"\\n+    _model = None\\n+    if hasattr(model, \"layers\"):\\n+        _model = model\\n+    elif hasattr(model, \"model\"):\\n+        if hasattr(model.model, \"layers\"):\\n+            _model = model.model\\n+    if _model is None:\\n+        raise TypeError(\"`model` or `model.model` does not have attribute `layers`. Are you sure this is a model?\")\\n+    pass\\n+\\n+    if use_reentrant is False:\\n+        use_reentrant = True\\n+    pass\\n+\\n+    n_layers = len(_model.layers)\\n+    boundaries = calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+    _model._gradient_checkpointing_boundaries    = boundaries\\n+    _model._gradient_checkpointing_use_reentrant = use_reentrant\\n+pass\\n',\n",
       " '@@ -593,6 +593,13 @@ def LlamaModel_fast_forward(\\n     all_self_attns = () if output_attentions else None\\n     next_decoder_cache = () if use_cache else None\\n \\n+    # Gradient checkpointing methods (ie sqrt)\\n+    if hasattr(self, \"_gradient_checkpointing_boundaries\"):\\n+        boundaries = self._gradient_checkpointing_boundaries\\n+    else:\\n+        boundaries = None\\n+    pass\\n+    \\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n',\n",
       " '@@ -24,6 +24,7 @@ from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n import subprocess\\n import psutil\\n+import re\\n \\n __all__ = [\\n     \"print_quantization_methods\",\\n@@ -176,19 +177,6 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n-    # First check for a token!\\n-    if push_to_hub:\\n-        from huggingface_hub import whoami\\n-        try: \\n-            username = whoami(token = token)[\"name\"]\\n-        except:\\n-            raise RuntimeError(\\n-                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n-                \"Go to https://huggingface.co/settings/tokens\"\\n-            )\\n-        pass\\n-    pass\\n-\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -215,7 +203,19 @@ def unsloth_save_model(\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n-    import re\\n+\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n \\n     assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n \\n@@ -588,7 +588,7 @@ def unsloth_save_model(\\n         from huggingface_hub import HfApi\\n         hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n \\n-        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        print(\"Unsloth: Uploading all files... Please wait...\")\\n         hf_api.upload_folder(\\n             folder_path = new_save_directory,\\n             path_in_repo = \".\",\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers import AutoTokenizer\\n from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n+import numpy as np\\n \\n __version__ = \"2024.3\"\\n \\n@@ -269,3 +270,88 @@ except:\\n         \"Luckily, your training run will still work in the meantime!\"\\n     )\\n pass\\n+\\n+\\n+def _calculate_n_gradient_checkpoints(\\n+    n_layers : int,\\n+    method   : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if method is None: method = \"sqrt\"\\n+\\n+    if method == \"sqrt\":\\n+        n_checkpoints = int(n_layers**0.5)\\n+    elif type(method) is int and method > 0:\\n+        n_checkpoints = int(np.ceil(n_layers / method))\\n+    else:\\n+        raise ValueError(\"method must be \\'sqrt\\' or an int >0 and <= n_layers.\")\\n+\\n+    size = n_layers // n_checkpoints\\n+    sizes = np.full(n_checkpoints, size, dtype = int)\\n+    leftovers = n_layers % n_checkpoints\\n+    # We append leftovers from the right\\n+    for k in range(leftovers):\\n+        sizes[n_checkpoints-1-k] += 1\\n+    boundaries = np.hstack((0, np.cumsum(sizes)))\\n+    boundaries = boundaries.tolist()\\n+    return boundaries\\n+pass\\n+\\n+\\n+def calculate_n_gradient_checkpoints(\\n+    n_layers              : int,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if layers_per_checkpoint is None or layers_per_checkpoint == 1:\\n+        return None\\n+\\n+    boundaries = _calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+\\n+    assert(boundaries[0] == 0 and boundaries[-1] == n_layers)\\n+    assert(min(boundaries) == 0 and max(boundaries) == n_layers)\\n+    assert(np.diff(boundaries).min() >= 0)\\n+    return boundaries\\n+pass\\n+\\n+\\n+def prepare_n_gradient_checkpoints(\\n+    model                 : Any,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+    use_reentrant         : Optional[bool] = True,\\n+) -> None:\\n+    \"\"\"\\n+    Calculates where to place the gradient checkpoints given n_layers.\\n+\\n+    Args:\\n+        model: Any LlamaModel with layers.\\n+        layers_per_checkpoint (`Union[str, int]`, *optional*):\\n+            Can either be `sqrt` or an integer for how many layers per checkpoint you want.\\n+            The more, the less memory usage, but can be slower. Default is `sqrt`.\\n+            Choose 1 for Pytorch gradient checkpointing. 2 to wrap 2 layers in 1 module etc.\\n+        use_reentrant (`bool`, *optional*):\\n+            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354\\n+            Optimal gradient checkpointing algorithm `use_reentrant=False` which will\\n+            be the default in future Pytorch versions doesn\\'t seem to work??\\n+    \"\"\"\\n+    _model = None\\n+    if hasattr(model, \"layers\"):\\n+        _model = model\\n+    elif hasattr(model, \"model\"):\\n+        if hasattr(model.model, \"layers\"):\\n+            _model = model.model\\n+    if _model is None:\\n+        raise TypeError(\"`model` or `model.model` does not have attribute `layers`. Are you sure this is a model?\")\\n+    pass\\n+\\n+    if use_reentrant is False:\\n+        use_reentrant = True\\n+    pass\\n+\\n+    n_layers = len(_model.layers)\\n+    boundaries = calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+    _model._gradient_checkpointing_boundaries    = boundaries\\n+    _model._gradient_checkpointing_use_reentrant = use_reentrant\\n+pass\\n',\n",
       " '@@ -593,6 +593,13 @@ def LlamaModel_fast_forward(\\n     all_self_attns = () if output_attentions else None\\n     next_decoder_cache = () if use_cache else None\\n \\n+    # Gradient checkpointing methods (ie sqrt)\\n+    if hasattr(self, \"_gradient_checkpointing_boundaries\"):\\n+        boundaries = self._gradient_checkpointing_boundaries\\n+    else:\\n+        boundaries = None\\n+    pass\\n+    \\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n',\n",
       " '@@ -24,6 +24,7 @@ from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n import subprocess\\n import psutil\\n+import re\\n \\n __all__ = [\\n     \"print_quantization_methods\",\\n@@ -176,19 +177,6 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n-    # First check for a token!\\n-    if push_to_hub:\\n-        from huggingface_hub import whoami\\n-        try: \\n-            username = whoami(token = token)[\"name\"]\\n-        except:\\n-            raise RuntimeError(\\n-                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n-                \"Go to https://huggingface.co/settings/tokens\"\\n-            )\\n-        pass\\n-    pass\\n-\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -215,7 +203,19 @@ def unsloth_save_model(\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n-    import re\\n+\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n \\n     assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n \\n@@ -588,7 +588,7 @@ def unsloth_save_model(\\n         from huggingface_hub import HfApi\\n         hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n \\n-        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        print(\"Unsloth: Uploading all files... Please wait...\")\\n         hf_api.upload_folder(\\n             folder_path = new_save_directory,\\n             path_in_repo = \".\",\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers import AutoTokenizer\\n from platform import system as platform_system\\n platform_system = platform_system()\\n import math\\n+import numpy as np\\n \\n __version__ = \"2024.3\"\\n \\n@@ -269,3 +270,88 @@ except:\\n         \"Luckily, your training run will still work in the meantime!\"\\n     )\\n pass\\n+\\n+\\n+def _calculate_n_gradient_checkpoints(\\n+    n_layers : int,\\n+    method   : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if method is None: method = \"sqrt\"\\n+\\n+    if method == \"sqrt\":\\n+        n_checkpoints = int(n_layers**0.5)\\n+    elif type(method) is int and method > 0:\\n+        n_checkpoints = int(np.ceil(n_layers / method))\\n+    else:\\n+        raise ValueError(\"method must be \\'sqrt\\' or an int >0 and <= n_layers.\")\\n+\\n+    size = n_layers // n_checkpoints\\n+    sizes = np.full(n_checkpoints, size, dtype = int)\\n+    leftovers = n_layers % n_checkpoints\\n+    # We append leftovers from the right\\n+    for k in range(leftovers):\\n+        sizes[n_checkpoints-1-k] += 1\\n+    boundaries = np.hstack((0, np.cumsum(sizes)))\\n+    boundaries = boundaries.tolist()\\n+    return boundaries\\n+pass\\n+\\n+\\n+def calculate_n_gradient_checkpoints(\\n+    n_layers              : int,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+) -> List[int]:\\n+    assert(type(n_layers) is int and n_layers > 0)\\n+\\n+    if layers_per_checkpoint is None or layers_per_checkpoint == 1:\\n+        return None\\n+\\n+    boundaries = _calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+\\n+    assert(boundaries[0] == 0 and boundaries[-1] == n_layers)\\n+    assert(min(boundaries) == 0 and max(boundaries) == n_layers)\\n+    assert(np.diff(boundaries).min() >= 0)\\n+    return boundaries\\n+pass\\n+\\n+\\n+def prepare_n_gradient_checkpoints(\\n+    model                 : Any,\\n+    layers_per_checkpoint : Optional[Union[str, int]] = \"sqrt\",\\n+    use_reentrant         : Optional[bool] = True,\\n+) -> None:\\n+    \"\"\"\\n+    Calculates where to place the gradient checkpoints given n_layers.\\n+\\n+    Args:\\n+        model: Any LlamaModel with layers.\\n+        layers_per_checkpoint (`Union[str, int]`, *optional*):\\n+            Can either be `sqrt` or an integer for how many layers per checkpoint you want.\\n+            The more, the less memory usage, but can be slower. Default is `sqrt`.\\n+            Choose 1 for Pytorch gradient checkpointing. 2 to wrap 2 layers in 1 module etc.\\n+        use_reentrant (`bool`, *optional*):\\n+            https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py#L354\\n+            Optimal gradient checkpointing algorithm `use_reentrant=False` which will\\n+            be the default in future Pytorch versions doesn\\'t seem to work??\\n+    \"\"\"\\n+    _model = None\\n+    if hasattr(model, \"layers\"):\\n+        _model = model\\n+    elif hasattr(model, \"model\"):\\n+        if hasattr(model.model, \"layers\"):\\n+            _model = model.model\\n+    if _model is None:\\n+        raise TypeError(\"`model` or `model.model` does not have attribute `layers`. Are you sure this is a model?\")\\n+    pass\\n+\\n+    if use_reentrant is False:\\n+        use_reentrant = True\\n+    pass\\n+\\n+    n_layers = len(_model.layers)\\n+    boundaries = calculate_n_gradient_checkpoints(n_layers, layers_per_checkpoint)\\n+    _model._gradient_checkpointing_boundaries    = boundaries\\n+    _model._gradient_checkpointing_use_reentrant = use_reentrant\\n+pass\\n',\n",
       " '@@ -593,6 +593,13 @@ def LlamaModel_fast_forward(\\n     all_self_attns = () if output_attentions else None\\n     next_decoder_cache = () if use_cache else None\\n \\n+    # Gradient checkpointing methods (ie sqrt)\\n+    if hasattr(self, \"_gradient_checkpointing_boundaries\"):\\n+        boundaries = self._gradient_checkpointing_boundaries\\n+    else:\\n+        boundaries = None\\n+    pass\\n+    \\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n',\n",
       " '@@ -24,6 +24,7 @@ from transformers.models.llama.modeling_llama import logger\\n from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n import subprocess\\n import psutil\\n+import re\\n \\n __all__ = [\\n     \"print_quantization_methods\",\\n@@ -176,19 +177,6 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n-    # First check for a token!\\n-    if push_to_hub:\\n-        from huggingface_hub import whoami\\n-        try: \\n-            username = whoami(token = token)[\"name\"]\\n-        except:\\n-            raise RuntimeError(\\n-                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n-                \"Go to https://huggingface.co/settings/tokens\"\\n-            )\\n-        pass\\n-    pass\\n-\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -215,7 +203,19 @@ def unsloth_save_model(\\n     for deletion in (\"model\", \"tokenizer\", \"save_method\", \"temporary_location\", \"maximum_memory_usage\"):\\n         del save_pretrained_settings[deletion]\\n     pass\\n-    import re\\n+\\n+    # First check for a token!\\n+    if push_to_hub:\\n+        from huggingface_hub import whoami\\n+        try: \\n+            username = whoami(token = token)[\"name\"]\\n+        except:\\n+            raise RuntimeError(\\n+                \"Unsloth: Please supply a token!\\\\n\"\\\\\\n+                \"Go to https://huggingface.co/settings/tokens\"\\n+            )\\n+        pass\\n+    pass\\n \\n     assert(maximum_memory_usage > 0 and maximum_memory_usage <= 0.95)\\n \\n@@ -588,7 +588,7 @@ def unsloth_save_model(\\n         from huggingface_hub import HfApi\\n         hf_api = HfApi(token = save_pretrained_settings[\"token\"])\\n \\n-        print(\"Unsloth: Uploading all files... Please wait!\")\\n+        print(\"Unsloth: Uploading all files... Please wait...\")\\n         hf_api.upload_folder(\\n             folder_path = new_save_directory,\\n             path_in_repo = \".\",\\n',\n",
       " '@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(\\n     else:\\n         boundaries = None\\n     pass\\n-    \\n+\\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n@@ -1601,7 +1601,11 @@ class FastLlamaModel:\\n         pass\\n \\n         # Also check if lm_head / embeddings are trained\\n-        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        internal_model = model\\n+        while not hasattr(internal_model, \"lm_head\"):\\n+            internal_model = internal_model.model\\n+        pass\\n+        lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n \\n',\n",
       " '@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(\\n     else:\\n         boundaries = None\\n     pass\\n-    \\n+\\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n@@ -1601,7 +1601,11 @@ class FastLlamaModel:\\n         pass\\n \\n         # Also check if lm_head / embeddings are trained\\n-        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        internal_model = model\\n+        while not hasattr(internal_model, \"lm_head\"):\\n+            internal_model = internal_model.model\\n+        pass\\n+        lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n \\n',\n",
       " '@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(\\n     else:\\n         boundaries = None\\n     pass\\n-    \\n+\\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n@@ -1601,7 +1601,11 @@ class FastLlamaModel:\\n         pass\\n \\n         # Also check if lm_head / embeddings are trained\\n-        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        internal_model = model\\n+        while not hasattr(internal_model, \"lm_head\"):\\n+            internal_model = internal_model.model\\n+        pass\\n+        lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n \\n',\n",
       " '@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(\\n     else:\\n         boundaries = None\\n     pass\\n-    \\n+\\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n@@ -1601,7 +1601,11 @@ class FastLlamaModel:\\n         pass\\n \\n         # Also check if lm_head / embeddings are trained\\n-        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        internal_model = model\\n+        while not hasattr(internal_model, \"lm_head\"):\\n+            internal_model = internal_model.model\\n+        pass\\n+        lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n \\n',\n",
       " '@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(\\n     else:\\n         boundaries = None\\n     pass\\n-    \\n+\\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n@@ -1601,7 +1601,11 @@ class FastLlamaModel:\\n         pass\\n \\n         # Also check if lm_head / embeddings are trained\\n-        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        internal_model = model\\n+        while not hasattr(internal_model, \"lm_head\"):\\n+            internal_model = internal_model.model\\n+        pass\\n+        lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n \\n',\n",
       " '@@ -599,7 +599,7 @@ def LlamaModel_fast_forward(\\n     else:\\n         boundaries = None\\n     pass\\n-    \\n+\\n     for idx, decoder_layer in enumerate(self.layers):\\n         if output_hidden_states:\\n             all_hidden_states += (hidden_states,)\\n@@ -1601,7 +1601,11 @@ class FastLlamaModel:\\n         pass\\n \\n         # Also check if lm_head / embeddings are trained\\n-        lm_head = getattr(model, \"model\", model).lm_head.weight\\n+        internal_model = model\\n+        while not hasattr(internal_model, \"lm_head\"):\\n+            internal_model = internal_model.model\\n+        pass\\n+        lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n \\n',\n",
       " \"@@ -1,160 +0,0 @@\\n-# Byte-compiled / optimized / DLL files\\n-__pycache__/\\n-*.py[cod]\\n-*$py.class\\n-\\n-# C extensions\\n-*.so\\n-\\n-# Distribution / packaging\\n-.Python\\n-build/\\n-develop-eggs/\\n-dist/\\n-downloads/\\n-eggs/\\n-.eggs/\\n-lib/\\n-lib64/\\n-parts/\\n-sdist/\\n-var/\\n-wheels/\\n-share/python-wheels/\\n-*.egg-info/\\n-.installed.cfg\\n-*.egg\\n-MANIFEST\\n-\\n-# PyInstaller\\n-#  Usually these files are written by a python script from a template\\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n-*.manifest\\n-*.spec\\n-\\n-# Installer logs\\n-pip-log.txt\\n-pip-delete-this-directory.txt\\n-\\n-# Unit test / coverage reports\\n-htmlcov/\\n-.tox/\\n-.nox/\\n-.coverage\\n-.coverage.*\\n-.cache\\n-nosetests.xml\\n-coverage.xml\\n-*.cover\\n-*.py,cover\\n-.hypothesis/\\n-.pytest_cache/\\n-cover/\\n-\\n-# Translations\\n-*.mo\\n-*.pot\\n-\\n-# Django stuff:\\n-*.log\\n-local_settings.py\\n-db.sqlite3\\n-db.sqlite3-journal\\n-\\n-# Flask stuff:\\n-instance/\\n-.webassets-cache\\n-\\n-# Scrapy stuff:\\n-.scrapy\\n-\\n-# Sphinx documentation\\n-docs/_build/\\n-\\n-# PyBuilder\\n-.pybuilder/\\n-target/\\n-\\n-# Jupyter Notebook\\n-.ipynb_checkpoints\\n-\\n-# IPython\\n-profile_default/\\n-ipython_config.py\\n-\\n-# pyenv\\n-#   For a library or package, you might want to ignore these files since the code is\\n-#   intended to run in multiple environments; otherwise, check them in:\\n-# .python-version\\n-\\n-# pipenv\\n-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n-#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n-#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n-#   install all needed dependencies.\\n-#Pipfile.lock\\n-\\n-# poetry\\n-#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n-#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n-#   commonly ignored for libraries.\\n-#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n-#poetry.lock\\n-\\n-# pdm\\n-#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n-#pdm.lock\\n-#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n-#   in version control.\\n-#   https://pdm.fming.dev/#use-with-ide\\n-.pdm.toml\\n-\\n-# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n-__pypackages__/\\n-\\n-# Celery stuff\\n-celerybeat-schedule\\n-celerybeat.pid\\n-\\n-# SageMath parsed files\\n-*.sage.py\\n-\\n-# Environments\\n-.env\\n-.venv\\n-env/\\n-venv/\\n-ENV/\\n-env.bak/\\n-venv.bak/\\n-\\n-# Spyder project settings\\n-.spyderproject\\n-.spyproject\\n-\\n-# Rope project settings\\n-.ropeproject\\n-\\n-# mkdocs documentation\\n-/site\\n-\\n-# mypy\\n-.mypy_cache/\\n-.dmypy.json\\n-dmypy.json\\n-\\n-# Pyre type checker\\n-.pyre/\\n-\\n-# pytype static type analyzer\\n-.pytype/\\n-\\n-# Cython debug symbols\\n-cython_debug/\\n-\\n-# PyCharm\\n-#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n-#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n-#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n-#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n-#.idea/\\n\",\n",
       " '@@ -186,7 +186,7 @@\\n       same \"printed page\" as the copyright notice for easier\\n       identification within third-party archives.\\n \\n-   Copyright [yyyy] [name of copyright owner]\\n+   Copyright [2024-] [Unsloth AI, Daniel Han-Chen & Michael Han-Chen]\\n \\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n',\n",
       " '@@ -113,3 +113,4 @@ pass\\n from .models import *\\n from .save import *\\n from .chat_templates import *\\n+from .tokenizer_utils import *\\n',\n",
       " '@@ -15,15 +15,19 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n-    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n-from .models._utils import patch_tokenizer\\n+from .save import patch_saving_functions\\n import os\\n import shutil\\n+from .tokenizer_utils import (\\n+    load_correct_tokenizer,\\n+    fix_sentencepiece_tokenizer,\\n+)\\n+from .models._utils import patch_tokenizer\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -251,84 +255,23 @@ gemma_chatml_eos_token = (\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n-def fix_sentencepiece_tokenizer(\\n-    old_tokenizer,\\n-    new_tokenizer,\\n-    token_mapping,\\n-    temporary_location = \"_unsloth_sentencepiece_temp\",\\n-):\\n-    # From https://github.com/google/sentencepiece/issues/121\\n-    # We need to manually edit the sentencepiece tokenizer!\\n-    try:\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    except:\\n-        if not os.path.exists(temporary_location):\\n-            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n-            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            shutil.rmtree(temporary_location)\\n-        pass\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    pass\\n-\\n-    if not os.path.exists(temporary_location):\\n-        os.makedirs(temporary_location)\\n-    pass\\n-\\n-    # First save the old tokenizer\\n-    old_tokenizer.save_pretrained(temporary_location)\\n-\\n-    from sentencepiece import SentencePieceProcessor\\n-    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n-    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n-\\n-    # Now save the new tokenizer\\n-    new_tokenizer.save_pretrained(temporary_location)\\n-\\n-    # Now correct the old tokenizer\\'s .model file\\n-    for old_token, new_token in token_mapping.items():\\n-        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n-        ids = ids[0]\\n-        if (len(ids) != 1):\\n-            # Skip this token!\\n-            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n-            continue\\n-        pass\\n-        ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n-        assert(tokenizer_piece.piece == old_token)\\n-        tokenizer_piece.piece = new_token\\n-    pass\\n-\\n-    # And now write it\\n-    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n-        file.write(tokenizer_file.SerializeToString())\\n-    pass\\n-\\n-    # And load it!\\n-    from transformers import AutoTokenizer\\n-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n-    return tokenizer\\n-pass\\n-\\n-\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    assert(type(map_eos_token) is bool)\\n     old_tokenizer = tokenizer\\n \\n-    if map_eos_token is False:\\n-        assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n-    pass\\n-\\n     IS_GEMMA = False\\n     if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n         if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n         IS_GEMMA = True\\n     pass\\n \\n+    # We first check if the tokenizer is a fast one. If not, we cannot convert this!\\n+    is_fast_tokenizer = getattr(tokenizer, \"is_fast\", False)\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n@@ -348,9 +291,17 @@ def get_chat_template(\\n \\n         assert(type(stop_word) is str)\\n \\n-        # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n-        # For Gemma :)\\n-        if token_mapping is not None:\\n+        # Check fast tokenizer\\n+        if not is_fast_tokenizer:\\n+            logger.warning_once(\\n+                f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n+                \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n+                \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n+            )\\n+\\n+        elif token_mapping is not None:\\n+            # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n+            # For Gemma :)\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n@@ -368,7 +319,7 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            if not stop_word in token_mapping.values():\\n+            if map_eos_token and (not stop_word in token_mapping.values()):\\n                 # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n                 logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n@@ -376,14 +327,19 @@ def get_chat_template(\\n \\n             if skipped != len(token_mapping):\\n                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                if map_eos_token:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+                else:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)\\n+                pass\\n \\n                 # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n                 tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n             else:\\n                 pass\\n \\n-        elif stop_word != \"eos_token\":\\n+        elif map_eos_token and (stop_word != \"eos_token\"):\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n \\n             # Replaces the old EOS token with a new one.\\n@@ -393,9 +349,14 @@ def get_chat_template(\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            old_eos_token = tokenizer.eos_token\\n+            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+            # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+            token_mapping = { old_eos_token : stop_word, }\\n+            tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n         pass\\n \\n     else:\\n@@ -433,7 +394,10 @@ def get_chat_template(\\n     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n \\n-    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+    # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+\\n+    # Patch saving functions\\n+    tokenizer = patch_saving_functions(tokenizer)\\n \\n     return tokenizer#, stopping_criteria\\n pass\\n',\n",
       " '@@ -60,22 +60,16 @@ from xformers import __version__ as xformers_version\\n \\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n-    \"patch_tokenizer\",\\n-    \"check_tokenizer\",\\n     \"xformers\",\\n     \"xformers_attention\",\\n     \"xformers_version\",\\n     \"__version__\",\\n     \"HAS_FLASH_ATTENTION\",\\n     \"platform_system\",\\n+    \"patch_tokenizer\",\\n ]\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -144,103 +138,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-def check_tokenizer(\\n-    model,\\n-    tokenizer,\\n-    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n-    model_max_length = 4096,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    _reload = True,\\n-):\\n-    # Checks tokenizer for out of bounds ids.\\n-    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n-    # where <sep> had token id=32002.\\n-    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n-    # Seems like the Fast tokenizer in Rust breaks things!\\n-\\n-    # We ignore some of them!\\n-    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n-        return tokenizer\\n-    pass\\n-\\n-    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n-    added_tokens_fast = tokenizer.added_tokens_decoder\\n-    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n-    sorted_keys = sorted(added_tokens_fast)\\n-    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n-\\n-    for j, index in enumerate(added_tokens_fast.keys()):\\n-        if index >= max_embedding_size:\\n-            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n-            bad_tokens  = list(added_tokens_fast.values())[j:]\\n-\\n-            if not _reload:\\n-                # Try removing the token\\n-                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n-                special_tokens = tokenizer.special_tokens_map\\n-                import itertools\\n-                special_tokens = frozenset(\\n-                    itertools.chain.from_iterable(\\n-                        [x] if type(x) is str else x for x in special_tokens.values()\\n-                    )\\n-                )\\n-                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n-                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n-\\n-                # Check of extra tokens can in fact we removed!\\n-\\n-                if  (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n-                    (len(can_be_removed2) == len(bad_tokens)):\\n-                    # Yes it can be fixed!\\n-                    for bad_token in can_be_removed1:\\n-                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n-                        del tokenizer._added_tokens_decoder[remove_id]\\n-                        del tokenizer._added_tokens_encoder[bad_token]\\n-                    pass\\n-                    # Confirm 1 more time!\\n-                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n-                        logger.warning_once(\\n-                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n-                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n-                        )\\n-                        return tokenizer\\n-                    pass\\n-                pass\\n-\\n-                # :( Failure\\n-                raise RuntimeError(\\n-                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n-                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n-                )\\n-            pass\\n-            \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n-        pass\\n-    pass\\n-    return tokenizer\\n-pass\\n-\\n-\\n # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??\\n # For mixed precision, we need it to be in float32 not float16.\\n from peft.tuners.lora.layer import LoraLayer\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers.modeling_attn_mask_utils import (\\n from ..kernels import *\\n from ._utils import *\\n from ._utils import __version__\\n+from ..tokenizer_utils import *\\n if HAS_FLASH_ATTENTION:\\n     from flash_attn import flash_attn_func\\n \\n@@ -1014,8 +1015,8 @@ class FastLlamaModel:\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n+        tokenizer = load_correct_tokenizer(\\n+            tokenizer_name    = tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n             token             = token,\\n',\n",
       " '@@ -362,7 +362,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer = load_correct_tokenizer(\\n             tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n',\n",
       " '@@ -276,7 +276,8 @@ def unsloth_save_model(\\n             old_username = None, private = private,\\n         )\\n \\n-        model.original_push_to_hub(\\n+        getattr(model, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+        (\\n             repo_id            = save_directory,\\n             use_temp_dir       = use_temp_dir,\\n             commit_message     = commit_message,\\n@@ -290,7 +291,8 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n-            tokenizer.original_push_to_hub(\\n+            getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+            (\\n                 repo_id            = save_directory,\\n                 use_temp_dir       = use_temp_dir,\\n                 commit_message     = commit_message,\\n',\n",
       " '@@ -0,0 +1,414 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from transformers import AutoTokenizer\\n+from transformers.convert_slow_tokenizer import convert_slow_tokenizer\\n+from transformers import PreTrainedTokenizerFast\\n+import re\\n+import os\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n+__all__ = [\\n+    \"load_correct_tokenizer\",\\n+    \"fix_sentencepiece_tokenizer\",\\n+    \"check_tokenizer\",\\n+]\\n+\\n+\\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n+\\n+def try_fix_tokenizer(tokenizer, prepend = True):\\n+\\n+    if hasattr(tokenizer, \"_tokenizer\"):\\n+        converted_tokenizer = tokenizer._tokenizer\\n+    else:\\n+        converted_tokenizer = convert_slow_tokenizer(tokenizer)\\n+    pass\\n+\\n+    tokenizer_string = converted_tokenizer.to_str()\\n+\\n+    # Llama does ▁apple. Sometimes this is wrong!!\\n+    prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n+    if not prepend and prepend_text in tokenizer_string:\\n+        tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n+    pass\\n+\\n+    dir_names = dir(tokenizer)\\n+    # Get eos_token, bos_token etc\\n+    token_names = [x for x in dir_names if x.endswith(\"_token\") and x.count(\"_\") == 1]\\n+\\n+    for token_name in token_names:\\n+        token = getattr(tokenizer, token_name, None)\\n+        if token is None: continue\\n+        token_id = getattr(tokenizer, token_name + \"_id\", None)\\n+\\n+        # Locate the token\\'s id mapping in the string\\n+        find_text = f\\'\"id\":{token_id},\"content\":\"\\'\\n+        start = tokenizer_string.find(find_text) + len(find_text)\\n+        if start == -1: continue\\n+        end   = tokenizer_string.find(\\'\",\\', start)\\n+\\n+        bad_token = tokenizer_string[start : end]\\n+        # Check if token is the actual same one - if not, edit it\\n+        if bad_token != token:\\n+            bad_text  = f\\'{find_text}{bad_token}\",\\'\\n+            good_text = f\\'{find_text}{token}\",\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+\\n+            # And replace vocab section\\n+            bad_text = f\\'\"{bad_token}\":{token_id},\\'\\n+            good_text = f\\'\"{token}\":{token_id},\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+        pass\\n+    pass\\n+\\n+    fixed_tokenizer = converted_tokenizer.from_str(tokenizer_string)\\n+    return fixed_tokenizer\\n+pass\\n+\\n+\\n+def get_sorted_dict(dictionary):\\n+    sorted_keys = sorted(dictionary.values())\\n+    inverted_dictionary = { value : key for key, value in dictionary.items() }\\n+\\n+    sorted_dictionary = {}\\n+    for key in sorted_keys:\\n+        value = inverted_dictionary[key]\\n+        sorted_dictionary[value] = key\\n+    return sorted_dictionary\\n+pass\\n+\\n+\\n+def convert_to_fast_tokenizer(\\n+    slow_tokenizer,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    is_fast = getattr(slow_tokenizer, \"is_fast\", False)\\n+    if is_fast: return slow_tokenizer\\n+    \\n+    try:\\n+        tokenizer_name = slow_tokenizer.__class__.__name__\\n+        lowered_tokenizer_name = tokenizer_name.lower()\\n+        if lowered_tokenizer_name.endswith(\"tokenizer\"):\\n+            class_name = lowered_tokenizer_name[:-len(\"tokenizer\")]\\n+            FastTokenizer = eval(\\n+                f\\'__import__(f\"transformers.models.{class_name}\").{tokenizer_name}Fast\\'\\n+            )\\n+        else:\\n+            FastTokenizer = PreTrainedTokenizerFast\\n+    except:\\n+        FastTokenizer = PreTrainedTokenizerFast\\n+    pass\\n+\\n+    # Get all arguments (bos_token, etc)\\n+    docs = FastTokenizer.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args = [x for x in args if not x.endswith(\"_file\")]\\n+\\n+    # Also some missing maybe!\\n+    docs = PreTrainedTokenizerFast.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args2 = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args2 = [x for x in args2 if not x.endswith(\"_file\")]\\n+    args = list(set(args + args2))\\n+\\n+    kwargs = {}\\n+    for arg in args: kwargs[arg] = getattr(slow_tokenizer, arg, None)\\n+    kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = True)\\n+    fast_tokenizer = FastTokenizer( **kwargs )\\n+\\n+    # Check if they\\'re similar!\\n+    sorted_slow_tokenizer = get_sorted_dict(slow_tokenizer.get_vocab())\\n+    sorted_fast_tokenizer = get_sorted_dict(fast_tokenizer.get_vocab())\\n+\\n+    check_vocab   = (sorted_slow_tokenizer == sorted_fast_tokenizer)\\n+    check_special = (slow_tokenizer.all_special_tokens == fast_tokenizer.all_special_tokens)\\n+\\n+    # Failure so return slow_tokenizer\\n+    if not check_vocab or not check_special: return slow_tokenizer\\n+\\n+    # Now confirm if they match\\n+    if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        # Maybe remove prepending of __apple?\\n+        kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = False)\\n+        fast_tokenizer = FastTokenizer( **kwargs )\\n+        if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            # Failure :(\\n+            return slow_tokenizer\\n+        pass\\n+    pass\\n+\\n+    # Also tokenizer.model is missing!\\n+    name = slow_tokenizer.name_or_path.replace(\"/\", \"_\")\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+    new_location = f\"{temporary_location}/{name}\"\\n+    slow_tokenizer.save_pretrained(new_location)\\n+    fast_tokenizer.save_pretrained(new_location)\\n+\\n+    # Now load it!\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(new_location)\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    return slow_tokenizer\\n+pass\\n+\\n+\\n+def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+    # Get eos_token, bos_token etc\\n+    dir_names = dir(slow_tokenizer)\\n+    special_tokens = list(filter(None, (\\n+        getattr(slow_tokenizer, x) for x in dir_names\\n+        if x.endswith(\"_token\") and x.count(\"_\") == 1\\n+    )))\\n+    all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n+    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+        \"\".join(all_special_tokens)\\n+    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+pass\\n+\\n+\\n+global sentencepiece_model_pb2\\n+sentencepiece_model_pb2 = None\\n+\\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    global sentencepiece_model_pb2\\n+    if sentencepiece_model_pb2 is None:\\n+        try:\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        except:\\n+            if not os.path.exists(temporary_location):\\n+                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n+                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            pass\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n+def load_correct_tokenizer(\\n+    tokenizer_name,\\n+    model_max_length = None,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    trust_remote_code = False,\\n+):\\n+    slow_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+        use_fast          = False,\\n+    )\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+    )\\n+    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+    \\n+    # Confirm if slow and fast are equivalent!\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    else:\\n+        return convert_to_fast_tokenizer(slow_tokenizer)\\n+    pass\\n+pass\\n+\\n+\\n+def check_tokenizer(\\n+    model,\\n+    tokenizer,\\n+    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+    model_max_length = 4096,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    _reload = True,\\n+):\\n+    # Checks tokenizer for out of bounds ids.\\n+    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n+    # where <sep> had token id=32002.\\n+    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n+    # Seems like the Fast tokenizer in Rust breaks things!\\n+\\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n+    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n+    added_tokens_fast = tokenizer.added_tokens_decoder\\n+    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n+    sorted_keys = sorted(added_tokens_fast)\\n+    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n+\\n+    for j, index in enumerate(added_tokens_fast.keys()):\\n+        if index >= max_embedding_size:\\n+            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n+            bad_tokens  = list(added_tokens_fast.values())[j:]\\n+            if not _reload:\\n+                # Try removing the token\\n+                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+                special_tokens = tokenizer.special_tokens_map\\n+                import itertools\\n+                special_tokens = frozenset(\\n+                    itertools.chain.from_iterable(\\n+                        [x] if type(x) is str else x for x in special_tokens.values()\\n+                    )\\n+                )\\n+                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n+                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n+\\n+                # Check of extra tokens can in fact we removed!\\n+                can_be_removed = \\\\\\n+                    (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n+                    (len(can_be_removed2) == len(bad_tokens))\\n+\\n+                # Check if sep_token or other generic types\\n+                remove_generic = False\\n+                try_mapper = []\\n+                if not can_be_removed:\\n+                    names = dir(tokenizer)\\n+                    names = (x for x in names if x.endswith(\"_token\") and x.count(\"_\") == 1)\\n+                    generic_tokens = [(x, getattr(tokenizer, x, None)) for x in names]\\n+\\n+                    try_removal = []\\n+                    for token in bad_tokens:\\n+                        for (name_token, check_token) in generic_tokens:\\n+                            if check_token == token:\\n+                                try_removal.append(token)\\n+                                try_mapper.append(name_token)\\n+                            pass\\n+                        pass\\n+                    pass\\n+\\n+                    # Recheck!\\n+                    can_be_removed = (len(try_removal) == len(bad_tokens))\\n+                    if can_be_removed: remove_generic = True\\n+                    can_be_removed1 = bad_tokens\\n+                pass\\n+\\n+                if can_be_removed:\\n+                    # Yes it can be fixed!\\n+                    for j, bad_token in enumerate(can_be_removed1):\\n+                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n+                        del tokenizer._added_tokens_decoder[remove_id]\\n+                        del tokenizer._added_tokens_encoder[bad_token]\\n+\\n+                        if remove_generic and (try_removal[j] == bad_token):\\n+                            # Remove sep token for example\\n+                            setattr(tokenizer, try_mapper[j], None)\\n+                            setattr(tokenizer, try_mapper[j] + \"_id\", None)\\n+                        pass\\n+                    pass\\n+                    # Confirm 1 more time!\\n+                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n+                        logger.warning_once(\\n+                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n+                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n+                        )\\n+                        return convert_to_fast_tokenizer(tokenizer)\\n+                    pass\\n+                pass\\n+\\n+                # :( Failure\\n+                raise RuntimeError(\\n+                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n+                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n+                )\\n+            pass\\n+            \\n+            # Try slow tokenizer which can fix things!\\n+            tokenizer = AutoTokenizer.from_pretrained(\\n+                model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                use_fast = False,\\n+            )\\n+            return check_tokenizer(\\n+                model = model,\\n+                tokenizer = tokenizer,\\n+                model_name = model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                _reload = False,\\n+            )\\n+            break\\n+        pass\\n+    pass\\n+    return convert_to_fast_tokenizer(tokenizer)\\n+pass\\n',\n",
       " \"@@ -1,160 +0,0 @@\\n-# Byte-compiled / optimized / DLL files\\n-__pycache__/\\n-*.py[cod]\\n-*$py.class\\n-\\n-# C extensions\\n-*.so\\n-\\n-# Distribution / packaging\\n-.Python\\n-build/\\n-develop-eggs/\\n-dist/\\n-downloads/\\n-eggs/\\n-.eggs/\\n-lib/\\n-lib64/\\n-parts/\\n-sdist/\\n-var/\\n-wheels/\\n-share/python-wheels/\\n-*.egg-info/\\n-.installed.cfg\\n-*.egg\\n-MANIFEST\\n-\\n-# PyInstaller\\n-#  Usually these files are written by a python script from a template\\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n-*.manifest\\n-*.spec\\n-\\n-# Installer logs\\n-pip-log.txt\\n-pip-delete-this-directory.txt\\n-\\n-# Unit test / coverage reports\\n-htmlcov/\\n-.tox/\\n-.nox/\\n-.coverage\\n-.coverage.*\\n-.cache\\n-nosetests.xml\\n-coverage.xml\\n-*.cover\\n-*.py,cover\\n-.hypothesis/\\n-.pytest_cache/\\n-cover/\\n-\\n-# Translations\\n-*.mo\\n-*.pot\\n-\\n-# Django stuff:\\n-*.log\\n-local_settings.py\\n-db.sqlite3\\n-db.sqlite3-journal\\n-\\n-# Flask stuff:\\n-instance/\\n-.webassets-cache\\n-\\n-# Scrapy stuff:\\n-.scrapy\\n-\\n-# Sphinx documentation\\n-docs/_build/\\n-\\n-# PyBuilder\\n-.pybuilder/\\n-target/\\n-\\n-# Jupyter Notebook\\n-.ipynb_checkpoints\\n-\\n-# IPython\\n-profile_default/\\n-ipython_config.py\\n-\\n-# pyenv\\n-#   For a library or package, you might want to ignore these files since the code is\\n-#   intended to run in multiple environments; otherwise, check them in:\\n-# .python-version\\n-\\n-# pipenv\\n-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n-#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n-#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n-#   install all needed dependencies.\\n-#Pipfile.lock\\n-\\n-# poetry\\n-#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n-#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n-#   commonly ignored for libraries.\\n-#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n-#poetry.lock\\n-\\n-# pdm\\n-#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n-#pdm.lock\\n-#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n-#   in version control.\\n-#   https://pdm.fming.dev/#use-with-ide\\n-.pdm.toml\\n-\\n-# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n-__pypackages__/\\n-\\n-# Celery stuff\\n-celerybeat-schedule\\n-celerybeat.pid\\n-\\n-# SageMath parsed files\\n-*.sage.py\\n-\\n-# Environments\\n-.env\\n-.venv\\n-env/\\n-venv/\\n-ENV/\\n-env.bak/\\n-venv.bak/\\n-\\n-# Spyder project settings\\n-.spyderproject\\n-.spyproject\\n-\\n-# Rope project settings\\n-.ropeproject\\n-\\n-# mkdocs documentation\\n-/site\\n-\\n-# mypy\\n-.mypy_cache/\\n-.dmypy.json\\n-dmypy.json\\n-\\n-# Pyre type checker\\n-.pyre/\\n-\\n-# pytype static type analyzer\\n-.pytype/\\n-\\n-# Cython debug symbols\\n-cython_debug/\\n-\\n-# PyCharm\\n-#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n-#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n-#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n-#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n-#.idea/\\n\",\n",
       " '@@ -186,7 +186,7 @@\\n       same \"printed page\" as the copyright notice for easier\\n       identification within third-party archives.\\n \\n-   Copyright [yyyy] [name of copyright owner]\\n+   Copyright [2024-] [Unsloth AI, Daniel Han-Chen & Michael Han-Chen]\\n \\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n',\n",
       " '@@ -113,3 +113,4 @@ pass\\n from .models import *\\n from .save import *\\n from .chat_templates import *\\n+from .tokenizer_utils import *\\n',\n",
       " '@@ -15,15 +15,19 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n-    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n-from .models._utils import patch_tokenizer\\n+from .save import patch_saving_functions\\n import os\\n import shutil\\n+from .tokenizer_utils import (\\n+    load_correct_tokenizer,\\n+    fix_sentencepiece_tokenizer,\\n+)\\n+from .models._utils import patch_tokenizer\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -251,84 +255,23 @@ gemma_chatml_eos_token = (\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n-def fix_sentencepiece_tokenizer(\\n-    old_tokenizer,\\n-    new_tokenizer,\\n-    token_mapping,\\n-    temporary_location = \"_unsloth_sentencepiece_temp\",\\n-):\\n-    # From https://github.com/google/sentencepiece/issues/121\\n-    # We need to manually edit the sentencepiece tokenizer!\\n-    try:\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    except:\\n-        if not os.path.exists(temporary_location):\\n-            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n-            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            shutil.rmtree(temporary_location)\\n-        pass\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    pass\\n-\\n-    if not os.path.exists(temporary_location):\\n-        os.makedirs(temporary_location)\\n-    pass\\n-\\n-    # First save the old tokenizer\\n-    old_tokenizer.save_pretrained(temporary_location)\\n-\\n-    from sentencepiece import SentencePieceProcessor\\n-    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n-    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n-\\n-    # Now save the new tokenizer\\n-    new_tokenizer.save_pretrained(temporary_location)\\n-\\n-    # Now correct the old tokenizer\\'s .model file\\n-    for old_token, new_token in token_mapping.items():\\n-        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n-        ids = ids[0]\\n-        if (len(ids) != 1):\\n-            # Skip this token!\\n-            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n-            continue\\n-        pass\\n-        ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n-        assert(tokenizer_piece.piece == old_token)\\n-        tokenizer_piece.piece = new_token\\n-    pass\\n-\\n-    # And now write it\\n-    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n-        file.write(tokenizer_file.SerializeToString())\\n-    pass\\n-\\n-    # And load it!\\n-    from transformers import AutoTokenizer\\n-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n-    return tokenizer\\n-pass\\n-\\n-\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    assert(type(map_eos_token) is bool)\\n     old_tokenizer = tokenizer\\n \\n-    if map_eos_token is False:\\n-        assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n-    pass\\n-\\n     IS_GEMMA = False\\n     if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n         if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n         IS_GEMMA = True\\n     pass\\n \\n+    # We first check if the tokenizer is a fast one. If not, we cannot convert this!\\n+    is_fast_tokenizer = getattr(tokenizer, \"is_fast\", False)\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n@@ -348,9 +291,17 @@ def get_chat_template(\\n \\n         assert(type(stop_word) is str)\\n \\n-        # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n-        # For Gemma :)\\n-        if token_mapping is not None:\\n+        # Check fast tokenizer\\n+        if not is_fast_tokenizer:\\n+            logger.warning_once(\\n+                f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n+                \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n+                \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n+            )\\n+\\n+        elif token_mapping is not None:\\n+            # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n+            # For Gemma :)\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n@@ -368,7 +319,7 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            if not stop_word in token_mapping.values():\\n+            if map_eos_token and (not stop_word in token_mapping.values()):\\n                 # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n                 logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n@@ -376,14 +327,19 @@ def get_chat_template(\\n \\n             if skipped != len(token_mapping):\\n                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                if map_eos_token:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+                else:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)\\n+                pass\\n \\n                 # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n                 tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n             else:\\n                 pass\\n \\n-        elif stop_word != \"eos_token\":\\n+        elif map_eos_token and (stop_word != \"eos_token\"):\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n \\n             # Replaces the old EOS token with a new one.\\n@@ -393,9 +349,14 @@ def get_chat_template(\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            old_eos_token = tokenizer.eos_token\\n+            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+            # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+            token_mapping = { old_eos_token : stop_word, }\\n+            tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n         pass\\n \\n     else:\\n@@ -433,7 +394,10 @@ def get_chat_template(\\n     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n \\n-    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+    # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+\\n+    # Patch saving functions\\n+    tokenizer = patch_saving_functions(tokenizer)\\n \\n     return tokenizer#, stopping_criteria\\n pass\\n',\n",
       " '@@ -60,22 +60,16 @@ from xformers import __version__ as xformers_version\\n \\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n-    \"patch_tokenizer\",\\n-    \"check_tokenizer\",\\n     \"xformers\",\\n     \"xformers_attention\",\\n     \"xformers_version\",\\n     \"__version__\",\\n     \"HAS_FLASH_ATTENTION\",\\n     \"platform_system\",\\n+    \"patch_tokenizer\",\\n ]\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -144,103 +138,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-def check_tokenizer(\\n-    model,\\n-    tokenizer,\\n-    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n-    model_max_length = 4096,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    _reload = True,\\n-):\\n-    # Checks tokenizer for out of bounds ids.\\n-    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n-    # where <sep> had token id=32002.\\n-    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n-    # Seems like the Fast tokenizer in Rust breaks things!\\n-\\n-    # We ignore some of them!\\n-    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n-        return tokenizer\\n-    pass\\n-\\n-    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n-    added_tokens_fast = tokenizer.added_tokens_decoder\\n-    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n-    sorted_keys = sorted(added_tokens_fast)\\n-    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n-\\n-    for j, index in enumerate(added_tokens_fast.keys()):\\n-        if index >= max_embedding_size:\\n-            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n-            bad_tokens  = list(added_tokens_fast.values())[j:]\\n-\\n-            if not _reload:\\n-                # Try removing the token\\n-                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n-                special_tokens = tokenizer.special_tokens_map\\n-                import itertools\\n-                special_tokens = frozenset(\\n-                    itertools.chain.from_iterable(\\n-                        [x] if type(x) is str else x for x in special_tokens.values()\\n-                    )\\n-                )\\n-                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n-                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n-\\n-                # Check of extra tokens can in fact we removed!\\n-\\n-                if  (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n-                    (len(can_be_removed2) == len(bad_tokens)):\\n-                    # Yes it can be fixed!\\n-                    for bad_token in can_be_removed1:\\n-                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n-                        del tokenizer._added_tokens_decoder[remove_id]\\n-                        del tokenizer._added_tokens_encoder[bad_token]\\n-                    pass\\n-                    # Confirm 1 more time!\\n-                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n-                        logger.warning_once(\\n-                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n-                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n-                        )\\n-                        return tokenizer\\n-                    pass\\n-                pass\\n-\\n-                # :( Failure\\n-                raise RuntimeError(\\n-                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n-                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n-                )\\n-            pass\\n-            \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n-        pass\\n-    pass\\n-    return tokenizer\\n-pass\\n-\\n-\\n # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??\\n # For mixed precision, we need it to be in float32 not float16.\\n from peft.tuners.lora.layer import LoraLayer\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers.modeling_attn_mask_utils import (\\n from ..kernels import *\\n from ._utils import *\\n from ._utils import __version__\\n+from ..tokenizer_utils import *\\n if HAS_FLASH_ATTENTION:\\n     from flash_attn import flash_attn_func\\n \\n@@ -1014,8 +1015,8 @@ class FastLlamaModel:\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n+        tokenizer = load_correct_tokenizer(\\n+            tokenizer_name    = tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n             token             = token,\\n',\n",
       " '@@ -362,7 +362,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer = load_correct_tokenizer(\\n             tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n',\n",
       " '@@ -276,7 +276,8 @@ def unsloth_save_model(\\n             old_username = None, private = private,\\n         )\\n \\n-        model.original_push_to_hub(\\n+        getattr(model, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+        (\\n             repo_id            = save_directory,\\n             use_temp_dir       = use_temp_dir,\\n             commit_message     = commit_message,\\n@@ -290,7 +291,8 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n-            tokenizer.original_push_to_hub(\\n+            getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+            (\\n                 repo_id            = save_directory,\\n                 use_temp_dir       = use_temp_dir,\\n                 commit_message     = commit_message,\\n',\n",
       " '@@ -0,0 +1,414 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from transformers import AutoTokenizer\\n+from transformers.convert_slow_tokenizer import convert_slow_tokenizer\\n+from transformers import PreTrainedTokenizerFast\\n+import re\\n+import os\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n+__all__ = [\\n+    \"load_correct_tokenizer\",\\n+    \"fix_sentencepiece_tokenizer\",\\n+    \"check_tokenizer\",\\n+]\\n+\\n+\\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n+\\n+def try_fix_tokenizer(tokenizer, prepend = True):\\n+\\n+    if hasattr(tokenizer, \"_tokenizer\"):\\n+        converted_tokenizer = tokenizer._tokenizer\\n+    else:\\n+        converted_tokenizer = convert_slow_tokenizer(tokenizer)\\n+    pass\\n+\\n+    tokenizer_string = converted_tokenizer.to_str()\\n+\\n+    # Llama does ▁apple. Sometimes this is wrong!!\\n+    prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n+    if not prepend and prepend_text in tokenizer_string:\\n+        tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n+    pass\\n+\\n+    dir_names = dir(tokenizer)\\n+    # Get eos_token, bos_token etc\\n+    token_names = [x for x in dir_names if x.endswith(\"_token\") and x.count(\"_\") == 1]\\n+\\n+    for token_name in token_names:\\n+        token = getattr(tokenizer, token_name, None)\\n+        if token is None: continue\\n+        token_id = getattr(tokenizer, token_name + \"_id\", None)\\n+\\n+        # Locate the token\\'s id mapping in the string\\n+        find_text = f\\'\"id\":{token_id},\"content\":\"\\'\\n+        start = tokenizer_string.find(find_text) + len(find_text)\\n+        if start == -1: continue\\n+        end   = tokenizer_string.find(\\'\",\\', start)\\n+\\n+        bad_token = tokenizer_string[start : end]\\n+        # Check if token is the actual same one - if not, edit it\\n+        if bad_token != token:\\n+            bad_text  = f\\'{find_text}{bad_token}\",\\'\\n+            good_text = f\\'{find_text}{token}\",\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+\\n+            # And replace vocab section\\n+            bad_text = f\\'\"{bad_token}\":{token_id},\\'\\n+            good_text = f\\'\"{token}\":{token_id},\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+        pass\\n+    pass\\n+\\n+    fixed_tokenizer = converted_tokenizer.from_str(tokenizer_string)\\n+    return fixed_tokenizer\\n+pass\\n+\\n+\\n+def get_sorted_dict(dictionary):\\n+    sorted_keys = sorted(dictionary.values())\\n+    inverted_dictionary = { value : key for key, value in dictionary.items() }\\n+\\n+    sorted_dictionary = {}\\n+    for key in sorted_keys:\\n+        value = inverted_dictionary[key]\\n+        sorted_dictionary[value] = key\\n+    return sorted_dictionary\\n+pass\\n+\\n+\\n+def convert_to_fast_tokenizer(\\n+    slow_tokenizer,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    is_fast = getattr(slow_tokenizer, \"is_fast\", False)\\n+    if is_fast: return slow_tokenizer\\n+    \\n+    try:\\n+        tokenizer_name = slow_tokenizer.__class__.__name__\\n+        lowered_tokenizer_name = tokenizer_name.lower()\\n+        if lowered_tokenizer_name.endswith(\"tokenizer\"):\\n+            class_name = lowered_tokenizer_name[:-len(\"tokenizer\")]\\n+            FastTokenizer = eval(\\n+                f\\'__import__(f\"transformers.models.{class_name}\").{tokenizer_name}Fast\\'\\n+            )\\n+        else:\\n+            FastTokenizer = PreTrainedTokenizerFast\\n+    except:\\n+        FastTokenizer = PreTrainedTokenizerFast\\n+    pass\\n+\\n+    # Get all arguments (bos_token, etc)\\n+    docs = FastTokenizer.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args = [x for x in args if not x.endswith(\"_file\")]\\n+\\n+    # Also some missing maybe!\\n+    docs = PreTrainedTokenizerFast.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args2 = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args2 = [x for x in args2 if not x.endswith(\"_file\")]\\n+    args = list(set(args + args2))\\n+\\n+    kwargs = {}\\n+    for arg in args: kwargs[arg] = getattr(slow_tokenizer, arg, None)\\n+    kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = True)\\n+    fast_tokenizer = FastTokenizer( **kwargs )\\n+\\n+    # Check if they\\'re similar!\\n+    sorted_slow_tokenizer = get_sorted_dict(slow_tokenizer.get_vocab())\\n+    sorted_fast_tokenizer = get_sorted_dict(fast_tokenizer.get_vocab())\\n+\\n+    check_vocab   = (sorted_slow_tokenizer == sorted_fast_tokenizer)\\n+    check_special = (slow_tokenizer.all_special_tokens == fast_tokenizer.all_special_tokens)\\n+\\n+    # Failure so return slow_tokenizer\\n+    if not check_vocab or not check_special: return slow_tokenizer\\n+\\n+    # Now confirm if they match\\n+    if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        # Maybe remove prepending of __apple?\\n+        kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = False)\\n+        fast_tokenizer = FastTokenizer( **kwargs )\\n+        if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            # Failure :(\\n+            return slow_tokenizer\\n+        pass\\n+    pass\\n+\\n+    # Also tokenizer.model is missing!\\n+    name = slow_tokenizer.name_or_path.replace(\"/\", \"_\")\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+    new_location = f\"{temporary_location}/{name}\"\\n+    slow_tokenizer.save_pretrained(new_location)\\n+    fast_tokenizer.save_pretrained(new_location)\\n+\\n+    # Now load it!\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(new_location)\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    return slow_tokenizer\\n+pass\\n+\\n+\\n+def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+    # Get eos_token, bos_token etc\\n+    dir_names = dir(slow_tokenizer)\\n+    special_tokens = list(filter(None, (\\n+        getattr(slow_tokenizer, x) for x in dir_names\\n+        if x.endswith(\"_token\") and x.count(\"_\") == 1\\n+    )))\\n+    all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n+    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+        \"\".join(all_special_tokens)\\n+    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+pass\\n+\\n+\\n+global sentencepiece_model_pb2\\n+sentencepiece_model_pb2 = None\\n+\\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    global sentencepiece_model_pb2\\n+    if sentencepiece_model_pb2 is None:\\n+        try:\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        except:\\n+            if not os.path.exists(temporary_location):\\n+                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n+                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            pass\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n+def load_correct_tokenizer(\\n+    tokenizer_name,\\n+    model_max_length = None,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    trust_remote_code = False,\\n+):\\n+    slow_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+        use_fast          = False,\\n+    )\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+    )\\n+    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+    \\n+    # Confirm if slow and fast are equivalent!\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    else:\\n+        return convert_to_fast_tokenizer(slow_tokenizer)\\n+    pass\\n+pass\\n+\\n+\\n+def check_tokenizer(\\n+    model,\\n+    tokenizer,\\n+    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+    model_max_length = 4096,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    _reload = True,\\n+):\\n+    # Checks tokenizer for out of bounds ids.\\n+    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n+    # where <sep> had token id=32002.\\n+    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n+    # Seems like the Fast tokenizer in Rust breaks things!\\n+\\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n+    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n+    added_tokens_fast = tokenizer.added_tokens_decoder\\n+    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n+    sorted_keys = sorted(added_tokens_fast)\\n+    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n+\\n+    for j, index in enumerate(added_tokens_fast.keys()):\\n+        if index >= max_embedding_size:\\n+            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n+            bad_tokens  = list(added_tokens_fast.values())[j:]\\n+            if not _reload:\\n+                # Try removing the token\\n+                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+                special_tokens = tokenizer.special_tokens_map\\n+                import itertools\\n+                special_tokens = frozenset(\\n+                    itertools.chain.from_iterable(\\n+                        [x] if type(x) is str else x for x in special_tokens.values()\\n+                    )\\n+                )\\n+                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n+                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n+\\n+                # Check of extra tokens can in fact we removed!\\n+                can_be_removed = \\\\\\n+                    (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n+                    (len(can_be_removed2) == len(bad_tokens))\\n+\\n+                # Check if sep_token or other generic types\\n+                remove_generic = False\\n+                try_mapper = []\\n+                if not can_be_removed:\\n+                    names = dir(tokenizer)\\n+                    names = (x for x in names if x.endswith(\"_token\") and x.count(\"_\") == 1)\\n+                    generic_tokens = [(x, getattr(tokenizer, x, None)) for x in names]\\n+\\n+                    try_removal = []\\n+                    for token in bad_tokens:\\n+                        for (name_token, check_token) in generic_tokens:\\n+                            if check_token == token:\\n+                                try_removal.append(token)\\n+                                try_mapper.append(name_token)\\n+                            pass\\n+                        pass\\n+                    pass\\n+\\n+                    # Recheck!\\n+                    can_be_removed = (len(try_removal) == len(bad_tokens))\\n+                    if can_be_removed: remove_generic = True\\n+                    can_be_removed1 = bad_tokens\\n+                pass\\n+\\n+                if can_be_removed:\\n+                    # Yes it can be fixed!\\n+                    for j, bad_token in enumerate(can_be_removed1):\\n+                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n+                        del tokenizer._added_tokens_decoder[remove_id]\\n+                        del tokenizer._added_tokens_encoder[bad_token]\\n+\\n+                        if remove_generic and (try_removal[j] == bad_token):\\n+                            # Remove sep token for example\\n+                            setattr(tokenizer, try_mapper[j], None)\\n+                            setattr(tokenizer, try_mapper[j] + \"_id\", None)\\n+                        pass\\n+                    pass\\n+                    # Confirm 1 more time!\\n+                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n+                        logger.warning_once(\\n+                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n+                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n+                        )\\n+                        return convert_to_fast_tokenizer(tokenizer)\\n+                    pass\\n+                pass\\n+\\n+                # :( Failure\\n+                raise RuntimeError(\\n+                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n+                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n+                )\\n+            pass\\n+            \\n+            # Try slow tokenizer which can fix things!\\n+            tokenizer = AutoTokenizer.from_pretrained(\\n+                model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                use_fast = False,\\n+            )\\n+            return check_tokenizer(\\n+                model = model,\\n+                tokenizer = tokenizer,\\n+                model_name = model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                _reload = False,\\n+            )\\n+            break\\n+        pass\\n+    pass\\n+    return convert_to_fast_tokenizer(tokenizer)\\n+pass\\n',\n",
       " \"@@ -1,160 +0,0 @@\\n-# Byte-compiled / optimized / DLL files\\n-__pycache__/\\n-*.py[cod]\\n-*$py.class\\n-\\n-# C extensions\\n-*.so\\n-\\n-# Distribution / packaging\\n-.Python\\n-build/\\n-develop-eggs/\\n-dist/\\n-downloads/\\n-eggs/\\n-.eggs/\\n-lib/\\n-lib64/\\n-parts/\\n-sdist/\\n-var/\\n-wheels/\\n-share/python-wheels/\\n-*.egg-info/\\n-.installed.cfg\\n-*.egg\\n-MANIFEST\\n-\\n-# PyInstaller\\n-#  Usually these files are written by a python script from a template\\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n-*.manifest\\n-*.spec\\n-\\n-# Installer logs\\n-pip-log.txt\\n-pip-delete-this-directory.txt\\n-\\n-# Unit test / coverage reports\\n-htmlcov/\\n-.tox/\\n-.nox/\\n-.coverage\\n-.coverage.*\\n-.cache\\n-nosetests.xml\\n-coverage.xml\\n-*.cover\\n-*.py,cover\\n-.hypothesis/\\n-.pytest_cache/\\n-cover/\\n-\\n-# Translations\\n-*.mo\\n-*.pot\\n-\\n-# Django stuff:\\n-*.log\\n-local_settings.py\\n-db.sqlite3\\n-db.sqlite3-journal\\n-\\n-# Flask stuff:\\n-instance/\\n-.webassets-cache\\n-\\n-# Scrapy stuff:\\n-.scrapy\\n-\\n-# Sphinx documentation\\n-docs/_build/\\n-\\n-# PyBuilder\\n-.pybuilder/\\n-target/\\n-\\n-# Jupyter Notebook\\n-.ipynb_checkpoints\\n-\\n-# IPython\\n-profile_default/\\n-ipython_config.py\\n-\\n-# pyenv\\n-#   For a library or package, you might want to ignore these files since the code is\\n-#   intended to run in multiple environments; otherwise, check them in:\\n-# .python-version\\n-\\n-# pipenv\\n-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n-#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n-#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n-#   install all needed dependencies.\\n-#Pipfile.lock\\n-\\n-# poetry\\n-#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n-#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n-#   commonly ignored for libraries.\\n-#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n-#poetry.lock\\n-\\n-# pdm\\n-#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n-#pdm.lock\\n-#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n-#   in version control.\\n-#   https://pdm.fming.dev/#use-with-ide\\n-.pdm.toml\\n-\\n-# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n-__pypackages__/\\n-\\n-# Celery stuff\\n-celerybeat-schedule\\n-celerybeat.pid\\n-\\n-# SageMath parsed files\\n-*.sage.py\\n-\\n-# Environments\\n-.env\\n-.venv\\n-env/\\n-venv/\\n-ENV/\\n-env.bak/\\n-venv.bak/\\n-\\n-# Spyder project settings\\n-.spyderproject\\n-.spyproject\\n-\\n-# Rope project settings\\n-.ropeproject\\n-\\n-# mkdocs documentation\\n-/site\\n-\\n-# mypy\\n-.mypy_cache/\\n-.dmypy.json\\n-dmypy.json\\n-\\n-# Pyre type checker\\n-.pyre/\\n-\\n-# pytype static type analyzer\\n-.pytype/\\n-\\n-# Cython debug symbols\\n-cython_debug/\\n-\\n-# PyCharm\\n-#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n-#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n-#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n-#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n-#.idea/\\n\",\n",
       " '@@ -186,7 +186,7 @@\\n       same \"printed page\" as the copyright notice for easier\\n       identification within third-party archives.\\n \\n-   Copyright [yyyy] [name of copyright owner]\\n+   Copyright [2024-] [Unsloth AI, Daniel Han-Chen & Michael Han-Chen]\\n \\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n',\n",
       " '@@ -113,3 +113,4 @@ pass\\n from .models import *\\n from .save import *\\n from .chat_templates import *\\n+from .tokenizer_utils import *\\n',\n",
       " '@@ -15,15 +15,19 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n-    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n-from .models._utils import patch_tokenizer\\n+from .save import patch_saving_functions\\n import os\\n import shutil\\n+from .tokenizer_utils import (\\n+    load_correct_tokenizer,\\n+    fix_sentencepiece_tokenizer,\\n+)\\n+from .models._utils import patch_tokenizer\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -251,84 +255,23 @@ gemma_chatml_eos_token = (\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n-def fix_sentencepiece_tokenizer(\\n-    old_tokenizer,\\n-    new_tokenizer,\\n-    token_mapping,\\n-    temporary_location = \"_unsloth_sentencepiece_temp\",\\n-):\\n-    # From https://github.com/google/sentencepiece/issues/121\\n-    # We need to manually edit the sentencepiece tokenizer!\\n-    try:\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    except:\\n-        if not os.path.exists(temporary_location):\\n-            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n-            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            shutil.rmtree(temporary_location)\\n-        pass\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    pass\\n-\\n-    if not os.path.exists(temporary_location):\\n-        os.makedirs(temporary_location)\\n-    pass\\n-\\n-    # First save the old tokenizer\\n-    old_tokenizer.save_pretrained(temporary_location)\\n-\\n-    from sentencepiece import SentencePieceProcessor\\n-    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n-    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n-\\n-    # Now save the new tokenizer\\n-    new_tokenizer.save_pretrained(temporary_location)\\n-\\n-    # Now correct the old tokenizer\\'s .model file\\n-    for old_token, new_token in token_mapping.items():\\n-        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n-        ids = ids[0]\\n-        if (len(ids) != 1):\\n-            # Skip this token!\\n-            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n-            continue\\n-        pass\\n-        ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n-        assert(tokenizer_piece.piece == old_token)\\n-        tokenizer_piece.piece = new_token\\n-    pass\\n-\\n-    # And now write it\\n-    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n-        file.write(tokenizer_file.SerializeToString())\\n-    pass\\n-\\n-    # And load it!\\n-    from transformers import AutoTokenizer\\n-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n-    return tokenizer\\n-pass\\n-\\n-\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    assert(type(map_eos_token) is bool)\\n     old_tokenizer = tokenizer\\n \\n-    if map_eos_token is False:\\n-        assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n-    pass\\n-\\n     IS_GEMMA = False\\n     if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n         if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n         IS_GEMMA = True\\n     pass\\n \\n+    # We first check if the tokenizer is a fast one. If not, we cannot convert this!\\n+    is_fast_tokenizer = getattr(tokenizer, \"is_fast\", False)\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n@@ -348,9 +291,17 @@ def get_chat_template(\\n \\n         assert(type(stop_word) is str)\\n \\n-        # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n-        # For Gemma :)\\n-        if token_mapping is not None:\\n+        # Check fast tokenizer\\n+        if not is_fast_tokenizer:\\n+            logger.warning_once(\\n+                f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n+                \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n+                \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n+            )\\n+\\n+        elif token_mapping is not None:\\n+            # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n+            # For Gemma :)\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n@@ -368,7 +319,7 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            if not stop_word in token_mapping.values():\\n+            if map_eos_token and (not stop_word in token_mapping.values()):\\n                 # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n                 logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n@@ -376,14 +327,19 @@ def get_chat_template(\\n \\n             if skipped != len(token_mapping):\\n                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                if map_eos_token:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+                else:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)\\n+                pass\\n \\n                 # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n                 tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n             else:\\n                 pass\\n \\n-        elif stop_word != \"eos_token\":\\n+        elif map_eos_token and (stop_word != \"eos_token\"):\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n \\n             # Replaces the old EOS token with a new one.\\n@@ -393,9 +349,14 @@ def get_chat_template(\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            old_eos_token = tokenizer.eos_token\\n+            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+            # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+            token_mapping = { old_eos_token : stop_word, }\\n+            tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n         pass\\n \\n     else:\\n@@ -433,7 +394,10 @@ def get_chat_template(\\n     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n \\n-    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+    # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+\\n+    # Patch saving functions\\n+    tokenizer = patch_saving_functions(tokenizer)\\n \\n     return tokenizer#, stopping_criteria\\n pass\\n',\n",
       " '@@ -60,22 +60,16 @@ from xformers import __version__ as xformers_version\\n \\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n-    \"patch_tokenizer\",\\n-    \"check_tokenizer\",\\n     \"xformers\",\\n     \"xformers_attention\",\\n     \"xformers_version\",\\n     \"__version__\",\\n     \"HAS_FLASH_ATTENTION\",\\n     \"platform_system\",\\n+    \"patch_tokenizer\",\\n ]\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -144,103 +138,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-def check_tokenizer(\\n-    model,\\n-    tokenizer,\\n-    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n-    model_max_length = 4096,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    _reload = True,\\n-):\\n-    # Checks tokenizer for out of bounds ids.\\n-    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n-    # where <sep> had token id=32002.\\n-    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n-    # Seems like the Fast tokenizer in Rust breaks things!\\n-\\n-    # We ignore some of them!\\n-    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n-        return tokenizer\\n-    pass\\n-\\n-    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n-    added_tokens_fast = tokenizer.added_tokens_decoder\\n-    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n-    sorted_keys = sorted(added_tokens_fast)\\n-    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n-\\n-    for j, index in enumerate(added_tokens_fast.keys()):\\n-        if index >= max_embedding_size:\\n-            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n-            bad_tokens  = list(added_tokens_fast.values())[j:]\\n-\\n-            if not _reload:\\n-                # Try removing the token\\n-                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n-                special_tokens = tokenizer.special_tokens_map\\n-                import itertools\\n-                special_tokens = frozenset(\\n-                    itertools.chain.from_iterable(\\n-                        [x] if type(x) is str else x for x in special_tokens.values()\\n-                    )\\n-                )\\n-                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n-                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n-\\n-                # Check of extra tokens can in fact we removed!\\n-\\n-                if  (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n-                    (len(can_be_removed2) == len(bad_tokens)):\\n-                    # Yes it can be fixed!\\n-                    for bad_token in can_be_removed1:\\n-                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n-                        del tokenizer._added_tokens_decoder[remove_id]\\n-                        del tokenizer._added_tokens_encoder[bad_token]\\n-                    pass\\n-                    # Confirm 1 more time!\\n-                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n-                        logger.warning_once(\\n-                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n-                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n-                        )\\n-                        return tokenizer\\n-                    pass\\n-                pass\\n-\\n-                # :( Failure\\n-                raise RuntimeError(\\n-                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n-                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n-                )\\n-            pass\\n-            \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n-        pass\\n-    pass\\n-    return tokenizer\\n-pass\\n-\\n-\\n # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??\\n # For mixed precision, we need it to be in float32 not float16.\\n from peft.tuners.lora.layer import LoraLayer\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers.modeling_attn_mask_utils import (\\n from ..kernels import *\\n from ._utils import *\\n from ._utils import __version__\\n+from ..tokenizer_utils import *\\n if HAS_FLASH_ATTENTION:\\n     from flash_attn import flash_attn_func\\n \\n@@ -1014,8 +1015,8 @@ class FastLlamaModel:\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n+        tokenizer = load_correct_tokenizer(\\n+            tokenizer_name    = tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n             token             = token,\\n',\n",
       " '@@ -362,7 +362,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer = load_correct_tokenizer(\\n             tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n',\n",
       " '@@ -276,7 +276,8 @@ def unsloth_save_model(\\n             old_username = None, private = private,\\n         )\\n \\n-        model.original_push_to_hub(\\n+        getattr(model, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+        (\\n             repo_id            = save_directory,\\n             use_temp_dir       = use_temp_dir,\\n             commit_message     = commit_message,\\n@@ -290,7 +291,8 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n-            tokenizer.original_push_to_hub(\\n+            getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+            (\\n                 repo_id            = save_directory,\\n                 use_temp_dir       = use_temp_dir,\\n                 commit_message     = commit_message,\\n',\n",
       " '@@ -0,0 +1,414 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from transformers import AutoTokenizer\\n+from transformers.convert_slow_tokenizer import convert_slow_tokenizer\\n+from transformers import PreTrainedTokenizerFast\\n+import re\\n+import os\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n+__all__ = [\\n+    \"load_correct_tokenizer\",\\n+    \"fix_sentencepiece_tokenizer\",\\n+    \"check_tokenizer\",\\n+]\\n+\\n+\\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n+\\n+def try_fix_tokenizer(tokenizer, prepend = True):\\n+\\n+    if hasattr(tokenizer, \"_tokenizer\"):\\n+        converted_tokenizer = tokenizer._tokenizer\\n+    else:\\n+        converted_tokenizer = convert_slow_tokenizer(tokenizer)\\n+    pass\\n+\\n+    tokenizer_string = converted_tokenizer.to_str()\\n+\\n+    # Llama does ▁apple. Sometimes this is wrong!!\\n+    prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n+    if not prepend and prepend_text in tokenizer_string:\\n+        tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n+    pass\\n+\\n+    dir_names = dir(tokenizer)\\n+    # Get eos_token, bos_token etc\\n+    token_names = [x for x in dir_names if x.endswith(\"_token\") and x.count(\"_\") == 1]\\n+\\n+    for token_name in token_names:\\n+        token = getattr(tokenizer, token_name, None)\\n+        if token is None: continue\\n+        token_id = getattr(tokenizer, token_name + \"_id\", None)\\n+\\n+        # Locate the token\\'s id mapping in the string\\n+        find_text = f\\'\"id\":{token_id},\"content\":\"\\'\\n+        start = tokenizer_string.find(find_text) + len(find_text)\\n+        if start == -1: continue\\n+        end   = tokenizer_string.find(\\'\",\\', start)\\n+\\n+        bad_token = tokenizer_string[start : end]\\n+        # Check if token is the actual same one - if not, edit it\\n+        if bad_token != token:\\n+            bad_text  = f\\'{find_text}{bad_token}\",\\'\\n+            good_text = f\\'{find_text}{token}\",\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+\\n+            # And replace vocab section\\n+            bad_text = f\\'\"{bad_token}\":{token_id},\\'\\n+            good_text = f\\'\"{token}\":{token_id},\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+        pass\\n+    pass\\n+\\n+    fixed_tokenizer = converted_tokenizer.from_str(tokenizer_string)\\n+    return fixed_tokenizer\\n+pass\\n+\\n+\\n+def get_sorted_dict(dictionary):\\n+    sorted_keys = sorted(dictionary.values())\\n+    inverted_dictionary = { value : key for key, value in dictionary.items() }\\n+\\n+    sorted_dictionary = {}\\n+    for key in sorted_keys:\\n+        value = inverted_dictionary[key]\\n+        sorted_dictionary[value] = key\\n+    return sorted_dictionary\\n+pass\\n+\\n+\\n+def convert_to_fast_tokenizer(\\n+    slow_tokenizer,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    is_fast = getattr(slow_tokenizer, \"is_fast\", False)\\n+    if is_fast: return slow_tokenizer\\n+    \\n+    try:\\n+        tokenizer_name = slow_tokenizer.__class__.__name__\\n+        lowered_tokenizer_name = tokenizer_name.lower()\\n+        if lowered_tokenizer_name.endswith(\"tokenizer\"):\\n+            class_name = lowered_tokenizer_name[:-len(\"tokenizer\")]\\n+            FastTokenizer = eval(\\n+                f\\'__import__(f\"transformers.models.{class_name}\").{tokenizer_name}Fast\\'\\n+            )\\n+        else:\\n+            FastTokenizer = PreTrainedTokenizerFast\\n+    except:\\n+        FastTokenizer = PreTrainedTokenizerFast\\n+    pass\\n+\\n+    # Get all arguments (bos_token, etc)\\n+    docs = FastTokenizer.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args = [x for x in args if not x.endswith(\"_file\")]\\n+\\n+    # Also some missing maybe!\\n+    docs = PreTrainedTokenizerFast.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args2 = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args2 = [x for x in args2 if not x.endswith(\"_file\")]\\n+    args = list(set(args + args2))\\n+\\n+    kwargs = {}\\n+    for arg in args: kwargs[arg] = getattr(slow_tokenizer, arg, None)\\n+    kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = True)\\n+    fast_tokenizer = FastTokenizer( **kwargs )\\n+\\n+    # Check if they\\'re similar!\\n+    sorted_slow_tokenizer = get_sorted_dict(slow_tokenizer.get_vocab())\\n+    sorted_fast_tokenizer = get_sorted_dict(fast_tokenizer.get_vocab())\\n+\\n+    check_vocab   = (sorted_slow_tokenizer == sorted_fast_tokenizer)\\n+    check_special = (slow_tokenizer.all_special_tokens == fast_tokenizer.all_special_tokens)\\n+\\n+    # Failure so return slow_tokenizer\\n+    if not check_vocab or not check_special: return slow_tokenizer\\n+\\n+    # Now confirm if they match\\n+    if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        # Maybe remove prepending of __apple?\\n+        kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = False)\\n+        fast_tokenizer = FastTokenizer( **kwargs )\\n+        if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            # Failure :(\\n+            return slow_tokenizer\\n+        pass\\n+    pass\\n+\\n+    # Also tokenizer.model is missing!\\n+    name = slow_tokenizer.name_or_path.replace(\"/\", \"_\")\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+    new_location = f\"{temporary_location}/{name}\"\\n+    slow_tokenizer.save_pretrained(new_location)\\n+    fast_tokenizer.save_pretrained(new_location)\\n+\\n+    # Now load it!\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(new_location)\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    return slow_tokenizer\\n+pass\\n+\\n+\\n+def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+    # Get eos_token, bos_token etc\\n+    dir_names = dir(slow_tokenizer)\\n+    special_tokens = list(filter(None, (\\n+        getattr(slow_tokenizer, x) for x in dir_names\\n+        if x.endswith(\"_token\") and x.count(\"_\") == 1\\n+    )))\\n+    all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n+    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+        \"\".join(all_special_tokens)\\n+    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+pass\\n+\\n+\\n+global sentencepiece_model_pb2\\n+sentencepiece_model_pb2 = None\\n+\\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    global sentencepiece_model_pb2\\n+    if sentencepiece_model_pb2 is None:\\n+        try:\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        except:\\n+            if not os.path.exists(temporary_location):\\n+                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n+                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            pass\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n+def load_correct_tokenizer(\\n+    tokenizer_name,\\n+    model_max_length = None,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    trust_remote_code = False,\\n+):\\n+    slow_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+        use_fast          = False,\\n+    )\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+    )\\n+    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+    \\n+    # Confirm if slow and fast are equivalent!\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    else:\\n+        return convert_to_fast_tokenizer(slow_tokenizer)\\n+    pass\\n+pass\\n+\\n+\\n+def check_tokenizer(\\n+    model,\\n+    tokenizer,\\n+    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+    model_max_length = 4096,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    _reload = True,\\n+):\\n+    # Checks tokenizer for out of bounds ids.\\n+    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n+    # where <sep> had token id=32002.\\n+    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n+    # Seems like the Fast tokenizer in Rust breaks things!\\n+\\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n+    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n+    added_tokens_fast = tokenizer.added_tokens_decoder\\n+    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n+    sorted_keys = sorted(added_tokens_fast)\\n+    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n+\\n+    for j, index in enumerate(added_tokens_fast.keys()):\\n+        if index >= max_embedding_size:\\n+            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n+            bad_tokens  = list(added_tokens_fast.values())[j:]\\n+            if not _reload:\\n+                # Try removing the token\\n+                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+                special_tokens = tokenizer.special_tokens_map\\n+                import itertools\\n+                special_tokens = frozenset(\\n+                    itertools.chain.from_iterable(\\n+                        [x] if type(x) is str else x for x in special_tokens.values()\\n+                    )\\n+                )\\n+                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n+                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n+\\n+                # Check of extra tokens can in fact we removed!\\n+                can_be_removed = \\\\\\n+                    (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n+                    (len(can_be_removed2) == len(bad_tokens))\\n+\\n+                # Check if sep_token or other generic types\\n+                remove_generic = False\\n+                try_mapper = []\\n+                if not can_be_removed:\\n+                    names = dir(tokenizer)\\n+                    names = (x for x in names if x.endswith(\"_token\") and x.count(\"_\") == 1)\\n+                    generic_tokens = [(x, getattr(tokenizer, x, None)) for x in names]\\n+\\n+                    try_removal = []\\n+                    for token in bad_tokens:\\n+                        for (name_token, check_token) in generic_tokens:\\n+                            if check_token == token:\\n+                                try_removal.append(token)\\n+                                try_mapper.append(name_token)\\n+                            pass\\n+                        pass\\n+                    pass\\n+\\n+                    # Recheck!\\n+                    can_be_removed = (len(try_removal) == len(bad_tokens))\\n+                    if can_be_removed: remove_generic = True\\n+                    can_be_removed1 = bad_tokens\\n+                pass\\n+\\n+                if can_be_removed:\\n+                    # Yes it can be fixed!\\n+                    for j, bad_token in enumerate(can_be_removed1):\\n+                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n+                        del tokenizer._added_tokens_decoder[remove_id]\\n+                        del tokenizer._added_tokens_encoder[bad_token]\\n+\\n+                        if remove_generic and (try_removal[j] == bad_token):\\n+                            # Remove sep token for example\\n+                            setattr(tokenizer, try_mapper[j], None)\\n+                            setattr(tokenizer, try_mapper[j] + \"_id\", None)\\n+                        pass\\n+                    pass\\n+                    # Confirm 1 more time!\\n+                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n+                        logger.warning_once(\\n+                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n+                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n+                        )\\n+                        return convert_to_fast_tokenizer(tokenizer)\\n+                    pass\\n+                pass\\n+\\n+                # :( Failure\\n+                raise RuntimeError(\\n+                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n+                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n+                )\\n+            pass\\n+            \\n+            # Try slow tokenizer which can fix things!\\n+            tokenizer = AutoTokenizer.from_pretrained(\\n+                model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                use_fast = False,\\n+            )\\n+            return check_tokenizer(\\n+                model = model,\\n+                tokenizer = tokenizer,\\n+                model_name = model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                _reload = False,\\n+            )\\n+            break\\n+        pass\\n+    pass\\n+    return convert_to_fast_tokenizer(tokenizer)\\n+pass\\n',\n",
       " \"@@ -1,160 +0,0 @@\\n-# Byte-compiled / optimized / DLL files\\n-__pycache__/\\n-*.py[cod]\\n-*$py.class\\n-\\n-# C extensions\\n-*.so\\n-\\n-# Distribution / packaging\\n-.Python\\n-build/\\n-develop-eggs/\\n-dist/\\n-downloads/\\n-eggs/\\n-.eggs/\\n-lib/\\n-lib64/\\n-parts/\\n-sdist/\\n-var/\\n-wheels/\\n-share/python-wheels/\\n-*.egg-info/\\n-.installed.cfg\\n-*.egg\\n-MANIFEST\\n-\\n-# PyInstaller\\n-#  Usually these files are written by a python script from a template\\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n-*.manifest\\n-*.spec\\n-\\n-# Installer logs\\n-pip-log.txt\\n-pip-delete-this-directory.txt\\n-\\n-# Unit test / coverage reports\\n-htmlcov/\\n-.tox/\\n-.nox/\\n-.coverage\\n-.coverage.*\\n-.cache\\n-nosetests.xml\\n-coverage.xml\\n-*.cover\\n-*.py,cover\\n-.hypothesis/\\n-.pytest_cache/\\n-cover/\\n-\\n-# Translations\\n-*.mo\\n-*.pot\\n-\\n-# Django stuff:\\n-*.log\\n-local_settings.py\\n-db.sqlite3\\n-db.sqlite3-journal\\n-\\n-# Flask stuff:\\n-instance/\\n-.webassets-cache\\n-\\n-# Scrapy stuff:\\n-.scrapy\\n-\\n-# Sphinx documentation\\n-docs/_build/\\n-\\n-# PyBuilder\\n-.pybuilder/\\n-target/\\n-\\n-# Jupyter Notebook\\n-.ipynb_checkpoints\\n-\\n-# IPython\\n-profile_default/\\n-ipython_config.py\\n-\\n-# pyenv\\n-#   For a library or package, you might want to ignore these files since the code is\\n-#   intended to run in multiple environments; otherwise, check them in:\\n-# .python-version\\n-\\n-# pipenv\\n-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\n-#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\n-#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\n-#   install all needed dependencies.\\n-#Pipfile.lock\\n-\\n-# poetry\\n-#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\\n-#   This is especially recommended for binary packages to ensure reproducibility, and is more\\n-#   commonly ignored for libraries.\\n-#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\\n-#poetry.lock\\n-\\n-# pdm\\n-#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\\n-#pdm.lock\\n-#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\\n-#   in version control.\\n-#   https://pdm.fming.dev/#use-with-ide\\n-.pdm.toml\\n-\\n-# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\\n-__pypackages__/\\n-\\n-# Celery stuff\\n-celerybeat-schedule\\n-celerybeat.pid\\n-\\n-# SageMath parsed files\\n-*.sage.py\\n-\\n-# Environments\\n-.env\\n-.venv\\n-env/\\n-venv/\\n-ENV/\\n-env.bak/\\n-venv.bak/\\n-\\n-# Spyder project settings\\n-.spyderproject\\n-.spyproject\\n-\\n-# Rope project settings\\n-.ropeproject\\n-\\n-# mkdocs documentation\\n-/site\\n-\\n-# mypy\\n-.mypy_cache/\\n-.dmypy.json\\n-dmypy.json\\n-\\n-# Pyre type checker\\n-.pyre/\\n-\\n-# pytype static type analyzer\\n-.pytype/\\n-\\n-# Cython debug symbols\\n-cython_debug/\\n-\\n-# PyCharm\\n-#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\\n-#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\\n-#  and can be added to the global gitignore or merged into this file.  For a more nuclear\\n-#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\\n-#.idea/\\n\",\n",
       " '@@ -186,7 +186,7 @@\\n       same \"printed page\" as the copyright notice for easier\\n       identification within third-party archives.\\n \\n-   Copyright [yyyy] [name of copyright owner]\\n+   Copyright [2024-] [Unsloth AI, Daniel Han-Chen & Michael Han-Chen]\\n \\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n',\n",
       " '@@ -113,3 +113,4 @@ pass\\n from .models import *\\n from .save import *\\n from .chat_templates import *\\n+from .tokenizer_utils import *\\n',\n",
       " '@@ -15,15 +15,19 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n-    \"fix_sentencepiece_tokenizer\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n from torch import LongTensor, FloatTensor\\n from transformers.models.llama.modeling_llama import logger\\n-from .models._utils import patch_tokenizer\\n+from .save import patch_saving_functions\\n import os\\n import shutil\\n+from .tokenizer_utils import (\\n+    load_correct_tokenizer,\\n+    fix_sentencepiece_tokenizer,\\n+)\\n+from .models._utils import patch_tokenizer\\n \\n CHAT_TEMPLATES = {}\\n \\n@@ -251,84 +255,23 @@ gemma_chatml_eos_token = (\\n CHAT_TEMPLATES[\"gemma_chatml\"] = (gemma_chatml_template, gemma_chatml_eos_token,)\\n \\n \\n-def fix_sentencepiece_tokenizer(\\n-    old_tokenizer,\\n-    new_tokenizer,\\n-    token_mapping,\\n-    temporary_location = \"_unsloth_sentencepiece_temp\",\\n-):\\n-    # From https://github.com/google/sentencepiece/issues/121\\n-    # We need to manually edit the sentencepiece tokenizer!\\n-    try:\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    except:\\n-        if not os.path.exists(temporary_location):\\n-            os.system(\"git clone https://github.com/google/sentencepiece.git unsloth_sentencepiece_temp\")\\n-            os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            shutil.rmtree(temporary_location)\\n-        pass\\n-        import sentencepiece.sentencepiece_model_pb2 as sentencepiece_model_pb2\\n-    pass\\n-\\n-    if not os.path.exists(temporary_location):\\n-        os.makedirs(temporary_location)\\n-    pass\\n-\\n-    # First save the old tokenizer\\n-    old_tokenizer.save_pretrained(temporary_location)\\n-\\n-    from sentencepiece import SentencePieceProcessor\\n-    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n-    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n-\\n-    # Now save the new tokenizer\\n-    new_tokenizer.save_pretrained(temporary_location)\\n-\\n-    # Now correct the old tokenizer\\'s .model file\\n-    for old_token, new_token in token_mapping.items():\\n-        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n-        ids = ids[0]\\n-        if (len(ids) != 1):\\n-            # Skip this token!\\n-            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n-            continue\\n-        pass\\n-        ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n-        assert(tokenizer_piece.piece == old_token)\\n-        tokenizer_piece.piece = new_token\\n-    pass\\n-\\n-    # And now write it\\n-    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n-        file.write(tokenizer_file.SerializeToString())\\n-    pass\\n-\\n-    # And load it!\\n-    from transformers import AutoTokenizer\\n-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n-    return tokenizer\\n-pass\\n-\\n-\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n     mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"},\\n     map_eos_token = True,\\n ):\\n+    assert(type(map_eos_token) is bool)\\n     old_tokenizer = tokenizer\\n \\n-    if map_eos_token is False:\\n-        assert(\"Unsloth: Can only map new tokens to EOS for now. Adding new tokens is not yet supported.\")\\n-    pass\\n-\\n     IS_GEMMA = False\\n     if tokenizer.__class__.__name__.startswith(\"Gemma\"):\\n         if chat_template == \"chatml\": chat_template = \"gemma_chatml\"\\n         IS_GEMMA = True\\n     pass\\n \\n+    # We first check if the tokenizer is a fast one. If not, we cannot convert this!\\n+    is_fast_tokenizer = getattr(tokenizer, \"is_fast\", False)\\n     old_padding_side = tokenizer.padding_side\\n \\n     if type(chat_template) in (list, tuple,):\\n@@ -348,9 +291,17 @@ def get_chat_template(\\n \\n         assert(type(stop_word) is str)\\n \\n-        # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n-        # For Gemma :)\\n-        if token_mapping is not None:\\n+        # Check fast tokenizer\\n+        if not is_fast_tokenizer:\\n+            logger.warning_once(\\n+                f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n+                \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n+                \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n+            )\\n+\\n+        elif token_mapping is not None:\\n+            # token_mapping = {\"<start_of_turn>\" : \"<|im_start|>\", \"<end_of_turn>\" : \"<|im_end|>\"}\\n+            # For Gemma :)\\n \\n             string_vocab = tokenizer._tokenizer.to_str()\\n \\n@@ -368,7 +319,7 @@ def get_chat_template(\\n                 pass\\n             pass\\n \\n-            if not stop_word in token_mapping.values():\\n+            if map_eos_token and (not stop_word in token_mapping.values()):\\n                 # Do not map 107 = <|im_end|> and 1 = <|im_end|>. This will reduce the vocab size by 1\\n                 logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n                 string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n@@ -376,14 +327,19 @@ def get_chat_template(\\n \\n             if skipped != len(token_mapping):\\n                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-                new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+                if map_eos_token:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+                else:\\n+                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)\\n+                pass\\n \\n                 # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n                 tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n             else:\\n                 pass\\n \\n-        elif stop_word != \"eos_token\":\\n+        elif map_eos_token and (stop_word != \"eos_token\"):\\n             logger.warning_once(f\"Unsloth: Will map {stop_word} to EOS = {tokenizer.eos_token}.\")\\n \\n             # Replaces the old EOS token with a new one.\\n@@ -393,9 +349,14 @@ def get_chat_template(\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            string_vocab = string_vocab.replace(tokenizer.eos_token, stop_word)\\n+            old_eos_token = tokenizer.eos_token\\n+            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+\\n+            # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n+            token_mapping = { old_eos_token : stop_word, }\\n+            tokenizer = fix_sentencepiece_tokenizer(tokenizer, new_tokenizer, token_mapping,)\\n         pass\\n \\n     else:\\n@@ -433,7 +394,10 @@ def get_chat_template(\\n     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n \\n-    #stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+    # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n+\\n+    # Patch saving functions\\n+    tokenizer = patch_saving_functions(tokenizer)\\n \\n     return tokenizer#, stopping_criteria\\n pass\\n',\n",
       " '@@ -60,22 +60,16 @@ from xformers import __version__ as xformers_version\\n \\n __all__ = [\\n     \"prepare_model_for_kbit_training\",\\n-    \"patch_tokenizer\",\\n-    \"check_tokenizer\",\\n     \"xformers\",\\n     \"xformers_attention\",\\n     \"xformers_version\",\\n     \"__version__\",\\n     \"HAS_FLASH_ATTENTION\",\\n     \"platform_system\",\\n+    \"patch_tokenizer\",\\n ]\\n \\n \\n-IGNORED_TOKENIZER_CHECKING = frozenset((\\n-    \"CodeLlamaTokenizerFast\",\\n-    \"CodeLlamaTokenizer\",\\n-))\\n-\\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n     use_gradient_checkpointing : bool = True,\\n@@ -144,103 +138,6 @@ def patch_tokenizer(model, tokenizer):\\n pass\\n \\n \\n-def check_tokenizer(\\n-    model,\\n-    tokenizer,\\n-    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n-    model_max_length = 4096,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    _reload = True,\\n-):\\n-    # Checks tokenizer for out of bounds ids.\\n-    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n-    # where <sep> had token id=32002.\\n-    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n-    # Seems like the Fast tokenizer in Rust breaks things!\\n-\\n-    # We ignore some of them!\\n-    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n-        return tokenizer\\n-    pass\\n-\\n-    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n-    added_tokens_fast = tokenizer.added_tokens_decoder\\n-    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n-    sorted_keys = sorted(added_tokens_fast)\\n-    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n-\\n-    for j, index in enumerate(added_tokens_fast.keys()):\\n-        if index >= max_embedding_size:\\n-            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n-            bad_tokens  = list(added_tokens_fast.values())[j:]\\n-\\n-            if not _reload:\\n-                # Try removing the token\\n-                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n-                special_tokens = tokenizer.special_tokens_map\\n-                import itertools\\n-                special_tokens = frozenset(\\n-                    itertools.chain.from_iterable(\\n-                        [x] if type(x) is str else x for x in special_tokens.values()\\n-                    )\\n-                )\\n-                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n-                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n-\\n-                # Check of extra tokens can in fact we removed!\\n-\\n-                if  (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n-                    (len(can_be_removed2) == len(bad_tokens)):\\n-                    # Yes it can be fixed!\\n-                    for bad_token in can_be_removed1:\\n-                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n-                        del tokenizer._added_tokens_decoder[remove_id]\\n-                        del tokenizer._added_tokens_encoder[bad_token]\\n-                    pass\\n-                    # Confirm 1 more time!\\n-                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n-                        logger.warning_once(\\n-                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n-                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n-                        )\\n-                        return tokenizer\\n-                    pass\\n-                pass\\n-\\n-                # :( Failure\\n-                raise RuntimeError(\\n-                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n-                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n-                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n-                )\\n-            pass\\n-            \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n-        pass\\n-    pass\\n-    return tokenizer\\n-pass\\n-\\n-\\n # Weirdly LoraLayer.update_layer downcasts PEFT layers to float16??\\n # For mixed precision, we need it to be in float32 not float16.\\n from peft.tuners.lora.layer import LoraLayer\\n',\n",
       " '@@ -26,6 +26,7 @@ from transformers.modeling_attn_mask_utils import (\\n from ..kernels import *\\n from ._utils import *\\n from ._utils import __version__\\n+from ..tokenizer_utils import *\\n if HAS_FLASH_ATTENTION:\\n     from flash_attn import flash_attn_func\\n \\n@@ -1014,8 +1015,8 @@ class FastLlamaModel:\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n+        tokenizer = load_correct_tokenizer(\\n+            tokenizer_name    = tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n             token             = token,\\n',\n",
       " '@@ -362,7 +362,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer = load_correct_tokenizer(\\n             tokenizer_name,\\n             model_max_length  = max_position_embeddings,\\n             padding_side      = \"right\",\\n',\n",
       " '@@ -276,7 +276,8 @@ def unsloth_save_model(\\n             old_username = None, private = private,\\n         )\\n \\n-        model.original_push_to_hub(\\n+        getattr(model, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+        (\\n             repo_id            = save_directory,\\n             use_temp_dir       = use_temp_dir,\\n             commit_message     = commit_message,\\n@@ -290,7 +291,8 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n-            tokenizer.original_push_to_hub(\\n+            getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n+            (\\n                 repo_id            = save_directory,\\n                 use_temp_dir       = use_temp_dir,\\n                 commit_message     = commit_message,\\n',\n",
       " '@@ -0,0 +1,414 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from transformers import AutoTokenizer\\n+from transformers.convert_slow_tokenizer import convert_slow_tokenizer\\n+from transformers import PreTrainedTokenizerFast\\n+import re\\n+import os\\n+from transformers.models.llama.modeling_llama import logger\\n+\\n+__all__ = [\\n+    \"load_correct_tokenizer\",\\n+    \"fix_sentencepiece_tokenizer\",\\n+    \"check_tokenizer\",\\n+]\\n+\\n+\\n+IGNORED_TOKENIZER_CHECKING = frozenset((\\n+    \"CodeLlamaTokenizerFast\",\\n+    \"CodeLlamaTokenizer\",\\n+))\\n+\\n+\\n+def try_fix_tokenizer(tokenizer, prepend = True):\\n+\\n+    if hasattr(tokenizer, \"_tokenizer\"):\\n+        converted_tokenizer = tokenizer._tokenizer\\n+    else:\\n+        converted_tokenizer = convert_slow_tokenizer(tokenizer)\\n+    pass\\n+\\n+    tokenizer_string = converted_tokenizer.to_str()\\n+\\n+    # Llama does ▁apple. Sometimes this is wrong!!\\n+    prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n+    if not prepend and prepend_text in tokenizer_string:\\n+        tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n+    pass\\n+\\n+    dir_names = dir(tokenizer)\\n+    # Get eos_token, bos_token etc\\n+    token_names = [x for x in dir_names if x.endswith(\"_token\") and x.count(\"_\") == 1]\\n+\\n+    for token_name in token_names:\\n+        token = getattr(tokenizer, token_name, None)\\n+        if token is None: continue\\n+        token_id = getattr(tokenizer, token_name + \"_id\", None)\\n+\\n+        # Locate the token\\'s id mapping in the string\\n+        find_text = f\\'\"id\":{token_id},\"content\":\"\\'\\n+        start = tokenizer_string.find(find_text) + len(find_text)\\n+        if start == -1: continue\\n+        end   = tokenizer_string.find(\\'\",\\', start)\\n+\\n+        bad_token = tokenizer_string[start : end]\\n+        # Check if token is the actual same one - if not, edit it\\n+        if bad_token != token:\\n+            bad_text  = f\\'{find_text}{bad_token}\",\\'\\n+            good_text = f\\'{find_text}{token}\",\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+\\n+            # And replace vocab section\\n+            bad_text = f\\'\"{bad_token}\":{token_id},\\'\\n+            good_text = f\\'\"{token}\":{token_id},\\'\\n+            tokenizer_string = tokenizer_string.replace(bad_text, good_text, 1)\\n+        pass\\n+    pass\\n+\\n+    fixed_tokenizer = converted_tokenizer.from_str(tokenizer_string)\\n+    return fixed_tokenizer\\n+pass\\n+\\n+\\n+def get_sorted_dict(dictionary):\\n+    sorted_keys = sorted(dictionary.values())\\n+    inverted_dictionary = { value : key for key, value in dictionary.items() }\\n+\\n+    sorted_dictionary = {}\\n+    for key in sorted_keys:\\n+        value = inverted_dictionary[key]\\n+        sorted_dictionary[value] = key\\n+    return sorted_dictionary\\n+pass\\n+\\n+\\n+def convert_to_fast_tokenizer(\\n+    slow_tokenizer,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    is_fast = getattr(slow_tokenizer, \"is_fast\", False)\\n+    if is_fast: return slow_tokenizer\\n+    \\n+    try:\\n+        tokenizer_name = slow_tokenizer.__class__.__name__\\n+        lowered_tokenizer_name = tokenizer_name.lower()\\n+        if lowered_tokenizer_name.endswith(\"tokenizer\"):\\n+            class_name = lowered_tokenizer_name[:-len(\"tokenizer\")]\\n+            FastTokenizer = eval(\\n+                f\\'__import__(f\"transformers.models.{class_name}\").{tokenizer_name}Fast\\'\\n+            )\\n+        else:\\n+            FastTokenizer = PreTrainedTokenizerFast\\n+    except:\\n+        FastTokenizer = PreTrainedTokenizerFast\\n+    pass\\n+\\n+    # Get all arguments (bos_token, etc)\\n+    docs = FastTokenizer.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args = [x for x in args if not x.endswith(\"_file\")]\\n+\\n+    # Also some missing maybe!\\n+    docs = PreTrainedTokenizerFast.__doc__\\n+    docs = docs[docs.find(\"Args:\"):]\\n+    args2 = re.findall(r\"\\\\n[\\\\s]+([^\\\\s]{1,}) \\\\(\", docs, flags = re.MULTILINE)\\n+    args2 = [x for x in args2 if not x.endswith(\"_file\")]\\n+    args = list(set(args + args2))\\n+\\n+    kwargs = {}\\n+    for arg in args: kwargs[arg] = getattr(slow_tokenizer, arg, None)\\n+    kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = True)\\n+    fast_tokenizer = FastTokenizer( **kwargs )\\n+\\n+    # Check if they\\'re similar!\\n+    sorted_slow_tokenizer = get_sorted_dict(slow_tokenizer.get_vocab())\\n+    sorted_fast_tokenizer = get_sorted_dict(fast_tokenizer.get_vocab())\\n+\\n+    check_vocab   = (sorted_slow_tokenizer == sorted_fast_tokenizer)\\n+    check_special = (slow_tokenizer.all_special_tokens == fast_tokenizer.all_special_tokens)\\n+\\n+    # Failure so return slow_tokenizer\\n+    if not check_vocab or not check_special: return slow_tokenizer\\n+\\n+    # Now confirm if they match\\n+    if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        # Maybe remove prepending of __apple?\\n+        kwargs[\"tokenizer_object\"] = try_fix_tokenizer(slow_tokenizer, prepend = False)\\n+        fast_tokenizer = FastTokenizer( **kwargs )\\n+        if not assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            # Failure :(\\n+            return slow_tokenizer\\n+        pass\\n+    pass\\n+\\n+    # Also tokenizer.model is missing!\\n+    name = slow_tokenizer.name_or_path.replace(\"/\", \"_\")\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+    new_location = f\"{temporary_location}/{name}\"\\n+    slow_tokenizer.save_pretrained(new_location)\\n+    fast_tokenizer.save_pretrained(new_location)\\n+\\n+    # Now load it!\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(new_location)\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    return slow_tokenizer\\n+pass\\n+\\n+\\n+def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+    # Get eos_token, bos_token etc\\n+    dir_names = dir(slow_tokenizer)\\n+    special_tokens = list(filter(None, (\\n+        getattr(slow_tokenizer, x) for x in dir_names\\n+        if x.endswith(\"_token\") and x.count(\"_\") == 1\\n+    )))\\n+    all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n+    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+        \"\".join(all_special_tokens)\\n+    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+pass\\n+\\n+\\n+global sentencepiece_model_pb2\\n+sentencepiece_model_pb2 = None\\n+\\n+def fix_sentencepiece_tokenizer(\\n+    old_tokenizer,\\n+    new_tokenizer,\\n+    token_mapping,\\n+    temporary_location = \"_unsloth_sentencepiece_temp\",\\n+):\\n+    # From https://github.com/google/sentencepiece/issues/121\\n+    # We need to manually edit the sentencepiece tokenizer!\\n+    global sentencepiece_model_pb2\\n+    if sentencepiece_model_pb2 is None:\\n+        try:\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        except:\\n+            if not os.path.exists(temporary_location):\\n+                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n+                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n+            pass\\n+            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n+            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n+        pass\\n+\\n+    if not os.path.exists(temporary_location):\\n+        os.makedirs(temporary_location)\\n+    pass\\n+\\n+    # First save the old tokenizer\\n+    old_tokenizer.save_pretrained(temporary_location)\\n+\\n+    from sentencepiece import SentencePieceProcessor\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n+\\n+    # Now save the new tokenizer\\n+    new_tokenizer.save_pretrained(temporary_location)\\n+\\n+    # Now correct the old tokenizer\\'s .model file\\n+    for old_token, new_token in token_mapping.items():\\n+        ids = old_tokenizer([old_token], add_special_tokens = False).input_ids\\n+        ids = ids[0]\\n+        if (len(ids) != 1):\\n+            # Skip this token!\\n+            print(f\"Skip mapping {old_token} to {new_token} since {new_token} is already in the tokenizer!\")\\n+            continue\\n+        pass\\n+        ids = ids[0]\\n+        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        assert(tokenizer_piece.piece == old_token)\\n+        tokenizer_piece.piece = new_token\\n+    pass\\n+\\n+    # And now write it\\n+    with open(f\"{temporary_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # And load it!\\n+    from transformers import AutoTokenizer\\n+    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    return tokenizer\\n+pass\\n+\\n+\\n+def load_correct_tokenizer(\\n+    tokenizer_name,\\n+    model_max_length = None,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    trust_remote_code = False,\\n+):\\n+    slow_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+        use_fast          = False,\\n+    )\\n+    fast_tokenizer = AutoTokenizer.from_pretrained(\\n+        tokenizer_name,\\n+        model_max_length  = model_max_length,\\n+        padding_side      = padding_side,\\n+        token             = token,\\n+        trust_remote_code = trust_remote_code,\\n+    )\\n+    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+    \\n+    # Confirm if slow and fast are equivalent!\\n+    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+        return fast_tokenizer\\n+    else:\\n+        return convert_to_fast_tokenizer(slow_tokenizer)\\n+    pass\\n+pass\\n+\\n+\\n+def check_tokenizer(\\n+    model,\\n+    tokenizer,\\n+    model_name = \"unsloth/llama-2-7b-bnb-4bit\",\\n+    model_max_length = 4096,\\n+    padding_side = \"right\",\\n+    token = None,\\n+    _reload = True,\\n+):\\n+    # Checks tokenizer for out of bounds ids.\\n+    # Mainly a fix for https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\\n+    # where <sep> had token id=32002.\\n+    # See https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/discussions/25\\n+    # Seems like the Fast tokenizer in Rust breaks things!\\n+\\n+    # We ignore some of them!\\n+    if tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+        return tokenizer\\n+    pass\\n+\\n+    max_embedding_size = model.model.embed_tokens.weight.shape[0]\\n+    added_tokens_fast = tokenizer.added_tokens_decoder\\n+    added_tokens_fast = {index : str(value) for index, value in added_tokens_fast.items()}\\n+    sorted_keys = sorted(added_tokens_fast)\\n+    added_tokens_fast = {key : added_tokens_fast[key] for key in sorted_keys}\\n+\\n+    for j, index in enumerate(added_tokens_fast.keys()):\\n+        if index >= max_embedding_size:\\n+            bad_indices = list(added_tokens_fast.keys  ())[j:]\\n+            bad_tokens  = list(added_tokens_fast.values())[j:]\\n+            if not _reload:\\n+                # Try removing the token\\n+                added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+                special_tokens = tokenizer.special_tokens_map\\n+                import itertools\\n+                special_tokens = frozenset(\\n+                    itertools.chain.from_iterable(\\n+                        [x] if type(x) is str else x for x in special_tokens.values()\\n+                    )\\n+                )\\n+                can_be_removed1 = [x for x in bad_tokens if x not in special_tokens]\\n+                can_be_removed2 = [x for x in can_be_removed1 if x in tokenizer._added_tokens_encoder.keys()]\\n+\\n+                # Check of extra tokens can in fact we removed!\\n+                can_be_removed = \\\\\\n+                    (len(can_be_removed1) == len(bad_tokens)) and \\\\\\n+                    (len(can_be_removed2) == len(bad_tokens))\\n+\\n+                # Check if sep_token or other generic types\\n+                remove_generic = False\\n+                try_mapper = []\\n+                if not can_be_removed:\\n+                    names = dir(tokenizer)\\n+                    names = (x for x in names if x.endswith(\"_token\") and x.count(\"_\") == 1)\\n+                    generic_tokens = [(x, getattr(tokenizer, x, None)) for x in names]\\n+\\n+                    try_removal = []\\n+                    for token in bad_tokens:\\n+                        for (name_token, check_token) in generic_tokens:\\n+                            if check_token == token:\\n+                                try_removal.append(token)\\n+                                try_mapper.append(name_token)\\n+                            pass\\n+                        pass\\n+                    pass\\n+\\n+                    # Recheck!\\n+                    can_be_removed = (len(try_removal) == len(bad_tokens))\\n+                    if can_be_removed: remove_generic = True\\n+                    can_be_removed1 = bad_tokens\\n+                pass\\n+\\n+                if can_be_removed:\\n+                    # Yes it can be fixed!\\n+                    for j, bad_token in enumerate(can_be_removed1):\\n+                        remove_id = tokenizer._added_tokens_encoder[bad_token]\\n+                        del tokenizer._added_tokens_decoder[remove_id]\\n+                        del tokenizer._added_tokens_encoder[bad_token]\\n+\\n+                        if remove_generic and (try_removal[j] == bad_token):\\n+                            # Remove sep token for example\\n+                            setattr(tokenizer, try_mapper[j], None)\\n+                            setattr(tokenizer, try_mapper[j] + \"_id\", None)\\n+                        pass\\n+                    pass\\n+                    # Confirm 1 more time!\\n+                    if max(tokenizer.added_tokens_decoder.keys()) < max_embedding_size:\\n+                        logger.warning_once(\\n+                            f\"Unsloth loaded a broken tokenizer `{model_name}`, but managed to repair it!\\\\n\"\\\\\\n+                            f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                            \"We removed these bad tokens. If you think this is incorrect, fix your tokenizer first.\"\\n+                        )\\n+                        return convert_to_fast_tokenizer(tokenizer)\\n+                    pass\\n+                pass\\n+\\n+                # :( Failure\\n+                raise RuntimeError(\\n+                    f\"Unsloth tried to load `{model_name}`, but cannot succeed.\\\\n\"\\\\\\n+                    f\"Tokens {bad_tokens} with ids {bad_indices} exceeds the max vocab size of {max_embedding_size}.\\\\n\"\\\\\\n+                    f\"Fix your tokenizer since it\\'ll perform out of bounds memory accesses.\"\\n+                )\\n+            pass\\n+            \\n+            # Try slow tokenizer which can fix things!\\n+            tokenizer = AutoTokenizer.from_pretrained(\\n+                model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                use_fast = False,\\n+            )\\n+            return check_tokenizer(\\n+                model = model,\\n+                tokenizer = tokenizer,\\n+                model_name = model_name,\\n+                model_max_length = model_max_length,\\n+                padding_side = padding_side,\\n+                token = token,\\n+                _reload = False,\\n+            )\\n+            break\\n+        pass\\n+    pass\\n+    return convert_to_fast_tokenizer(tokenizer)\\n+pass\\n',\n",
       " '@@ -45,10 +45,9 @@ def fast_geglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -83,20 +82,30 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -129,13 +138,8 @@ def GemmaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +82,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -167,12 +168,12 @@ def LlamaAttention_fast_forward_inference(\\n     Kn *= cos; Kn.addcmul_(RH_K, sin);\\n     \\n     # New KV cache\\n-    # Kn = torch.cat([K1, Kn], dim = 2)\\n-    # Vn = torch.cat([V1, Vn], dim = 2)\\n-    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n-    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n-    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n-    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n+    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n+    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n+    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n \\n     # Handle sliding windows\\n     sliding_window = getattr(self.config, \"sliding_window\", None)\\n@@ -200,6 +201,7 @@ def LlamaAttention_fast_forward_inference(\\n     # Attention\\n     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n     A *= self.scalar\\n+    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n@@ -215,10 +217,9 @@ def fast_swiglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -375,19 +376,30 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        #     attention_mask = attention_mask,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -418,13 +430,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +609,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,22 +642,15 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n-\\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n@@ -670,12 +669,29 @@ def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    attention_mask = None,\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+\\n+    # Must use attention mask for batched processing\\n+    sliding_window = getattr(self.config, \"sliding_window\", None)\\n+    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = sliding_window,\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -686,7 +702,9 @@ def LlamaModel_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n             past_key_values[idx],\\n-            None,\\n+            position_ids = None,\\n+            do_prefill = False,\\n+            attention_mask = attention_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -726,11 +744,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n+    if False and past_key_values is not None and \\\\\\n         hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n         outputs = LlamaModel_fast_forward_inference(\\n             self.model,\\n             input_ids,\\n             past_key_values,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -45,10 +45,9 @@ def fast_geglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -83,20 +82,30 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -129,13 +138,8 @@ def GemmaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +82,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -167,12 +168,12 @@ def LlamaAttention_fast_forward_inference(\\n     Kn *= cos; Kn.addcmul_(RH_K, sin);\\n     \\n     # New KV cache\\n-    # Kn = torch.cat([K1, Kn], dim = 2)\\n-    # Vn = torch.cat([V1, Vn], dim = 2)\\n-    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n-    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n-    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n-    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n+    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n+    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n+    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n \\n     # Handle sliding windows\\n     sliding_window = getattr(self.config, \"sliding_window\", None)\\n@@ -200,6 +201,7 @@ def LlamaAttention_fast_forward_inference(\\n     # Attention\\n     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n     A *= self.scalar\\n+    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n@@ -215,10 +217,9 @@ def fast_swiglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -375,19 +376,30 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        #     attention_mask = attention_mask,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -418,13 +430,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +609,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,22 +642,15 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n-\\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n@@ -670,12 +669,29 @@ def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    attention_mask = None,\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+\\n+    # Must use attention mask for batched processing\\n+    sliding_window = getattr(self.config, \"sliding_window\", None)\\n+    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = sliding_window,\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -686,7 +702,9 @@ def LlamaModel_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n             past_key_values[idx],\\n-            None,\\n+            position_ids = None,\\n+            do_prefill = False,\\n+            attention_mask = attention_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -726,11 +744,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n+    if False and past_key_values is not None and \\\\\\n         hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n         outputs = LlamaModel_fast_forward_inference(\\n             self.model,\\n             input_ids,\\n             past_key_values,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -45,10 +45,9 @@ def fast_geglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -83,20 +82,30 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -129,13 +138,8 @@ def GemmaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +82,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -167,12 +168,12 @@ def LlamaAttention_fast_forward_inference(\\n     Kn *= cos; Kn.addcmul_(RH_K, sin);\\n     \\n     # New KV cache\\n-    # Kn = torch.cat([K1, Kn], dim = 2)\\n-    # Vn = torch.cat([V1, Vn], dim = 2)\\n-    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n-    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n-    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n-    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n+    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n+    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n+    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n \\n     # Handle sliding windows\\n     sliding_window = getattr(self.config, \"sliding_window\", None)\\n@@ -200,6 +201,7 @@ def LlamaAttention_fast_forward_inference(\\n     # Attention\\n     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n     A *= self.scalar\\n+    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n@@ -215,10 +217,9 @@ def fast_swiglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -375,19 +376,30 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        #     attention_mask = attention_mask,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -418,13 +430,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +609,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,22 +642,15 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n-\\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n@@ -670,12 +669,29 @@ def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    attention_mask = None,\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+\\n+    # Must use attention mask for batched processing\\n+    sliding_window = getattr(self.config, \"sliding_window\", None)\\n+    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = sliding_window,\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -686,7 +702,9 @@ def LlamaModel_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n             past_key_values[idx],\\n-            None,\\n+            position_ids = None,\\n+            do_prefill = False,\\n+            attention_mask = attention_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -726,11 +744,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n+    if False and past_key_values is not None and \\\\\\n         hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n         outputs = LlamaModel_fast_forward_inference(\\n             self.model,\\n             input_ids,\\n             past_key_values,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -45,10 +45,9 @@ def fast_geglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -83,20 +82,30 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -129,13 +138,8 @@ def GemmaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +82,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -167,12 +168,12 @@ def LlamaAttention_fast_forward_inference(\\n     Kn *= cos; Kn.addcmul_(RH_K, sin);\\n     \\n     # New KV cache\\n-    # Kn = torch.cat([K1, Kn], dim = 2)\\n-    # Vn = torch.cat([V1, Vn], dim = 2)\\n-    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n-    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n-    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n-    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n+    Kn = torch.cat([K1, Kn], dim = 2)\\n+    Vn = torch.cat([V1, Vn], dim = 2)\\n+    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n+    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n+    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n+    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n \\n     # Handle sliding windows\\n     sliding_window = getattr(self.config, \"sliding_window\", None)\\n@@ -200,6 +201,7 @@ def LlamaAttention_fast_forward_inference(\\n     # Attention\\n     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n     A *= self.scalar\\n+    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n@@ -215,10 +217,9 @@ def fast_swiglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)\\n+    up   = fast_linear_forward(self.  up_proj, X)\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -375,19 +376,30 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n+    if use_cache: #past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n+        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+        #     self.self_attn,\\n+        #     hidden_states,\\n+        #     past_key_value,\\n+        #     position_ids,\\n+        #     do_prefill = do_prefill,\\n+        #     attention_mask = attention_mask,\\n+        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -418,13 +430,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +609,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,22 +642,15 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n-\\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n@@ -670,12 +669,29 @@ def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    attention_mask = None,\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+\\n+    # Must use attention mask for batched processing\\n+    sliding_window = getattr(self.config, \"sliding_window\", None)\\n+    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = sliding_window,\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -686,7 +702,9 @@ def LlamaModel_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n             past_key_values[idx],\\n-            None,\\n+            position_ids = None,\\n+            do_prefill = False,\\n+            attention_mask = attention_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -726,11 +744,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n+    if False and past_key_values is not None and \\\\\\n         hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n         outputs = LlamaModel_fast_forward_inference(\\n             self.model,\\n             input_ids,\\n             past_key_values,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -45,9 +45,10 @@ def fast_geglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X)\\n-    up   = fast_linear_forward(self.  up_proj, X)\\n+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -82,30 +83,20 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if use_cache: #past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-            hidden_states=hidden_states,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_value=past_key_value,\\n-            output_attentions=output_attentions,\\n-            use_cache=use_cache,\\n-            padding_mask=padding_mask,\\n+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+            self.self_attn,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+            do_prefill = do_prefill,\\n         )\\n-        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-        #     self.self_attn,\\n-        #     hidden_states,\\n-        #     past_key_value,\\n-        #     position_ids,\\n-        #     do_prefill = do_prefill,\\n-        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -138,8 +129,13 @@ def GemmaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-    if output_attentions: outputs += (self_attn_weights,)\\n-    if use_cache: outputs += (present_key_value,)\\n+\\n+    if output_attentions:\\n+        outputs += (self_attn_weights,)\\n+\\n+    if use_cache:\\n+        outputs += (present_key_value,)\\n+\\n     return outputs\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 256 # KV Cache update size\\n+KV_CACHE_INCREMENT = 128 # KV Cache update size\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,7 +82,6 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n-    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -168,12 +167,12 @@ def LlamaAttention_fast_forward_inference(\\n     Kn *= cos; Kn.addcmul_(RH_K, sin);\\n     \\n     # New KV cache\\n-    Kn = torch.cat([K1, Kn], dim = 2)\\n-    Vn = torch.cat([V1, Vn], dim = 2)\\n-    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n-    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n-    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n-    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n+    # Kn = torch.cat([K1, Kn], dim = 2)\\n+    # Vn = torch.cat([V1, Vn], dim = 2)\\n+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n \\n     # Handle sliding windows\\n     sliding_window = getattr(self.config, \"sliding_window\", None)\\n@@ -201,7 +200,6 @@ def LlamaAttention_fast_forward_inference(\\n     # Attention\\n     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n     A *= self.scalar\\n-    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n@@ -217,9 +215,10 @@ def fast_swiglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X)\\n-    up   = fast_linear_forward(self.  up_proj, X)\\n+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -376,30 +375,19 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if use_cache: #past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-            hidden_states=hidden_states,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_value=past_key_value,\\n-            output_attentions=output_attentions,\\n-            use_cache=use_cache,\\n-            padding_mask=padding_mask,\\n+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+            self.self_attn,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+            do_prefill = do_prefill,\\n         )\\n-        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-        #     self.self_attn,\\n-        #     hidden_states,\\n-        #     past_key_value,\\n-        #     position_ids,\\n-        #     do_prefill = do_prefill,\\n-        #     attention_mask = attention_mask,\\n-        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -430,8 +418,13 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-    if output_attentions: outputs += (self_attn_weights,)\\n-    if use_cache: outputs += (present_key_value,)\\n+\\n+    if output_attentions:\\n+        outputs += (self_attn_weights,)\\n+\\n+    if use_cache:\\n+        outputs += (present_key_value,)\\n+\\n     return outputs\\n pass\\n \\n@@ -609,8 +602,9 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n+        if output_hidden_states:\\n+            all_hidden_states += (hidden_states,)\\n \\n-        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -642,15 +636,22 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n-        pass\\n \\n         hidden_states = layer_outputs[0]\\n-        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-        if output_attentions: all_self_attns += (layer_outputs[1],)\\n+\\n+        if use_cache:\\n+            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+\\n+        if output_attentions:\\n+            all_self_attns += (layer_outputs[1],)\\n     pass\\n+    \\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    if output_hidden_states: all_hidden_states += (hidden_states,)\\n+    # add hidden states from the last decoder layer\\n+    if output_hidden_states:\\n+        all_hidden_states += (hidden_states,)\\n+\\n     next_cache = next_decoder_cache if use_cache else None\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n@@ -669,29 +670,12 @@ def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n-    attention_mask = None,\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n-    bsz, q_len, hd = hidden_states.shape\\n-    seq_len = past_key_values[0][0].shape[-2]\\n-\\n-    # Must use attention mask for batched processing\\n-    sliding_window = getattr(self.config, \"sliding_window\", None)\\n-    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):\\n-        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n-            attention_mask,\\n-            (bsz, q_len),\\n-            hidden_states,\\n-            seq_len,\\n-            sliding_window = sliding_window,\\n-        )\\n-    else:\\n-        attention_mask = None\\n-    pass\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -702,9 +686,7 @@ def LlamaModel_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n             past_key_values[idx],\\n-            position_ids = None,\\n-            do_prefill = False,\\n-            attention_mask = attention_mask,\\n+            None,\\n         )\\n         hidden_states += residual\\n \\n@@ -744,12 +726,11 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n-                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,13 +200,12 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if False and past_key_values is not None and \\\\\\n+    if past_key_values is not None and \\\\\\n         hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n         outputs = LlamaModel_fast_forward_inference(\\n             self.model,\\n             input_ids,\\n             past_key_values,\\n-            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -45,9 +45,10 @@ def fast_geglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X)\\n-    up   = fast_linear_forward(self.  up_proj, X)\\n+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -82,30 +83,20 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if use_cache: #past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-            hidden_states=hidden_states,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_value=past_key_value,\\n-            output_attentions=output_attentions,\\n-            use_cache=use_cache,\\n-            padding_mask=padding_mask,\\n+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+            self.self_attn,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+            do_prefill = do_prefill,\\n         )\\n-        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-        #     self.self_attn,\\n-        #     hidden_states,\\n-        #     past_key_value,\\n-        #     position_ids,\\n-        #     do_prefill = do_prefill,\\n-        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -138,8 +129,13 @@ def GemmaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-    if output_attentions: outputs += (self_attn_weights,)\\n-    if use_cache: outputs += (present_key_value,)\\n+\\n+    if output_attentions:\\n+        outputs += (self_attn_weights,)\\n+\\n+    if use_cache:\\n+        outputs += (present_key_value,)\\n+\\n     return outputs\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,7 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 256 # KV Cache update size\\n+KV_CACHE_INCREMENT = 128 # KV Cache update size\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,7 +82,6 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n-    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -168,12 +167,12 @@ def LlamaAttention_fast_forward_inference(\\n     Kn *= cos; Kn.addcmul_(RH_K, sin);\\n     \\n     # New KV cache\\n-    Kn = torch.cat([K1, Kn], dim = 2)\\n-    Vn = torch.cat([V1, Vn], dim = 2)\\n-    # self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n-    # self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n-    # Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n-    # Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n+    # Kn = torch.cat([K1, Kn], dim = 2)\\n+    # Vn = torch.cat([V1, Vn], dim = 2)\\n+    self.paged_attention_K[seq_len] = Kn.permute(2, 0, 1, 3)\\n+    self.paged_attention_V[seq_len] = Vn.permute(2, 0, 1, 3)\\n+    Kn = self.paged_attention_K[:kv_seq_len].permute(1, 2, 0, 3)\\n+    Vn = self.paged_attention_V[:kv_seq_len].permute(1, 2, 0, 3)\\n \\n     # Handle sliding windows\\n     sliding_window = getattr(self.config, \"sliding_window\", None)\\n@@ -201,7 +200,6 @@ def LlamaAttention_fast_forward_inference(\\n     # Attention\\n     A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n     A *= self.scalar\\n-    if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n     A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n     A = torch.matmul(A, Vnn, out = Qn)\\n     A = A.transpose(1, 2)\\n@@ -217,9 +215,10 @@ def fast_swiglu_inference(self, X):\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n     mlp_size = self.config.intermediate_size\\n+    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X)\\n-    up   = fast_linear_forward(self.  up_proj, X)\\n+    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -376,30 +375,19 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if use_cache: #past_key_value is not None:\\n+    if past_key_value is not None:\\n         do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n-            hidden_states=hidden_states,\\n-            causal_mask=causal_mask,\\n-            attention_mask=attention_mask,\\n-            position_ids=position_ids,\\n-            past_key_value=past_key_value,\\n-            output_attentions=output_attentions,\\n-            use_cache=use_cache,\\n-            padding_mask=padding_mask,\\n+        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n+            self.self_attn,\\n+            hidden_states,\\n+            past_key_value,\\n+            position_ids,\\n+            do_prefill = do_prefill,\\n         )\\n-        # hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-        #     self.self_attn,\\n-        #     hidden_states,\\n-        #     past_key_value,\\n-        #     position_ids,\\n-        #     do_prefill = do_prefill,\\n-        #     attention_mask = attention_mask,\\n-        # )\\n         hidden_states += residual\\n \\n         # Fully Connected\\n@@ -430,8 +418,13 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-    if output_attentions: outputs += (self_attn_weights,)\\n-    if use_cache: outputs += (present_key_value,)\\n+\\n+    if output_attentions:\\n+        outputs += (self_attn_weights,)\\n+\\n+    if use_cache:\\n+        outputs += (present_key_value,)\\n+\\n     return outputs\\n pass\\n \\n@@ -609,8 +602,9 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n+        if output_hidden_states:\\n+            all_hidden_states += (hidden_states,)\\n \\n-        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -642,15 +636,22 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n-        pass\\n \\n         hidden_states = layer_outputs[0]\\n-        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-        if output_attentions: all_self_attns += (layer_outputs[1],)\\n+\\n+        if use_cache:\\n+            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+\\n+        if output_attentions:\\n+            all_self_attns += (layer_outputs[1],)\\n     pass\\n+    \\n     hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    if output_hidden_states: all_hidden_states += (hidden_states,)\\n+    # add hidden states from the last decoder layer\\n+    if output_hidden_states:\\n+        all_hidden_states += (hidden_states,)\\n+\\n     next_cache = next_decoder_cache if use_cache else None\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n@@ -669,29 +670,12 @@ def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n-    attention_mask = None,\\n ):\\n     # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n \\n     hidden_states = self.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n-    bsz, q_len, hd = hidden_states.shape\\n-    seq_len = past_key_values[0][0].shape[-2]\\n-\\n-    # Must use attention mask for batched processing\\n-    sliding_window = getattr(self.config, \"sliding_window\", None)\\n-    if (sliding_window is not None and seq_len >= sliding_window) or (bsz != 1):\\n-        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n-            attention_mask,\\n-            (bsz, q_len),\\n-            hidden_states,\\n-            seq_len,\\n-            sliding_window = sliding_window,\\n-        )\\n-    else:\\n-        attention_mask = None\\n-    pass\\n \\n     next_decoder_cache = []\\n     for idx, decoder_layer in enumerate(self.layers):\\n@@ -702,9 +686,7 @@ def LlamaModel_fast_forward_inference(\\n             decoder_layer.self_attn,\\n             hidden_states,\\n             past_key_values[idx],\\n-            position_ids = None,\\n-            do_prefill = False,\\n-            attention_mask = attention_mask,\\n+            None,\\n         )\\n         hidden_states += residual\\n \\n@@ -744,12 +726,11 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if False:#past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n             outputs = fast_forward_inference(\\n                 self.model,\\n                 input_ids,\\n                 past_key_values,\\n-                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,13 +200,12 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if False and past_key_values is not None and \\\\\\n+    if past_key_values is not None and \\\\\\n         hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n         outputs = LlamaModel_fast_forward_inference(\\n             self.model,\\n             input_ids,\\n             past_key_values,\\n-            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -118,7 +118,7 @@ def fast_gemv(X, W, quant_state, out = None):\\n     if quant_state is None: return torch.matmul(X, W, out = out)\\n     # For fast X @ W where seq_len == 1\\n     # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n-    bsz, q_len, hd = X.shape\\n+    _, q_len, hd = X.shape\\n     # assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n@@ -142,10 +142,10 @@ def fast_gemv(X, W, quant_state, out = None):\\n     bout = shape[0]\\n \\n     if out is None:\\n-        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n-    else:\\n-        assert(out.shape == (bsz, 1, bout,))\\n-    pass\\n+        out = torch.empty((1, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    # else:\\n+    #     assert(out.shape == (1, 1, bout,))\\n+    # pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -171,15 +171,9 @@ def fast_gemv(X, W, quant_state, out = None):\\n     fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \\\\\\n         cgemm_4bit_inference_naive_bf16\\n \\n-    ptr_W      = get_ptr(W)\\n-    ptr_absmax = get_ptr(absmax)\\n-    ptr_stats  = get_ptr(stats)\\n-    blocksize  = ctypes.c_int32(blocksize)\\n-\\n-    for row in range(bsz):\\n-        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n-           lda, ldb, ldc, blocksize)\\n-    pass\\n+    blocksize = ctypes.c_int32(blocksize)\\n+    fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),\\n+       lda, ldb, ldc, blocksize)\\n \\n     return out\\n pass\\n@@ -189,12 +183,11 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n \\n-    bsz, _, in_dim = X.shape\\n+    bsz, q_len, in_dim = X.shape\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n-    elif bsz <= 2:\\n-        # Only batches of 2 are faster with Gemv\\n+    elif bsz == 1 and q_len == 1:\\n         out = fast_gemv(X, W, W_quant, out = out)\\n     else:\\n         W = fast_dequantize(W.t(), W_quant)\\n',\n",
       " '@@ -44,11 +44,11 @@ def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -58,18 +58,6 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n-def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n-    XX = X.to(torch.float32)\\n-    variance = XX.square().mean(-1, keepdim = True)\\n-    variance += self.variance_epsilon\\n-    XX *= variance.rsqrt_()\\n-    out_weight[:] = self.weight\\n-    out_weight += 1.0\\n-    XX *= out_weight\\n-    return XX.to(X.dtype)\\n-pass\\n-\\n-\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -83,19 +71,21 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+    if use_cache: #past_key_value is not None:\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -107,7 +97,6 @@ def GemmaDecoderLayer_fast_forward(\\n     else:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n             causal_mask=causal_mask,\\n@@ -123,19 +112,13 @@ def GemmaDecoderLayer_fast_forward(\\n         # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -148,31 +131,37 @@ def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n-    input_ids = input_ids[:,:self.max_seq_length]\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    input_ids = input_ids[:,:self.max_seq_length]\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)\\n+    pass\\n+\\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -180,13 +169,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,8 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n+torch_nn_functional_softmax = torch.nn.functional.softmax\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +83,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -138,6 +140,7 @@ def LlamaAttention_fast_forward_inference(\\n         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = \"cuda\")\\n         self.scalar = 1.0 / math_sqrt(self.head_dim)\\n+        self.half_head_dim = head_dim // 2\\n     elif kv_seq_len >= self.paged_attention.shape[0]:\\n         self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))\\n         self.paged_attention_K = self.paged_attention[:,0]\\n@@ -154,17 +157,23 @@ def LlamaAttention_fast_forward_inference(\\n \\n     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n-    cos = self.rotary_emb.cos_cached[seq_len]\\n-    sin = self.rotary_emb.sin_cached[seq_len]\\n-    h = head_dim // 2\\n+    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)\\n+    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)\\n+    h = self.half_head_dim\\n \\n     RH_Q = self.RH_Q\\n-    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]; RH_Q[:,:,:,h:] = Qn[:,:,:,:h]; torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h]);\\n-    Qn *= cos; Qn.addcmul_(RH_Q, sin);\\n+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]\\n+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]\\n+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])\\n+    Qn *= cos\\n+    Qn.addcmul_(RH_Q, sin)\\n \\n     RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n-    RH_K[:,:,:,:h] = Kn[:,:,:,h:]; RH_K[:,:,:,h:] = Kn[:,:,:,:h]; torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h]);\\n-    Kn *= cos; Kn.addcmul_(RH_K, sin);\\n+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]\\n+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]\\n+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])\\n+    Kn *= cos\\n+    Kn.addcmul_(RH_K, sin)\\n     \\n     # New KV cache\\n     # Kn = torch.cat([K1, Kn], dim = 2)\\n@@ -198,10 +207,15 @@ def LlamaAttention_fast_forward_inference(\\n     # pass\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n-    A *= self.scalar\\n-    A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n-    A = torch.matmul(A, Vnn, out = Qn)\\n+    if bsz == 1:\\n+        A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n+        A *= self.scalar\\n+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n+        A = torch.matmul(A, Vnn, out = Qn)\\n+    else:\\n+        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)\\n+    pass\\n     A = A.transpose(1, 2)\\n     A = A.reshape(bsz, 1, attention_size)\\n     A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])\\n@@ -214,11 +228,11 @@ def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -240,6 +254,24 @@ def fast_rms_layernorm_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight = None):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+\\n+    if out_weight is None:\\n+        out_weight = self.weight + 1.0\\n+    else:\\n+        out_weight[:] = self.weight\\n+        out_weight += 1.0\\n+    pass\\n+\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -375,18 +407,18 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n-\\n-        # Self Attention\\n+    if use_cache:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -418,13 +450,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +629,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,23 +662,24 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n+    # Final layernorm\\n+    if use_cache:\\n+        hidden_states = (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\\\\\\n+            (self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n+    pass\\n \\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n+\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n     return BaseModelOutputWithPast(\\n@@ -665,32 +692,44 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -698,13 +737,13 @@ def LlamaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n@@ -726,11 +765,13 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None:\\n             outputs = fast_forward_inference(\\n-                self.model,\\n+                self,\\n                 input_ids,\\n                 past_key_values,\\n+                position_ids = position_ids,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+    if past_key_values is not None:\\n         outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n+            self,\\n             input_ids,\\n             past_key_values,\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -118,7 +118,7 @@ def fast_gemv(X, W, quant_state, out = None):\\n     if quant_state is None: return torch.matmul(X, W, out = out)\\n     # For fast X @ W where seq_len == 1\\n     # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n-    bsz, q_len, hd = X.shape\\n+    _, q_len, hd = X.shape\\n     # assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n@@ -142,10 +142,10 @@ def fast_gemv(X, W, quant_state, out = None):\\n     bout = shape[0]\\n \\n     if out is None:\\n-        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n-    else:\\n-        assert(out.shape == (bsz, 1, bout,))\\n-    pass\\n+        out = torch.empty((1, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    # else:\\n+    #     assert(out.shape == (1, 1, bout,))\\n+    # pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -171,15 +171,9 @@ def fast_gemv(X, W, quant_state, out = None):\\n     fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \\\\\\n         cgemm_4bit_inference_naive_bf16\\n \\n-    ptr_W      = get_ptr(W)\\n-    ptr_absmax = get_ptr(absmax)\\n-    ptr_stats  = get_ptr(stats)\\n-    blocksize  = ctypes.c_int32(blocksize)\\n-\\n-    for row in range(bsz):\\n-        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n-           lda, ldb, ldc, blocksize)\\n-    pass\\n+    blocksize = ctypes.c_int32(blocksize)\\n+    fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),\\n+       lda, ldb, ldc, blocksize)\\n \\n     return out\\n pass\\n@@ -189,12 +183,11 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n \\n-    bsz, _, in_dim = X.shape\\n+    bsz, q_len, in_dim = X.shape\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n-    elif bsz <= 2:\\n-        # Only batches of 2 are faster with Gemv\\n+    elif bsz == 1 and q_len == 1:\\n         out = fast_gemv(X, W, W_quant, out = out)\\n     else:\\n         W = fast_dequantize(W.t(), W_quant)\\n',\n",
       " '@@ -44,11 +44,11 @@ def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -58,18 +58,6 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n-def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n-    XX = X.to(torch.float32)\\n-    variance = XX.square().mean(-1, keepdim = True)\\n-    variance += self.variance_epsilon\\n-    XX *= variance.rsqrt_()\\n-    out_weight[:] = self.weight\\n-    out_weight += 1.0\\n-    XX *= out_weight\\n-    return XX.to(X.dtype)\\n-pass\\n-\\n-\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -83,19 +71,21 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+    if use_cache: #past_key_value is not None:\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -107,7 +97,6 @@ def GemmaDecoderLayer_fast_forward(\\n     else:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n             causal_mask=causal_mask,\\n@@ -123,19 +112,13 @@ def GemmaDecoderLayer_fast_forward(\\n         # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -148,31 +131,37 @@ def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n-    input_ids = input_ids[:,:self.max_seq_length]\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    input_ids = input_ids[:,:self.max_seq_length]\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)\\n+    pass\\n+\\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -180,13 +169,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,8 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n+torch_nn_functional_softmax = torch.nn.functional.softmax\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +83,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -138,6 +140,7 @@ def LlamaAttention_fast_forward_inference(\\n         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = \"cuda\")\\n         self.scalar = 1.0 / math_sqrt(self.head_dim)\\n+        self.half_head_dim = head_dim // 2\\n     elif kv_seq_len >= self.paged_attention.shape[0]:\\n         self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))\\n         self.paged_attention_K = self.paged_attention[:,0]\\n@@ -154,17 +157,23 @@ def LlamaAttention_fast_forward_inference(\\n \\n     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n-    cos = self.rotary_emb.cos_cached[seq_len]\\n-    sin = self.rotary_emb.sin_cached[seq_len]\\n-    h = head_dim // 2\\n+    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)\\n+    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)\\n+    h = self.half_head_dim\\n \\n     RH_Q = self.RH_Q\\n-    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]; RH_Q[:,:,:,h:] = Qn[:,:,:,:h]; torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h]);\\n-    Qn *= cos; Qn.addcmul_(RH_Q, sin);\\n+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]\\n+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]\\n+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])\\n+    Qn *= cos\\n+    Qn.addcmul_(RH_Q, sin)\\n \\n     RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n-    RH_K[:,:,:,:h] = Kn[:,:,:,h:]; RH_K[:,:,:,h:] = Kn[:,:,:,:h]; torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h]);\\n-    Kn *= cos; Kn.addcmul_(RH_K, sin);\\n+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]\\n+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]\\n+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])\\n+    Kn *= cos\\n+    Kn.addcmul_(RH_K, sin)\\n     \\n     # New KV cache\\n     # Kn = torch.cat([K1, Kn], dim = 2)\\n@@ -198,10 +207,15 @@ def LlamaAttention_fast_forward_inference(\\n     # pass\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n-    A *= self.scalar\\n-    A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n-    A = torch.matmul(A, Vnn, out = Qn)\\n+    if bsz == 1:\\n+        A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n+        A *= self.scalar\\n+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n+        A = torch.matmul(A, Vnn, out = Qn)\\n+    else:\\n+        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)\\n+    pass\\n     A = A.transpose(1, 2)\\n     A = A.reshape(bsz, 1, attention_size)\\n     A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])\\n@@ -214,11 +228,11 @@ def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -240,6 +254,24 @@ def fast_rms_layernorm_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight = None):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+\\n+    if out_weight is None:\\n+        out_weight = self.weight + 1.0\\n+    else:\\n+        out_weight[:] = self.weight\\n+        out_weight += 1.0\\n+    pass\\n+\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -375,18 +407,18 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n-\\n-        # Self Attention\\n+    if use_cache:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -418,13 +450,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +629,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,23 +662,24 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n+    # Final layernorm\\n+    if use_cache:\\n+        hidden_states = (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\\\\\\n+            (self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n+    pass\\n \\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n+\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n     return BaseModelOutputWithPast(\\n@@ -665,32 +692,44 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -698,13 +737,13 @@ def LlamaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n@@ -726,11 +765,13 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None:\\n             outputs = fast_forward_inference(\\n-                self.model,\\n+                self,\\n                 input_ids,\\n                 past_key_values,\\n+                position_ids = position_ids,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+    if past_key_values is not None:\\n         outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n+            self,\\n             input_ids,\\n             past_key_values,\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -118,7 +118,7 @@ def fast_gemv(X, W, quant_state, out = None):\\n     if quant_state is None: return torch.matmul(X, W, out = out)\\n     # For fast X @ W where seq_len == 1\\n     # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n-    bsz, q_len, hd = X.shape\\n+    _, q_len, hd = X.shape\\n     # assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n@@ -142,10 +142,10 @@ def fast_gemv(X, W, quant_state, out = None):\\n     bout = shape[0]\\n \\n     if out is None:\\n-        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n-    else:\\n-        assert(out.shape == (bsz, 1, bout,))\\n-    pass\\n+        out = torch.empty((1, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    # else:\\n+    #     assert(out.shape == (1, 1, bout,))\\n+    # pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -171,15 +171,9 @@ def fast_gemv(X, W, quant_state, out = None):\\n     fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \\\\\\n         cgemm_4bit_inference_naive_bf16\\n \\n-    ptr_W      = get_ptr(W)\\n-    ptr_absmax = get_ptr(absmax)\\n-    ptr_stats  = get_ptr(stats)\\n-    blocksize  = ctypes.c_int32(blocksize)\\n-\\n-    for row in range(bsz):\\n-        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n-           lda, ldb, ldc, blocksize)\\n-    pass\\n+    blocksize = ctypes.c_int32(blocksize)\\n+    fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),\\n+       lda, ldb, ldc, blocksize)\\n \\n     return out\\n pass\\n@@ -189,12 +183,11 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n \\n-    bsz, _, in_dim = X.shape\\n+    bsz, q_len, in_dim = X.shape\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n-    elif bsz <= 2:\\n-        # Only batches of 2 are faster with Gemv\\n+    elif bsz == 1 and q_len == 1:\\n         out = fast_gemv(X, W, W_quant, out = out)\\n     else:\\n         W = fast_dequantize(W.t(), W_quant)\\n',\n",
       " '@@ -44,11 +44,11 @@ def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -58,18 +58,6 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n-def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n-    XX = X.to(torch.float32)\\n-    variance = XX.square().mean(-1, keepdim = True)\\n-    variance += self.variance_epsilon\\n-    XX *= variance.rsqrt_()\\n-    out_weight[:] = self.weight\\n-    out_weight += 1.0\\n-    XX *= out_weight\\n-    return XX.to(X.dtype)\\n-pass\\n-\\n-\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -83,19 +71,21 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+    if use_cache: #past_key_value is not None:\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -107,7 +97,6 @@ def GemmaDecoderLayer_fast_forward(\\n     else:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n             causal_mask=causal_mask,\\n@@ -123,19 +112,13 @@ def GemmaDecoderLayer_fast_forward(\\n         # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -148,31 +131,37 @@ def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n-    input_ids = input_ids[:,:self.max_seq_length]\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    input_ids = input_ids[:,:self.max_seq_length]\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)\\n+    pass\\n+\\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -180,13 +169,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,8 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n+torch_nn_functional_softmax = torch.nn.functional.softmax\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +83,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -138,6 +140,7 @@ def LlamaAttention_fast_forward_inference(\\n         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = \"cuda\")\\n         self.scalar = 1.0 / math_sqrt(self.head_dim)\\n+        self.half_head_dim = head_dim // 2\\n     elif kv_seq_len >= self.paged_attention.shape[0]:\\n         self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))\\n         self.paged_attention_K = self.paged_attention[:,0]\\n@@ -154,17 +157,23 @@ def LlamaAttention_fast_forward_inference(\\n \\n     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n-    cos = self.rotary_emb.cos_cached[seq_len]\\n-    sin = self.rotary_emb.sin_cached[seq_len]\\n-    h = head_dim // 2\\n+    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)\\n+    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)\\n+    h = self.half_head_dim\\n \\n     RH_Q = self.RH_Q\\n-    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]; RH_Q[:,:,:,h:] = Qn[:,:,:,:h]; torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h]);\\n-    Qn *= cos; Qn.addcmul_(RH_Q, sin);\\n+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]\\n+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]\\n+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])\\n+    Qn *= cos\\n+    Qn.addcmul_(RH_Q, sin)\\n \\n     RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n-    RH_K[:,:,:,:h] = Kn[:,:,:,h:]; RH_K[:,:,:,h:] = Kn[:,:,:,:h]; torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h]);\\n-    Kn *= cos; Kn.addcmul_(RH_K, sin);\\n+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]\\n+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]\\n+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])\\n+    Kn *= cos\\n+    Kn.addcmul_(RH_K, sin)\\n     \\n     # New KV cache\\n     # Kn = torch.cat([K1, Kn], dim = 2)\\n@@ -198,10 +207,15 @@ def LlamaAttention_fast_forward_inference(\\n     # pass\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n-    A *= self.scalar\\n-    A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n-    A = torch.matmul(A, Vnn, out = Qn)\\n+    if bsz == 1:\\n+        A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n+        A *= self.scalar\\n+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n+        A = torch.matmul(A, Vnn, out = Qn)\\n+    else:\\n+        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)\\n+    pass\\n     A = A.transpose(1, 2)\\n     A = A.reshape(bsz, 1, attention_size)\\n     A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])\\n@@ -214,11 +228,11 @@ def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -240,6 +254,24 @@ def fast_rms_layernorm_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight = None):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+\\n+    if out_weight is None:\\n+        out_weight = self.weight + 1.0\\n+    else:\\n+        out_weight[:] = self.weight\\n+        out_weight += 1.0\\n+    pass\\n+\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -375,18 +407,18 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n-\\n-        # Self Attention\\n+    if use_cache:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -418,13 +450,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +629,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,23 +662,24 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n+    # Final layernorm\\n+    if use_cache:\\n+        hidden_states = (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\\\\\\n+            (self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n+    pass\\n \\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n+\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n     return BaseModelOutputWithPast(\\n@@ -665,32 +692,44 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -698,13 +737,13 @@ def LlamaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n@@ -726,11 +765,13 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None:\\n             outputs = fast_forward_inference(\\n-                self.model,\\n+                self,\\n                 input_ids,\\n                 past_key_values,\\n+                position_ids = position_ids,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+    if past_key_values is not None:\\n         outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n+            self,\\n             input_ids,\\n             past_key_values,\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -118,7 +118,7 @@ def fast_gemv(X, W, quant_state, out = None):\\n     if quant_state is None: return torch.matmul(X, W, out = out)\\n     # For fast X @ W where seq_len == 1\\n     # From https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/functional.py#L1469\\n-    bsz, q_len, hd = X.shape\\n+    _, q_len, hd = X.shape\\n     # assert(q_len == 1)\\n \\n     if type(quant_state) is not list:\\n@@ -142,10 +142,10 @@ def fast_gemv(X, W, quant_state, out = None):\\n     bout = shape[0]\\n \\n     if out is None:\\n-        out = torch.empty((bsz, 1, bout,), dtype = dtype, device = \"cuda\")\\n-    else:\\n-        assert(out.shape == (bsz, 1, bout,))\\n-    pass\\n+        out = torch.empty((1, 1, bout,), dtype = dtype, device = \"cuda\")\\n+    # else:\\n+    #     assert(out.shape == (1, 1, bout,))\\n+    # pass\\n \\n     n = 1\\n     m = shape[0]\\n@@ -171,15 +171,9 @@ def fast_gemv(X, W, quant_state, out = None):\\n     fx = cgemm_4bit_inference_naive_fp16 if dtype == torch.float16 else \\\\\\n         cgemm_4bit_inference_naive_bf16\\n \\n-    ptr_W      = get_ptr(W)\\n-    ptr_absmax = get_ptr(absmax)\\n-    ptr_stats  = get_ptr(stats)\\n-    blocksize  = ctypes.c_int32(blocksize)\\n-\\n-    for row in range(bsz):\\n-        fx(m, n, k, get_ptr(X[row]), ptr_W, ptr_absmax, ptr_stats, get_ptr(out[row]),\\n-           lda, ldb, ldc, blocksize)\\n-    pass\\n+    blocksize = ctypes.c_int32(blocksize)\\n+    fx(m, n, k, get_ptr(X), get_ptr(W), get_ptr(absmax), get_ptr(stats), get_ptr(out),\\n+       lda, ldb, ldc, blocksize)\\n \\n     return out\\n pass\\n@@ -189,12 +183,11 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n \\n-    bsz, _, in_dim = X.shape\\n+    bsz, q_len, in_dim = X.shape\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n-    elif bsz <= 2:\\n-        # Only batches of 2 are faster with Gemv\\n+    elif bsz == 1 and q_len == 1:\\n         out = fast_gemv(X, W, W_quant, out = out)\\n     else:\\n         W = fast_dequantize(W.t(), W_quant)\\n',\n",
       " '@@ -44,11 +44,11 @@ def fast_geglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_gelu(gate, approximate = \"tanh\")\\n     gate *= up\\n \\n@@ -58,18 +58,6 @@ def fast_geglu_inference(self, X):\\n pass\\n \\n \\n-def fast_rms_layernorm_inference_gemma(self, X, out_weight):\\n-    XX = X.to(torch.float32)\\n-    variance = XX.square().mean(-1, keepdim = True)\\n-    variance += self.variance_epsilon\\n-    XX *= variance.rsqrt_()\\n-    out_weight[:] = self.weight\\n-    out_weight += 1.0\\n-    XX *= out_weight\\n-    return XX.to(X.dtype)\\n-pass\\n-\\n-\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L590\\n def GemmaDecoderLayer_fast_forward(\\n     self,\\n@@ -83,19 +71,21 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n+    if use_cache: #past_key_value is not None:\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(self.input_layernorm, hidden_states, out_weight)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -107,7 +97,6 @@ def GemmaDecoderLayer_fast_forward(\\n     else:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.input_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.input_layernorm(hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n             hidden_states=hidden_states,\\n             causal_mask=causal_mask,\\n@@ -123,19 +112,13 @@ def GemmaDecoderLayer_fast_forward(\\n         # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm(self.post_attention_layernorm, hidden_states, gemma = True)\\n-        # hidden_states = self.post_attention_layernorm(hidden_states)\\n         hidden_states = self.mlp(hidden_states)\\n         hidden_states = residual + hidden_states\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -148,31 +131,37 @@ def GemmaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n-    input_ids = input_ids[:,:self.max_seq_length]\\n     out_weight = torch.empty_like(self.layers[0].input_layernorm.weight, dtype = torch.float32, device = \"cuda\")\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    input_ids = input_ids[:,:self.max_seq_length]\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n     # 3072**0.5 = 55.5000 in bfloat16, whilst 55.4256 in float32\\n     # 2048**0.5 = 45.2500 in bfloat16, whilst 45.2548 in float32\\n     hidden_states *= torch.tensor(math_sqrt(self.config.hidden_size), dtype = hidden_states.dtype)\\n \\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)\\n+    pass\\n+\\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.input_layernorm, hidden_states, out_weight)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference_gemma(decoder_layer.post_attention_layernorm, hidden_states, out_weight)\\n         hidden_states = fast_geglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -180,13 +169,13 @@ def GemmaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference_gemma(self.norm, hidden_states, out_weight)\\n+    hidden_states = fast_rms_layernorm_inference_gemma(self.model.norm, hidden_states, out_weight)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n',\n",
       " '@@ -74,7 +74,8 @@ pass\\n \\n \\n from math import sqrt as math_sqrt\\n-KV_CACHE_INCREMENT = 128 # KV Cache update size\\n+KV_CACHE_INCREMENT = 256 # KV Cache update size\\n+torch_nn_functional_softmax = torch.nn.functional.softmax\\n \\n def LlamaAttention_fast_forward_inference(\\n     self,\\n@@ -82,6 +83,7 @@ def LlamaAttention_fast_forward_inference(\\n     past_key_value: Optional[Tuple[torch.Tensor]],\\n     position_ids,\\n     do_prefill = False,\\n+    attention_mask = None,\\n ):\\n     \"\"\"\\n         https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L406\\n@@ -138,6 +140,7 @@ def LlamaAttention_fast_forward_inference(\\n         self.RH_Q = torch.empty((bsz, n_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n         self.attention = torch.empty((bsz, n_heads, 1, KV_CACHE_INCREMENT+seq_len), dtype = dtype, device = \"cuda\")\\n         self.scalar = 1.0 / math_sqrt(self.head_dim)\\n+        self.half_head_dim = head_dim // 2\\n     elif kv_seq_len >= self.paged_attention.shape[0]:\\n         self.paged_attention.resize_((self.paged_attention.shape[0]+KV_CACHE_INCREMENT, 2, bsz, n_kv_heads, head_dim))\\n         self.paged_attention_K = self.paged_attention[:,0]\\n@@ -154,17 +157,23 @@ def LlamaAttention_fast_forward_inference(\\n \\n     # cos, sin = self.rotary_emb(Vn, seq_len = kv_seq_len)\\n     # Qn, Kn = inplace_rope_embedding(Qn, Kn, cos, sin, position_ids)\\n-    cos = self.rotary_emb.cos_cached[seq_len]\\n-    sin = self.rotary_emb.sin_cached[seq_len]\\n-    h = head_dim // 2\\n+    cos = self.rotary_emb.cos_cached[position_ids].unsqueeze(1)\\n+    sin = self.rotary_emb.sin_cached[position_ids].unsqueeze(1)\\n+    h = self.half_head_dim\\n \\n     RH_Q = self.RH_Q\\n-    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]; RH_Q[:,:,:,h:] = Qn[:,:,:,:h]; torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h]);\\n-    Qn *= cos; Qn.addcmul_(RH_Q, sin);\\n+    RH_Q[:,:,:,:h] = Qn[:,:,:,h:]\\n+    RH_Q[:,:,:,h:] = Qn[:,:,:,:h]\\n+    torch.neg(RH_Q[:,:,:,:h], out = RH_Q[:,:,:,:h])\\n+    Qn *= cos\\n+    Qn.addcmul_(RH_Q, sin)\\n \\n     RH_K = RH_Q[:,:n_kv_heads,:,:] # torch.empty((n_kv_heads, 1, head_dim), dtype = dtype, device = \"cuda\")\\n-    RH_K[:,:,:,:h] = Kn[:,:,:,h:]; RH_K[:,:,:,h:] = Kn[:,:,:,:h]; torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h]);\\n-    Kn *= cos; Kn.addcmul_(RH_K, sin);\\n+    RH_K[:,:,:,:h] = Kn[:,:,:,h:]\\n+    RH_K[:,:,:,h:] = Kn[:,:,:,:h]\\n+    torch.neg(RH_K[:,:,:,:h], out = RH_K[:,:,:,:h])\\n+    Kn *= cos\\n+    Kn.addcmul_(RH_K, sin)\\n     \\n     # New KV cache\\n     # Kn = torch.cat([K1, Kn], dim = 2)\\n@@ -198,10 +207,15 @@ def LlamaAttention_fast_forward_inference(\\n     # pass\\n \\n     # Attention\\n-    A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n-    A *= self.scalar\\n-    A[:] = torch.nn.functional.softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n-    A = torch.matmul(A, Vnn, out = Qn)\\n+    if bsz == 1:\\n+        A = torch.matmul(Qn, Knn.transpose(2, 3), out = self.attention[:,:,:,:cached_len])\\n+        A *= self.scalar\\n+        # if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\\n+        A[:] = torch_nn_functional_softmax(A, dim = -1, dtype = torch.float32)#.to(A.dtype)\\n+        A = torch.matmul(A, Vnn, out = Qn)\\n+    else:\\n+        A = scaled_dot_product_attention(Qn, Knn, Vnn, attn_mask = attention_mask, is_causal = False)\\n+    pass\\n     A = A.transpose(1, 2)\\n     A = A.reshape(bsz, 1, attention_size)\\n     A = fast_linear_forward(self.o_proj, A, out = self.temp_QA[1][:,:,:self.hidden_size])\\n@@ -214,11 +228,11 @@ def fast_swiglu_inference(self, X):\\n     # gate = self.gate_proj(X)\\n     # up   = self.up_proj(X)\\n     bsz, _, hd = X.shape\\n-    mlp_size = self.config.intermediate_size\\n-    temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n+    # mlp_size = self.config.intermediate_size\\n+    # temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda\")\\n \\n-    gate = fast_linear_forward(self.gate_proj, X, out = temp[0])\\n-    up   = fast_linear_forward(self.  up_proj, X, out = temp[1])\\n+    gate = fast_linear_forward(self.gate_proj, X)#, out = temp[0])\\n+    up   = fast_linear_forward(self.  up_proj, X)#, out = temp[1])\\n     gate = torch_nn_functional_silu(gate, inplace = True)\\n     gate *= up\\n \\n@@ -240,6 +254,24 @@ def fast_rms_layernorm_inference(self, X):\\n pass\\n \\n \\n+def fast_rms_layernorm_inference_gemma(self, X, out_weight = None):\\n+    XX = X.to(torch.float32)\\n+    variance = XX.square().mean(-1, keepdim = True)\\n+    variance += self.variance_epsilon\\n+    XX *= variance.rsqrt_()\\n+\\n+    if out_weight is None:\\n+        out_weight = self.weight + 1.0\\n+    else:\\n+        out_weight[:] = self.weight\\n+        out_weight += 1.0\\n+    pass\\n+\\n+    XX *= out_weight\\n+    return XX.to(X.dtype)\\n+pass\\n+\\n+\\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L320\\n def LlamaAttention_fast_forward(\\n     self,\\n@@ -375,18 +407,18 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if past_key_value is not None:\\n-        do_prefill = not hasattr(self.self_attn, \"paged_attention\")\\n-\\n-        # Self Attention\\n+    if use_cache:\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n-        hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n-            self.self_attn,\\n-            hidden_states,\\n-            past_key_value,\\n-            position_ids,\\n-            do_prefill = do_prefill,\\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n+            hidden_states=hidden_states,\\n+            causal_mask=causal_mask,\\n+            attention_mask=attention_mask,\\n+            position_ids=position_ids,\\n+            past_key_value=past_key_value,\\n+            output_attentions=output_attentions,\\n+            use_cache=use_cache,\\n+            padding_mask=padding_mask,\\n         )\\n         hidden_states += residual\\n \\n@@ -418,13 +450,8 @@ def LlamaDecoderLayer_fast_forward(\\n     pass\\n \\n     outputs = (hidden_states,)\\n-\\n-    if output_attentions:\\n-        outputs += (self_attn_weights,)\\n-\\n-    if use_cache:\\n-        outputs += (present_key_value,)\\n-\\n+    if output_attentions: outputs += (self_attn_weights,)\\n+    if use_cache: outputs += (present_key_value,)\\n     return outputs\\n pass\\n \\n@@ -602,9 +629,8 @@ def LlamaModel_fast_forward(\\n     pass\\n \\n     for idx, decoder_layer in enumerate(self.layers):\\n-        if output_hidden_states:\\n-            all_hidden_states += (hidden_states,)\\n \\n+        if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if self.gradient_checkpointing and self.training:\\n@@ -636,23 +662,24 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+        pass\\n \\n         hidden_states = layer_outputs[0]\\n-\\n-        if use_cache:\\n-            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n-\\n-        if output_attentions:\\n-            all_self_attns += (layer_outputs[1],)\\n+        if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n+        if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n-    \\n-    hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n \\n-    # add hidden states from the last decoder layer\\n-    if output_hidden_states:\\n-        all_hidden_states += (hidden_states,)\\n+    # Final layernorm\\n+    if use_cache:\\n+        hidden_states = (fast_rms_layernorm_inference_gemma if IS_GEMMA else fast_rms_layernorm_inference)\\\\\\n+            (self.norm, hidden_states)\\n+    else:\\n+        hidden_states = fast_rms_layernorm(self.norm, hidden_states, gemma = IS_GEMMA)\\n+    pass\\n \\n+    if output_hidden_states: all_hidden_states += (hidden_states,)\\n     next_cache = next_decoder_cache if use_cache else None\\n+\\n     if not return_dict:\\n         return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\\n     return BaseModelOutputWithPast(\\n@@ -665,32 +692,44 @@ pass\\n \\n \\n # https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L825\\n-# @torch.inference_mode\\n def LlamaModel_fast_forward_inference(\\n     self,\\n     input_ids,\\n     past_key_values,\\n+    position_ids,\\n+    attention_mask = None,\\n ):\\n-    # Fix out of bounds tokenization\\n     input_ids = input_ids[:,:self.max_seq_length]\\n-\\n-    hidden_states = self.embed_tokens(input_ids)\\n+    hidden_states = self.model.embed_tokens(input_ids)\\n     hidden_states = hidden_states.to(self.config.torch_dtype)\\n+    bsz, q_len, hd = hidden_states.shape\\n+    seq_len = past_key_values[0][0].shape[-2]\\n+    if bsz != 1:\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+            sliding_window = getattr(self.config, \"sliding_window\", None),\\n+        )\\n+    else:\\n+        attention_mask = None\\n+    pass\\n \\n     next_decoder_cache = []\\n-    for idx, decoder_layer in enumerate(self.layers):\\n-        # Self Attention\\n+    for idx, decoder_layer in enumerate(self.model.layers):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.input_layernorm, hidden_states)\\n         hidden_states, present_key_value = LlamaAttention_fast_forward_inference(\\n             decoder_layer.self_attn,\\n-            hidden_states,\\n-            past_key_values[idx],\\n-            None,\\n+            hidden_states = hidden_states,\\n+            past_key_value = past_key_values[idx],\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n+            do_prefill = not hasattr(decoder_layer.self_attn, \"paged_attention\"),\\n         )\\n         hidden_states += residual\\n \\n-        # Fully Connected\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(decoder_layer.post_attention_layernorm, hidden_states)\\n         hidden_states = fast_swiglu_inference(decoder_layer.mlp, hidden_states)\\n@@ -698,13 +737,13 @@ def LlamaModel_fast_forward_inference(\\n \\n         next_decoder_cache.append(present_key_value)\\n     pass\\n-    hidden_states = fast_rms_layernorm_inference(self.norm, hidden_states)\\n+    hidden_states = fast_rms_layernorm_inference(self.model.norm, hidden_states)\\n \\n     return BaseModelOutputWithPast(\\n         last_hidden_state = hidden_states,\\n-        past_key_values   = next_decoder_cache,\\n-        hidden_states     = [],\\n-        attentions        = [],\\n+        past_key_values = next_decoder_cache,\\n+        hidden_states = [],\\n+        attentions = [],\\n     )\\n pass\\n \\n@@ -726,11 +765,13 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n \\n-        if past_key_values is not None and hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+        if past_key_values is not None:\\n             outputs = fast_forward_inference(\\n-                self.model,\\n+                self,\\n                 input_ids,\\n                 past_key_values,\\n+                position_ids = position_ids,\\n+                attention_mask = attention_mask,\\n             )\\n         else:\\n             causal_mask = xformers.attn_bias.LowerTriangularMask()\\n',\n",
       " '@@ -200,12 +200,13 @@ def MistralForCausalLM_fast_forward(\\n     # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\\n     self.model._has_no_labels = labels is None\\n \\n-    if past_key_values is not None and \\\\\\n-        hasattr(self.model.layers[0].self_attn, \"paged_attention\"):\\n+    if past_key_values is not None:\\n         outputs = LlamaModel_fast_forward_inference(\\n-            self.model,\\n+            self,\\n             input_ids,\\n             past_key_values,\\n+            position_ids = position_ids,\\n+            attention_mask = attention_mask,\\n         )\\n     else:\\n         outputs = self.model(\\n',\n",
       " '@@ -30,4 +30,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\\n',\n",
       " '@@ -13,32 +13,7 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters\\n-\\n-\\n-def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n-    dtype = X.dtype\\n-    W = fast_dequantize(W.t(), W_quant)\\n-\\n-    if X.dim() == 3:\\n-        batch, seq_len, d = X.shape\\n-        X = X.view(-1, X.shape[-1])\\n-        reshape = True\\n-    else:\\n-        reshape = False\\n-    pass\\n-\\n-    out = torch.matmul(X, W, out = out)\\n-    if W_quant is not None: del W\\n-\\n-    if A is not None:\\n-        # LoRA is enabled\\n-        A, B = A.t(), B.t()\\n-        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n-    pass\\n-    \\n-    return out.view(batch, seq_len, -1) if reshape else out\\n-pass\\n+from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -182,8 +182,8 @@ pass\\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-\\n     bsz, q_len, in_dim = X.shape\\n+    if q_len != 1: return matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n@@ -203,7 +203,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n             lora_A._fast_lora = lora_A.to(dtype)\\n             lora_B._fast_lora = lora_B.to(dtype)\\n         pass\\n-\\n+        \\n         if bsz == 1:\\n             out = out.view(out_dim)\\n             temp_lora = torch.mv(lora_A._fast_lora, X.ravel(), out = temp_lora)\\n@@ -218,3 +218,28 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     return out\\n pass\\n+\\n+\\n+def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n+    dtype = X.dtype\\n+    W = fast_dequantize(W.t(), W_quant)\\n+\\n+    if X.dim() == 3:\\n+        batch, seq_len, d = X.shape\\n+        X = X.view(-1, X.shape[-1])\\n+        reshape = True\\n+    else:\\n+        reshape = False\\n+    pass\\n+\\n+    out = torch.matmul(X, W, out = out)\\n+    if W_quant is not None: del W\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n+    return out.view(batch, seq_len, -1) if reshape else out\\n+pass\\n',\n",
       " '@@ -30,4 +30,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\\n',\n",
       " '@@ -13,32 +13,7 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters\\n-\\n-\\n-def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n-    dtype = X.dtype\\n-    W = fast_dequantize(W.t(), W_quant)\\n-\\n-    if X.dim() == 3:\\n-        batch, seq_len, d = X.shape\\n-        X = X.view(-1, X.shape[-1])\\n-        reshape = True\\n-    else:\\n-        reshape = False\\n-    pass\\n-\\n-    out = torch.matmul(X, W, out = out)\\n-    if W_quant is not None: del W\\n-\\n-    if A is not None:\\n-        # LoRA is enabled\\n-        A, B = A.t(), B.t()\\n-        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n-    pass\\n-    \\n-    return out.view(batch, seq_len, -1) if reshape else out\\n-pass\\n+from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -182,8 +182,8 @@ pass\\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-\\n     bsz, q_len, in_dim = X.shape\\n+    if q_len != 1: return matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n@@ -203,7 +203,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n             lora_A._fast_lora = lora_A.to(dtype)\\n             lora_B._fast_lora = lora_B.to(dtype)\\n         pass\\n-\\n+        \\n         if bsz == 1:\\n             out = out.view(out_dim)\\n             temp_lora = torch.mv(lora_A._fast_lora, X.ravel(), out = temp_lora)\\n@@ -218,3 +218,28 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     return out\\n pass\\n+\\n+\\n+def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n+    dtype = X.dtype\\n+    W = fast_dequantize(W.t(), W_quant)\\n+\\n+    if X.dim() == 3:\\n+        batch, seq_len, d = X.shape\\n+        X = X.view(-1, X.shape[-1])\\n+        reshape = True\\n+    else:\\n+        reshape = False\\n+    pass\\n+\\n+    out = torch.matmul(X, W, out = out)\\n+    if W_quant is not None: del W\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n+    return out.view(batch, seq_len, -1) if reshape else out\\n+pass\\n',\n",
       " '@@ -30,4 +30,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\\n',\n",
       " '@@ -13,32 +13,7 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters\\n-\\n-\\n-def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n-    dtype = X.dtype\\n-    W = fast_dequantize(W.t(), W_quant)\\n-\\n-    if X.dim() == 3:\\n-        batch, seq_len, d = X.shape\\n-        X = X.view(-1, X.shape[-1])\\n-        reshape = True\\n-    else:\\n-        reshape = False\\n-    pass\\n-\\n-    out = torch.matmul(X, W, out = out)\\n-    if W_quant is not None: del W\\n-\\n-    if A is not None:\\n-        # LoRA is enabled\\n-        A, B = A.t(), B.t()\\n-        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n-    pass\\n-    \\n-    return out.view(batch, seq_len, -1) if reshape else out\\n-pass\\n+from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -182,8 +182,8 @@ pass\\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-\\n     bsz, q_len, in_dim = X.shape\\n+    if q_len != 1: return matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n@@ -203,7 +203,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n             lora_A._fast_lora = lora_A.to(dtype)\\n             lora_B._fast_lora = lora_B.to(dtype)\\n         pass\\n-\\n+        \\n         if bsz == 1:\\n             out = out.view(out_dim)\\n             temp_lora = torch.mv(lora_A._fast_lora, X.ravel(), out = temp_lora)\\n@@ -218,3 +218,28 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     return out\\n pass\\n+\\n+\\n+def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n+    dtype = X.dtype\\n+    W = fast_dequantize(W.t(), W_quant)\\n+\\n+    if X.dim() == 3:\\n+        batch, seq_len, d = X.shape\\n+        X = X.view(-1, X.shape[-1])\\n+        reshape = True\\n+    else:\\n+        reshape = False\\n+    pass\\n+\\n+    out = torch.matmul(X, W, out = out)\\n+    if W_quant is not None: del W\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n+    return out.view(batch, seq_len, -1) if reshape else out\\n+pass\\n',\n",
       " '@@ -30,4 +30,4 @@ from .fast_lora import (\\n \\tapply_lora_qkv,\\n \\tapply_lora_o,\\n )\\n-from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward\\n+from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\\n',\n",
       " '@@ -13,32 +13,7 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters\\n-\\n-\\n-def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n-    dtype = X.dtype\\n-    W = fast_dequantize(W.t(), W_quant)\\n-\\n-    if X.dim() == 3:\\n-        batch, seq_len, d = X.shape\\n-        X = X.view(-1, X.shape[-1])\\n-        reshape = True\\n-    else:\\n-        reshape = False\\n-    pass\\n-\\n-    out = torch.matmul(X, W, out = out)\\n-    if W_quant is not None: del W\\n-\\n-    if A is not None:\\n-        # LoRA is enabled\\n-        A, B = A.t(), B.t()\\n-        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n-    pass\\n-    \\n-    return out.view(batch, seq_len, -1) if reshape else out\\n-pass\\n+from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -182,8 +182,8 @@ pass\\n def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     W, W_quant, lora_A, lora_B, lora_S = get_lora_parameters(proj)\\n-\\n     bsz, q_len, in_dim = X.shape\\n+    if q_len != 1: return matmul_lora(X, W, W_quant, lora_A, lora_B, lora_S)\\n \\n     if W_quant is None:\\n         out = torch.matmul(X, W.t(), out = out)\\n@@ -203,7 +203,7 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n             lora_A._fast_lora = lora_A.to(dtype)\\n             lora_B._fast_lora = lora_B.to(dtype)\\n         pass\\n-\\n+        \\n         if bsz == 1:\\n             out = out.view(out_dim)\\n             temp_lora = torch.mv(lora_A._fast_lora, X.ravel(), out = temp_lora)\\n@@ -218,3 +218,28 @@ def fast_linear_forward(proj, X, temp_lora = None, out = None):\\n \\n     return out\\n pass\\n+\\n+\\n+def matmul_lora(X, W, W_quant, A, B, s, out = None):\\n+    dtype = X.dtype\\n+    W = fast_dequantize(W.t(), W_quant)\\n+\\n+    if X.dim() == 3:\\n+        batch, seq_len, d = X.shape\\n+        X = X.view(-1, X.shape[-1])\\n+        reshape = True\\n+    else:\\n+        reshape = False\\n+    pass\\n+\\n+    out = torch.matmul(X, W, out = out)\\n+    if W_quant is not None: del W\\n+\\n+    if A is not None:\\n+        # LoRA is enabled\\n+        A, B = A.t(), B.t()\\n+        out += (X @ A.to(dtype)) @ (s * B.to(dtype))\\n+    pass\\n+    \\n+    return out.view(batch, seq_len, -1) if reshape else out\\n+pass\\n',\n",
       " '@@ -36,7 +36,7 @@ huggingface = [\\n     \"tyro\",\\n     \"transformers>=4.38.2\",\\n     \"datasets>=2.16.0\",\\n-    \"sentencepiece\",\\n+    \"sentencepiece>=0.2.0\",\\n     \"tqdm\",\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n',\n",
       " '@@ -145,7 +145,12 @@ def GemmaModel_fast_forward_inference(\\n     bsz, q_len, hd = hidden_states.shape\\n     seq_len = past_key_values[0][0].shape[-2]\\n     if bsz != 1:\\n-        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (bsz, q_len), hidden_states, seq_len,)\\n+        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\\n+            attention_mask,\\n+            (bsz, q_len),\\n+            hidden_states,\\n+            seq_len,\\n+        )\\n     pass\\n \\n     next_decoder_cache = []\\n',\n",
       " '@@ -30,7 +30,7 @@ import numpy as np\\n import os\\n import psutil\\n \\n-__version__ = \"2024.3\"\\n+__version__ = \"2024.4\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -70,12 +70,13 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n+    \"Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n-    use_gradient_checkpointing : bool = True,\\n+    use_gradient_checkpointing : Optional = True,\\n     use_reentrant              : Optional[bool] = True,\\n ) -> Any:\\n     \"\"\"\\n@@ -101,9 +102,23 @@ def prepare_model_for_kbit_training(\\n             param.requires_grad_(False)\\n     pass\\n \\n-    if use_gradient_checkpointing:\\n+    # Gradient checkpointing!\\n+    if use_gradient_checkpointing == \"offloaded\":\\n+\\n+        # Saves VRAM!\\n+        original_model = model\\n+        while hasattr(original_model, \"model\"):\\n+            original_model._offloaded_gradient_checkpointing = True\\n+            original_model = original_model.model\\n+        pass\\n+        original_model._offloaded_gradient_checkpointing = True\\n+        \\n         model.gradient_checkpointing_enable()\\n \\n+    elif use_gradient_checkpointing == True:\\n+        model.gradient_checkpointing_enable()\\n+    pass\\n+\\n     # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.\\n     if use_reentrant:\\n         if hasattr(model, \"enable_input_require_grads\"):\\n@@ -179,6 +194,7 @@ def get_statistics():\\n     try:\\n         from huggingface_hub import hf_hub_download\\n         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled\\n+        import psutil\\n         n_cpus = psutil.cpu_count(logical = False)\\n \\n         keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n@@ -291,3 +307,35 @@ def prepare_n_gradient_checkpoints(\\n     _model._gradient_checkpointing_boundaries    = boundaries\\n     _model._gradient_checkpointing_use_reentrant = use_reentrant\\n pass\\n+\\n+\\n+class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+    \"\"\"\\n+    Saves VRAM by smartly offloading to RAM.\\n+    Tiny hit to performance, since we mask the movement via non blocking calls.\\n+    [TODO] Load the backward pass earlier\\n+    \"\"\"\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_fwd\\n+    def forward(ctx, forward_function, hidden_states, *args):\\n+        saved_hidden_states = hidden_states.to(\"cpu\", non_blocking = True)\\n+        with torch.no_grad():\\n+            (output,) = forward_function(hidden_states, *args)\\n+        ctx.save_for_backward(saved_hidden_states)\\n+        ctx.forward_function = forward_function\\n+        ctx.args = args\\n+        return output\\n+    pass\\n+\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_bwd\\n+    def backward(ctx, dY):\\n+        (hidden_states,) = ctx.saved_tensors\\n+        hidden_states = hidden_states.to(\"cuda\", non_blocking = True).detach()\\n+        hidden_states.requires_grad = True\\n+        with torch.enable_grad():\\n+            (output,) = ctx.forward_function(hidden_states, *ctx.args)\\n+        torch.autograd.backward(output, dY)\\n+        return (None, hidden_states.grad,) + (None,)*len(ctx.args)\\n+    pass\\n+pass\\n',\n",
       " '@@ -628,19 +628,42 @@ def LlamaModel_fast_forward(\\n         boundaries = None\\n     pass\\n \\n+    # Check checkpointing method\\n+    gradient_checkpointing = False\\n+    offloaded_gradient_checkpointing = False\\n+\\n+    if (self.gradient_checkpointing and self.training and not use_cache):\\n+\\n+        gradient_checkpointing = True\\n+\\n+        if output_attentions is False and hasattr(self, \"_offloaded_gradient_checkpointing\"):\\n+            offloaded_gradient_checkpointing = True\\n+    pass\\n+\\n+    # Go through every layer!\\n     for idx, decoder_layer in enumerate(self.layers):\\n \\n         if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n-        if self.gradient_checkpointing and self.training:\\n+        if offloaded_gradient_checkpointing:\\n+            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+                decoder_layer,\\n+                hidden_states,\\n+                causal_mask,\\n+                attention_mask,\\n+                position_ids,\\n+                past_key_values,\\n+                output_attentions,\\n+                use_cache,\\n+            )\\n \\n+        elif gradient_checkpointing:\\n             def create_custom_forward(module):\\n                 def custom_forward(*inputs):\\n-                    # None for past_key_value\\n-                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\\n-\\n+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)\\n                 return custom_forward\\n+            pass\\n \\n             layer_outputs = torch.utils.checkpoint.checkpoint(\\n                 create_custom_forward(decoder_layer),\\n@@ -648,9 +671,11 @@ def LlamaModel_fast_forward(\\n                 causal_mask,\\n                 attention_mask,\\n                 position_ids,\\n-                use_reentrant=True,\\n-                preserve_rng_state=False,\\n+                use_reentrant = True,\\n+                preserve_rng_state = False,\\n             )\\n+            hidden_states = layer_outputs[0]\\n+\\n         else:\\n             layer_outputs = decoder_layer(\\n                 hidden_states,\\n@@ -662,9 +687,9 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+            hidden_states = layer_outputs[0]\\n         pass\\n \\n-        hidden_states = layer_outputs[0]\\n         if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n         if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n@@ -801,12 +826,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n \\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n+        lm_head = self.lm_head.weight\\n         if bsz == 1 and q_len == 1:\\n-            lm_head = self.lm_head.weight\\n             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n-            logits = self.lm_head(hidden_states)\\n+            logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n         pass\\n         logits = logits.to(self.config.torch_dtype)\\n \\n@@ -1402,6 +1427,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n+                else: modules_to_save.append(\"lm_head\")\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1409,6 +1436,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n+                else: modules_to_save.append(\"embed_tokens\")\\n \\n             else:\\n                 assert(module in accepted_modules)\\n',\n",
       " '@@ -225,12 +225,12 @@ def MistralForCausalLM_fast_forward(\\n \\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n+    lm_head = self.lm_head.weight\\n     if bsz == 1 and q_len == 1:\\n-        lm_head = self.lm_head.weight\\n         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n-        logits = self.lm_head(hidden_states)\\n+        logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n     pass\\n     logits = logits.to(self.config.torch_dtype)\\n \\n',\n",
       " '@@ -33,9 +33,13 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n-# Check Kaggle\\n-IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n \\n+# Weights\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -177,6 +181,9 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if token is None and \"HF_TOKEN\" in os.environ:\\n+        token = os.environ[\"HF_TOKEN\"]\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -291,6 +298,10 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n             (\\n                 repo_id            = save_directory,\\n@@ -305,6 +316,9 @@ def unsloth_save_model(\\n                 commit_description = commit_description,\\n                 tags               = tags,\\n             )\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n         pass\\n \\n         if hasattr(model, \"config\"):\\n@@ -361,7 +375,16 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n+\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -449,12 +472,12 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    # Check if Kaggle, since only 20GB of Disk space allowed.\\n-    if IS_A_KAGGLE_ENVIRONMENT:\\n+    # Check if Kaggle or Colab, since only 20GB of Disk space allowed.\\n+    if IS_KAGGLE_ENVIRONMENT or IS_COLAB_ENVIRONMENT:\\n         # We free up 4GB of space\\n         logger.warning_once(\\n-            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n-            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+            \"Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\"\\n         )\\n         _free_cached_model(internal_model)\\n     pass\\n@@ -462,7 +485,10 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n+\\n+    torch_dtype = model.config.torch_dtype\\n+    # Check modules to save float32 dtype\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n@@ -495,7 +521,8 @@ def unsloth_save_model(\\n     pass\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n+    # Check for modules_to_save float32 dtype\\n+    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -552,7 +579,16 @@ def unsloth_save_model(\\n     # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+        # Set padding side to left for inference\\n+        old_padding_side = tokenizer.padding_side\\n+        tokenizer.padding_side = \"left\"\\n+\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+        # Revert back padding side\\n+        tokenizer.padding_side = old_padding_side\\n+            \\n         print(\" Done.\")\\n     else:\\n         print()\\n@@ -1216,7 +1252,7 @@ def unsloth_save_pretrained_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1237,7 +1273,7 @@ def unsloth_save_pretrained_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n@@ -1336,7 +1372,7 @@ def unsloth_push_to_hub_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1357,7 +1393,7 @@ def unsloth_push_to_hub_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n',\n",
       " '@@ -186,9 +186,6 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n pass\\n \\n \\n-global sentencepiece_model_pb2\\n-sentencepiece_model_pb2 = None\\n-\\n def fix_sentencepiece_tokenizer(\\n     old_tokenizer,\\n     new_tokenizer,\\n@@ -197,19 +194,7 @@ def fix_sentencepiece_tokenizer(\\n ):\\n     # From https://github.com/google/sentencepiece/issues/121\\n     # We need to manually edit the sentencepiece tokenizer!\\n-    global sentencepiece_model_pb2\\n-    if sentencepiece_model_pb2 is None:\\n-        try:\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        except:\\n-            if not os.path.exists(temporary_location):\\n-                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n-                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            pass\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        pass\\n+    from transformers.utils import sentencepiece_model_pb2\\n \\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n',\n",
       " '@@ -30,7 +30,7 @@ import numpy as np\\n import os\\n import psutil\\n \\n-__version__ = \"2024.3\"\\n+__version__ = \"2024.4\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -70,12 +70,13 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n+    \"Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n-    use_gradient_checkpointing : bool = True,\\n+    use_gradient_checkpointing : Optional = True,\\n     use_reentrant              : Optional[bool] = True,\\n ) -> Any:\\n     \"\"\"\\n@@ -101,9 +102,23 @@ def prepare_model_for_kbit_training(\\n             param.requires_grad_(False)\\n     pass\\n \\n-    if use_gradient_checkpointing:\\n+    # Gradient checkpointing!\\n+    if use_gradient_checkpointing == \"offloaded\":\\n+\\n+        # Saves VRAM!\\n+        original_model = model\\n+        while hasattr(original_model, \"model\"):\\n+            original_model._offloaded_gradient_checkpointing = True\\n+            original_model = original_model.model\\n+        pass\\n+        original_model._offloaded_gradient_checkpointing = True\\n+        \\n         model.gradient_checkpointing_enable()\\n \\n+    elif use_gradient_checkpointing == True:\\n+        model.gradient_checkpointing_enable()\\n+    pass\\n+\\n     # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.\\n     if use_reentrant:\\n         if hasattr(model, \"enable_input_require_grads\"):\\n@@ -179,6 +194,7 @@ def get_statistics():\\n     try:\\n         from huggingface_hub import hf_hub_download\\n         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled\\n+        import psutil\\n         n_cpus = psutil.cpu_count(logical = False)\\n \\n         keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n@@ -291,3 +307,35 @@ def prepare_n_gradient_checkpoints(\\n     _model._gradient_checkpointing_boundaries    = boundaries\\n     _model._gradient_checkpointing_use_reentrant = use_reentrant\\n pass\\n+\\n+\\n+class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+    \"\"\"\\n+    Saves VRAM by smartly offloading to RAM.\\n+    Tiny hit to performance, since we mask the movement via non blocking calls.\\n+    [TODO] Load the backward pass earlier\\n+    \"\"\"\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_fwd\\n+    def forward(ctx, forward_function, hidden_states, *args):\\n+        saved_hidden_states = hidden_states.to(\"cpu\", non_blocking = True)\\n+        with torch.no_grad():\\n+            (output,) = forward_function(hidden_states, *args)\\n+        ctx.save_for_backward(saved_hidden_states)\\n+        ctx.forward_function = forward_function\\n+        ctx.args = args\\n+        return output\\n+    pass\\n+\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_bwd\\n+    def backward(ctx, dY):\\n+        (hidden_states,) = ctx.saved_tensors\\n+        hidden_states = hidden_states.to(\"cuda\", non_blocking = True).detach()\\n+        hidden_states.requires_grad = True\\n+        with torch.enable_grad():\\n+            (output,) = ctx.forward_function(hidden_states, *ctx.args)\\n+        torch.autograd.backward(output, dY)\\n+        return (None, hidden_states.grad,) + (None,)*len(ctx.args)\\n+    pass\\n+pass\\n',\n",
       " '@@ -628,19 +628,42 @@ def LlamaModel_fast_forward(\\n         boundaries = None\\n     pass\\n \\n+    # Check checkpointing method\\n+    gradient_checkpointing = False\\n+    offloaded_gradient_checkpointing = False\\n+\\n+    if (self.gradient_checkpointing and self.training and not use_cache):\\n+\\n+        gradient_checkpointing = True\\n+\\n+        if output_attentions is False and hasattr(self, \"_offloaded_gradient_checkpointing\"):\\n+            offloaded_gradient_checkpointing = True\\n+    pass\\n+\\n+    # Go through every layer!\\n     for idx, decoder_layer in enumerate(self.layers):\\n \\n         if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n-        if self.gradient_checkpointing and self.training:\\n+        if offloaded_gradient_checkpointing:\\n+            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+                decoder_layer,\\n+                hidden_states,\\n+                causal_mask,\\n+                attention_mask,\\n+                position_ids,\\n+                past_key_values,\\n+                output_attentions,\\n+                use_cache,\\n+            )\\n \\n+        elif gradient_checkpointing:\\n             def create_custom_forward(module):\\n                 def custom_forward(*inputs):\\n-                    # None for past_key_value\\n-                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\\n-\\n+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)\\n                 return custom_forward\\n+            pass\\n \\n             layer_outputs = torch.utils.checkpoint.checkpoint(\\n                 create_custom_forward(decoder_layer),\\n@@ -648,9 +671,11 @@ def LlamaModel_fast_forward(\\n                 causal_mask,\\n                 attention_mask,\\n                 position_ids,\\n-                use_reentrant=True,\\n-                preserve_rng_state=False,\\n+                use_reentrant = True,\\n+                preserve_rng_state = False,\\n             )\\n+            hidden_states = layer_outputs[0]\\n+\\n         else:\\n             layer_outputs = decoder_layer(\\n                 hidden_states,\\n@@ -662,9 +687,9 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+            hidden_states = layer_outputs[0]\\n         pass\\n \\n-        hidden_states = layer_outputs[0]\\n         if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n         if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n@@ -801,12 +826,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n \\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n+        lm_head = self.lm_head.weight\\n         if bsz == 1 and q_len == 1:\\n-            lm_head = self.lm_head.weight\\n             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n-            logits = self.lm_head(hidden_states)\\n+            logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n         pass\\n         logits = logits.to(self.config.torch_dtype)\\n \\n@@ -1402,6 +1427,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n+                else: modules_to_save.append(\"lm_head\")\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1409,6 +1436,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n+                else: modules_to_save.append(\"embed_tokens\")\\n \\n             else:\\n                 assert(module in accepted_modules)\\n',\n",
       " '@@ -225,12 +225,12 @@ def MistralForCausalLM_fast_forward(\\n \\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n+    lm_head = self.lm_head.weight\\n     if bsz == 1 and q_len == 1:\\n-        lm_head = self.lm_head.weight\\n         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n-        logits = self.lm_head(hidden_states)\\n+        logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n     pass\\n     logits = logits.to(self.config.torch_dtype)\\n \\n',\n",
       " '@@ -33,9 +33,13 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n-# Check Kaggle\\n-IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n \\n+# Weights\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -177,6 +181,9 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if token is None and \"HF_TOKEN\" in os.environ:\\n+        token = os.environ[\"HF_TOKEN\"]\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -291,6 +298,10 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n             (\\n                 repo_id            = save_directory,\\n@@ -305,6 +316,9 @@ def unsloth_save_model(\\n                 commit_description = commit_description,\\n                 tags               = tags,\\n             )\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n         pass\\n \\n         if hasattr(model, \"config\"):\\n@@ -361,7 +375,16 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n+\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -449,12 +472,12 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    # Check if Kaggle, since only 20GB of Disk space allowed.\\n-    if IS_A_KAGGLE_ENVIRONMENT:\\n+    # Check if Kaggle or Colab, since only 20GB of Disk space allowed.\\n+    if IS_KAGGLE_ENVIRONMENT or IS_COLAB_ENVIRONMENT:\\n         # We free up 4GB of space\\n         logger.warning_once(\\n-            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n-            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+            \"Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\"\\n         )\\n         _free_cached_model(internal_model)\\n     pass\\n@@ -462,7 +485,10 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n+\\n+    torch_dtype = model.config.torch_dtype\\n+    # Check modules to save float32 dtype\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n@@ -495,7 +521,8 @@ def unsloth_save_model(\\n     pass\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n+    # Check for modules_to_save float32 dtype\\n+    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -552,7 +579,16 @@ def unsloth_save_model(\\n     # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+        # Set padding side to left for inference\\n+        old_padding_side = tokenizer.padding_side\\n+        tokenizer.padding_side = \"left\"\\n+\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+        # Revert back padding side\\n+        tokenizer.padding_side = old_padding_side\\n+            \\n         print(\" Done.\")\\n     else:\\n         print()\\n@@ -1216,7 +1252,7 @@ def unsloth_save_pretrained_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1237,7 +1273,7 @@ def unsloth_save_pretrained_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n@@ -1336,7 +1372,7 @@ def unsloth_push_to_hub_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1357,7 +1393,7 @@ def unsloth_push_to_hub_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n',\n",
       " '@@ -186,9 +186,6 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n pass\\n \\n \\n-global sentencepiece_model_pb2\\n-sentencepiece_model_pb2 = None\\n-\\n def fix_sentencepiece_tokenizer(\\n     old_tokenizer,\\n     new_tokenizer,\\n@@ -197,19 +194,7 @@ def fix_sentencepiece_tokenizer(\\n ):\\n     # From https://github.com/google/sentencepiece/issues/121\\n     # We need to manually edit the sentencepiece tokenizer!\\n-    global sentencepiece_model_pb2\\n-    if sentencepiece_model_pb2 is None:\\n-        try:\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        except:\\n-            if not os.path.exists(temporary_location):\\n-                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n-                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            pass\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        pass\\n+    from transformers.utils import sentencepiece_model_pb2\\n \\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n',\n",
       " '@@ -30,7 +30,7 @@ import numpy as np\\n import os\\n import psutil\\n \\n-__version__ = \"2024.3\"\\n+__version__ = \"2024.4\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -70,12 +70,13 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n+    \"Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n-    use_gradient_checkpointing : bool = True,\\n+    use_gradient_checkpointing : Optional = True,\\n     use_reentrant              : Optional[bool] = True,\\n ) -> Any:\\n     \"\"\"\\n@@ -101,9 +102,23 @@ def prepare_model_for_kbit_training(\\n             param.requires_grad_(False)\\n     pass\\n \\n-    if use_gradient_checkpointing:\\n+    # Gradient checkpointing!\\n+    if use_gradient_checkpointing == \"offloaded\":\\n+\\n+        # Saves VRAM!\\n+        original_model = model\\n+        while hasattr(original_model, \"model\"):\\n+            original_model._offloaded_gradient_checkpointing = True\\n+            original_model = original_model.model\\n+        pass\\n+        original_model._offloaded_gradient_checkpointing = True\\n+        \\n         model.gradient_checkpointing_enable()\\n \\n+    elif use_gradient_checkpointing == True:\\n+        model.gradient_checkpointing_enable()\\n+    pass\\n+\\n     # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.\\n     if use_reentrant:\\n         if hasattr(model, \"enable_input_require_grads\"):\\n@@ -179,6 +194,7 @@ def get_statistics():\\n     try:\\n         from huggingface_hub import hf_hub_download\\n         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled\\n+        import psutil\\n         n_cpus = psutil.cpu_count(logical = False)\\n \\n         keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n@@ -291,3 +307,35 @@ def prepare_n_gradient_checkpoints(\\n     _model._gradient_checkpointing_boundaries    = boundaries\\n     _model._gradient_checkpointing_use_reentrant = use_reentrant\\n pass\\n+\\n+\\n+class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+    \"\"\"\\n+    Saves VRAM by smartly offloading to RAM.\\n+    Tiny hit to performance, since we mask the movement via non blocking calls.\\n+    [TODO] Load the backward pass earlier\\n+    \"\"\"\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_fwd\\n+    def forward(ctx, forward_function, hidden_states, *args):\\n+        saved_hidden_states = hidden_states.to(\"cpu\", non_blocking = True)\\n+        with torch.no_grad():\\n+            (output,) = forward_function(hidden_states, *args)\\n+        ctx.save_for_backward(saved_hidden_states)\\n+        ctx.forward_function = forward_function\\n+        ctx.args = args\\n+        return output\\n+    pass\\n+\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_bwd\\n+    def backward(ctx, dY):\\n+        (hidden_states,) = ctx.saved_tensors\\n+        hidden_states = hidden_states.to(\"cuda\", non_blocking = True).detach()\\n+        hidden_states.requires_grad = True\\n+        with torch.enable_grad():\\n+            (output,) = ctx.forward_function(hidden_states, *ctx.args)\\n+        torch.autograd.backward(output, dY)\\n+        return (None, hidden_states.grad,) + (None,)*len(ctx.args)\\n+    pass\\n+pass\\n',\n",
       " '@@ -628,19 +628,42 @@ def LlamaModel_fast_forward(\\n         boundaries = None\\n     pass\\n \\n+    # Check checkpointing method\\n+    gradient_checkpointing = False\\n+    offloaded_gradient_checkpointing = False\\n+\\n+    if (self.gradient_checkpointing and self.training and not use_cache):\\n+\\n+        gradient_checkpointing = True\\n+\\n+        if output_attentions is False and hasattr(self, \"_offloaded_gradient_checkpointing\"):\\n+            offloaded_gradient_checkpointing = True\\n+    pass\\n+\\n+    # Go through every layer!\\n     for idx, decoder_layer in enumerate(self.layers):\\n \\n         if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n-        if self.gradient_checkpointing and self.training:\\n+        if offloaded_gradient_checkpointing:\\n+            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+                decoder_layer,\\n+                hidden_states,\\n+                causal_mask,\\n+                attention_mask,\\n+                position_ids,\\n+                past_key_values,\\n+                output_attentions,\\n+                use_cache,\\n+            )\\n \\n+        elif gradient_checkpointing:\\n             def create_custom_forward(module):\\n                 def custom_forward(*inputs):\\n-                    # None for past_key_value\\n-                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\\n-\\n+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)\\n                 return custom_forward\\n+            pass\\n \\n             layer_outputs = torch.utils.checkpoint.checkpoint(\\n                 create_custom_forward(decoder_layer),\\n@@ -648,9 +671,11 @@ def LlamaModel_fast_forward(\\n                 causal_mask,\\n                 attention_mask,\\n                 position_ids,\\n-                use_reentrant=True,\\n-                preserve_rng_state=False,\\n+                use_reentrant = True,\\n+                preserve_rng_state = False,\\n             )\\n+            hidden_states = layer_outputs[0]\\n+\\n         else:\\n             layer_outputs = decoder_layer(\\n                 hidden_states,\\n@@ -662,9 +687,9 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+            hidden_states = layer_outputs[0]\\n         pass\\n \\n-        hidden_states = layer_outputs[0]\\n         if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n         if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n@@ -801,12 +826,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n \\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n+        lm_head = self.lm_head.weight\\n         if bsz == 1 and q_len == 1:\\n-            lm_head = self.lm_head.weight\\n             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n-            logits = self.lm_head(hidden_states)\\n+            logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n         pass\\n         logits = logits.to(self.config.torch_dtype)\\n \\n@@ -1402,6 +1427,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n+                else: modules_to_save.append(\"lm_head\")\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1409,6 +1436,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n+                else: modules_to_save.append(\"embed_tokens\")\\n \\n             else:\\n                 assert(module in accepted_modules)\\n',\n",
       " '@@ -225,12 +225,12 @@ def MistralForCausalLM_fast_forward(\\n \\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n+    lm_head = self.lm_head.weight\\n     if bsz == 1 and q_len == 1:\\n-        lm_head = self.lm_head.weight\\n         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n-        logits = self.lm_head(hidden_states)\\n+        logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n     pass\\n     logits = logits.to(self.config.torch_dtype)\\n \\n',\n",
       " '@@ -33,9 +33,13 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n-# Check Kaggle\\n-IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n \\n+# Weights\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -177,6 +181,9 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if token is None and \"HF_TOKEN\" in os.environ:\\n+        token = os.environ[\"HF_TOKEN\"]\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -291,6 +298,10 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n             (\\n                 repo_id            = save_directory,\\n@@ -305,6 +316,9 @@ def unsloth_save_model(\\n                 commit_description = commit_description,\\n                 tags               = tags,\\n             )\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n         pass\\n \\n         if hasattr(model, \"config\"):\\n@@ -361,7 +375,16 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n+\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -449,12 +472,12 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    # Check if Kaggle, since only 20GB of Disk space allowed.\\n-    if IS_A_KAGGLE_ENVIRONMENT:\\n+    # Check if Kaggle or Colab, since only 20GB of Disk space allowed.\\n+    if IS_KAGGLE_ENVIRONMENT or IS_COLAB_ENVIRONMENT:\\n         # We free up 4GB of space\\n         logger.warning_once(\\n-            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n-            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+            \"Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\"\\n         )\\n         _free_cached_model(internal_model)\\n     pass\\n@@ -462,7 +485,10 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n+\\n+    torch_dtype = model.config.torch_dtype\\n+    # Check modules to save float32 dtype\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n@@ -495,7 +521,8 @@ def unsloth_save_model(\\n     pass\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n+    # Check for modules_to_save float32 dtype\\n+    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -552,7 +579,16 @@ def unsloth_save_model(\\n     # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+        # Set padding side to left for inference\\n+        old_padding_side = tokenizer.padding_side\\n+        tokenizer.padding_side = \"left\"\\n+\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+        # Revert back padding side\\n+        tokenizer.padding_side = old_padding_side\\n+            \\n         print(\" Done.\")\\n     else:\\n         print()\\n@@ -1216,7 +1252,7 @@ def unsloth_save_pretrained_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1237,7 +1273,7 @@ def unsloth_save_pretrained_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n@@ -1336,7 +1372,7 @@ def unsloth_push_to_hub_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1357,7 +1393,7 @@ def unsloth_push_to_hub_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n',\n",
       " '@@ -186,9 +186,6 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n pass\\n \\n \\n-global sentencepiece_model_pb2\\n-sentencepiece_model_pb2 = None\\n-\\n def fix_sentencepiece_tokenizer(\\n     old_tokenizer,\\n     new_tokenizer,\\n@@ -197,19 +194,7 @@ def fix_sentencepiece_tokenizer(\\n ):\\n     # From https://github.com/google/sentencepiece/issues/121\\n     # We need to manually edit the sentencepiece tokenizer!\\n-    global sentencepiece_model_pb2\\n-    if sentencepiece_model_pb2 is None:\\n-        try:\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        except:\\n-            if not os.path.exists(temporary_location):\\n-                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n-                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            pass\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        pass\\n+    from transformers.utils import sentencepiece_model_pb2\\n \\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n',\n",
       " '@@ -30,7 +30,7 @@ import numpy as np\\n import os\\n import psutil\\n \\n-__version__ = \"2024.3\"\\n+__version__ = \"2024.4\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -70,12 +70,13 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n+    \"Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n-    use_gradient_checkpointing : bool = True,\\n+    use_gradient_checkpointing : Optional = True,\\n     use_reentrant              : Optional[bool] = True,\\n ) -> Any:\\n     \"\"\"\\n@@ -101,9 +102,23 @@ def prepare_model_for_kbit_training(\\n             param.requires_grad_(False)\\n     pass\\n \\n-    if use_gradient_checkpointing:\\n+    # Gradient checkpointing!\\n+    if use_gradient_checkpointing == \"offloaded\":\\n+\\n+        # Saves VRAM!\\n+        original_model = model\\n+        while hasattr(original_model, \"model\"):\\n+            original_model._offloaded_gradient_checkpointing = True\\n+            original_model = original_model.model\\n+        pass\\n+        original_model._offloaded_gradient_checkpointing = True\\n+        \\n         model.gradient_checkpointing_enable()\\n \\n+    elif use_gradient_checkpointing == True:\\n+        model.gradient_checkpointing_enable()\\n+    pass\\n+\\n     # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.\\n     if use_reentrant:\\n         if hasattr(model, \"enable_input_require_grads\"):\\n@@ -179,6 +194,7 @@ def get_statistics():\\n     try:\\n         from huggingface_hub import hf_hub_download\\n         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled\\n+        import psutil\\n         n_cpus = psutil.cpu_count(logical = False)\\n \\n         keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n@@ -291,3 +307,35 @@ def prepare_n_gradient_checkpoints(\\n     _model._gradient_checkpointing_boundaries    = boundaries\\n     _model._gradient_checkpointing_use_reentrant = use_reentrant\\n pass\\n+\\n+\\n+class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+    \"\"\"\\n+    Saves VRAM by smartly offloading to RAM.\\n+    Tiny hit to performance, since we mask the movement via non blocking calls.\\n+    [TODO] Load the backward pass earlier\\n+    \"\"\"\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_fwd\\n+    def forward(ctx, forward_function, hidden_states, *args):\\n+        saved_hidden_states = hidden_states.to(\"cpu\", non_blocking = True)\\n+        with torch.no_grad():\\n+            (output,) = forward_function(hidden_states, *args)\\n+        ctx.save_for_backward(saved_hidden_states)\\n+        ctx.forward_function = forward_function\\n+        ctx.args = args\\n+        return output\\n+    pass\\n+\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_bwd\\n+    def backward(ctx, dY):\\n+        (hidden_states,) = ctx.saved_tensors\\n+        hidden_states = hidden_states.to(\"cuda\", non_blocking = True).detach()\\n+        hidden_states.requires_grad = True\\n+        with torch.enable_grad():\\n+            (output,) = ctx.forward_function(hidden_states, *ctx.args)\\n+        torch.autograd.backward(output, dY)\\n+        return (None, hidden_states.grad,) + (None,)*len(ctx.args)\\n+    pass\\n+pass\\n',\n",
       " '@@ -628,19 +628,42 @@ def LlamaModel_fast_forward(\\n         boundaries = None\\n     pass\\n \\n+    # Check checkpointing method\\n+    gradient_checkpointing = False\\n+    offloaded_gradient_checkpointing = False\\n+\\n+    if (self.gradient_checkpointing and self.training and not use_cache):\\n+\\n+        gradient_checkpointing = True\\n+\\n+        if output_attentions is False and hasattr(self, \"_offloaded_gradient_checkpointing\"):\\n+            offloaded_gradient_checkpointing = True\\n+    pass\\n+\\n+    # Go through every layer!\\n     for idx, decoder_layer in enumerate(self.layers):\\n \\n         if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n-        if self.gradient_checkpointing and self.training:\\n+        if offloaded_gradient_checkpointing:\\n+            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+                decoder_layer,\\n+                hidden_states,\\n+                causal_mask,\\n+                attention_mask,\\n+                position_ids,\\n+                past_key_values,\\n+                output_attentions,\\n+                use_cache,\\n+            )\\n \\n+        elif gradient_checkpointing:\\n             def create_custom_forward(module):\\n                 def custom_forward(*inputs):\\n-                    # None for past_key_value\\n-                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\\n-\\n+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)\\n                 return custom_forward\\n+            pass\\n \\n             layer_outputs = torch.utils.checkpoint.checkpoint(\\n                 create_custom_forward(decoder_layer),\\n@@ -648,9 +671,11 @@ def LlamaModel_fast_forward(\\n                 causal_mask,\\n                 attention_mask,\\n                 position_ids,\\n-                use_reentrant=True,\\n-                preserve_rng_state=False,\\n+                use_reentrant = True,\\n+                preserve_rng_state = False,\\n             )\\n+            hidden_states = layer_outputs[0]\\n+\\n         else:\\n             layer_outputs = decoder_layer(\\n                 hidden_states,\\n@@ -662,9 +687,9 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+            hidden_states = layer_outputs[0]\\n         pass\\n \\n-        hidden_states = layer_outputs[0]\\n         if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n         if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n@@ -801,12 +826,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n \\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n+        lm_head = self.lm_head.weight\\n         if bsz == 1 and q_len == 1:\\n-            lm_head = self.lm_head.weight\\n             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n-            logits = self.lm_head(hidden_states)\\n+            logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n         pass\\n         logits = logits.to(self.config.torch_dtype)\\n \\n@@ -1402,6 +1427,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n+                else: modules_to_save.append(\"lm_head\")\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1409,6 +1436,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n+                else: modules_to_save.append(\"embed_tokens\")\\n \\n             else:\\n                 assert(module in accepted_modules)\\n',\n",
       " '@@ -225,12 +225,12 @@ def MistralForCausalLM_fast_forward(\\n \\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n+    lm_head = self.lm_head.weight\\n     if bsz == 1 and q_len == 1:\\n-        lm_head = self.lm_head.weight\\n         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n-        logits = self.lm_head(hidden_states)\\n+        logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n     pass\\n     logits = logits.to(self.config.torch_dtype)\\n \\n',\n",
       " '@@ -33,9 +33,13 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n-# Check Kaggle\\n-IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n \\n+# Weights\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -177,6 +181,9 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if token is None and \"HF_TOKEN\" in os.environ:\\n+        token = os.environ[\"HF_TOKEN\"]\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -291,6 +298,10 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n             (\\n                 repo_id            = save_directory,\\n@@ -305,6 +316,9 @@ def unsloth_save_model(\\n                 commit_description = commit_description,\\n                 tags               = tags,\\n             )\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n         pass\\n \\n         if hasattr(model, \"config\"):\\n@@ -361,7 +375,16 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n+\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -449,12 +472,12 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    # Check if Kaggle, since only 20GB of Disk space allowed.\\n-    if IS_A_KAGGLE_ENVIRONMENT:\\n+    # Check if Kaggle or Colab, since only 20GB of Disk space allowed.\\n+    if IS_KAGGLE_ENVIRONMENT or IS_COLAB_ENVIRONMENT:\\n         # We free up 4GB of space\\n         logger.warning_once(\\n-            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n-            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+            \"Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\"\\n         )\\n         _free_cached_model(internal_model)\\n     pass\\n@@ -462,7 +485,10 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n+\\n+    torch_dtype = model.config.torch_dtype\\n+    # Check modules to save float32 dtype\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n@@ -495,7 +521,8 @@ def unsloth_save_model(\\n     pass\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n+    # Check for modules_to_save float32 dtype\\n+    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -552,7 +579,16 @@ def unsloth_save_model(\\n     # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+        # Set padding side to left for inference\\n+        old_padding_side = tokenizer.padding_side\\n+        tokenizer.padding_side = \"left\"\\n+\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+        # Revert back padding side\\n+        tokenizer.padding_side = old_padding_side\\n+            \\n         print(\" Done.\")\\n     else:\\n         print()\\n@@ -1216,7 +1252,7 @@ def unsloth_save_pretrained_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1237,7 +1273,7 @@ def unsloth_save_pretrained_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n@@ -1336,7 +1372,7 @@ def unsloth_push_to_hub_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1357,7 +1393,7 @@ def unsloth_push_to_hub_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n',\n",
       " '@@ -186,9 +186,6 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n pass\\n \\n \\n-global sentencepiece_model_pb2\\n-sentencepiece_model_pb2 = None\\n-\\n def fix_sentencepiece_tokenizer(\\n     old_tokenizer,\\n     new_tokenizer,\\n@@ -197,19 +194,7 @@ def fix_sentencepiece_tokenizer(\\n ):\\n     # From https://github.com/google/sentencepiece/issues/121\\n     # We need to manually edit the sentencepiece tokenizer!\\n-    global sentencepiece_model_pb2\\n-    if sentencepiece_model_pb2 is None:\\n-        try:\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        except:\\n-            if not os.path.exists(temporary_location):\\n-                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n-                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            pass\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        pass\\n+    from transformers.utils import sentencepiece_model_pb2\\n \\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n',\n",
       " '@@ -30,7 +30,7 @@ import numpy as np\\n import os\\n import psutil\\n \\n-__version__ = \"2024.3\"\\n+__version__ = \"2024.4\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n@@ -70,12 +70,13 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n+    \"Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n def prepare_model_for_kbit_training(\\n     model                      : Any,\\n-    use_gradient_checkpointing : bool = True,\\n+    use_gradient_checkpointing : Optional = True,\\n     use_reentrant              : Optional[bool] = True,\\n ) -> Any:\\n     \"\"\"\\n@@ -101,9 +102,23 @@ def prepare_model_for_kbit_training(\\n             param.requires_grad_(False)\\n     pass\\n \\n-    if use_gradient_checkpointing:\\n+    # Gradient checkpointing!\\n+    if use_gradient_checkpointing == \"offloaded\":\\n+\\n+        # Saves VRAM!\\n+        original_model = model\\n+        while hasattr(original_model, \"model\"):\\n+            original_model._offloaded_gradient_checkpointing = True\\n+            original_model = original_model.model\\n+        pass\\n+        original_model._offloaded_gradient_checkpointing = True\\n+        \\n         model.gradient_checkpointing_enable()\\n \\n+    elif use_gradient_checkpointing == True:\\n+        model.gradient_checkpointing_enable()\\n+    pass\\n+\\n     # If use_reentrant = True which is the Pytorch default, we just make the input requires_grad.\\n     if use_reentrant:\\n         if hasattr(model, \"enable_input_require_grads\"):\\n@@ -179,6 +194,7 @@ def get_statistics():\\n     try:\\n         from huggingface_hub import hf_hub_download\\n         from huggingface_hub.utils import disable_progress_bars, enable_progress_bars, are_progress_bars_disabled\\n+        import psutil\\n         n_cpus = psutil.cpu_count(logical = False)\\n \\n         keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n@@ -291,3 +307,35 @@ def prepare_n_gradient_checkpoints(\\n     _model._gradient_checkpointing_boundaries    = boundaries\\n     _model._gradient_checkpointing_use_reentrant = use_reentrant\\n pass\\n+\\n+\\n+class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+    \"\"\"\\n+    Saves VRAM by smartly offloading to RAM.\\n+    Tiny hit to performance, since we mask the movement via non blocking calls.\\n+    [TODO] Load the backward pass earlier\\n+    \"\"\"\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_fwd\\n+    def forward(ctx, forward_function, hidden_states, *args):\\n+        saved_hidden_states = hidden_states.to(\"cpu\", non_blocking = True)\\n+        with torch.no_grad():\\n+            (output,) = forward_function(hidden_states, *args)\\n+        ctx.save_for_backward(saved_hidden_states)\\n+        ctx.forward_function = forward_function\\n+        ctx.args = args\\n+        return output\\n+    pass\\n+\\n+    @staticmethod\\n+    @torch.cuda.amp.custom_bwd\\n+    def backward(ctx, dY):\\n+        (hidden_states,) = ctx.saved_tensors\\n+        hidden_states = hidden_states.to(\"cuda\", non_blocking = True).detach()\\n+        hidden_states.requires_grad = True\\n+        with torch.enable_grad():\\n+            (output,) = ctx.forward_function(hidden_states, *ctx.args)\\n+        torch.autograd.backward(output, dY)\\n+        return (None, hidden_states.grad,) + (None,)*len(ctx.args)\\n+    pass\\n+pass\\n',\n",
       " '@@ -628,19 +628,42 @@ def LlamaModel_fast_forward(\\n         boundaries = None\\n     pass\\n \\n+    # Check checkpointing method\\n+    gradient_checkpointing = False\\n+    offloaded_gradient_checkpointing = False\\n+\\n+    if (self.gradient_checkpointing and self.training and not use_cache):\\n+\\n+        gradient_checkpointing = True\\n+\\n+        if output_attentions is False and hasattr(self, \"_offloaded_gradient_checkpointing\"):\\n+            offloaded_gradient_checkpointing = True\\n+    pass\\n+\\n+    # Go through every layer!\\n     for idx, decoder_layer in enumerate(self.layers):\\n \\n         if output_hidden_states: all_hidden_states += (hidden_states,)\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n-        if self.gradient_checkpointing and self.training:\\n+        if offloaded_gradient_checkpointing:\\n+            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+                decoder_layer,\\n+                hidden_states,\\n+                causal_mask,\\n+                attention_mask,\\n+                position_ids,\\n+                past_key_values,\\n+                output_attentions,\\n+                use_cache,\\n+            )\\n \\n+        elif gradient_checkpointing:\\n             def create_custom_forward(module):\\n                 def custom_forward(*inputs):\\n-                    # None for past_key_value\\n-                    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\\n-\\n+                    return module(*inputs, past_key_value, output_attentions, padding_mask = padding_mask)\\n                 return custom_forward\\n+            pass\\n \\n             layer_outputs = torch.utils.checkpoint.checkpoint(\\n                 create_custom_forward(decoder_layer),\\n@@ -648,9 +671,11 @@ def LlamaModel_fast_forward(\\n                 causal_mask,\\n                 attention_mask,\\n                 position_ids,\\n-                use_reentrant=True,\\n-                preserve_rng_state=False,\\n+                use_reentrant = True,\\n+                preserve_rng_state = False,\\n             )\\n+            hidden_states = layer_outputs[0]\\n+\\n         else:\\n             layer_outputs = decoder_layer(\\n                 hidden_states,\\n@@ -662,9 +687,9 @@ def LlamaModel_fast_forward(\\n                 use_cache=use_cache,\\n                 padding_mask=padding_mask,\\n             )\\n+            hidden_states = layer_outputs[0]\\n         pass\\n \\n-        hidden_states = layer_outputs[0]\\n         if use_cache: next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\\n         if output_attentions: all_self_attns += (layer_outputs[1],)\\n     pass\\n@@ -801,12 +826,12 @@ def CausalLM_fast_forward(fast_forward_inference):\\n \\n         hidden_states = outputs[0]\\n         bsz, q_len, hd = hidden_states.shape\\n+        lm_head = self.lm_head.weight\\n         if bsz == 1 and q_len == 1:\\n-            lm_head = self.lm_head.weight\\n             logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n             logits = logits.unsqueeze(0).unsqueeze(0)\\n         else:\\n-            logits = self.lm_head(hidden_states)\\n+            logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n         pass\\n         logits = logits.to(self.config.torch_dtype)\\n \\n@@ -1402,6 +1427,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n+                if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n+                else: modules_to_save.append(\"lm_head\")\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n@@ -1409,6 +1436,8 @@ class FastLlamaModel:\\n                     \"We shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n+                if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n+                else: modules_to_save.append(\"embed_tokens\")\\n \\n             else:\\n                 assert(module in accepted_modules)\\n',\n",
       " '@@ -225,12 +225,12 @@ def MistralForCausalLM_fast_forward(\\n \\n     hidden_states = outputs[0]\\n     bsz, q_len, hd = hidden_states.shape\\n+    lm_head = self.lm_head.weight\\n     if bsz == 1 and q_len == 1:\\n-        lm_head = self.lm_head.weight\\n         logits = torch.mv(lm_head, hidden_states.ravel().to(lm_head.dtype))\\n         logits = logits.unsqueeze(0).unsqueeze(0)\\n     else:\\n-        logits = self.lm_head(hidden_states)\\n+        logits = self.lm_head(hidden_states.to(lm_head.dtype))\\n     pass\\n     logits = logits.to(self.config.torch_dtype)\\n \\n',\n",
       " '@@ -33,9 +33,13 @@ __all__ = [\\n     \"patch_saving_functions\",\\n ]\\n \\n-# Check Kaggle\\n-IS_A_KAGGLE_ENVIRONMENT = \"KAGGLE_CONTAINER_NAME\" in os.environ\\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n \\n+# Weights\\n LLAMA_WEIGHTS = (\\n     \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\\n     \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\",\\n@@ -177,6 +181,9 @@ def unsloth_save_model(\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n     maximum_memory_usage : float = 0.9,\\n ):\\n+    if token is None and \"HF_TOKEN\" in os.environ:\\n+        token = os.environ[\"HF_TOKEN\"]\\n+\\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n         commit_message += \" (Trained with Unsloth)\"\\n@@ -291,6 +298,10 @@ def unsloth_save_model(\\n             tags               = tags,\\n         )\\n         if tokenizer is not None:\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             getattr(tokenizer, \"original_push_to_hub\", tokenizer.push_to_hub)\\\\\\n             (\\n                 repo_id            = save_directory,\\n@@ -305,6 +316,9 @@ def unsloth_save_model(\\n                 commit_description = commit_description,\\n                 tags               = tags,\\n             )\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n         pass\\n \\n         if hasattr(model, \"config\"):\\n@@ -361,7 +375,16 @@ def unsloth_save_model(\\n \\n         if tokenizer is not None:\\n             print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+            # Set padding side to left for inference\\n+            old_padding_side = tokenizer.padding_side\\n+            tokenizer.padding_side = \"left\"\\n+\\n             tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+            # Revert back padding side\\n+            tokenizer.padding_side = old_padding_side\\n+\\n             print(\" Done.\")\\n         else:\\n             print()\\n@@ -449,12 +472,12 @@ def unsloth_save_model(\\n         os.makedirs(temporary_location)\\n     pass\\n \\n-    # Check if Kaggle, since only 20GB of Disk space allowed.\\n-    if IS_A_KAGGLE_ENVIRONMENT:\\n+    # Check if Kaggle or Colab, since only 20GB of Disk space allowed.\\n+    if IS_KAGGLE_ENVIRONMENT or IS_COLAB_ENVIRONMENT:\\n         # We free up 4GB of space\\n         logger.warning_once(\\n-            \"Unsloth: Kaggle only allows 20GB of disk space. We need to delete the downloaded\\\\n\"\\\\\\n-            \"model which will save 4GB of disk space, allowing you to save on Kaggle.\"\\n+            \"Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\\\\n\"\\\\\\n+            \"model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\"\\n         )\\n         _free_cached_model(internal_model)\\n     pass\\n@@ -462,7 +485,10 @@ def unsloth_save_model(\\n     # HF also uses a OrderedDict\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n-    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data\\n+\\n+    torch_dtype = model.config.torch_dtype\\n+    # Check modules to save float32 dtype\\n+    state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n     max_vram = int(torch.cuda.get_device_properties(0).total_memory * maximum_memory_usage)\\n \\n@@ -495,7 +521,8 @@ def unsloth_save_model(\\n     pass\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n-    state_dict[\"lm_head.weight\"]    = internal_model.lm_head.weight.data\\n+    # Check for modules_to_save float32 dtype\\n+    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -552,7 +579,16 @@ def unsloth_save_model(\\n     # Save tokenizer\\n     if tokenizer is not None:\\n         print(\"Unsloth: Saving tokenizer...\", end = \"\")\\n+\\n+        # Set padding side to left for inference\\n+        old_padding_side = tokenizer.padding_side\\n+        tokenizer.padding_side = \"left\"\\n+\\n         tokenizer.save_pretrained(**tokenizer_save_settings)\\n+\\n+        # Revert back padding side\\n+        tokenizer.padding_side = old_padding_side\\n+            \\n         print(\" Done.\")\\n     else:\\n         print()\\n@@ -1216,7 +1252,7 @@ def unsloth_save_pretrained_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1237,7 +1273,7 @@ def unsloth_save_pretrained_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n@@ -1336,7 +1372,7 @@ def unsloth_push_to_hub_gguf(\\n     # Non blocking install GGUF first\\n     if not os.path.exists(\"llama.cpp\"):\\n \\n-        if IS_A_KAGGLE_ENVIRONMENT:\\n+        if IS_KAGGLE_ENVIRONMENT:\\n             # Kaggle is weird - no blocking installs, and no CUDA?\\n             python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n             python_install.wait()\\n@@ -1357,7 +1393,7 @@ def unsloth_push_to_hub_gguf(\\n             makefile = None\\n         except:\\n             # Retry by recloning llama.cpp\\n-            if IS_A_KAGGLE_ENVIRONMENT:\\n+            if IS_KAGGLE_ENVIRONMENT:\\n                 # Kaggle is weird - no blocking installs, and no CUDA?\\n                 python_install = install_python_non_blocking([\"gguf\", \"protobuf\"])\\n                 python_install.wait()\\n',\n",
       " '@@ -186,9 +186,6 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n pass\\n \\n \\n-global sentencepiece_model_pb2\\n-sentencepiece_model_pb2 = None\\n-\\n def fix_sentencepiece_tokenizer(\\n     old_tokenizer,\\n     new_tokenizer,\\n@@ -197,19 +194,7 @@ def fix_sentencepiece_tokenizer(\\n ):\\n     # From https://github.com/google/sentencepiece/issues/121\\n     # We need to manually edit the sentencepiece tokenizer!\\n-    global sentencepiece_model_pb2\\n-    if sentencepiece_model_pb2 is None:\\n-        try:\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        except:\\n-            if not os.path.exists(temporary_location):\\n-                os.system(f\"git clone https://github.com/google/sentencepiece.git {temporary_location}\")\\n-                os.system(f\"cd {temporary_location}/src && protoc --python_out=. sentencepiece_model.proto\")\\n-            pass\\n-            import sentencepiece.sentencepiece_model_pb2 as _sentencepiece_model_pb2\\n-            sentencepiece_model_pb2 = _sentencepiece_model_pb2\\n-        pass\\n+    from transformers.utils import sentencepiece_model_pb2\\n \\n     if not os.path.exists(temporary_location):\\n         os.makedirs(temporary_location)\\n',\n",
       " '@@ -44,6 +44,7 @@ huggingface = [\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n     \"peft>=0.7.1\",\\n+    \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -170,6 +171,7 @@ colab-new = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab-no-deps = [\\n     \"accelerate>=0.26.1\",\\n@@ -177,6 +179,7 @@ colab-no-deps = [\\n     \"peft>=0.7.1\",\\n     \"xformers\",\\n     \"bitsandbytes\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab = [\\n     \"unsloth[cu121]\",\\n',\n",
       " '@@ -1068,17 +1068,39 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map              = device_map,\\n-            torch_dtype             = dtype,\\n-            quantization_config     = bnb_config,\\n-            token                   = token,\\n-            rope_scaling            = rope_scaling,\\n-            max_position_embeddings = max_position_embeddings,\\n-            trust_remote_code       = trust_remote_code,\\n-            **kwargs,\\n-        )\\n+        try:\\n+            model = AutoModelForCausalLM.from_pretrained(\\n+                model_name,\\n+                device_map              = device_map,\\n+                torch_dtype             = dtype,\\n+                quantization_config     = bnb_config,\\n+                token                   = token,\\n+                rope_scaling            = rope_scaling,\\n+                max_position_embeddings = max_position_embeddings,\\n+                trust_remote_code       = trust_remote_code,\\n+                **kwargs,\\n+            )\\n+        except Exception as error:\\n+            if \"rope_scaling\" in str(error):\\n+                if rope_scaling is not None:\\n+                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n+                pass\\n+\\n+                # Counteract missing rope_scaling\\n+                model = AutoModelForCausalLM.from_pretrained(\\n+                    model_name,\\n+                    device_map              = device_map,\\n+                    torch_dtype             = dtype,\\n+                    quantization_config     = bnb_config,\\n+                    token                   = token,\\n+                    max_position_embeddings = max_position_embeddings,\\n+                    trust_remote_code       = trust_remote_code,\\n+                    **kwargs,\\n+                )\\n+            else:\\n+                raise error\\n+            pass\\n+        pass\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n',\n",
       " '@@ -684,7 +684,7 @@ pass\\n \\n \\n def install_llama_cpp_make_non_blocking():\\n-    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n@@ -752,7 +752,7 @@ pass\\n \\n \\n def install_llama_cpp_blocking(use_cuda = True):\\n-    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+    use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n@@ -937,7 +937,7 @@ def save_to_gguf(\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n             \"Once that\\'s done, redo the quantization.\"\\n         )\\n     pass\\n@@ -966,7 +966,7 @@ def save_to_gguf(\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                 \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n         pass\\n',\n",
       " '@@ -31,6 +31,12 @@ IGNORED_TOKENIZER_CHECKING = frozenset((\\n     \"CodeLlamaTokenizer\",\\n ))\\n \\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n+\\n \\n def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n@@ -179,10 +185,19 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n         if x.endswith(\"_token\") and x.count(\"_\") == 1\\n     )))\\n     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n-    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n-        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n-        \"\".join(all_special_tokens)\\n-    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    try:\\n+        string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+            \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+            \"\".join(all_special_tokens)\\n+        return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    except:\\n+        # For eg see https://github.com/unslothai/unsloth/issues/292\\n+        # Sometimes tokenizer has weird tokens, causing a combined tokenization to fail.\\n+        # [TODO] We temporarily disable this for CodeLlama tokenizers\\n+        if slow_tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+            return True\\n+        else:\\n+            return False\\n pass\\n \\n \\n@@ -203,7 +218,6 @@ def fix_sentencepiece_tokenizer(\\n     # First save the old tokenizer\\n     old_tokenizer.save_pretrained(temporary_location)\\n \\n-    from sentencepiece import SentencePieceProcessor\\n     tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n     tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n \\n@@ -220,7 +234,11 @@ def fix_sentencepiece_tokenizer(\\n             continue\\n         pass\\n         ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        # [TODO] Hack for Starling - try except\\n+        try:\\n+            tokenizer_piece = tokenizer_file.pieces[ids]\\n+        except:\\n+            continue\\n         assert(tokenizer_piece.piece == old_token)\\n         tokenizer_piece.piece = new_token\\n     pass\\n@@ -243,7 +261,14 @@ def load_correct_tokenizer(\\n     padding_side = \"right\",\\n     token = None,\\n     trust_remote_code = False,\\n+    cache_dir = \"huggingface_tokenizers_cache\",\\n ):\\n+    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+        cache_dir = cache_dir\\n+    else:\\n+        cache_dir = None\\n+    pass\\n+\\n     slow_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -251,6 +276,7 @@ def load_correct_tokenizer(\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n         use_fast          = False,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n@@ -258,6 +284,7 @@ def load_correct_tokenizer(\\n         padding_side      = padding_side,\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n     fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n@@ -375,6 +402,12 @@ def check_tokenizer(\\n                 )\\n             pass\\n             \\n+            if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+                cache_dir = \"huggingface_tokenizers_cache\"\\n+            else:\\n+                cache_dir = None\\n+            pass\\n+\\n             # Try slow tokenizer which can fix things!\\n             tokenizer = AutoTokenizer.from_pretrained(\\n                 model_name,\\n@@ -382,6 +415,7 @@ def check_tokenizer(\\n                 padding_side = padding_side,\\n                 token = token,\\n                 use_fast = False,\\n+                cache_dir = cache_dir,\\n             )\\n             return check_tokenizer(\\n                 model = model,\\n',\n",
       " '@@ -44,6 +44,7 @@ huggingface = [\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n     \"peft>=0.7.1\",\\n+    \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -170,6 +171,7 @@ colab-new = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab-no-deps = [\\n     \"accelerate>=0.26.1\",\\n@@ -177,6 +179,7 @@ colab-no-deps = [\\n     \"peft>=0.7.1\",\\n     \"xformers\",\\n     \"bitsandbytes\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab = [\\n     \"unsloth[cu121]\",\\n',\n",
       " '@@ -1068,17 +1068,39 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map              = device_map,\\n-            torch_dtype             = dtype,\\n-            quantization_config     = bnb_config,\\n-            token                   = token,\\n-            rope_scaling            = rope_scaling,\\n-            max_position_embeddings = max_position_embeddings,\\n-            trust_remote_code       = trust_remote_code,\\n-            **kwargs,\\n-        )\\n+        try:\\n+            model = AutoModelForCausalLM.from_pretrained(\\n+                model_name,\\n+                device_map              = device_map,\\n+                torch_dtype             = dtype,\\n+                quantization_config     = bnb_config,\\n+                token                   = token,\\n+                rope_scaling            = rope_scaling,\\n+                max_position_embeddings = max_position_embeddings,\\n+                trust_remote_code       = trust_remote_code,\\n+                **kwargs,\\n+            )\\n+        except Exception as error:\\n+            if \"rope_scaling\" in str(error):\\n+                if rope_scaling is not None:\\n+                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n+                pass\\n+\\n+                # Counteract missing rope_scaling\\n+                model = AutoModelForCausalLM.from_pretrained(\\n+                    model_name,\\n+                    device_map              = device_map,\\n+                    torch_dtype             = dtype,\\n+                    quantization_config     = bnb_config,\\n+                    token                   = token,\\n+                    max_position_embeddings = max_position_embeddings,\\n+                    trust_remote_code       = trust_remote_code,\\n+                    **kwargs,\\n+                )\\n+            else:\\n+                raise error\\n+            pass\\n+        pass\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n',\n",
       " '@@ -684,7 +684,7 @@ pass\\n \\n \\n def install_llama_cpp_make_non_blocking():\\n-    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n@@ -752,7 +752,7 @@ pass\\n \\n \\n def install_llama_cpp_blocking(use_cuda = True):\\n-    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+    use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n@@ -937,7 +937,7 @@ def save_to_gguf(\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n             \"Once that\\'s done, redo the quantization.\"\\n         )\\n     pass\\n@@ -966,7 +966,7 @@ def save_to_gguf(\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                 \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n         pass\\n',\n",
       " '@@ -31,6 +31,12 @@ IGNORED_TOKENIZER_CHECKING = frozenset((\\n     \"CodeLlamaTokenizer\",\\n ))\\n \\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n+\\n \\n def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n@@ -179,10 +185,19 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n         if x.endswith(\"_token\") and x.count(\"_\") == 1\\n     )))\\n     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n-    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n-        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n-        \"\".join(all_special_tokens)\\n-    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    try:\\n+        string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+            \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+            \"\".join(all_special_tokens)\\n+        return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    except:\\n+        # For eg see https://github.com/unslothai/unsloth/issues/292\\n+        # Sometimes tokenizer has weird tokens, causing a combined tokenization to fail.\\n+        # [TODO] We temporarily disable this for CodeLlama tokenizers\\n+        if slow_tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+            return True\\n+        else:\\n+            return False\\n pass\\n \\n \\n@@ -203,7 +218,6 @@ def fix_sentencepiece_tokenizer(\\n     # First save the old tokenizer\\n     old_tokenizer.save_pretrained(temporary_location)\\n \\n-    from sentencepiece import SentencePieceProcessor\\n     tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n     tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n \\n@@ -220,7 +234,11 @@ def fix_sentencepiece_tokenizer(\\n             continue\\n         pass\\n         ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        # [TODO] Hack for Starling - try except\\n+        try:\\n+            tokenizer_piece = tokenizer_file.pieces[ids]\\n+        except:\\n+            continue\\n         assert(tokenizer_piece.piece == old_token)\\n         tokenizer_piece.piece = new_token\\n     pass\\n@@ -243,7 +261,14 @@ def load_correct_tokenizer(\\n     padding_side = \"right\",\\n     token = None,\\n     trust_remote_code = False,\\n+    cache_dir = \"huggingface_tokenizers_cache\",\\n ):\\n+    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+        cache_dir = cache_dir\\n+    else:\\n+        cache_dir = None\\n+    pass\\n+\\n     slow_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -251,6 +276,7 @@ def load_correct_tokenizer(\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n         use_fast          = False,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n@@ -258,6 +284,7 @@ def load_correct_tokenizer(\\n         padding_side      = padding_side,\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n     fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n@@ -375,6 +402,12 @@ def check_tokenizer(\\n                 )\\n             pass\\n             \\n+            if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+                cache_dir = \"huggingface_tokenizers_cache\"\\n+            else:\\n+                cache_dir = None\\n+            pass\\n+\\n             # Try slow tokenizer which can fix things!\\n             tokenizer = AutoTokenizer.from_pretrained(\\n                 model_name,\\n@@ -382,6 +415,7 @@ def check_tokenizer(\\n                 padding_side = padding_side,\\n                 token = token,\\n                 use_fast = False,\\n+                cache_dir = cache_dir,\\n             )\\n             return check_tokenizer(\\n                 model = model,\\n',\n",
       " '@@ -44,6 +44,7 @@ huggingface = [\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n     \"peft>=0.7.1\",\\n+    \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -170,6 +171,7 @@ colab-new = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab-no-deps = [\\n     \"accelerate>=0.26.1\",\\n@@ -177,6 +179,7 @@ colab-no-deps = [\\n     \"peft>=0.7.1\",\\n     \"xformers\",\\n     \"bitsandbytes\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab = [\\n     \"unsloth[cu121]\",\\n',\n",
       " '@@ -1068,17 +1068,39 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map              = device_map,\\n-            torch_dtype             = dtype,\\n-            quantization_config     = bnb_config,\\n-            token                   = token,\\n-            rope_scaling            = rope_scaling,\\n-            max_position_embeddings = max_position_embeddings,\\n-            trust_remote_code       = trust_remote_code,\\n-            **kwargs,\\n-        )\\n+        try:\\n+            model = AutoModelForCausalLM.from_pretrained(\\n+                model_name,\\n+                device_map              = device_map,\\n+                torch_dtype             = dtype,\\n+                quantization_config     = bnb_config,\\n+                token                   = token,\\n+                rope_scaling            = rope_scaling,\\n+                max_position_embeddings = max_position_embeddings,\\n+                trust_remote_code       = trust_remote_code,\\n+                **kwargs,\\n+            )\\n+        except Exception as error:\\n+            if \"rope_scaling\" in str(error):\\n+                if rope_scaling is not None:\\n+                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n+                pass\\n+\\n+                # Counteract missing rope_scaling\\n+                model = AutoModelForCausalLM.from_pretrained(\\n+                    model_name,\\n+                    device_map              = device_map,\\n+                    torch_dtype             = dtype,\\n+                    quantization_config     = bnb_config,\\n+                    token                   = token,\\n+                    max_position_embeddings = max_position_embeddings,\\n+                    trust_remote_code       = trust_remote_code,\\n+                    **kwargs,\\n+                )\\n+            else:\\n+                raise error\\n+            pass\\n+        pass\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n',\n",
       " '@@ -684,7 +684,7 @@ pass\\n \\n \\n def install_llama_cpp_make_non_blocking():\\n-    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n@@ -752,7 +752,7 @@ pass\\n \\n \\n def install_llama_cpp_blocking(use_cuda = True):\\n-    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+    use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n@@ -937,7 +937,7 @@ def save_to_gguf(\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n             \"Once that\\'s done, redo the quantization.\"\\n         )\\n     pass\\n@@ -966,7 +966,7 @@ def save_to_gguf(\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                 \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n         pass\\n',\n",
       " '@@ -31,6 +31,12 @@ IGNORED_TOKENIZER_CHECKING = frozenset((\\n     \"CodeLlamaTokenizer\",\\n ))\\n \\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n+\\n \\n def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n@@ -179,10 +185,19 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n         if x.endswith(\"_token\") and x.count(\"_\") == 1\\n     )))\\n     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n-    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n-        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n-        \"\".join(all_special_tokens)\\n-    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    try:\\n+        string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+            \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+            \"\".join(all_special_tokens)\\n+        return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    except:\\n+        # For eg see https://github.com/unslothai/unsloth/issues/292\\n+        # Sometimes tokenizer has weird tokens, causing a combined tokenization to fail.\\n+        # [TODO] We temporarily disable this for CodeLlama tokenizers\\n+        if slow_tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+            return True\\n+        else:\\n+            return False\\n pass\\n \\n \\n@@ -203,7 +218,6 @@ def fix_sentencepiece_tokenizer(\\n     # First save the old tokenizer\\n     old_tokenizer.save_pretrained(temporary_location)\\n \\n-    from sentencepiece import SentencePieceProcessor\\n     tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n     tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n \\n@@ -220,7 +234,11 @@ def fix_sentencepiece_tokenizer(\\n             continue\\n         pass\\n         ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        # [TODO] Hack for Starling - try except\\n+        try:\\n+            tokenizer_piece = tokenizer_file.pieces[ids]\\n+        except:\\n+            continue\\n         assert(tokenizer_piece.piece == old_token)\\n         tokenizer_piece.piece = new_token\\n     pass\\n@@ -243,7 +261,14 @@ def load_correct_tokenizer(\\n     padding_side = \"right\",\\n     token = None,\\n     trust_remote_code = False,\\n+    cache_dir = \"huggingface_tokenizers_cache\",\\n ):\\n+    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+        cache_dir = cache_dir\\n+    else:\\n+        cache_dir = None\\n+    pass\\n+\\n     slow_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -251,6 +276,7 @@ def load_correct_tokenizer(\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n         use_fast          = False,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n@@ -258,6 +284,7 @@ def load_correct_tokenizer(\\n         padding_side      = padding_side,\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n     fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n@@ -375,6 +402,12 @@ def check_tokenizer(\\n                 )\\n             pass\\n             \\n+            if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+                cache_dir = \"huggingface_tokenizers_cache\"\\n+            else:\\n+                cache_dir = None\\n+            pass\\n+\\n             # Try slow tokenizer which can fix things!\\n             tokenizer = AutoTokenizer.from_pretrained(\\n                 model_name,\\n@@ -382,6 +415,7 @@ def check_tokenizer(\\n                 padding_side = padding_side,\\n                 token = token,\\n                 use_fast = False,\\n+                cache_dir = cache_dir,\\n             )\\n             return check_tokenizer(\\n                 model = model,\\n',\n",
       " '@@ -44,6 +44,7 @@ huggingface = [\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n     \"peft>=0.7.1\",\\n+    \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -170,6 +171,7 @@ colab-new = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab-no-deps = [\\n     \"accelerate>=0.26.1\",\\n@@ -177,6 +179,7 @@ colab-no-deps = [\\n     \"peft>=0.7.1\",\\n     \"xformers\",\\n     \"bitsandbytes\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab = [\\n     \"unsloth[cu121]\",\\n',\n",
       " '@@ -1068,17 +1068,39 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map              = device_map,\\n-            torch_dtype             = dtype,\\n-            quantization_config     = bnb_config,\\n-            token                   = token,\\n-            rope_scaling            = rope_scaling,\\n-            max_position_embeddings = max_position_embeddings,\\n-            trust_remote_code       = trust_remote_code,\\n-            **kwargs,\\n-        )\\n+        try:\\n+            model = AutoModelForCausalLM.from_pretrained(\\n+                model_name,\\n+                device_map              = device_map,\\n+                torch_dtype             = dtype,\\n+                quantization_config     = bnb_config,\\n+                token                   = token,\\n+                rope_scaling            = rope_scaling,\\n+                max_position_embeddings = max_position_embeddings,\\n+                trust_remote_code       = trust_remote_code,\\n+                **kwargs,\\n+            )\\n+        except Exception as error:\\n+            if \"rope_scaling\" in str(error):\\n+                if rope_scaling is not None:\\n+                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n+                pass\\n+\\n+                # Counteract missing rope_scaling\\n+                model = AutoModelForCausalLM.from_pretrained(\\n+                    model_name,\\n+                    device_map              = device_map,\\n+                    torch_dtype             = dtype,\\n+                    quantization_config     = bnb_config,\\n+                    token                   = token,\\n+                    max_position_embeddings = max_position_embeddings,\\n+                    trust_remote_code       = trust_remote_code,\\n+                    **kwargs,\\n+                )\\n+            else:\\n+                raise error\\n+            pass\\n+        pass\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n',\n",
       " '@@ -684,7 +684,7 @@ pass\\n \\n \\n def install_llama_cpp_make_non_blocking():\\n-    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n@@ -752,7 +752,7 @@ pass\\n \\n \\n def install_llama_cpp_blocking(use_cuda = True):\\n-    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+    use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n@@ -937,7 +937,7 @@ def save_to_gguf(\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n             \"Once that\\'s done, redo the quantization.\"\\n         )\\n     pass\\n@@ -966,7 +966,7 @@ def save_to_gguf(\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                 \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n         pass\\n',\n",
       " '@@ -31,6 +31,12 @@ IGNORED_TOKENIZER_CHECKING = frozenset((\\n     \"CodeLlamaTokenizer\",\\n ))\\n \\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n+\\n \\n def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n@@ -179,10 +185,19 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n         if x.endswith(\"_token\") and x.count(\"_\") == 1\\n     )))\\n     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n-    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n-        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n-        \"\".join(all_special_tokens)\\n-    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    try:\\n+        string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+            \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+            \"\".join(all_special_tokens)\\n+        return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    except:\\n+        # For eg see https://github.com/unslothai/unsloth/issues/292\\n+        # Sometimes tokenizer has weird tokens, causing a combined tokenization to fail.\\n+        # [TODO] We temporarily disable this for CodeLlama tokenizers\\n+        if slow_tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+            return True\\n+        else:\\n+            return False\\n pass\\n \\n \\n@@ -203,7 +218,6 @@ def fix_sentencepiece_tokenizer(\\n     # First save the old tokenizer\\n     old_tokenizer.save_pretrained(temporary_location)\\n \\n-    from sentencepiece import SentencePieceProcessor\\n     tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n     tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n \\n@@ -220,7 +234,11 @@ def fix_sentencepiece_tokenizer(\\n             continue\\n         pass\\n         ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        # [TODO] Hack for Starling - try except\\n+        try:\\n+            tokenizer_piece = tokenizer_file.pieces[ids]\\n+        except:\\n+            continue\\n         assert(tokenizer_piece.piece == old_token)\\n         tokenizer_piece.piece = new_token\\n     pass\\n@@ -243,7 +261,14 @@ def load_correct_tokenizer(\\n     padding_side = \"right\",\\n     token = None,\\n     trust_remote_code = False,\\n+    cache_dir = \"huggingface_tokenizers_cache\",\\n ):\\n+    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+        cache_dir = cache_dir\\n+    else:\\n+        cache_dir = None\\n+    pass\\n+\\n     slow_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -251,6 +276,7 @@ def load_correct_tokenizer(\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n         use_fast          = False,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n@@ -258,6 +284,7 @@ def load_correct_tokenizer(\\n         padding_side      = padding_side,\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n     fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n@@ -375,6 +402,12 @@ def check_tokenizer(\\n                 )\\n             pass\\n             \\n+            if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+                cache_dir = \"huggingface_tokenizers_cache\"\\n+            else:\\n+                cache_dir = None\\n+            pass\\n+\\n             # Try slow tokenizer which can fix things!\\n             tokenizer = AutoTokenizer.from_pretrained(\\n                 model_name,\\n@@ -382,6 +415,7 @@ def check_tokenizer(\\n                 padding_side = padding_side,\\n                 token = token,\\n                 use_fast = False,\\n+                cache_dir = cache_dir,\\n             )\\n             return check_tokenizer(\\n                 model = model,\\n',\n",
       " '@@ -44,6 +44,7 @@ huggingface = [\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n     \"peft>=0.7.1\",\\n+    \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n     \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.22.post7%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n@@ -170,6 +171,7 @@ colab-new = [\\n     \"psutil\",\\n     \"wheel>=0.42.0\",\\n     \"numpy\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab-no-deps = [\\n     \"accelerate>=0.26.1\",\\n@@ -177,6 +179,7 @@ colab-no-deps = [\\n     \"peft>=0.7.1\",\\n     \"xformers\",\\n     \"bitsandbytes\",\\n+    \"protobuf<4.0.0\",\\n ]\\n colab = [\\n     \"unsloth[cu121]\",\\n',\n",
       " '@@ -1068,17 +1068,39 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map              = device_map,\\n-            torch_dtype             = dtype,\\n-            quantization_config     = bnb_config,\\n-            token                   = token,\\n-            rope_scaling            = rope_scaling,\\n-            max_position_embeddings = max_position_embeddings,\\n-            trust_remote_code       = trust_remote_code,\\n-            **kwargs,\\n-        )\\n+        try:\\n+            model = AutoModelForCausalLM.from_pretrained(\\n+                model_name,\\n+                device_map              = device_map,\\n+                torch_dtype             = dtype,\\n+                quantization_config     = bnb_config,\\n+                token                   = token,\\n+                rope_scaling            = rope_scaling,\\n+                max_position_embeddings = max_position_embeddings,\\n+                trust_remote_code       = trust_remote_code,\\n+                **kwargs,\\n+            )\\n+        except Exception as error:\\n+            if \"rope_scaling\" in str(error):\\n+                if rope_scaling is not None:\\n+                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n+                pass\\n+\\n+                # Counteract missing rope_scaling\\n+                model = AutoModelForCausalLM.from_pretrained(\\n+                    model_name,\\n+                    device_map              = device_map,\\n+                    torch_dtype             = dtype,\\n+                    quantization_config     = bnb_config,\\n+                    token                   = token,\\n+                    max_position_embeddings = max_position_embeddings,\\n+                    trust_remote_code       = trust_remote_code,\\n+                    **kwargs,\\n+                )\\n+            else:\\n+                raise error\\n+            pass\\n+        pass\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n',\n",
       " '@@ -684,7 +684,7 @@ pass\\n \\n \\n def install_llama_cpp_make_non_blocking():\\n-    env = { **os.environ, \"LLAMA_CUBLAS\": \"1\", }\\n+    env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n@@ -752,7 +752,7 @@ pass\\n \\n \\n def install_llama_cpp_blocking(use_cuda = True):\\n-    use_cuda = \"LLAMA_CUBLAS=1\" if use_cuda else \"\"\\n+    use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n@@ -937,7 +937,7 @@ def save_to_gguf(\\n             \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n             \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n             \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n             \"Once that\\'s done, redo the quantization.\"\\n         )\\n     pass\\n@@ -966,7 +966,7 @@ def save_to_gguf(\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                 \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUBLAS=1 make all -j\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n         pass\\n',\n",
       " '@@ -31,6 +31,12 @@ IGNORED_TOKENIZER_CHECKING = frozenset((\\n     \"CodeLlamaTokenizer\",\\n ))\\n \\n+# Check environments\\n+keynames = \"\\\\n\" + \"\\\\n\".join(os.environ.keys())\\n+IS_COLAB_ENVIRONMENT  = \"\\\\nCOLAB_\"  in keynames\\n+IS_KAGGLE_ENVIRONMENT = \"\\\\nKAGGLE_\" in keynames\\n+del keynames\\n+\\n \\n def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n@@ -179,10 +185,19 @@ def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n         if x.endswith(\"_token\") and x.count(\"_\") == 1\\n     )))\\n     all_special_tokens = list(set(special_tokens + slow_tokenizer.all_special_tokens))\\n-    string = \"\\\\n\".join(all_special_tokens) + \\\\\\n-        \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n-        \"\".join(all_special_tokens)\\n-    return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    try:\\n+        string = \"\\\\n\".join(all_special_tokens) + \\\\\\n+            \"A quick brown fox jumps over the lazy dog!!\\\\n\\\\n\" + \\\\\\n+            \"\".join(all_special_tokens)\\n+        return slow_tokenizer(string).input_ids == fast_tokenizer(string).input_ids\\n+    except:\\n+        # For eg see https://github.com/unslothai/unsloth/issues/292\\n+        # Sometimes tokenizer has weird tokens, causing a combined tokenization to fail.\\n+        # [TODO] We temporarily disable this for CodeLlama tokenizers\\n+        if slow_tokenizer.__repr__().split(\"(\", 1)[0] in IGNORED_TOKENIZER_CHECKING:\\n+            return True\\n+        else:\\n+            return False\\n pass\\n \\n \\n@@ -203,7 +218,6 @@ def fix_sentencepiece_tokenizer(\\n     # First save the old tokenizer\\n     old_tokenizer.save_pretrained(temporary_location)\\n \\n-    from sentencepiece import SentencePieceProcessor\\n     tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n     tokenizer_file.ParseFromString(open(f\"{temporary_location}/tokenizer.model\", \"rb\").read())\\n \\n@@ -220,7 +234,11 @@ def fix_sentencepiece_tokenizer(\\n             continue\\n         pass\\n         ids = ids[0]\\n-        tokenizer_piece = tokenizer_file.pieces[ids]\\n+        # [TODO] Hack for Starling - try except\\n+        try:\\n+            tokenizer_piece = tokenizer_file.pieces[ids]\\n+        except:\\n+            continue\\n         assert(tokenizer_piece.piece == old_token)\\n         tokenizer_piece.piece = new_token\\n     pass\\n@@ -243,7 +261,14 @@ def load_correct_tokenizer(\\n     padding_side = \"right\",\\n     token = None,\\n     trust_remote_code = False,\\n+    cache_dir = \"huggingface_tokenizers_cache\",\\n ):\\n+    if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+        cache_dir = cache_dir\\n+    else:\\n+        cache_dir = None\\n+    pass\\n+\\n     slow_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -251,6 +276,7 @@ def load_correct_tokenizer(\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n         use_fast          = False,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n@@ -258,6 +284,7 @@ def load_correct_tokenizer(\\n         padding_side      = padding_side,\\n         token             = token,\\n         trust_remote_code = trust_remote_code,\\n+        cache_dir         = cache_dir,\\n     )\\n     fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n     fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n@@ -375,6 +402,12 @@ def check_tokenizer(\\n                 )\\n             pass\\n             \\n+            if IS_COLAB_ENVIRONMENT or IS_KAGGLE_ENVIRONMENT:\\n+                cache_dir = \"huggingface_tokenizers_cache\"\\n+            else:\\n+                cache_dir = None\\n+            pass\\n+\\n             # Try slow tokenizer which can fix things!\\n             tokenizer = AutoTokenizer.from_pretrained(\\n                 model_name,\\n@@ -382,6 +415,7 @@ def check_tokenizer(\\n                 padding_side = padding_side,\\n                 token = token,\\n                 use_fast = False,\\n+                cache_dir = cache_dir,\\n             )\\n             return check_tokenizer(\\n                 model = model,\\n',\n",
       " '@@ -70,7 +70,7 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n-    \"Offloaded_Gradient_Checkpointer\",\\n+    \"Unsloth_Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n@@ -103,7 +103,7 @@ def prepare_model_for_kbit_training(\\n     pass\\n \\n     # Gradient checkpointing!\\n-    if use_gradient_checkpointing == \"offloaded\":\\n+    if use_gradient_checkpointing == \"unsloth\":\\n \\n         # Saves VRAM!\\n         original_model = model\\n@@ -309,11 +309,10 @@ def prepare_n_gradient_checkpoints(\\n pass\\n \\n \\n-class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n     \"\"\"\\n     Saves VRAM by smartly offloading to RAM.\\n     Tiny hit to performance, since we mask the movement via non blocking calls.\\n-    [TODO] Load the backward pass earlier\\n     \"\"\"\\n     @staticmethod\\n     @torch.cuda.amp.custom_fwd\\n',\n",
       " '@@ -647,7 +647,7 @@ def LlamaModel_fast_forward(\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if offloaded_gradient_checkpointing:\\n-            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+            hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(\\n                 decoder_layer,\\n                 hidden_states,\\n                 causal_mask,\\n',\n",
       " '@@ -93,7 +93,27 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/mistral-7b-v0.2-bnb-4bit\" : (\\n         \"unsloth/mistral-7b-v0.2\",\\n         \"alpindale/Mistral-7B-v0.2-hf\",\\n-    )\\n+    ),\\n+    \"unsloth/gemma-1.1-2b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-1.1-2b-it\",\\n+        \"google/gemma-1.1-2b-it\",\\n+    ),\\n+    \"unsloth/gemma-1.1-7b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-1.1-7b-it\",\\n+        \"google/gemma-1.1-7b-it\",\\n+    ),\\n+    \"unsloth/Starling-LM-7B-beta-bnb-4bit\" : (\\n+        \"unsloth/Starling-LM-7B-beta\",\\n+        \"Nexusflow/Starling-LM-7B-beta\",\\n+    ),\\n+    \"unsloth/Hermes-2-Pro-Mistral-7B-bnb-4bit\" : (\\n+        \"unsloth/Hermes-2-Pro-Mistral-7B\",\\n+        \"NousResearch/Hermes-2-Pro-Mistral-7B\",\\n+    ),\\n+    \"unsloth/OpenHermes-2.5-Mistral-7B-bnb-4bit\" : (\\n+        \"unsloth/OpenHermes-2.5-Mistral-7B\",\\n+        \"teknium/OpenHermes-2.5-Mistral-7B\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -183,6 +183,9 @@ def unsloth_save_model(\\n ):\\n     if token is None and \"HF_TOKEN\" in os.environ:\\n         token = os.environ[\"HF_TOKEN\"]\\n+    \\n+    if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+        token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n \\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n@@ -522,7 +525,11 @@ def unsloth_save_model(\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n     # Check for modules_to_save float32 dtype\\n-    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n+\\n+    # Check for tied weights\\n+    if internal_model.model.embed_tokens.weight.data_ptr() != internal_model.lm_head.weight.data_ptr():\\n+        state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n+    pass\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -731,9 +738,9 @@ def install_llama_cpp_old(version = -10):\\n     # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n-        \"pip install gguf protobuf\",\\n+        f\"cd llama.cpp && git reset --hard {version} && git clean -df\",\\n+        \"make clean -C llama.cpp\",\\n+        f\"make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n@@ -756,7 +763,8 @@ def install_llama_cpp_blocking(use_cuda = True):\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n+        \"make clean -C llama.cpp\",\\n+        f\"{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -931,15 +939,26 @@ def save_to_gguf(\\n \\n     # Check if quantization succeeded!\\n     if not os.path.isfile(final_location):\\n-        raise RuntimeError(\\n-            f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n-            \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n-            \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n-            \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-            \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n-            \"Once that\\'s done, redo the quantization.\"\\n-        )\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            raise RuntimeError(\\n+                f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                \"You are in a Kaggle environment, which might be the reason this is failing.\\\\n\"\\\\\\n+                \"Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\\\\n\"\\\\\\n+                \"This means using `model.{save_pretrained/push_to_hub}_merged` works, but\\\\n\"\\\\\\n+                \"`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\\\\n\"\\\\\\n+                \"I suggest you to save the 16bit model first, then use manual llama.cpp conversion.\"\\n+            )\\n+        else:\\n+            raise RuntimeError(\\n+                f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                \"Once that\\'s done, redo the quantization.\"\\n+            )\\n+        pass\\n     pass\\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n@@ -961,14 +980,25 @@ def save_to_gguf(\\n \\n         # Check if quantization succeeded!\\n         if not os.path.isfile(final_location):\\n-            raise RuntimeError(\\n-                \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n-                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n-                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n-                \"Once that\\'s done, redo the quantization.\"\\n-            )\\n+            if IS_KAGGLE_ENVIRONMENT:\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                    \"You are in a Kaggle environment, which might be the reason this is failing.\\\\n\"\\\\\\n+                    \"Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\\\\n\"\\\\\\n+                    \"This means using `model.{save_pretrained/push_to_hub}_merged` works, but\\\\n\"\\\\\\n+                    \"`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\\\\n\"\\\\\\n+                    \"I suggest you to save the 16bit model first, then use manual llama.cpp conversion.\"\\n+                )\\n+            else:\\n+                raise RuntimeError(\\n+                    \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                    \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                    \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                    \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                    \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                    \"Once that\\'s done, redo the quantization.\"\\n+                )\\n+            pass\\n         pass\\n \\n         print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n',\n",
       " '@@ -70,7 +70,7 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n-    \"Offloaded_Gradient_Checkpointer\",\\n+    \"Unsloth_Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n@@ -103,7 +103,7 @@ def prepare_model_for_kbit_training(\\n     pass\\n \\n     # Gradient checkpointing!\\n-    if use_gradient_checkpointing == \"offloaded\":\\n+    if use_gradient_checkpointing == \"unsloth\":\\n \\n         # Saves VRAM!\\n         original_model = model\\n@@ -309,11 +309,10 @@ def prepare_n_gradient_checkpoints(\\n pass\\n \\n \\n-class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n     \"\"\"\\n     Saves VRAM by smartly offloading to RAM.\\n     Tiny hit to performance, since we mask the movement via non blocking calls.\\n-    [TODO] Load the backward pass earlier\\n     \"\"\"\\n     @staticmethod\\n     @torch.cuda.amp.custom_fwd\\n',\n",
       " '@@ -647,7 +647,7 @@ def LlamaModel_fast_forward(\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if offloaded_gradient_checkpointing:\\n-            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+            hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(\\n                 decoder_layer,\\n                 hidden_states,\\n                 causal_mask,\\n',\n",
       " '@@ -93,7 +93,27 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/mistral-7b-v0.2-bnb-4bit\" : (\\n         \"unsloth/mistral-7b-v0.2\",\\n         \"alpindale/Mistral-7B-v0.2-hf\",\\n-    )\\n+    ),\\n+    \"unsloth/gemma-1.1-2b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-1.1-2b-it\",\\n+        \"google/gemma-1.1-2b-it\",\\n+    ),\\n+    \"unsloth/gemma-1.1-7b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-1.1-7b-it\",\\n+        \"google/gemma-1.1-7b-it\",\\n+    ),\\n+    \"unsloth/Starling-LM-7B-beta-bnb-4bit\" : (\\n+        \"unsloth/Starling-LM-7B-beta\",\\n+        \"Nexusflow/Starling-LM-7B-beta\",\\n+    ),\\n+    \"unsloth/Hermes-2-Pro-Mistral-7B-bnb-4bit\" : (\\n+        \"unsloth/Hermes-2-Pro-Mistral-7B\",\\n+        \"NousResearch/Hermes-2-Pro-Mistral-7B\",\\n+    ),\\n+    \"unsloth/OpenHermes-2.5-Mistral-7B-bnb-4bit\" : (\\n+        \"unsloth/OpenHermes-2.5-Mistral-7B\",\\n+        \"teknium/OpenHermes-2.5-Mistral-7B\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -183,6 +183,9 @@ def unsloth_save_model(\\n ):\\n     if token is None and \"HF_TOKEN\" in os.environ:\\n         token = os.environ[\"HF_TOKEN\"]\\n+    \\n+    if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+        token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n \\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n@@ -522,7 +525,11 @@ def unsloth_save_model(\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n     # Check for modules_to_save float32 dtype\\n-    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n+\\n+    # Check for tied weights\\n+    if internal_model.model.embed_tokens.weight.data_ptr() != internal_model.lm_head.weight.data_ptr():\\n+        state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n+    pass\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -731,9 +738,9 @@ def install_llama_cpp_old(version = -10):\\n     # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n-        \"pip install gguf protobuf\",\\n+        f\"cd llama.cpp && git reset --hard {version} && git clean -df\",\\n+        \"make clean -C llama.cpp\",\\n+        f\"make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n@@ -756,7 +763,8 @@ def install_llama_cpp_blocking(use_cuda = True):\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n+        \"make clean -C llama.cpp\",\\n+        f\"{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -931,15 +939,26 @@ def save_to_gguf(\\n \\n     # Check if quantization succeeded!\\n     if not os.path.isfile(final_location):\\n-        raise RuntimeError(\\n-            f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n-            \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n-            \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n-            \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-            \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n-            \"Once that\\'s done, redo the quantization.\"\\n-        )\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            raise RuntimeError(\\n+                f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                \"You are in a Kaggle environment, which might be the reason this is failing.\\\\n\"\\\\\\n+                \"Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\\\\n\"\\\\\\n+                \"This means using `model.{save_pretrained/push_to_hub}_merged` works, but\\\\n\"\\\\\\n+                \"`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\\\\n\"\\\\\\n+                \"I suggest you to save the 16bit model first, then use manual llama.cpp conversion.\"\\n+            )\\n+        else:\\n+            raise RuntimeError(\\n+                f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                \"Once that\\'s done, redo the quantization.\"\\n+            )\\n+        pass\\n     pass\\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n@@ -961,14 +980,25 @@ def save_to_gguf(\\n \\n         # Check if quantization succeeded!\\n         if not os.path.isfile(final_location):\\n-            raise RuntimeError(\\n-                \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n-                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n-                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n-                \"Once that\\'s done, redo the quantization.\"\\n-            )\\n+            if IS_KAGGLE_ENVIRONMENT:\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                    \"You are in a Kaggle environment, which might be the reason this is failing.\\\\n\"\\\\\\n+                    \"Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\\\\n\"\\\\\\n+                    \"This means using `model.{save_pretrained/push_to_hub}_merged` works, but\\\\n\"\\\\\\n+                    \"`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\\\\n\"\\\\\\n+                    \"I suggest you to save the 16bit model first, then use manual llama.cpp conversion.\"\\n+                )\\n+            else:\\n+                raise RuntimeError(\\n+                    \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                    \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                    \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                    \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                    \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                    \"Once that\\'s done, redo the quantization.\"\\n+                )\\n+            pass\\n         pass\\n \\n         print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n',\n",
       " '@@ -70,7 +70,7 @@ __all__ = [\\n     \"platform_system\",\\n     \"patch_tokenizer\",\\n     \"get_statistics\",\\n-    \"Offloaded_Gradient_Checkpointer\",\\n+    \"Unsloth_Offloaded_Gradient_Checkpointer\",\\n ]\\n \\n \\n@@ -103,7 +103,7 @@ def prepare_model_for_kbit_training(\\n     pass\\n \\n     # Gradient checkpointing!\\n-    if use_gradient_checkpointing == \"offloaded\":\\n+    if use_gradient_checkpointing == \"unsloth\":\\n \\n         # Saves VRAM!\\n         original_model = model\\n@@ -309,11 +309,10 @@ def prepare_n_gradient_checkpoints(\\n pass\\n \\n \\n-class Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n+class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n     \"\"\"\\n     Saves VRAM by smartly offloading to RAM.\\n     Tiny hit to performance, since we mask the movement via non blocking calls.\\n-    [TODO] Load the backward pass earlier\\n     \"\"\"\\n     @staticmethod\\n     @torch.cuda.amp.custom_fwd\\n',\n",
       " '@@ -647,7 +647,7 @@ def LlamaModel_fast_forward(\\n         past_key_value = past_key_values[idx] if past_key_values is not None else None\\n \\n         if offloaded_gradient_checkpointing:\\n-            hidden_states = Offloaded_Gradient_Checkpointer.apply(\\n+            hidden_states = Unsloth_Offloaded_Gradient_Checkpointer.apply(\\n                 decoder_layer,\\n                 hidden_states,\\n                 causal_mask,\\n',\n",
       " '@@ -93,7 +93,27 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/mistral-7b-v0.2-bnb-4bit\" : (\\n         \"unsloth/mistral-7b-v0.2\",\\n         \"alpindale/Mistral-7B-v0.2-hf\",\\n-    )\\n+    ),\\n+    \"unsloth/gemma-1.1-2b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-1.1-2b-it\",\\n+        \"google/gemma-1.1-2b-it\",\\n+    ),\\n+    \"unsloth/gemma-1.1-7b-it-bnb-4bit\" : (\\n+        \"unsloth/gemma-1.1-7b-it\",\\n+        \"google/gemma-1.1-7b-it\",\\n+    ),\\n+    \"unsloth/Starling-LM-7B-beta-bnb-4bit\" : (\\n+        \"unsloth/Starling-LM-7B-beta\",\\n+        \"Nexusflow/Starling-LM-7B-beta\",\\n+    ),\\n+    \"unsloth/Hermes-2-Pro-Mistral-7B-bnb-4bit\" : (\\n+        \"unsloth/Hermes-2-Pro-Mistral-7B\",\\n+        \"NousResearch/Hermes-2-Pro-Mistral-7B\",\\n+    ),\\n+    \"unsloth/OpenHermes-2.5-Mistral-7B-bnb-4bit\" : (\\n+        \"unsloth/OpenHermes-2.5-Mistral-7B\",\\n+        \"teknium/OpenHermes-2.5-Mistral-7B\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -183,6 +183,9 @@ def unsloth_save_model(\\n ):\\n     if token is None and \"HF_TOKEN\" in os.environ:\\n         token = os.environ[\"HF_TOKEN\"]\\n+    \\n+    if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+        token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n \\n     if commit_message is None: commit_message = \"\"\\n     if \"Unsloth\" not in commit_message:\\n@@ -522,7 +525,11 @@ def unsloth_save_model(\\n \\n     state_dict[\"model.norm.weight\"] = internal_model.model.norm.weight.data\\n     # Check for modules_to_save float32 dtype\\n-    state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n+\\n+    # Check for tied weights\\n+    if internal_model.model.embed_tokens.weight.data_ptr() != internal_model.lm_head.weight.data_ptr():\\n+        state_dict[\"lm_head.weight\"] = internal_model.lm_head.weight.data.to(torch_dtype)\\n+    pass\\n \\n     # All tensors MUST be type torch.Tensor and not torch.nn.parameter.Parameter\\n     for key, value in state_dict.items():\\n@@ -731,9 +738,9 @@ def install_llama_cpp_old(version = -10):\\n     # Also don\\'t use the GPU!\\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && git reset --hard {version} && git clean -df && \"\\\\\\n-        f\"make clean make all -j{psutil.cpu_count()*2}\",\\n-        \"pip install gguf protobuf\",\\n+        f\"cd llama.cpp && git reset --hard {version} && git clean -df\",\\n+        \"make clean -C llama.cpp\",\\n+        f\"make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n     ]\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n@@ -756,7 +763,8 @@ def install_llama_cpp_blocking(use_cuda = True):\\n \\n     commands = [\\n         \"git clone https://github.com/ggerganov/llama.cpp\",\\n-        f\"cd llama.cpp && make clean && {use_cuda} make all -j{psutil.cpu_count()*2}\",\\n+        \"make clean -C llama.cpp\",\\n+        f\"{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -931,15 +939,26 @@ def save_to_gguf(\\n \\n     # Check if quantization succeeded!\\n     if not os.path.isfile(final_location):\\n-        raise RuntimeError(\\n-            f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n-            \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n-            \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n-            \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-            \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-            \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n-            \"Once that\\'s done, redo the quantization.\"\\n-        )\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            raise RuntimeError(\\n+                f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                \"You are in a Kaggle environment, which might be the reason this is failing.\\\\n\"\\\\\\n+                \"Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\\\\n\"\\\\\\n+                \"This means using `model.{save_pretrained/push_to_hub}_merged` works, but\\\\n\"\\\\\\n+                \"`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\\\\n\"\\\\\\n+                \"I suggest you to save the 16bit model first, then use manual llama.cpp conversion.\"\\n+            )\\n+        else:\\n+            raise RuntimeError(\\n+                f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                \"Once that\\'s done, redo the quantization.\"\\n+            )\\n+        pass\\n     pass\\n     print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n \\n@@ -961,14 +980,25 @@ def save_to_gguf(\\n \\n         # Check if quantization succeeded!\\n         if not os.path.isfile(final_location):\\n-            raise RuntimeError(\\n-                \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n-                \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n-                \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n-                \"Once that\\'s done, redo the quantization.\"\\n-            )\\n+            if IS_KAGGLE_ENVIRONMENT:\\n+                raise RuntimeError(\\n+                    f\"Unsloth: Quantization failed for {final_location}\\\\n\"\\\\\\n+                    \"You are in a Kaggle environment, which might be the reason this is failing.\\\\n\"\\\\\\n+                    \"Kaggle only provides 20GB of disk space. Merging to 16bit for 7b models use 16GB of space.\\\\n\"\\\\\\n+                    \"This means using `model.{save_pretrained/push_to_hub}_merged` works, but\\\\n\"\\\\\\n+                    \"`model.{save_pretrained/push_to_hub}_gguf will use too much disk space.\\\\n\"\\\\\\n+                    \"I suggest you to save the 16bit model first, then use manual llama.cpp conversion.\"\\n+                )\\n+            else:\\n+                raise RuntimeError(\\n+                    \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n+                    \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n+                    \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n+                    \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                    \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                    \"Once that\\'s done, redo the quantization.\"\\n+                )\\n+            pass\\n         pass\\n \\n         print(f\"Unsloth: Conversion completed! Output location: {final_location}\")\\n',\n",
       " '@@ -1702,6 +1702,11 @@ class FastLlamaModel:\\n         lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n+        \\n+        if type(dtype) is str:\\n+            if   dtype ==  \"float16\": dtype = torch.float16\\n+            elif dtype == \"bfloat16\": dtype = torch.bfloat16\\n+        pass\\n \\n         # Wrap model.generate\\n         model._unwrapped_old_generate = model.generate\\n',\n",
       " '@@ -183,7 +183,7 @@ def unsloth_save_model(\\n ):\\n     if token is None and \"HF_TOKEN\" in os.environ:\\n         token = os.environ[\"HF_TOKEN\"]\\n-    \\n+\\n     if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n         token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n \\n@@ -489,7 +489,12 @@ def unsloth_save_model(\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n \\n-    torch_dtype = model.config.torch_dtype\\n+    torch_dtype = internal_model.config.torch_dtype\\n+    if type(torch_dtype) is str:\\n+        if   torch_dtype ==  \"float16\": torch_dtype = torch.float16\\n+        elif torch_dtype == \"bfloat16\": torch_dtype = torch.bfloat16\\n+    pass\\n+\\n     # Check modules to save float32 dtype\\n     state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n',\n",
       " '@@ -1702,6 +1702,11 @@ class FastLlamaModel:\\n         lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n+        \\n+        if type(dtype) is str:\\n+            if   dtype ==  \"float16\": dtype = torch.float16\\n+            elif dtype == \"bfloat16\": dtype = torch.bfloat16\\n+        pass\\n \\n         # Wrap model.generate\\n         model._unwrapped_old_generate = model.generate\\n',\n",
       " '@@ -183,7 +183,7 @@ def unsloth_save_model(\\n ):\\n     if token is None and \"HF_TOKEN\" in os.environ:\\n         token = os.environ[\"HF_TOKEN\"]\\n-    \\n+\\n     if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n         token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n \\n@@ -489,7 +489,12 @@ def unsloth_save_model(\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n \\n-    torch_dtype = model.config.torch_dtype\\n+    torch_dtype = internal_model.config.torch_dtype\\n+    if type(torch_dtype) is str:\\n+        if   torch_dtype ==  \"float16\": torch_dtype = torch.float16\\n+        elif torch_dtype == \"bfloat16\": torch_dtype = torch.bfloat16\\n+    pass\\n+\\n     # Check modules to save float32 dtype\\n     state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n',\n",
       " '@@ -1702,6 +1702,11 @@ class FastLlamaModel:\\n         lm_head = internal_model.lm_head.weight\\n         device_type = lm_head.device.type\\n         dtype = model.config.torch_dtype\\n+        \\n+        if type(dtype) is str:\\n+            if   dtype ==  \"float16\": dtype = torch.float16\\n+            elif dtype == \"bfloat16\": dtype = torch.bfloat16\\n+        pass\\n \\n         # Wrap model.generate\\n         model._unwrapped_old_generate = model.generate\\n',\n",
       " '@@ -183,7 +183,7 @@ def unsloth_save_model(\\n ):\\n     if token is None and \"HF_TOKEN\" in os.environ:\\n         token = os.environ[\"HF_TOKEN\"]\\n-    \\n+\\n     if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n         token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n \\n@@ -489,7 +489,12 @@ def unsloth_save_model(\\n     from collections import OrderedDict\\n     state_dict = OrderedDict()\\n \\n-    torch_dtype = model.config.torch_dtype\\n+    torch_dtype = internal_model.config.torch_dtype\\n+    if type(torch_dtype) is str:\\n+        if   torch_dtype ==  \"float16\": torch_dtype = torch.float16\\n+        elif torch_dtype == \"bfloat16\": torch_dtype = torch.bfloat16\\n+    pass\\n+\\n     # Check modules to save float32 dtype\\n     state_dict[\"model.embed_tokens.weight\"] = internal_model.model.embed_tokens.weight.data.to(torch_dtype)\\n \\n',\n",
       " '@@ -114,6 +114,22 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/OpenHermes-2.5-Mistral-7B\",\\n         \"teknium/OpenHermes-2.5-Mistral-7B\",\\n     ),\\n+    \"unsloth/codegemma-2b-bnb-4bit\" : (\\n+        \"unsloth/codegemma-2b\",\\n+        \"google/codegemma-2b\",\\n+    ),\\n+    \"unsloth/codegemma-7b-bnb-4bit\" : (\\n+        \"unsloth/codegemma-7b\",\\n+        \"google/codegemma-7b\",\\n+    ),\\n+    \"unsloth/codegemma-2b-it-bnb-4bit\" : (\\n+        \"unsloth/codegemma-2b-it\",\\n+        \"google/codegemma-2b-it\",\\n+    ),\\n+    \"unsloth/codegemma-7b-it-bnb-4bit\" : (\\n+        \"unsloth/codegemma-7b-it\",\\n+        \"google/codegemma-7b-it\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -114,6 +114,22 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/OpenHermes-2.5-Mistral-7B\",\\n         \"teknium/OpenHermes-2.5-Mistral-7B\",\\n     ),\\n+    \"unsloth/codegemma-2b-bnb-4bit\" : (\\n+        \"unsloth/codegemma-2b\",\\n+        \"google/codegemma-2b\",\\n+    ),\\n+    \"unsloth/codegemma-7b-bnb-4bit\" : (\\n+        \"unsloth/codegemma-7b\",\\n+        \"google/codegemma-7b\",\\n+    ),\\n+    \"unsloth/codegemma-2b-it-bnb-4bit\" : (\\n+        \"unsloth/codegemma-2b-it\",\\n+        \"google/codegemma-2b-it\",\\n+    ),\\n+    \"unsloth/codegemma-7b-it-bnb-4bit\" : (\\n+        \"unsloth/codegemma-7b-it\",\\n+        \"google/codegemma-7b-it\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -114,6 +114,22 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/OpenHermes-2.5-Mistral-7B\",\\n         \"teknium/OpenHermes-2.5-Mistral-7B\",\\n     ),\\n+    \"unsloth/codegemma-2b-bnb-4bit\" : (\\n+        \"unsloth/codegemma-2b\",\\n+        \"google/codegemma-2b\",\\n+    ),\\n+    \"unsloth/codegemma-7b-bnb-4bit\" : (\\n+        \"unsloth/codegemma-7b\",\\n+        \"google/codegemma-7b\",\\n+    ),\\n+    \"unsloth/codegemma-2b-it-bnb-4bit\" : (\\n+        \"unsloth/codegemma-2b-it\",\\n+        \"google/codegemma-2b-it\",\\n+    ),\\n+    \"unsloth/codegemma-7b-it-bnb-4bit\" : (\\n+        \"unsloth/codegemma-7b-it\",\\n+        \"google/codegemma-7b-it\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -95,11 +95,22 @@ def prepare_model_for_kbit_training(\\n     \"\"\"\\n \\n     # Freeze all parameters except LoRA\\n-    for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n-            param.requires_grad_(True)\\n-        else:\\n-            param.requires_grad_(False)\\n+    import re\\n+    with torch.inference_mode():\\n+        for name, param in model.named_parameters():\\n+            if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n+                param.requires_grad_(True)\\n+                # Also must be in float32!\\n+                if param.dtype != torch.float32:\\n+                    name = name.replace(\"base_model\", \"model\", 1)\\n+                    layer_number = re.search(r\"\\\\.[\\\\d]{1,}\\\\.\", name).group(0)\\n+                    name = name.replace(layer_number, f\"[{layer_number[1:-1]}].\")\\n+                    name = name.replace(\".weight\", \"\", 1)\\n+                    exec(f\"{name}.to(torch.float32)\")\\n+                pass\\n+            else:\\n+                param.requires_grad_(False)\\n+        pass\\n     pass\\n \\n     # Gradient checkpointing!\\n',\n",
       " '@@ -1030,7 +1030,7 @@ class FastLlamaModel:\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n         model_patcher.pre_patch()\\n-        get_statistics()\\n+        # get_statistics()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n',\n",
       " '@@ -122,10 +122,6 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/codegemma-7b\",\\n         \"google/codegemma-7b\",\\n     ),\\n-    \"unsloth/codegemma-2b-it-bnb-4bit\" : (\\n-        \"unsloth/codegemma-2b-it\",\\n-        \"google/codegemma-2b-it\",\\n-    ),\\n     \"unsloth/codegemma-7b-it-bnb-4bit\" : (\\n         \"unsloth/codegemma-7b-it\",\\n         \"google/codegemma-7b-it\",\\n',\n",
       " '@@ -319,7 +319,7 @@ class FastMistralModel(FastLlamaModel):\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n         model_patcher.pre_patch()\\n-        get_statistics()\\n+        # get_statistics()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n',\n",
       " '@@ -95,11 +95,22 @@ def prepare_model_for_kbit_training(\\n     \"\"\"\\n \\n     # Freeze all parameters except LoRA\\n-    for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n-            param.requires_grad_(True)\\n-        else:\\n-            param.requires_grad_(False)\\n+    import re\\n+    with torch.inference_mode():\\n+        for name, param in model.named_parameters():\\n+            if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n+                param.requires_grad_(True)\\n+                # Also must be in float32!\\n+                if param.dtype != torch.float32:\\n+                    name = name.replace(\"base_model\", \"model\", 1)\\n+                    layer_number = re.search(r\"\\\\.[\\\\d]{1,}\\\\.\", name).group(0)\\n+                    name = name.replace(layer_number, f\"[{layer_number[1:-1]}].\")\\n+                    name = name.replace(\".weight\", \"\", 1)\\n+                    exec(f\"{name}.to(torch.float32)\")\\n+                pass\\n+            else:\\n+                param.requires_grad_(False)\\n+        pass\\n     pass\\n \\n     # Gradient checkpointing!\\n',\n",
       " '@@ -1030,7 +1030,7 @@ class FastLlamaModel:\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n         model_patcher.pre_patch()\\n-        get_statistics()\\n+        # get_statistics()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n',\n",
       " '@@ -122,10 +122,6 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/codegemma-7b\",\\n         \"google/codegemma-7b\",\\n     ),\\n-    \"unsloth/codegemma-2b-it-bnb-4bit\" : (\\n-        \"unsloth/codegemma-2b-it\",\\n-        \"google/codegemma-2b-it\",\\n-    ),\\n     \"unsloth/codegemma-7b-it-bnb-4bit\" : (\\n         \"unsloth/codegemma-7b-it\",\\n         \"google/codegemma-7b-it\",\\n',\n",
       " '@@ -319,7 +319,7 @@ class FastMistralModel(FastLlamaModel):\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n         model_patcher.pre_patch()\\n-        get_statistics()\\n+        # get_statistics()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n',\n",
       " '@@ -95,11 +95,22 @@ def prepare_model_for_kbit_training(\\n     \"\"\"\\n \\n     # Freeze all parameters except LoRA\\n-    for name, param in model.named_parameters():\\n-        if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n-            param.requires_grad_(True)\\n-        else:\\n-            param.requires_grad_(False)\\n+    import re\\n+    with torch.inference_mode():\\n+        for name, param in model.named_parameters():\\n+            if \".lora_A.\" in name or \".lora_B.\" in name or \".lora_magnitude_vector\" in name:\\n+                param.requires_grad_(True)\\n+                # Also must be in float32!\\n+                if param.dtype != torch.float32:\\n+                    name = name.replace(\"base_model\", \"model\", 1)\\n+                    layer_number = re.search(r\"\\\\.[\\\\d]{1,}\\\\.\", name).group(0)\\n+                    name = name.replace(layer_number, f\"[{layer_number[1:-1]}].\")\\n+                    name = name.replace(\".weight\", \"\", 1)\\n+                    exec(f\"{name}.to(torch.float32)\")\\n+                pass\\n+            else:\\n+                param.requires_grad_(False)\\n+        pass\\n     pass\\n \\n     # Gradient checkpointing!\\n',\n",
       " '@@ -1030,7 +1030,7 @@ class FastLlamaModel:\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n         model_patcher.pre_patch()\\n-        get_statistics()\\n+        # get_statistics()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n',\n",
       " '@@ -122,10 +122,6 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n         \"unsloth/codegemma-7b\",\\n         \"google/codegemma-7b\",\\n     ),\\n-    \"unsloth/codegemma-2b-it-bnb-4bit\" : (\\n-        \"unsloth/codegemma-2b-it\",\\n-        \"google/codegemma-2b-it\",\\n-    ),\\n     \"unsloth/codegemma-7b-it-bnb-4bit\" : (\\n         \"unsloth/codegemma-7b-it\",\\n         \"google/codegemma-7b-it\",\\n',\n",
       " '@@ -319,7 +319,7 @@ class FastMistralModel(FastLlamaModel):\\n            f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n         print(statistics)\\n         model_patcher.pre_patch()\\n-        get_statistics()\\n+        # get_statistics()\\n \\n         if dtype is None:\\n             dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n',\n",
       " '@@ -293,7 +293,7 @@ def get_chat_template(\\n \\n         # Check fast tokenizer\\n         if not is_fast_tokenizer:\\n-            logger.warning_once(\\n+            print(\\n                 f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n                 \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n                 \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n@@ -348,11 +348,31 @@ def get_chat_template(\\n             # But training the lm_head and embeddings are slow!\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n+\\n+            old_bos_token = getattr(tokenizer, \"bos_token\", None)\\n+            old_eos_token = getattr(tokenizer, \"eos_token\", None)\\n+            old_pad_token = getattr(tokenizer, \"pad_token\", None)\\n+            old_unk_token = getattr(tokenizer, \"unk_token\", None)\\n+\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            old_eos_token = tokenizer.eos_token\\n-            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n+            # First check if new stop_word is in the tokenizer\\n+            if stop_word in string_vocab:\\n+                # We shall swap them around\\n+                temporary_stop_token = \"<|:__TEMP//STOP//TOKEN__:|>\"\\n+                string_vocab = string_vocab.replace(old_eos_token, temporary_stop_token)\\n+                string_vocab = string_vocab.replace(stop_word, old_eos_token)\\n+                string_vocab = string_vocab.replace(temporary_stop_token, stop_word)\\n+            else:\\n+                string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n+            pass\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(\\n+                tokenizer_object = new_tokenizer,\\n+                bos_token = old_bos_token,\\n+                eos_token = stop_word,\\n+                unk_token = old_unk_token,\\n+                pad_token = old_pad_token,\\n+            )\\n \\n             # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n             token_mapping = { old_eos_token : stop_word, }\\n',\n",
       " '@@ -1017,6 +1017,12 @@ class FastLlamaModel:\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -1445,8 +1451,8 @@ class FastLlamaModel:\\n         for module in target_modules:\\n             if module == \"lm_head\":\\n                 logger.warning_once(\\n-                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n-                    \"We shall do it for you!\"\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`. \"\\\\\\n+                    \"Luckily, we shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n                 if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n@@ -1454,8 +1460,8 @@ class FastLlamaModel:\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n-                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n-                    \"We shall do it for you!\"\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`. \"\\\\\\n+                    \"Luckily, we shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n                 if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n',\n",
       " '@@ -78,6 +78,12 @@ class FastLanguageModel(FastLlamaModel):\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         old_model_name = model_name\\n         model_name = _get_model_name(model_name, load_in_4bit)\\n \\n',\n",
       " '@@ -13,6 +13,7 @@\\n # limitations under the License.\\n \\n from .llama import *\\n+import os\\n from ._utils import __version__\\n \\n from transformers.models.mistral.modeling_mistral import (\\n@@ -301,6 +302,12 @@ class FastMistralModel(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         if model_patcher is None: model_patcher = FastMistralModel\\n         # Mistral does NOT support RoPE Scaling!\\n         if rope_scaling is not None:\\n',\n",
       " '@@ -327,7 +327,7 @@ def unsloth_save_model(\\n         if hasattr(model, \"config\"):\\n             print(f\"Saved {save_method} model to https://huggingface.co/\" + save_directory)\\n         pass\\n-        return save_directory\\n+        return save_directory, None\\n     pass\\n \\n     # Tokenizer has different saving arguments\\n@@ -402,7 +402,7 @@ def unsloth_save_model(\\n         pass\\n \\n         print(\" Done.\")\\n-        return save_directory\\n+        return save_directory, None\\n     pass\\n \\n     # If push_to_hub, we must remove the .../ part of a repo\\n',\n",
       " '@@ -48,7 +48,7 @@ def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n     tokenizer_string = converted_tokenizer.to_str()\\n \\n-    # Llama does ▁apple. Sometimes this is wrong!!\\n+    # Llama does _apple. Sometimes this is wrong!!\\n     prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n     if not prepend and prepend_text in tokenizer_string:\\n         tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n@@ -269,15 +269,26 @@ def load_correct_tokenizer(\\n         cache_dir = None\\n     pass\\n \\n-    slow_tokenizer = AutoTokenizer.from_pretrained(\\n-        tokenizer_name,\\n-        model_max_length  = model_max_length,\\n-        padding_side      = padding_side,\\n-        token             = token,\\n-        trust_remote_code = trust_remote_code,\\n-        use_fast          = False,\\n-        cache_dir         = cache_dir,\\n-    )\\n+    # Try loading the slow tokenizer. If it fails, then try Fast only\\n+    # Mainly to solve Deepseek models with no tokenizer.model file\\n+    slow_tokenizer = None\\n+    try:\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n+            tokenizer_name,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n+            use_fast          = False,\\n+            cache_dir         = cache_dir,\\n+        )\\n+    except:\\n+        print(\\n+            f\"Unsloth: {tokenizer_name} has no tokenizer.model file.\\\\n\"\\\\\\n+            \"Just informing you about this - this is not a critical error.\"\\n+        )\\n+    pass\\n+\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -286,14 +297,19 @@ def load_correct_tokenizer(\\n         trust_remote_code = trust_remote_code,\\n         cache_dir         = cache_dir,\\n     )\\n-    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n-    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n-    \\n-    # Confirm if slow and fast are equivalent!\\n-    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n-        return fast_tokenizer\\n+\\n+    if slow_tokenizer is not None:\\n+        fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+        fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+        \\n+        # Confirm if slow and fast are equivalent!\\n+        if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            return fast_tokenizer\\n+        else:\\n+            return convert_to_fast_tokenizer(slow_tokenizer)\\n+        pass\\n     else:\\n-        return convert_to_fast_tokenizer(slow_tokenizer)\\n+        return fast_tokenizer\\n     pass\\n pass\\n \\n@@ -408,25 +424,37 @@ def check_tokenizer(\\n                 cache_dir = None\\n             pass\\n \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-                cache_dir = cache_dir,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n+            # Sometimes slow tokenizer does not work like Deepseek\\n+            try:\\n+                # Try slow tokenizer which can fix things!\\n+                tokenizer = AutoTokenizer.from_pretrained(\\n+                    model_name,\\n+                    model_max_length = model_max_length,\\n+                    padding_side = padding_side,\\n+                    token = token,\\n+                    use_fast = False,\\n+                    cache_dir = cache_dir,\\n+                )\\n+                return check_tokenizer(\\n+                    model = model,\\n+                    tokenizer = tokenizer,\\n+                    model_name = model_name,\\n+                    model_max_length = model_max_length,\\n+                    padding_side = padding_side,\\n+                    token = token,\\n+                    _reload = False,\\n+                )\\n+                break\\n+            except:\\n+                # Tokenizer has out of bounds issues and we can\\'t\\n+                # load the slow tokenizer version :(\\n+                logger.warning_once(\\n+                    \"Unsloth: Tokenizer is most likely buggy, and Unsloth failed to repair it.\\\\n\"\\\\\\n+                    \"It will still work, but beware of out of bounds memory accesses.\\\\n\"\\\\\\n+                    \"Please file an issue on the model owner\\'s repo about this issue.\"\\n+                )\\n+                return tokenizer\\n+            pass\\n         pass\\n     pass\\n     return convert_to_fast_tokenizer(tokenizer)\\n',\n",
       " '@@ -293,7 +293,7 @@ def get_chat_template(\\n \\n         # Check fast tokenizer\\n         if not is_fast_tokenizer:\\n-            logger.warning_once(\\n+            print(\\n                 f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n                 \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n                 \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n@@ -348,11 +348,31 @@ def get_chat_template(\\n             # But training the lm_head and embeddings are slow!\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n+\\n+            old_bos_token = getattr(tokenizer, \"bos_token\", None)\\n+            old_eos_token = getattr(tokenizer, \"eos_token\", None)\\n+            old_pad_token = getattr(tokenizer, \"pad_token\", None)\\n+            old_unk_token = getattr(tokenizer, \"unk_token\", None)\\n+\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            old_eos_token = tokenizer.eos_token\\n-            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n+            # First check if new stop_word is in the tokenizer\\n+            if stop_word in string_vocab:\\n+                # We shall swap them around\\n+                temporary_stop_token = \"<|:__TEMP//STOP//TOKEN__:|>\"\\n+                string_vocab = string_vocab.replace(old_eos_token, temporary_stop_token)\\n+                string_vocab = string_vocab.replace(stop_word, old_eos_token)\\n+                string_vocab = string_vocab.replace(temporary_stop_token, stop_word)\\n+            else:\\n+                string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n+            pass\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(\\n+                tokenizer_object = new_tokenizer,\\n+                bos_token = old_bos_token,\\n+                eos_token = stop_word,\\n+                unk_token = old_unk_token,\\n+                pad_token = old_pad_token,\\n+            )\\n \\n             # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n             token_mapping = { old_eos_token : stop_word, }\\n',\n",
       " '@@ -1017,6 +1017,12 @@ class FastLlamaModel:\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -1445,8 +1451,8 @@ class FastLlamaModel:\\n         for module in target_modules:\\n             if module == \"lm_head\":\\n                 logger.warning_once(\\n-                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n-                    \"We shall do it for you!\"\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`. \"\\\\\\n+                    \"Luckily, we shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n                 if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n@@ -1454,8 +1460,8 @@ class FastLlamaModel:\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n-                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n-                    \"We shall do it for you!\"\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`. \"\\\\\\n+                    \"Luckily, we shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n                 if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n',\n",
       " '@@ -78,6 +78,12 @@ class FastLanguageModel(FastLlamaModel):\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         old_model_name = model_name\\n         model_name = _get_model_name(model_name, load_in_4bit)\\n \\n',\n",
       " '@@ -13,6 +13,7 @@\\n # limitations under the License.\\n \\n from .llama import *\\n+import os\\n from ._utils import __version__\\n \\n from transformers.models.mistral.modeling_mistral import (\\n@@ -301,6 +302,12 @@ class FastMistralModel(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         if model_patcher is None: model_patcher = FastMistralModel\\n         # Mistral does NOT support RoPE Scaling!\\n         if rope_scaling is not None:\\n',\n",
       " '@@ -327,7 +327,7 @@ def unsloth_save_model(\\n         if hasattr(model, \"config\"):\\n             print(f\"Saved {save_method} model to https://huggingface.co/\" + save_directory)\\n         pass\\n-        return save_directory\\n+        return save_directory, None\\n     pass\\n \\n     # Tokenizer has different saving arguments\\n@@ -402,7 +402,7 @@ def unsloth_save_model(\\n         pass\\n \\n         print(\" Done.\")\\n-        return save_directory\\n+        return save_directory, None\\n     pass\\n \\n     # If push_to_hub, we must remove the .../ part of a repo\\n',\n",
       " '@@ -48,7 +48,7 @@ def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n     tokenizer_string = converted_tokenizer.to_str()\\n \\n-    # Llama does ▁apple. Sometimes this is wrong!!\\n+    # Llama does _apple. Sometimes this is wrong!!\\n     prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n     if not prepend and prepend_text in tokenizer_string:\\n         tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n@@ -269,15 +269,26 @@ def load_correct_tokenizer(\\n         cache_dir = None\\n     pass\\n \\n-    slow_tokenizer = AutoTokenizer.from_pretrained(\\n-        tokenizer_name,\\n-        model_max_length  = model_max_length,\\n-        padding_side      = padding_side,\\n-        token             = token,\\n-        trust_remote_code = trust_remote_code,\\n-        use_fast          = False,\\n-        cache_dir         = cache_dir,\\n-    )\\n+    # Try loading the slow tokenizer. If it fails, then try Fast only\\n+    # Mainly to solve Deepseek models with no tokenizer.model file\\n+    slow_tokenizer = None\\n+    try:\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n+            tokenizer_name,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n+            use_fast          = False,\\n+            cache_dir         = cache_dir,\\n+        )\\n+    except:\\n+        print(\\n+            f\"Unsloth: {tokenizer_name} has no tokenizer.model file.\\\\n\"\\\\\\n+            \"Just informing you about this - this is not a critical error.\"\\n+        )\\n+    pass\\n+\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -286,14 +297,19 @@ def load_correct_tokenizer(\\n         trust_remote_code = trust_remote_code,\\n         cache_dir         = cache_dir,\\n     )\\n-    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n-    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n-    \\n-    # Confirm if slow and fast are equivalent!\\n-    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n-        return fast_tokenizer\\n+\\n+    if slow_tokenizer is not None:\\n+        fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+        fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+        \\n+        # Confirm if slow and fast are equivalent!\\n+        if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            return fast_tokenizer\\n+        else:\\n+            return convert_to_fast_tokenizer(slow_tokenizer)\\n+        pass\\n     else:\\n-        return convert_to_fast_tokenizer(slow_tokenizer)\\n+        return fast_tokenizer\\n     pass\\n pass\\n \\n@@ -408,25 +424,37 @@ def check_tokenizer(\\n                 cache_dir = None\\n             pass\\n \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-                cache_dir = cache_dir,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n+            # Sometimes slow tokenizer does not work like Deepseek\\n+            try:\\n+                # Try slow tokenizer which can fix things!\\n+                tokenizer = AutoTokenizer.from_pretrained(\\n+                    model_name,\\n+                    model_max_length = model_max_length,\\n+                    padding_side = padding_side,\\n+                    token = token,\\n+                    use_fast = False,\\n+                    cache_dir = cache_dir,\\n+                )\\n+                return check_tokenizer(\\n+                    model = model,\\n+                    tokenizer = tokenizer,\\n+                    model_name = model_name,\\n+                    model_max_length = model_max_length,\\n+                    padding_side = padding_side,\\n+                    token = token,\\n+                    _reload = False,\\n+                )\\n+                break\\n+            except:\\n+                # Tokenizer has out of bounds issues and we can\\'t\\n+                # load the slow tokenizer version :(\\n+                logger.warning_once(\\n+                    \"Unsloth: Tokenizer is most likely buggy, and Unsloth failed to repair it.\\\\n\"\\\\\\n+                    \"It will still work, but beware of out of bounds memory accesses.\\\\n\"\\\\\\n+                    \"Please file an issue on the model owner\\'s repo about this issue.\"\\n+                )\\n+                return tokenizer\\n+            pass\\n         pass\\n     pass\\n     return convert_to_fast_tokenizer(tokenizer)\\n',\n",
       " '@@ -293,7 +293,7 @@ def get_chat_template(\\n \\n         # Check fast tokenizer\\n         if not is_fast_tokenizer:\\n-            logger.warning_once(\\n+            print(\\n                 f\"Unsloth: Not a fast tokenizer, so can\\'t process it as of yet :(\\\\n\"\\\\\\n                 \"Please log a Github issue if you want this as a new feature!\\\\n\"\\\\\\n                 \"Your chat template will still work, but it won\\'t add or edit tokens.\"\\n@@ -348,11 +348,31 @@ def get_chat_template(\\n             # But training the lm_head and embeddings are slow!\\n             # This is a HACK!\\n             # Idea from https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\\n+\\n+            old_bos_token = getattr(tokenizer, \"bos_token\", None)\\n+            old_eos_token = getattr(tokenizer, \"eos_token\", None)\\n+            old_pad_token = getattr(tokenizer, \"pad_token\", None)\\n+            old_unk_token = getattr(tokenizer, \"unk_token\", None)\\n+\\n             string_vocab = tokenizer._tokenizer.to_str()\\n-            old_eos_token = tokenizer.eos_token\\n-            string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n+            # First check if new stop_word is in the tokenizer\\n+            if stop_word in string_vocab:\\n+                # We shall swap them around\\n+                temporary_stop_token = \"<|:__TEMP//STOP//TOKEN__:|>\"\\n+                string_vocab = string_vocab.replace(old_eos_token, temporary_stop_token)\\n+                string_vocab = string_vocab.replace(stop_word, old_eos_token)\\n+                string_vocab = string_vocab.replace(temporary_stop_token, stop_word)\\n+            else:\\n+                string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n+            pass\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n-            new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+            new_tokenizer = tokenizer.__class__(\\n+                tokenizer_object = new_tokenizer,\\n+                bos_token = old_bos_token,\\n+                eos_token = stop_word,\\n+                unk_token = old_unk_token,\\n+                pad_token = old_pad_token,\\n+            )\\n \\n             # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n             token_mapping = { old_eos_token : stop_word, }\\n',\n",
       " '@@ -1017,6 +1017,12 @@ class FastLlamaModel:\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         if model_patcher is None: model_patcher = FastLlamaModel\\n         SUPPORTS_BFLOAT16 = torch.cuda.is_bf16_supported()\\n         gpu_stats = torch.cuda.get_device_properties(0)\\n@@ -1445,8 +1451,8 @@ class FastLlamaModel:\\n         for module in target_modules:\\n             if module == \"lm_head\":\\n                 logger.warning_once(\\n-                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n-                    \"We shall do it for you!\"\\n+                    \"Unsloth: `lm_head` should be placed in `modules_to_save` and not `target_modules`. \"\\\\\\n+                    \"Luckily, we shall do it for you!\"\\n                 )\\n                 train_lm_head = True\\n                 if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n@@ -1454,8 +1460,8 @@ class FastLlamaModel:\\n \\n             elif module == \"embed_tokens\":\\n                 logger.warning_once(\\n-                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`.\"\\\\\\n-                    \"We shall do it for you!\"\\n+                    \"Unsloth: `embed_tokens` should be placed in `modules_to_save` and not `target_modules`. \"\\\\\\n+                    \"Luckily, we shall do it for you!\"\\n                 )\\n                 train_embed_tokens = True\\n                 if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n',\n",
       " '@@ -78,6 +78,12 @@ class FastLanguageModel(FastLlamaModel):\\n         use_gradient_checkpointing = True,\\n         *args, **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         old_model_name = model_name\\n         model_name = _get_model_name(model_name, load_in_4bit)\\n \\n',\n",
       " '@@ -13,6 +13,7 @@\\n # limitations under the License.\\n \\n from .llama import *\\n+import os\\n from ._utils import __version__\\n \\n from transformers.models.mistral.modeling_mistral import (\\n@@ -301,6 +302,12 @@ class FastMistralModel(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n+        if token is None and \"HF_TOKEN\" in os.environ:\\n+            token = os.environ[\"HF_TOKEN\"]\\n+\\n+        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n+            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n+\\n         if model_patcher is None: model_patcher = FastMistralModel\\n         # Mistral does NOT support RoPE Scaling!\\n         if rope_scaling is not None:\\n',\n",
       " '@@ -327,7 +327,7 @@ def unsloth_save_model(\\n         if hasattr(model, \"config\"):\\n             print(f\"Saved {save_method} model to https://huggingface.co/\" + save_directory)\\n         pass\\n-        return save_directory\\n+        return save_directory, None\\n     pass\\n \\n     # Tokenizer has different saving arguments\\n@@ -402,7 +402,7 @@ def unsloth_save_model(\\n         pass\\n \\n         print(\" Done.\")\\n-        return save_directory\\n+        return save_directory, None\\n     pass\\n \\n     # If push_to_hub, we must remove the .../ part of a repo\\n',\n",
       " '@@ -48,7 +48,7 @@ def try_fix_tokenizer(tokenizer, prepend = True):\\n \\n     tokenizer_string = converted_tokenizer.to_str()\\n \\n-    # Llama does ▁apple. Sometimes this is wrong!!\\n+    # Llama does _apple. Sometimes this is wrong!!\\n     prepend_text = \\'{\"type\":\"Prepend\",\"prepend\":\"▁\"},\\'\\n     if not prepend and prepend_text in tokenizer_string:\\n         tokenizer_string = tokenizer_string.replace(prepend_text, \"\", 1)\\n@@ -269,15 +269,26 @@ def load_correct_tokenizer(\\n         cache_dir = None\\n     pass\\n \\n-    slow_tokenizer = AutoTokenizer.from_pretrained(\\n-        tokenizer_name,\\n-        model_max_length  = model_max_length,\\n-        padding_side      = padding_side,\\n-        token             = token,\\n-        trust_remote_code = trust_remote_code,\\n-        use_fast          = False,\\n-        cache_dir         = cache_dir,\\n-    )\\n+    # Try loading the slow tokenizer. If it fails, then try Fast only\\n+    # Mainly to solve Deepseek models with no tokenizer.model file\\n+    slow_tokenizer = None\\n+    try:\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n+            tokenizer_name,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n+            trust_remote_code = trust_remote_code,\\n+            use_fast          = False,\\n+            cache_dir         = cache_dir,\\n+        )\\n+    except:\\n+        print(\\n+            f\"Unsloth: {tokenizer_name} has no tokenizer.model file.\\\\n\"\\\\\\n+            \"Just informing you about this - this is not a critical error.\"\\n+        )\\n+    pass\\n+\\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n         tokenizer_name,\\n         model_max_length  = model_max_length,\\n@@ -286,14 +297,19 @@ def load_correct_tokenizer(\\n         trust_remote_code = trust_remote_code,\\n         cache_dir         = cache_dir,\\n     )\\n-    fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n-    fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n-    \\n-    # Confirm if slow and fast are equivalent!\\n-    if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n-        return fast_tokenizer\\n+\\n+    if slow_tokenizer is not None:\\n+        fast_tokenizer.add_bos_token = slow_tokenizer.add_bos_token\\n+        fast_tokenizer.add_eos_token = slow_tokenizer.add_eos_token\\n+        \\n+        # Confirm if slow and fast are equivalent!\\n+        if assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n+            return fast_tokenizer\\n+        else:\\n+            return convert_to_fast_tokenizer(slow_tokenizer)\\n+        pass\\n     else:\\n-        return convert_to_fast_tokenizer(slow_tokenizer)\\n+        return fast_tokenizer\\n     pass\\n pass\\n \\n@@ -408,25 +424,37 @@ def check_tokenizer(\\n                 cache_dir = None\\n             pass\\n \\n-            # Try slow tokenizer which can fix things!\\n-            tokenizer = AutoTokenizer.from_pretrained(\\n-                model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                use_fast = False,\\n-                cache_dir = cache_dir,\\n-            )\\n-            return check_tokenizer(\\n-                model = model,\\n-                tokenizer = tokenizer,\\n-                model_name = model_name,\\n-                model_max_length = model_max_length,\\n-                padding_side = padding_side,\\n-                token = token,\\n-                _reload = False,\\n-            )\\n-            break\\n+            # Sometimes slow tokenizer does not work like Deepseek\\n+            try:\\n+                # Try slow tokenizer which can fix things!\\n+                tokenizer = AutoTokenizer.from_pretrained(\\n+                    model_name,\\n+                    model_max_length = model_max_length,\\n+                    padding_side = padding_side,\\n+                    token = token,\\n+                    use_fast = False,\\n+                    cache_dir = cache_dir,\\n+                )\\n+                return check_tokenizer(\\n+                    model = model,\\n+                    tokenizer = tokenizer,\\n+                    model_name = model_name,\\n+                    model_max_length = model_max_length,\\n+                    padding_side = padding_side,\\n+                    token = token,\\n+                    _reload = False,\\n+                )\\n+                break\\n+            except:\\n+                # Tokenizer has out of bounds issues and we can\\'t\\n+                # load the slow tokenizer version :(\\n+                logger.warning_once(\\n+                    \"Unsloth: Tokenizer is most likely buggy, and Unsloth failed to repair it.\\\\n\"\\\\\\n+                    \"It will still work, but beware of out of bounds memory accesses.\\\\n\"\\\\\\n+                    \"Please file an issue on the model owner\\'s repo about this issue.\"\\n+                )\\n+                return tokenizer\\n+            pass\\n         pass\\n     pass\\n     return convert_to_fast_tokenizer(tokenizer)\\n',\n",
       " '@@ -23,10 +23,7 @@ from transformers.models.llama.modeling_llama import logger\\n from .save import patch_saving_functions\\n import os\\n import shutil\\n-from .tokenizer_utils import (\\n-    load_correct_tokenizer,\\n-    fix_sentencepiece_tokenizer,\\n-)\\n+from .tokenizer_utils import *\\n from .models._utils import patch_tokenizer\\n \\n CHAT_TEMPLATES = {}\\n@@ -266,7 +263,7 @@ llama3_template = \\\\\\n         \"{{ \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\"\\\\\\n     \"{% endif %}\"\\n llama3_template_eos_token = \"eos_token\"\\n-CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, gemma_chatml_eos_token,)\\n+CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, llama3_template_eos_token,)\\n \\n \\n def get_chat_template(\\n@@ -288,6 +285,8 @@ def get_chat_template(\\n     is_fast_tokenizer = getattr(tokenizer, \"is_fast\", False)\\n     old_padding_side = tokenizer.padding_side\\n \\n+    same_padding_token = False\\n+\\n     if type(chat_template) in (list, tuple,):\\n         chat_template, stop_word = chat_template\\n         assert(type(chat_template) is str)\\n@@ -342,10 +341,24 @@ def get_chat_template(\\n             if skipped != len(token_mapping):\\n                 new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n \\n+                # Careful on pad_token\\n+                old_pad_token = tokenizer.pad_token\\n+                if old_pad_token == tokenizer.eos_token:\\n+                    old_pad_token = stop_word\\n+                    same_padding_token = True\\n+                pass\\n+\\n                 if map_eos_token:\\n-                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer, eos_token = stop_word)\\n+                    new_tokenizer = tokenizer.__class__(\\n+                        tokenizer_object = new_tokenizer,\\n+                        eos_token = stop_word,\\n+                        pad_token = old_pad_token,\\n+                    )\\n                 else:\\n-                    new_tokenizer = tokenizer.__class__(tokenizer_object = new_tokenizer)\\n+                    new_tokenizer = tokenizer.__class__(\\n+                        tokenizer_object = new_tokenizer,\\n+                        pad_token = old_pad_token,\\n+                    )\\n                 pass\\n \\n                 # Must fix the sentence piece tokenizer since there\\'s no tokenizer.model file!\\n@@ -380,6 +393,13 @@ def get_chat_template(\\n                 string_vocab = string_vocab.replace(old_eos_token, stop_word)\\n             pass\\n             new_tokenizer = tokenizer._tokenizer.from_str(string_vocab)\\n+\\n+            # Careful on pad_token\\n+            if old_pad_token == old_eos_token:\\n+                old_pad_token = stop_word\\n+                same_padding_token = True\\n+            pass\\n+\\n             new_tokenizer = tokenizer.__class__(\\n                 tokenizer_object = new_tokenizer,\\n                 bos_token = old_bos_token,\\n@@ -424,9 +444,11 @@ def get_chat_template(\\n     new_pad_token = getattr(tokenizer,     \"pad_token\", None)\\n     new_bos_token = getattr(tokenizer,     \"bos_token\", None)\\n     new_unk_token = getattr(tokenizer,     \"unk_token\", None)\\n-    if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token\\n     if old_bos_token != new_bos_token: tokenizer.bos_token = old_bos_token\\n     if old_unk_token != new_unk_token: tokenizer.unk_token = old_unk_token\\n+    if not same_padding_token:\\n+        if old_pad_token != new_pad_token: tokenizer.pad_token = old_pad_token\\n+    pass\\n \\n     # stopping_criteria = create_stopping_criteria(tokenizer, stop_word)\\n \\n',\n",
       " '@@ -349,3 +349,4 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n         return (None, hidden_states.grad,) + (None,)*len(ctx.args)\\n     pass\\n pass\\n+\\n',\n",
       " '@@ -1445,6 +1445,10 @@ class FastLlamaModel:\\n                                       \"gate_proj\", \"up_proj\", \"down_proj\",),)\\n         model.config.update({\"unsloth_version\" : __version__})\\n \\n+        if type(modules_to_save) is tuple:\\n+            modules_to_save = list(modules_to_save)\\n+        pass\\n+\\n         train_lm_head = False\\n         train_embed_tokens = False\\n         final_modules = []\\n@@ -1472,6 +1476,29 @@ class FastLlamaModel:\\n                 final_modules.append(module)\\n         pass\\n \\n+        # Check if we added new tokens!\\n+        if hasattr(model, \"_need_to_train_embeddings\"):\\n+            if not train_lm_head or not train_embed_tokens:\\n+                print(\\n+                    \"Unsloth: You added new tokens but did not specify if you wanted to \"\\\\\\n+                    \"train the lm_head and embed_tokens.\\\\nWe must turn it on for you.\"\\n+                )\\n+                train_lm_head = True\\n+                train_embed_tokens = True\\n+\\n+                if modules_to_save is None: modules_to_save = [\"embed_tokens\"]\\n+                else: modules_to_save.append(\"embed_tokens\")\\n+\\n+                if modules_to_save is None: modules_to_save = [\"lm_head\"]\\n+                else: modules_to_save.append(\"lm_head\")\\n+            pass\\n+        pass\\n+\\n+        # First fix untrained tokens\\n+        if train_embed_tokens or train_lm_head:\\n+            fix_untrained_tokens(model, eps = 1e-16)\\n+        pass\\n+\\n         # Check modules_to_save\\n         if modules_to_save is not None:\\n             for module in modules_to_save:\\n@@ -1479,8 +1506,15 @@ class FastLlamaModel:\\n                     train_lm_head = True\\n                 elif module == \"embed_tokens\":\\n                     train_embed_tokens = True\\n+                else:\\n+                    raise TypeError(\\n+                        f\"Unsloth: Module = {module} is not allowed. Only \\'lm_head\\' and \\'embed_tokens\\' is allowed.\"\\n+                    )\\n             pass\\n         pass\\n+        if isinstance(modules_to_save, (tuple, list)):\\n+            modules_to_save = list(set(modules_to_save))\\n+        pass\\n \\n         # Get LoRA\\n         arguments = dict(\\n',\n",
       " '@@ -922,9 +922,16 @@ def save_to_gguf(\\n           f\"The output location will be {final_location}\\\\n\"\\\\\\n           \"This will take 3 minutes...\")\\n \\n+    # We first check if tokenizer.model exists in the model_directory\\n+    if os.path.exists(f\"{model_directory}/tokenizer.model\"):\\n+        vocab_type = \"hfft\"\\n+    else:\\n+        vocab_type = \"bpe\"\\n+    pass\\n+\\n     if use_fast_convert:\\n         command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-            f\"--outfile {final_location} --vocab-type hfft \"\\\\\\n+            f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n',\n",
       " '@@ -18,11 +18,15 @@ from transformers import PreTrainedTokenizerFast\\n import re\\n import os\\n from transformers.models.llama.modeling_llama import logger\\n+from peft import PeftModelForCausalLM\\n+import torch\\n \\n __all__ = [\\n     \"load_correct_tokenizer\",\\n     \"fix_sentencepiece_tokenizer\",\\n     \"check_tokenizer\",\\n+    \"fix_untrained_tokens\",\\n+    \"add_new_tokens\",\\n ]\\n \\n \\n@@ -255,7 +259,11 @@ def fix_sentencepiece_tokenizer(\\n \\n     # And load it!\\n     from transformers import AutoTokenizer\\n-    tokenizer = AutoTokenizer.from_pretrained(temporary_location, eos_token = new_tokenizer.eos_token)\\n+    tokenizer = AutoTokenizer.from_pretrained(\\n+        temporary_location,\\n+        eos_token = new_tokenizer.eos_token,\\n+        pad_token = new_tokenizer.pad_token,\\n+    )\\n     return tokenizer\\n pass\\n \\n@@ -466,3 +474,124 @@ def check_tokenizer(\\n     pass\\n     return convert_to_fast_tokenizer(tokenizer)\\n pass\\n+\\n+\\n+@torch.inference_mode\\n+def fix_untrained_tokens(model, eps = 1e-16):\\n+    \"\"\"\\n+    Llama-3 for eg has untrained vectors in the base model.\\n+    These include <|eot_id|>, <|start_header_id|>, <|end_header_id|>\\n+    We reset them to the mean of the rest of the tokens\\n+    \"\"\"\\n+    embedding_matrix = model.get_input_embeddings ().weight.data\\n+    lm_head_matrix   = model.get_output_embeddings().weight.data\\n+\\n+    # Get untrained tokens\\n+    indicator_untrained = torch.amax(embedding_matrix, axis = 1) <= eps\\n+    where_untrained = torch.where(indicator_untrained)[0]\\n+    n_untrained = where_untrained.shape[0]\\n+    n_trained = embedding_matrix.shape[0] - n_untrained\\n+    if n_untrained != 0:\\n+        print(\\n+            f\"Unsloth: Not an error, but your model has {n_untrained} untrained tokens.\\\\n\"\\\\\\n+            \"We shall set them to the mean of the other trained tokens.\"\\n+        )\\n+    pass\\n+\\n+    # First set untrained to all 0s - sometimes it\\'s not! 1e-23 for bfloat16\\n+    embedding_matrix[where_untrained] = 0\\n+    lm_head_matrix  [where_untrained] = 0\\n+\\n+    # Find sum\\n+    sum_embedding  = torch.sum(embedding_matrix, dtype = torch.float32, axis = 0)\\n+    sum_lm_head    = torch.sum(lm_head_matrix,   dtype = torch.float32, axis = 0)\\n+\\n+    # Find correct average by dividing by sum of trained tokens\\n+    mean_embedding = (sum_embedding / n_trained).to(embedding_matrix.dtype)\\n+    mean_lm_head   = (sum_lm_head   / n_trained).to(lm_head_matrix  .dtype)\\n+\\n+    # Set them to the mean\\n+    embedding_matrix[where_untrained] = mean_embedding\\n+    lm_head_matrix  [where_untrained] = mean_lm_head\\n+\\n+    return mean_embedding, mean_lm_head\\n+pass\\n+\\n+\\n+@torch.inference_mode\\n+def add_new_tokens(\\n+    model,\\n+    tokenizer,\\n+    new_tokens = [],\\n+    method = \"mean\",\\n+    interpolation = 0.05,\\n+):\\n+    \"\"\"\\n+    Smartly resizes the tokenizer and adds new tokens to the model.\\n+    We also disregard untrained tokens by removing them from the mean calculation.\\n+    \"\"\"\\n+    assert(isinstance(new_tokens, (list, tuple)))\\n+    assert(len(new_tokens) > 0)\\n+    assert(method == \"mean\" or method == \"interpolation\")\\n+    assert(interpolation >= 0 and interpolation <= 1)\\n+\\n+    # Check if tokens already exist\\n+    overlapping_tokens = set(new_tokens) & set(tokenizer.vocab.keys())\\n+    if len(overlapping_tokens) != 0:\\n+        print(\\n+            f\"Unsloth: You\\'re adding new_tokens = {new_tokens}\\\\n\"\\\\\\n+            f\"There are tokens which are overlapping = {list(overlapping_tokens)}\\\\n\"\\\\\\n+            f\"We shall safely ignore these overlapping tokens.\"\\n+        )\\n+        new_tokens = [x for x in new_tokens if x not in overlapping_tokens]\\n+    pass\\n+\\n+    # Get mean of trained tokens\\n+    mean_embedding, mean_lm_head = fix_untrained_tokens(model)\\n+    mean_embedding = mean_embedding.to(torch.float32)\\n+    mean_lm_head   = mean_lm_head  .to(torch.float32)\\n+\\n+    # Add tokens!\\n+    old_length = len(tokenizer)\\n+    tokenizer.add_tokens(new_tokens)\\n+    model.resize_token_embeddings(len(tokenizer))\\n+\\n+    # If we use interpolation, we interpolate between the mean embeddings and\\n+    # the Word2Vec sum of the other vectors\\n+    embedding_matrix = model.get_input_embeddings ().weight.data\\n+    lm_head_matrix   = model.get_output_embeddings().weight.data\\n+\\n+    if method == \"interpolation\":\\n+        print(\\n+            \"Unsloth: You are using interpolation to add new tokens.\\\\n\"\\\\\\n+            f\"We shall set new tokens = mean(embeddings)*{1-interpolation} + mean(new_tokens)*{interpolation}\"\\n+        )\\n+        for j, token in enumerate(new_tokens):\\n+            input_ids = tokenizer(token, add_special_tokens = False).input_ids\\n+            mean_embedding_token = embedding_matrix[input_ids].mean(axis = 0, dtype = torch.float32)\\n+            mean_lm_head_token   = lm_head_matrix  [input_ids].mean(axis = 0, dtype = torch.float32)\\n+\\n+            # Interpolate\\n+            mean_embedding_token = mean_embedding*(1-interpolation) + mean_embedding_token*interpolation\\n+            mean_lm_head_token   = mean_lm_head  *(1-interpolation) + mean_lm_head_token  *interpolation\\n+\\n+            # Set the new vector\\n+            embedding_matrix[old_length+j] = mean_embedding_token\\n+            lm_head_matrix  [old_length+j] = mean_lm_head_token\\n+        pass\\n+    else:\\n+        # Now set the new tokens to the mean!\\n+        embedding_matrix[old_length:] = mean_embedding\\n+        lm_head_matrix  [old_length:] = mean_lm_head\\n+    pass\\n+\\n+    # We set a flag to say we need to train embeddings\\n+    internal_model = model\\n+    while hasattr(internal_model, \"model\"):\\n+        internal_model._need_to_train_embeddings = True\\n+        internal_model = internal_model.model\\n+    pass\\n+    internal_model._need_to_train_embeddings = True\\n+    \\n+    return\\n+pass\\n',\n",
       " '@@ -10,7 +10,7 @@\\n <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n \\n-### Finetune Mistral, Gemma, Llama 2-5x faster with 80% less memory!\\n+### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!\\n \\n ![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n \\n@@ -22,12 +22,11 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n \\n | Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n-| **Llama-3 8b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n-| **Gemma 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n-| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n-| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 82% less |\\n-| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 49% less |\\n-| **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\\\* | 73% less |\\n+| **Llama 3 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n+| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n+| **Gemma (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n+| **Llama 3 (8B)** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook) | 5x faster\\\\* | 73% less |\\n+| **ORPO**     | [▶️ Start on Colab](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n | **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n \\n - Benchmarking compared to FA2 + Hugging Face combined.\\n@@ -36,7 +35,8 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\\n \\n ## 🦥 Unsloth.ai News\\n-- 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (just change the model name in the notebook).\\n+- 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n+- 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n - 📣 NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you\\'re using our notebooks. To enable, simply change 1 line:\\n ```python\\n model = FastLanguageModel.get_peft_model(\\n@@ -46,8 +46,6 @@ model = FastLanguageModel.get_peft_model(\\n ```\\n - 📣 [CodeGemma](https://colab.research.google.com/drive/19lwcRk_ZQ_ZtX-qzFP3qZBBHZNcMD1hh?usp=sharing) now works along with [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) and [Gemma 2b](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)\\n - 📣 [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models\\n-- 📣 [DPO support](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) is now included. [More info](#DPO) on DPO\\n-- 📣 We did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗Hugging Face and are in their official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)\\n \\n ## 🔗 Links and Resources\\n | Type                            | Links                               |\\n@@ -182,18 +180,20 @@ max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\\n url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\\n dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\\n \\n-# 4bit pre quantized models we support - 4x faster downloading!\\n+# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\\n fourbit_models = [\\n     \"unsloth/mistral-7b-bnb-4bit\",\\n+    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\\n     \"unsloth/llama-2-7b-bnb-4bit\",\\n-    \"unsloth/llama-2-13b-bnb-4bit\",\\n-    \"unsloth/codellama-34b-bnb-4bit\",\\n-    \"unsloth/tinyllama-bnb-4bit\",\\n-] # Go to https://huggingface.co/unsloth for more 4-bit models!\\n+    \"unsloth/gemma-7b-bnb-4bit\",\\n+    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\\n+    \"unsloth/gemma-2b-bnb-4bit\",\\n+    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\\n+    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\\n+] # More models at https://huggingface.co/unsloth\\n \\n-# Load Llama model\\n model, tokenizer = FastLanguageModel.from_pretrained(\\n-    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Supports Llama, Mistral - replace this!\\n+    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\\n     max_seq_length = max_seq_length,\\n     dtype = None,\\n     load_in_4bit = True,\\n@@ -208,7 +208,8 @@ model = FastLanguageModel.get_peft_model(\\n     lora_alpha = 16,\\n     lora_dropout = 0, # Supports any, but = 0 is optimized\\n     bias = \"none\",    # Supports any, but = \"none\" is optimized\\n-    use_gradient_checkpointing = True,\\n+    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\\n+    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\\n     random_state = 3407,\\n     max_seq_length = max_seq_length,\\n     use_rslora = False,  # We support rank stabilized LoRA\\n@@ -272,7 +273,8 @@ model = FastLanguageModel.get_peft_model(\\n     lora_alpha = 64,\\n     lora_dropout = 0, # Supports any, but = 0 is optimized\\n     bias = \"none\",    # Supports any, but = \"none\" is optimized\\n-    use_gradient_checkpointing = True,\\n+    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\\n+    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\\n     random_state = 3407,\\n     max_seq_length = max_seq_length,\\n )\\n',\n",
       " '@@ -281,6 +281,17 @@ def get_chat_template(\\n         IS_GEMMA = True\\n     pass\\n \\n+    # We add a check for Llama-3\\n+    # if chat_template == \"llama-3\":\\n+    #     tokenizer._using_llama3_template = True\\n+    # else:\\n+    #     llama3_tokens = set([\"<|end_header_id|>\", \"<|eot_id|>\", \"<|start_header_id|>\"])\\n+    #     check_llama3_tokens = llama3_tokens & set(str(x) for x in tokenizer.added_tokens_decoder.values())\\n+    #     if len(check_llama3_tokens) == len(llama3_tokens):\\n+    #         tokenizer._using_llama3_template = True\\n+    #     pass\\n+    # pass\\n+\\n     # We first check if the tokenizer is a fast one. If not, we cannot convert this!\\n     is_fast_tokenizer = getattr(tokenizer, \"is_fast\", False)\\n     old_padding_side = tokenizer.padding_side\\n',\n",
       " '@@ -1284,6 +1284,15 @@ class FastLlamaModel:\\n         # Add save modules\\n         patch_saving_functions(model)\\n \\n+        # Save tokenizer for inference purposes\\n+        tokenizer.padding_side = \"left\" # Force inference\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model._saved_temp_tokenizer = tokenizer\\n+            internal_model = internal_model.model\\n+        pass\\n+        internal_model._saved_temp_tokenizer = tokenizer\\n+        \\n         return model, tokenizer\\n     pass\\n \\n@@ -1534,8 +1543,12 @@ class FastLlamaModel:\\n         if not SUPPORTS_LOFTQ:  del arguments[\"loftq_config\"]\\n         if not SUPPORTS_RSLORA: del arguments[\"use_rslora\"]\\n \\n+        _saved_temp_tokenizer = model._saved_temp_tokenizer\\n+\\n         lora_config = LoraConfig(**arguments)\\n         model = _get_peft_model(model, lora_config)\\n+        \\n+        model._saved_temp_tokenizer = _saved_temp_tokenizer\\n \\n         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n \\n@@ -1554,6 +1567,18 @@ class FastLlamaModel:\\n             model.model.lm_head.modules_to_save.default.requires_grad_(True)\\n         pass\\n \\n+        # Patch tokenizer to pad to the right\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            if hasattr(internal_model, \"_saved_temp_tokenizer\"):\\n+                internal_model._saved_temp_tokenizer.padding_side = \"right\"\\n+            pass\\n+            internal_model = internal_model.model\\n+        pass\\n+        if hasattr(internal_model, \"_saved_temp_tokenizer\"):\\n+            internal_model._saved_temp_tokenizer.padding_side = \"right\"\\n+        pass\\n+\\n         return model\\n     pass\\n \\n@@ -1751,6 +1776,18 @@ class FastLlamaModel:\\n         # Wrap model.generate\\n         model._unwrapped_old_generate = model.generate\\n         model.generate = _wrap_fast_inference(model.generate, device_type, dtype)\\n+\\n+        # Patch tokenizer to pad to the left\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            if hasattr(internal_model, \"_saved_temp_tokenizer\"):\\n+                internal_model._saved_temp_tokenizer.padding_side = \"left\"\\n+            pass\\n+            internal_model = internal_model.model\\n+        pass\\n+        if hasattr(internal_model, \"_saved_temp_tokenizer\"):\\n+            internal_model._saved_temp_tokenizer.padding_side = \"left\"\\n+        pass\\n     pass\\n \\n \\n@@ -1777,8 +1814,18 @@ class FastLlamaModel:\\n             model.generate = model._unwrapped_old_generate\\n             del model._unwrapped_old_generate\\n         pass\\n+\\n+        # Patch tokenizer to pad to the right\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            if hasattr(internal_model, \"_saved_temp_tokenizer\"):\\n+                internal_model._saved_temp_tokenizer.padding_side = \"right\"\\n+            pass\\n+            internal_model = internal_model.model\\n+        pass\\n+        if hasattr(internal_model, \"_saved_temp_tokenizer\"):\\n+            internal_model._saved_temp_tokenizer.padding_side = \"right\"\\n+        pass\\n     pass\\n pass\\n \\n-\\n-\\n',\n",
       " '@@ -76,6 +76,7 @@ class FastLanguageModel(FastLlamaModel):\\n         fix_tokenizer  = True,\\n         trust_remote_code = False,\\n         use_gradient_checkpointing = True,\\n+        resize_model_vocab = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -149,6 +150,9 @@ class FastLanguageModel(FastLlamaModel):\\n             trust_remote_code = trust_remote_code,\\n             *args, **kwargs,\\n         )\\n+        \\n+        if resize_model_vocab is not None:\\n+            model.resize_token_embeddings(resize_model_vocab)\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n',\n",
       " '@@ -559,6 +559,15 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+\\n+        # Save tokenizer for inference purposes\\n+        tokenizer.padding_side = \"left\" # Force inference\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model._saved_temp_tokenizer = tokenizer\\n+            internal_model = internal_model.model\\n+        pass\\n+        internal_model._saved_temp_tokenizer = tokenizer\\n         \\n         return model, tokenizer\\n     pass\\n',\n",
       " '@@ -689,7 +689,7 @@ pass\\n \\n \\n def install_llama_cpp_clone_non_blocking():\\n-    full_command = [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"]\\n+    full_command = [\"git\", \"clone\", \"--recursive\", \"https://github.com/ggerganov/llama.cpp\"]\\n     run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n pass\\n@@ -742,7 +742,7 @@ def install_llama_cpp_old(version = -10):\\n     # Clone a specific commit\\n     # Also don\\'t use the GPU!\\n     commands = [\\n-        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        \"git clone --recursive https://github.com/ggerganov/llama.cpp\",\\n         f\"cd llama.cpp && git reset --hard {version} && git clean -df\",\\n         \"make clean -C llama.cpp\",\\n         f\"make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n@@ -767,7 +767,7 @@ def install_llama_cpp_blocking(use_cuda = True):\\n     use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n-        \"git clone https://github.com/ggerganov/llama.cpp\",\\n+        \"git clone --recursive https://github.com/ggerganov/llama.cpp\",\\n         \"make clean -C llama.cpp\",\\n         f\"{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n         \"pip install gguf protobuf\",\\n@@ -922,16 +922,9 @@ def save_to_gguf(\\n           f\"The output location will be {final_location}\\\\n\"\\\\\\n           \"This will take 3 minutes...\")\\n \\n-    # We first check if tokenizer.model exists in the model_directory\\n-    if os.path.exists(f\"{model_directory}/tokenizer.model\"):\\n-        vocab_type = \"hfft\"\\n-    else:\\n-        vocab_type = \"bpe\"\\n-    pass\\n-\\n     if use_fast_convert:\\n         command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n-            f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n+            f\"--outfile {final_location} --vocab-type spm,hfft,bpe \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n@@ -966,7 +959,7 @@ def save_to_gguf(\\n                 \"You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-                \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                \"git clone --recursive https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n                 \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n@@ -1006,7 +999,7 @@ def save_to_gguf(\\n                     \"Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\\\\n\"\\\\\\n                     \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                     \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n-                    \"git clone https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n+                    \"git clone --recursive https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n                     \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n                     \"Once that\\'s done, redo the quantization.\"\\n                 )\\n',\n",
       " '@@ -524,7 +524,7 @@ def add_new_tokens(\\n     tokenizer,\\n     new_tokens = [],\\n     method = \"mean\",\\n-    interpolation = 0.05,\\n+    interpolation = 0.5,\\n ):\\n     \"\"\"\\n     Smartly resizes the tokenizer and adds new tokens to the model.\\n',\n",
       " '@@ -20,23 +20,25 @@\\n \\n All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you\\'ll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\\n \\n-| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\\n-|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\\n-| **Llama 3 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n-| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n-| **Gemma (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n-| **Llama 3 (8B)** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook) | 5x faster\\\\* | 73% less |\\n-| **ORPO**     | [▶️ Start on Colab](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n-| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n+| Unsloth supports | Free Notebooks | Performance | Memory use |\\n+|-----------|---------|--------|----------|\\n+| **Llama 3 (8B)**      | [▶️ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n+| **Mistral (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n+| **Gemma (7B)**      | [▶️ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n+| **ORPO**     | [▶️ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n+| **DPO Zephyr**     | [▶️ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n+| **Phi-3 (3.8B)** | [▶️ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |\\n+| **TinyLlama**  | [▶️ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |\\n \\n - Benchmarking compared to FA2 + Hugging Face combined.\\n-- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.\\n-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\\n-- \\\\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\\n+- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n+- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for Llama-3. And ChatML for [Mistral 7b](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing).\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n - 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n+- 📣 NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!\\n - 📣 NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you\\'re using our notebooks. To enable, simply change 1 line:\\n ```python\\n model = FastLanguageModel.get_peft_model(\\n@@ -45,7 +47,7 @@ model = FastLanguageModel.get_peft_model(\\n )\\n ```\\n - 📣 [CodeGemma](https://colab.research.google.com/drive/19lwcRk_ZQ_ZtX-qzFP3qZBBHZNcMD1hh?usp=sharing) now works along with [Gemma 7b](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing) and [Gemma 2b](https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing)\\n-- 📣 [2x faster inference](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) added for all our models\\n+- 📣 [2x faster inference](https://colab.research.google.com/drive/1aqlNQi7MMJbynFDyOQteD2t0yVfjb9Zh?usp=sharing) added for all our models\\n \\n ## 🔗 Links and Resources\\n | Type                            | Links                               |\\n@@ -190,6 +192,7 @@ fourbit_models = [\\n     \"unsloth/gemma-2b-bnb-4bit\",\\n     \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\\n     \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\\n+    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\\n ] # More models at https://huggingface.co/unsloth\\n \\n model, tokenizer = FastLanguageModel.from_pretrained(\\n',\n",
       " '@@ -140,6 +140,10 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/llama-3-70b-Instruct-bnb-4bit\" : (\\n         \"meta-llama/Meta-Llama-3-70B-Instruct\",\\n     ),\\n+    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\" : (\\n+        \"unsloth/Phi-3-mini-4k-instruct\",\\n+        \"microsoft/Phi-3-mini-4k-instruct\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -1503,10 +1503,16 @@ class FastLlamaModel:\\n             pass\\n         pass\\n \\n+        # Check for Llama-3\\n+        # if hasattr(model._saved_temp_tokenizer, \"_using_llama3_template\"):\\n+        #     if not train_embed_tokens and not train_lm_head:\\n+        #         raise RuntimeError(\"\")\\n+\\n         # First fix untrained tokens\\n-        if train_embed_tokens or train_lm_head:\\n-            fix_untrained_tokens(model, eps = 1e-16)\\n-        pass\\n+        # Wrong - can cause reserved tokens to pop out!!\\n+        # if train_embed_tokens or train_lm_head:\\n+        #     fix_untrained_tokens(model, eps = 1e-16)\\n+        # pass\\n \\n         # Check modules_to_save\\n         if modules_to_save is not None:\\n@@ -1547,7 +1553,7 @@ class FastLlamaModel:\\n \\n         lora_config = LoraConfig(**arguments)\\n         model = _get_peft_model(model, lora_config)\\n-        \\n+\\n         model._saved_temp_tokenizer = _saved_temp_tokenizer\\n \\n         model = FastLlamaModel.patch_peft_model(model, use_gradient_checkpointing)\\n',\n",
       " '@@ -118,14 +118,14 @@ def _merge_lora(layer, name):\\n             W = fast_dequantize(W, quant_state)\\n         else:\\n             dtype = W.dtype\\n-        # W = W.to(torch.float32).t()\\n-        W = W.t()\\n+        W = W.to(torch.float32).t()\\n+        # W = W.t()\\n \\n         if A is not None:\\n             # sAB = (A.t().to(torch.float32) @ (s * B.t().to(torch.float32)))\\n             # W += sAB\\n-            # W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n-            W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n+            W.addmm_(A.t().to(torch.float32), B.t().to(torch.float32), alpha = s)\\n+            # W.addmm_(A.t().to(W.dtype), B.t().to(W.dtype), alpha = s)\\n             # if not torch.isfinite(W).all():\\n             maximum_element = torch.max(W.min().abs(), W.max())\\n             if not torch.isfinite(maximum_element).item():\\n@@ -696,12 +696,18 @@ pass\\n \\n \\n def install_llama_cpp_make_non_blocking():\\n-    env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n+    # https://github.com/ggerganov/llama.cpp/issues/7062\\n+    # Weirdly GPU conversion for GGUF breaks??\\n+    # env = { **os.environ, \"LLAMA_CUDA\": \"1\", }\\n     n_jobs = max(int(psutil.cpu_count()*1.5), 1)\\n     # Force make clean\\n     os.system(\"make clean -C llama.cpp\")\\n     full_command = [\"make\", \"all\", \"-j\"+str(n_jobs), \"-C\", \"llama.cpp\"]\\n-    run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+\\n+    # https://github.com/ggerganov/llama.cpp/issues/7062\\n+    # Weirdly GPU conversion for GGUF breaks??\\n+    # run_installer = subprocess.Popen(full_command, env = env, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n+    run_installer = subprocess.Popen(full_command, stdout = subprocess.DEVNULL, stderr = subprocess.STDOUT)\\n     return run_installer\\n pass\\n \\n@@ -764,12 +770,17 @@ pass\\n \\n \\n def install_llama_cpp_blocking(use_cuda = True):\\n-    use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n+    # https://github.com/ggerganov/llama.cpp/issues/7062\\n+    # Weirdly GPU conversion for GGUF breaks??\\n+    # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n \\n     commands = [\\n         \"git clone --recursive https://github.com/ggerganov/llama.cpp\",\\n         \"make clean -C llama.cpp\",\\n-        f\"{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n+        # https://github.com/ggerganov/llama.cpp/issues/7062\\n+        # Weirdly GPU conversion for GGUF breaks??\\n+        # f\"{use_cuda} make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n+        f\"make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n         \"pip install gguf protobuf\",\\n     ]\\n     if os.path.exists(\"llama.cpp\"): return\\n@@ -833,6 +844,12 @@ def save_to_gguf(\\n     first_conversion     : str = \"f16\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n+    logger.warning(\\n+        \"WARNING: llama.cpp GGUF conversion is currently unstable, since llama.cpp is\\\\n\"\\\\\\n+        \"undergoing some major bug fixes as at 5th of May 2024. This is not an Unsloth issue.\\\\n\"\\\\\\n+        \"Please be patient - GGUF saving should still work, but might not work as well.\"\\n+    )\\n+\\n     from transformers.models.llama.modeling_llama import logger\\n \\n     if quantization_method.startswith(\"iq2\"):\\n@@ -967,7 +984,7 @@ def save_to_gguf(\\n                 \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                 \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                 \"git clone --recursive https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                \"cd llama.cpp && make clean && make all -j\\\\n\"\\\\\\n                 \"Once that\\'s done, redo the quantization.\"\\n             )\\n         pass\\n@@ -1007,7 +1024,7 @@ def save_to_gguf(\\n                     \"You do not need to close this Python program. Run the following commands in a new terminal:\\\\n\"\\\\\\n                     \"You must run this in the same folder as you\\'re saving your model.\\\\n\"\\\\\\n                     \"git clone --recursive https://github.com/ggerganov/llama.cpp\\\\n\"\\\\\\n-                    \"cd llama.cpp && make clean && LLAMA_CUDA=1 make all -j\\\\n\"\\\\\\n+                    \"cd llama.cpp && make clean && make all -j\\\\n\"\\\\\\n                     \"Once that\\'s done, redo the quantization.\"\\n                 )\\n             pass\\n',\n",
       " '@@ -25,7 +25,6 @@ __all__ = [\\n     \"load_correct_tokenizer\",\\n     \"fix_sentencepiece_tokenizer\",\\n     \"check_tokenizer\",\\n-    \"fix_untrained_tokens\",\\n     \"add_new_tokens\",\\n ]\\n \\n@@ -518,6 +517,44 @@ def fix_untrained_tokens(model, eps = 1e-16):\\n pass\\n \\n \\n+@torch.inference_mode\\n+def mean_of_trained_tokens(model, eps = 1e-16):\\n+    \"\"\"\\n+    Llama-3 for eg has untrained vectors in the base model.\\n+    These include <|eot_id|>, <|start_header_id|>, <|end_header_id|>\\n+    We reset them to the mean of the rest of the tokens\\n+    \"\"\"\\n+    embedding_matrix = model.get_input_embeddings ().weight.data.clone()\\n+    lm_head_matrix   = model.get_output_embeddings().weight.data.clone()\\n+\\n+    # Get untrained tokens\\n+    indicator_untrained = torch.amax(embedding_matrix, axis = 1) <= eps\\n+    where_untrained = torch.where(indicator_untrained)[0]\\n+    n_untrained = where_untrained.shape[0]\\n+    n_trained = embedding_matrix.shape[0] - n_untrained\\n+    if n_untrained != 0:\\n+        print(\\n+            f\"Unsloth: Not an error, but your model has {n_untrained} untrained tokens.\\\\n\"\\\\\\n+            \"We shall set them to the mean of the other trained tokens.\"\\n+        )\\n+    pass\\n+\\n+    # First set untrained to all 0s - sometimes it\\'s not! 1e-23 for bfloat16\\n+    embedding_matrix[where_untrained] = 0\\n+    lm_head_matrix  [where_untrained] = 0\\n+\\n+    # Find sum\\n+    sum_embedding  = torch.sum(embedding_matrix, dtype = torch.float32, axis = 0)\\n+    sum_lm_head    = torch.sum(lm_head_matrix,   dtype = torch.float32, axis = 0)\\n+\\n+    # Find correct average by dividing by sum of trained tokens\\n+    mean_embedding = (sum_embedding / n_trained).to(embedding_matrix.dtype)\\n+    mean_lm_head   = (sum_lm_head   / n_trained).to(lm_head_matrix  .dtype)\\n+\\n+    return mean_embedding, mean_lm_head\\n+pass\\n+\\n+\\n @torch.inference_mode\\n def add_new_tokens(\\n     model,\\n@@ -547,7 +584,10 @@ def add_new_tokens(\\n     pass\\n \\n     # Get mean of trained tokens\\n-    mean_embedding, mean_lm_head = fix_untrained_tokens(model)\\n+    # mean_embedding, mean_lm_head = fix_untrained_tokens(model)\\n+\\n+    # Weirdly be careful reserved tokens can pop out\\n+    mean_embedding, mean_lm_head = mean_of_trained_tokens(model)\\n     mean_embedding = mean_embedding.to(torch.float32)\\n     mean_lm_head   = mean_lm_head  .to(torch.float32)\\n \\n@@ -595,3 +635,43 @@ def add_new_tokens(\\n     \\n     return\\n pass\\n+\\n+\\n+from inspect import getsource\\n+import trl.trainer.sft_trainer\\n+from trl.trainer.sft_trainer import *\\n+\\n+def fix_sft_trainer_tokenizer():\\n+    \"\"\"\\n+        Fixes double adding BOS tokens like in llama-3\\n+    \"\"\"\\n+    for function_name, replacer in (\\n+        (\"_prepare_non_packed_dataloader\", \"def tokenize(element):\",),\\n+        # (\"_prepare_packed_dataloader\", \"if dataset_text_field is not None\",),\\n+    ):\\n+        function = getsource(eval(f\"trl.trainer.sft_trainer.SFTTrainer.{function_name}\"))\\n+        where = function.find(\"def\")\\n+        function = function.split(\"\\\\n\")\\n+        function = \"\\\\n\".join(x[where:] for x in function)\\n+\\n+        check_text = \\\\\\n+        \"\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n+        \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n+        \"has_bos_token_already = test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template\\\\n\"\\\\\\n+        \"add_special_tokens = False if has_bos_token_already else add_special_tokens\\\\n\\\\n\"\\n+\\n+        check_text = check_text.split(\"\\\\n\")\\n+        check_text = \"\\\\n\".join(\" \"*where + x for x in check_text)\\n+\\n+        function = function.replace(replacer, check_text + replacer)\\n+        exec(function, globals())\\n+\\n+        # Replace TRL\\'s SFTTrainer\\n+        exec(f\"trl.trainer.sft_trainer.SFTTrainer.{function_name} = {function_name}\", globals())\\n+    pass\\n+pass\\n+\\n+# Fixes double adding BOS tokens like in llama-3\\n+fix_sft_trainer_tokenizer()\\n',\n",
       " '@@ -266,6 +266,20 @@ llama3_template_eos_token = \"eos_token\"\\n CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, llama3_template_eos_token,)\\n \\n \\n+# Phi-3\\n+phi3_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if (message[\\'role\\'] == \\'user\\') %}\"\\\\\\n+            \"{{\\'<|user|>\\' + \\'\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\' + \\'<|assistant|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% elif (message[\\'role\\'] == \\'assistant\\') %}\"\\\\\\n+            \"{{message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\n+phi3_template_eos_token = \"<|end|>\"\\n+CHAT_TEMPLATES[\"phi-3\"] = (phi3_template, phi3_template_eos_token,)\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -595,4 +609,12 @@ def test_chat_templates():\\n     correct_tokenizer.chat_template = template\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n+\\n+    # Phi-3\\n+    template = phi3_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n pass\\n',\n",
       " '@@ -144,24 +144,60 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    \"\"\"\\n+        Phi3\\'s pad_token isn\\'t set. We set it to <|placeholder...\\n+        Llama-3 is <|reserved...\\n+        Llama-2 is <unk>\\n+        Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        Fixes https://github.com/unslothai/unsloth/issues/5\\n+    \"\"\"\\n+    possible_reserved_tokens = (\"<|reserved\", \"<|placeholder\",)\\n+\\n     if model is not None:\\n         model.config.update({\"unsloth_version\" : __version__})\\n-    if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n-        # Fixes https://github.com/unslothai/unsloth/issues/5\\n-        if hasattr(tokenizer, \"unk_token\") and tokenizer.unk_token is not None:\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n-            tokenizer.pad_token = tokenizer.unk_token\\n-        else:\\n-            name = model.config._name_or_path if model is not None else \"Model\"\\n-            logger.warning_once(\\n-                f\"{name} does not have a padding or unknown token!\\\\n\"\\\\\\n-                f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+\\n+    bad_pad_token = False\\n+    if hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is not None:\\n+        # Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        bad_pad_token = tokenizer.eos_token == tokenizer.pad_token\\n+    elif hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is None:\\n+        bad_pad_token = True\\n+    else:\\n+        bad_pad_token = False\\n+    pass\\n+\\n+    if bad_pad_token:\\n+        # Find a better pad token\\n+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+        possible_pad_token = None\\n+        for added_token in added_tokens[::-1]:\\n+            if added_token.startswith(possible_reserved_tokens):\\n+                possible_pad_token = added_token\\n+                break\\n+            pass\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Try unk_token\\n+            possible_pad_token = tokenizer.unk_token\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Failure!!\\n+            raise RuntimeError(\\n+                \"Unsloth: Tokenizer\\'s pad_token cannot be = eos_token, and we couldn\\'t find a\\\\n\"\\\\\\n+                \"replacement of either <|reserved... or <|placeholder...\"\\n             )\\n-            assert(hasattr(tokenizer, \"eos_token\"))\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n-            tokenizer.pad_token = tokenizer.eos_token\\n+        pass\\n+\\n+        name = model.config._name_or_path if model is not None else \"Model\"\\n+        logger.warning_once(\\n+            f\"{name} does not have a padding token! Will use pad_token = {possible_pad_token}.\"\\n+        )\\n+        \\n+        # Edit pad_token\\n+        tokenizer.add_special_tokens({\"pad_token\" : possible_pad_token})\\n+        tokenizer.pad_token = possible_pad_token\\n         if model is not None:\\n-            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.pad_token_id})\\n     pass\\n     return model, tokenizer\\n pass\\n',\n",
       " '@@ -18,6 +18,7 @@ from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n+import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n@@ -87,6 +88,24 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def check_if_sentencepiece_model(model, temporary_location = \"_unsloth_sentencepiece_temp\"):\\n+    if not hasattr(model, \"_saved_temp_tokenizer\"): return False\\n+\\n+    temp_tokenizer = model._saved_temp_tokenizer\\n+    sentencepiece_model = False\\n+    file_location = f\"{temporary_location}/{temp_tokenizer.name_or_path}\"\\n+    if not os.path.exists(file_location):\\n+        os.makedirs(file_location)\\n+    pass\\n+    temp_tokenizer.save_pretrained(file_location)\\n+    if os.path.isfile(f\"{file_location}/tokenizer.model\"):\\n+        sentencepiece_model = True\\n+    pass\\n+    shutil.rmtree(file_location)\\n+    return sentencepiece_model\\n+pass\\n+\\n+\\n def _free_cached_model(model):\\n     from huggingface_hub import scan_cache_dir\\n     cached_repos = list(scan_cache_dir().repos)\\n@@ -840,6 +859,7 @@ pass\\n \\n def save_to_gguf(\\n     model_type           : str,\\n+    is_sentencepiece     : bool = False,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -856,7 +876,8 @@ def save_to_gguf(\\n \\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n-    if   model_type == \"llama\":   use_fast_convert = True\\n+    if not is_sentencepiece:      use_fast_convert = False # Llama-3\\n+    elif model_type == \"llama\":   use_fast_convert = True\\n     elif model_type == \"mistral\": use_fast_convert = True\\n     pass\\n     logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n@@ -951,7 +972,7 @@ def save_to_gguf(\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n-        _fix_gemma_gguf()\\n+        # _fix_gemma_gguf()\\n \\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n@@ -1353,7 +1374,10 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1473,7 +1497,10 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -266,6 +266,20 @@ llama3_template_eos_token = \"eos_token\"\\n CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, llama3_template_eos_token,)\\n \\n \\n+# Phi-3\\n+phi3_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if (message[\\'role\\'] == \\'user\\') %}\"\\\\\\n+            \"{{\\'<|user|>\\' + \\'\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\' + \\'<|assistant|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% elif (message[\\'role\\'] == \\'assistant\\') %}\"\\\\\\n+            \"{{message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\n+phi3_template_eos_token = \"<|end|>\"\\n+CHAT_TEMPLATES[\"phi-3\"] = (phi3_template, phi3_template_eos_token,)\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -595,4 +609,12 @@ def test_chat_templates():\\n     correct_tokenizer.chat_template = template\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n+\\n+    # Phi-3\\n+    template = phi3_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n pass\\n',\n",
       " '@@ -144,24 +144,60 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    \"\"\"\\n+        Phi3\\'s pad_token isn\\'t set. We set it to <|placeholder...\\n+        Llama-3 is <|reserved...\\n+        Llama-2 is <unk>\\n+        Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        Fixes https://github.com/unslothai/unsloth/issues/5\\n+    \"\"\"\\n+    possible_reserved_tokens = (\"<|reserved\", \"<|placeholder\",)\\n+\\n     if model is not None:\\n         model.config.update({\"unsloth_version\" : __version__})\\n-    if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n-        # Fixes https://github.com/unslothai/unsloth/issues/5\\n-        if hasattr(tokenizer, \"unk_token\") and tokenizer.unk_token is not None:\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n-            tokenizer.pad_token = tokenizer.unk_token\\n-        else:\\n-            name = model.config._name_or_path if model is not None else \"Model\"\\n-            logger.warning_once(\\n-                f\"{name} does not have a padding or unknown token!\\\\n\"\\\\\\n-                f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+\\n+    bad_pad_token = False\\n+    if hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is not None:\\n+        # Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        bad_pad_token = tokenizer.eos_token == tokenizer.pad_token\\n+    elif hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is None:\\n+        bad_pad_token = True\\n+    else:\\n+        bad_pad_token = False\\n+    pass\\n+\\n+    if bad_pad_token:\\n+        # Find a better pad token\\n+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+        possible_pad_token = None\\n+        for added_token in added_tokens[::-1]:\\n+            if added_token.startswith(possible_reserved_tokens):\\n+                possible_pad_token = added_token\\n+                break\\n+            pass\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Try unk_token\\n+            possible_pad_token = tokenizer.unk_token\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Failure!!\\n+            raise RuntimeError(\\n+                \"Unsloth: Tokenizer\\'s pad_token cannot be = eos_token, and we couldn\\'t find a\\\\n\"\\\\\\n+                \"replacement of either <|reserved... or <|placeholder...\"\\n             )\\n-            assert(hasattr(tokenizer, \"eos_token\"))\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n-            tokenizer.pad_token = tokenizer.eos_token\\n+        pass\\n+\\n+        name = model.config._name_or_path if model is not None else \"Model\"\\n+        logger.warning_once(\\n+            f\"{name} does not have a padding token! Will use pad_token = {possible_pad_token}.\"\\n+        )\\n+        \\n+        # Edit pad_token\\n+        tokenizer.add_special_tokens({\"pad_token\" : possible_pad_token})\\n+        tokenizer.pad_token = possible_pad_token\\n         if model is not None:\\n-            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.pad_token_id})\\n     pass\\n     return model, tokenizer\\n pass\\n',\n",
       " '@@ -18,6 +18,7 @@ from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n+import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n@@ -87,6 +88,24 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def check_if_sentencepiece_model(model, temporary_location = \"_unsloth_sentencepiece_temp\"):\\n+    if not hasattr(model, \"_saved_temp_tokenizer\"): return False\\n+\\n+    temp_tokenizer = model._saved_temp_tokenizer\\n+    sentencepiece_model = False\\n+    file_location = f\"{temporary_location}/{temp_tokenizer.name_or_path}\"\\n+    if not os.path.exists(file_location):\\n+        os.makedirs(file_location)\\n+    pass\\n+    temp_tokenizer.save_pretrained(file_location)\\n+    if os.path.isfile(f\"{file_location}/tokenizer.model\"):\\n+        sentencepiece_model = True\\n+    pass\\n+    shutil.rmtree(file_location)\\n+    return sentencepiece_model\\n+pass\\n+\\n+\\n def _free_cached_model(model):\\n     from huggingface_hub import scan_cache_dir\\n     cached_repos = list(scan_cache_dir().repos)\\n@@ -840,6 +859,7 @@ pass\\n \\n def save_to_gguf(\\n     model_type           : str,\\n+    is_sentencepiece     : bool = False,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -856,7 +876,8 @@ def save_to_gguf(\\n \\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n-    if   model_type == \"llama\":   use_fast_convert = True\\n+    if not is_sentencepiece:      use_fast_convert = False # Llama-3\\n+    elif model_type == \"llama\":   use_fast_convert = True\\n     elif model_type == \"mistral\": use_fast_convert = True\\n     pass\\n     logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n@@ -951,7 +972,7 @@ def save_to_gguf(\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n-        _fix_gemma_gguf()\\n+        # _fix_gemma_gguf()\\n \\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n@@ -1353,7 +1374,10 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1473,7 +1497,10 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -266,6 +266,20 @@ llama3_template_eos_token = \"eos_token\"\\n CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, llama3_template_eos_token,)\\n \\n \\n+# Phi-3\\n+phi3_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if (message[\\'role\\'] == \\'user\\') %}\"\\\\\\n+            \"{{\\'<|user|>\\' + \\'\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\' + \\'<|assistant|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% elif (message[\\'role\\'] == \\'assistant\\') %}\"\\\\\\n+            \"{{message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\n+phi3_template_eos_token = \"<|end|>\"\\n+CHAT_TEMPLATES[\"phi-3\"] = (phi3_template, phi3_template_eos_token,)\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -595,4 +609,12 @@ def test_chat_templates():\\n     correct_tokenizer.chat_template = template\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n+\\n+    # Phi-3\\n+    template = phi3_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n pass\\n',\n",
       " '@@ -144,24 +144,60 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    \"\"\"\\n+        Phi3\\'s pad_token isn\\'t set. We set it to <|placeholder...\\n+        Llama-3 is <|reserved...\\n+        Llama-2 is <unk>\\n+        Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        Fixes https://github.com/unslothai/unsloth/issues/5\\n+    \"\"\"\\n+    possible_reserved_tokens = (\"<|reserved\", \"<|placeholder\",)\\n+\\n     if model is not None:\\n         model.config.update({\"unsloth_version\" : __version__})\\n-    if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n-        # Fixes https://github.com/unslothai/unsloth/issues/5\\n-        if hasattr(tokenizer, \"unk_token\") and tokenizer.unk_token is not None:\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n-            tokenizer.pad_token = tokenizer.unk_token\\n-        else:\\n-            name = model.config._name_or_path if model is not None else \"Model\"\\n-            logger.warning_once(\\n-                f\"{name} does not have a padding or unknown token!\\\\n\"\\\\\\n-                f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+\\n+    bad_pad_token = False\\n+    if hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is not None:\\n+        # Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        bad_pad_token = tokenizer.eos_token == tokenizer.pad_token\\n+    elif hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is None:\\n+        bad_pad_token = True\\n+    else:\\n+        bad_pad_token = False\\n+    pass\\n+\\n+    if bad_pad_token:\\n+        # Find a better pad token\\n+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+        possible_pad_token = None\\n+        for added_token in added_tokens[::-1]:\\n+            if added_token.startswith(possible_reserved_tokens):\\n+                possible_pad_token = added_token\\n+                break\\n+            pass\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Try unk_token\\n+            possible_pad_token = tokenizer.unk_token\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Failure!!\\n+            raise RuntimeError(\\n+                \"Unsloth: Tokenizer\\'s pad_token cannot be = eos_token, and we couldn\\'t find a\\\\n\"\\\\\\n+                \"replacement of either <|reserved... or <|placeholder...\"\\n             )\\n-            assert(hasattr(tokenizer, \"eos_token\"))\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n-            tokenizer.pad_token = tokenizer.eos_token\\n+        pass\\n+\\n+        name = model.config._name_or_path if model is not None else \"Model\"\\n+        logger.warning_once(\\n+            f\"{name} does not have a padding token! Will use pad_token = {possible_pad_token}.\"\\n+        )\\n+        \\n+        # Edit pad_token\\n+        tokenizer.add_special_tokens({\"pad_token\" : possible_pad_token})\\n+        tokenizer.pad_token = possible_pad_token\\n         if model is not None:\\n-            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.pad_token_id})\\n     pass\\n     return model, tokenizer\\n pass\\n',\n",
       " '@@ -18,6 +18,7 @@ from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n+import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n@@ -87,6 +88,24 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def check_if_sentencepiece_model(model, temporary_location = \"_unsloth_sentencepiece_temp\"):\\n+    if not hasattr(model, \"_saved_temp_tokenizer\"): return False\\n+\\n+    temp_tokenizer = model._saved_temp_tokenizer\\n+    sentencepiece_model = False\\n+    file_location = f\"{temporary_location}/{temp_tokenizer.name_or_path}\"\\n+    if not os.path.exists(file_location):\\n+        os.makedirs(file_location)\\n+    pass\\n+    temp_tokenizer.save_pretrained(file_location)\\n+    if os.path.isfile(f\"{file_location}/tokenizer.model\"):\\n+        sentencepiece_model = True\\n+    pass\\n+    shutil.rmtree(file_location)\\n+    return sentencepiece_model\\n+pass\\n+\\n+\\n def _free_cached_model(model):\\n     from huggingface_hub import scan_cache_dir\\n     cached_repos = list(scan_cache_dir().repos)\\n@@ -840,6 +859,7 @@ pass\\n \\n def save_to_gguf(\\n     model_type           : str,\\n+    is_sentencepiece     : bool = False,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -856,7 +876,8 @@ def save_to_gguf(\\n \\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n-    if   model_type == \"llama\":   use_fast_convert = True\\n+    if not is_sentencepiece:      use_fast_convert = False # Llama-3\\n+    elif model_type == \"llama\":   use_fast_convert = True\\n     elif model_type == \"mistral\": use_fast_convert = True\\n     pass\\n     logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n@@ -951,7 +972,7 @@ def save_to_gguf(\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n-        _fix_gemma_gguf()\\n+        # _fix_gemma_gguf()\\n \\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n@@ -1353,7 +1374,10 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1473,7 +1497,10 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -266,6 +266,20 @@ llama3_template_eos_token = \"eos_token\"\\n CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, llama3_template_eos_token,)\\n \\n \\n+# Phi-3\\n+phi3_template = \\\\\\n+    \"{{ bos_token }}\"\\\\\\n+    \"{% for message in messages %}\"\\\\\\n+        \"{% if (message[\\'role\\'] == \\'user\\') %}\"\\\\\\n+            \"{{\\'<|user|>\\' + \\'\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\' + \\'<|assistant|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% elif (message[\\'role\\'] == \\'assistant\\') %}\"\\\\\\n+            \"{{message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\'}}\"\\\\\\n+        \"{% endif %}\"\\\\\\n+    \"{% endfor %}\"\\n+phi3_template_eos_token = \"<|end|>\"\\n+CHAT_TEMPLATES[\"phi-3\"] = (phi3_template, phi3_template_eos_token,)\\n+\\n+\\n def get_chat_template(\\n     tokenizer,\\n     chat_template = \"chatml\",\\n@@ -595,4 +609,12 @@ def test_chat_templates():\\n     correct_tokenizer.chat_template = template\\n     our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n+\\n+    # Phi-3\\n+    template = phi3_template\\n+    correct_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_tokenizer.chat_template = template\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    assert(correct_prompt == our_prompt)\\n pass\\n',\n",
       " '@@ -144,24 +144,60 @@ pass\\n \\n \\n def patch_tokenizer(model, tokenizer):\\n+    \"\"\"\\n+        Phi3\\'s pad_token isn\\'t set. We set it to <|placeholder...\\n+        Llama-3 is <|reserved...\\n+        Llama-2 is <unk>\\n+        Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        Fixes https://github.com/unslothai/unsloth/issues/5\\n+    \"\"\"\\n+    possible_reserved_tokens = (\"<|reserved\", \"<|placeholder\",)\\n+\\n     if model is not None:\\n         model.config.update({\"unsloth_version\" : __version__})\\n-    if not hasattr(tokenizer, \"pad_token\") or tokenizer.pad_token is None:\\n-        # Fixes https://github.com/unslothai/unsloth/issues/5\\n-        if hasattr(tokenizer, \"unk_token\") and tokenizer.unk_token is not None:\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.unk_token})\\n-            tokenizer.pad_token = tokenizer.unk_token\\n-        else:\\n-            name = model.config._name_or_path if model is not None else \"Model\"\\n-            logger.warning_once(\\n-                f\"{name} does not have a padding or unknown token!\\\\n\"\\\\\\n-                f\"Will use the EOS token of id {tokenizer.eos_token_id} as padding.\"\\n+\\n+    bad_pad_token = False\\n+    if hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is not None:\\n+        # Check if pad_token is not the same as eos_token otherwise the loss will ignore it!!\\n+        bad_pad_token = tokenizer.eos_token == tokenizer.pad_token\\n+    elif hasattr(tokenizer, \"pad_token\") and tokenizer.pad_token is None:\\n+        bad_pad_token = True\\n+    else:\\n+        bad_pad_token = False\\n+    pass\\n+\\n+    if bad_pad_token:\\n+        # Find a better pad token\\n+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+        possible_pad_token = None\\n+        for added_token in added_tokens[::-1]:\\n+            if added_token.startswith(possible_reserved_tokens):\\n+                possible_pad_token = added_token\\n+                break\\n+            pass\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Try unk_token\\n+            possible_pad_token = tokenizer.unk_token\\n+        pass\\n+        if possible_pad_token is None:\\n+            # Failure!!\\n+            raise RuntimeError(\\n+                \"Unsloth: Tokenizer\\'s pad_token cannot be = eos_token, and we couldn\\'t find a\\\\n\"\\\\\\n+                \"replacement of either <|reserved... or <|placeholder...\"\\n             )\\n-            assert(hasattr(tokenizer, \"eos_token\"))\\n-            tokenizer.add_special_tokens({\"pad_token\" : tokenizer.eos_token})\\n-            tokenizer.pad_token = tokenizer.eos_token\\n+        pass\\n+\\n+        name = model.config._name_or_path if model is not None else \"Model\"\\n+        logger.warning_once(\\n+            f\"{name} does not have a padding token! Will use pad_token = {possible_pad_token}.\"\\n+        )\\n+        \\n+        # Edit pad_token\\n+        tokenizer.add_special_tokens({\"pad_token\" : possible_pad_token})\\n+        tokenizer.pad_token = possible_pad_token\\n         if model is not None:\\n-            config = model.config.update({\"pad_token_id\" : tokenizer.eos_token_id})\\n+            config = model.config.update({\"pad_token_id\" : tokenizer.pad_token_id})\\n     pass\\n     return model, tokenizer\\n pass\\n',\n",
       " '@@ -18,6 +18,7 @@ from peft.tuners.lora import Linear as Peft_Linear\\n from typing import Optional, Callable, Union, List\\n import torch\\n import os\\n+import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n@@ -87,6 +88,24 @@ def print_quantization_methods():\\n pass\\n \\n \\n+def check_if_sentencepiece_model(model, temporary_location = \"_unsloth_sentencepiece_temp\"):\\n+    if not hasattr(model, \"_saved_temp_tokenizer\"): return False\\n+\\n+    temp_tokenizer = model._saved_temp_tokenizer\\n+    sentencepiece_model = False\\n+    file_location = f\"{temporary_location}/{temp_tokenizer.name_or_path}\"\\n+    if not os.path.exists(file_location):\\n+        os.makedirs(file_location)\\n+    pass\\n+    temp_tokenizer.save_pretrained(file_location)\\n+    if os.path.isfile(f\"{file_location}/tokenizer.model\"):\\n+        sentencepiece_model = True\\n+    pass\\n+    shutil.rmtree(file_location)\\n+    return sentencepiece_model\\n+pass\\n+\\n+\\n def _free_cached_model(model):\\n     from huggingface_hub import scan_cache_dir\\n     cached_repos = list(scan_cache_dir().repos)\\n@@ -840,6 +859,7 @@ pass\\n \\n def save_to_gguf(\\n     model_type           : str,\\n+    is_sentencepiece     : bool = False,\\n     model_directory      : str = \"unsloth_finetuned_model\",\\n     quantization_method  : str = \"fast_quantized\",\\n     first_conversion     : str = \"f16\",\\n@@ -856,7 +876,8 @@ def save_to_gguf(\\n \\n     # Careful convert.py is only for Llama / Mistral based archs\\n     use_fast_convert = False\\n-    if   model_type == \"llama\":   use_fast_convert = True\\n+    if not is_sentencepiece:      use_fast_convert = False # Llama-3\\n+    elif model_type == \"llama\":   use_fast_convert = True\\n     elif model_type == \"mistral\": use_fast_convert = True\\n     pass\\n     logger.warning_once(f\"Unsloth: Converting {model_type} model. Can use fast conversion = {use_fast_convert}.\")\\n@@ -951,7 +972,7 @@ def save_to_gguf(\\n             f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n-        _fix_gemma_gguf()\\n+        # _fix_gemma_gguf()\\n \\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n@@ -1353,7 +1374,10 @@ def unsloth_save_pretrained_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     if push_to_hub:\\n         print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n@@ -1473,7 +1497,10 @@ def unsloth_push_to_hub_gguf(\\n         gc.collect()\\n \\n     model_type = self.config.model_type\\n-    file_location = save_to_gguf(model_type, new_save_directory, quantization_method, first_conversion, makefile)\\n+    is_sentencepiece_model = check_if_sentencepiece_model(self)\\n+    file_location = save_to_gguf(model_type, is_sentencepiece_model, \\n+        new_save_directory, quantization_method, first_conversion, makefile,\\n+    )\\n \\n     print(\"Unsloth: Uploading GGUF to Huggingface Hub...\")\\n     username = upload_to_huggingface(\\n',\n",
       " '@@ -36,6 +36,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.\\n \\n ## 🦥 Unsloth.ai News\\n+- 📣 NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly\\'s PR [#428](https://github.com/unslothai/unsloth/pull/428)\\n - 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n - 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n - 📣 NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!\\n@@ -159,7 +160,14 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n pip install --no-deps xformers trl peft accelerate bitsandbytes\\n ```\\n-7. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n+7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n+```bash\\n+pip install \"unsloth[cu118-torch230] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu118-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\\n+pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\\n+```\\n+8. To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.\\n ```bash\\n nvcc\\n python -m xformers.info\\n',\n",
       " '@@ -86,6 +86,17 @@ cu121onlytorch220 = [\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n     \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n ]\\n+cu118onlytorch230 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.26.post1%2Bcu118-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.26.post1%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu118/xformers-0.0.26.post1%2Bcu118-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+cu121onlytorch230 = [\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp39-cp39-manylinux2014_x86_64.whl ; python_version==\\'3.9\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl ; python_version==\\'3.10\\'\",\\n+    \"xformers @ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl ; python_version==\\'3.11\\'\",\\n+]\\n+\\n cu118 = [\\n     \"unsloth[huggingface]\",\\n     \"bitsandbytes\",\\n@@ -126,6 +137,16 @@ cu121-torch220 = [\\n     \"bitsandbytes\",\\n     \"unsloth[cu121onlytorch220]\",\\n ]\\n+cu118-torch230 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118onlytorch230]\",\\n+]\\n+cu121-torch230 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121onlytorch230]\",\\n+]\\n kaggle = [\\n     \"unsloth[huggingface]\",\\n ]\\n@@ -238,6 +259,22 @@ cu121-ampere-torch220 = [\\n     \"ninja\",\\n     \"flash-attn\",\\n ]\\n+cu118-ampere-torch230 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu118onlytorch230]\",\\n+    \"packaging\",\\n+    \"ninja\",\\n+    \"flash-attn\",\\n+]\\n+cu121-ampere-torch230 = [\\n+    \"unsloth[huggingface]\",\\n+    \"bitsandbytes\",\\n+    \"unsloth[cu121onlytorch230]\",\\n+    \"packaging\",\\n+    \"ninja\",\\n+    \"flash-attn\",\\n+]\\n \\n [project.urls]\\n homepage = \"http://www.unsloth.ai\"\\n',\n",
       " '@@ -15,6 +15,7 @@\\n __all__ = [\\n     \"get_chat_template\",\\n     \"test_chat_templates\",\\n+    \"test_hf_gguf_equivalence\",\\n ]\\n \\n from transformers import StoppingCriteria, StoppingCriteriaList\\n@@ -270,12 +271,11 @@ CHAT_TEMPLATES[\"llama-3\"] = (llama3_template, llama3_template_eos_token,)\\n phi3_template = \\\\\\n     \"{{ bos_token }}\"\\\\\\n     \"{% for message in messages %}\"\\\\\\n-        \"{% if (message[\\'role\\'] == \\'user\\') %}\"\\\\\\n-            \"{{\\'<|user|>\\' + \\'\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\' + \\'<|assistant|>\\' + \\'\\\\n\\'}}\"\\\\\\n-        \"{% elif (message[\\'role\\'] == \\'assistant\\') %}\"\\\\\\n-            \"{{message[\\'content\\'] + \\'<|end|>\\' + \\'\\\\n\\'}}\"\\\\\\n-        \"{% endif %}\"\\\\\\n-    \"{% endfor %}\"\\n+        \"{{\\'<|\\' + message[\\'role\\'] + \\'|>\\\\n\\' + message[\\'content\\'] + \\'<|end|>\\\\n\\'}}\"\\\\\\n+    \"{% endfor %}\"\\\\\\n+    \"{% if add_generation_prompt %}\"\\\\\\n+        \"{{ \\'<|assistant|>\\\\n\\' }}\"\\\\\\n+    \"{% endif %}\"\\n phi3_template_eos_token = \"<|end|>\"\\n CHAT_TEMPLATES[\"phi-3\"] = (phi3_template, phi3_template_eos_token,)\\n \\n@@ -613,8 +613,80 @@ def test_chat_templates():\\n     # Phi-3\\n     template = phi3_template\\n     correct_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\\n-    correct_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    correct_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n     correct_tokenizer.chat_template = template\\n-    our_prompt = correct_tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+    our_prompt = correct_tokenizer.apply_chat_template(messages[1:], tokenize = False, add_generation_prompt = True)\\n     assert(correct_prompt == our_prompt)\\n pass\\n+\\n+\\n+def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\"):\\n+    \"\"\"\\n+        Carefully checks the output of GGUF\\'s tokenization and HF.\\n+        Can catch all tokenization bugs.\\n+    \"\"\"\\n+    import subprocess\\n+    import re\\n+    messages = [\\n+        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n+        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n+        {\"role\": \"user\", \"content\": \"  But 2+2 is equal to 5. \"},\\n+        {\"role\": \"assistant\", \"content\": \"No I\\'m sure its 4.\"},\\n+        {\"role\": \"user\", \"content\": \"  No it\\'s 100% 5! \"},\\n+    ]\\n+\\n+    prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n+\\n+    ### Instruction:\\n+    {}\\n+\\n+    ### Input:\\n+    {}\\n+\\n+    ### Response:\\n+    {}\"\"\".format(\\n+        \"Describe the city given eloquently.\", # instruction\\n+        \"The lost city of Atlantis.\", # input\\n+        \"\", # output - leave this blank for generation!\\n+    )\\n+    prompts = [ prompt, ]\\n+\\n+    if tokenizer.chat_template is not None:\\n+        prompt = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\\n+        prompt = prompt.replace(\"\\'\", \"\") # Subprocess does not like \\'\\'\\n+        prompts.append(prompts)\\n+    pass\\n+    \\n+    for prompt in prompts:\\n+        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+            f\"--check-tensors -p \\'{prompt}\\'\"\\n+\\n+        datas = []\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n+                datas.append(line.decode(\"utf-8\", errors = \"replace\"))\\n+        pass\\n+        gguf_tokens = \"\".join(datas)\\n+\\n+        # Now extract GGUF tokenization attempt\\n+        gguf_tokenized = re.findall(\"([\\\\d]{1,}) \\\\-\\\\> \\\\\\'([^\\\\\\']{1,})\\\\\\'\", gguf_tokens, flags = re.MULTILINE)\\n+        gguf_tokenized = [(int(x[0]), x[1],) for x in gguf_tokenized]\\n+        input_ids = tokenizer(prompt).input_ids\\n+        tokens = tokenizer.batch_decode(input_ids)\\n+        hf_tokenized = list(zip(input_ids, tokens))\\n+        print(gguf_tokenized[:5])\\n+\\n+        # Compare to Huggingface\\n+        for j, (hf_token, gguf_token) in enumerate(zip(hf_tokenized, gguf_tokenized)):\\n+            if (hf_token[0] != gguf_token[0]):\\n+                print(\"Failed GGUF != HF at\", j)\\n+                print(\"HF =\", hf_token)\\n+                print(\"GGUF =\", gguf_token)\\n+                print(hf_tokenized[:j+1])\\n+                print(gguf_tokenized[:j+1])\\n+                print(gguf_tokens)\\n+                raise RuntimeError(\"Failed comparing GGUF to HF.\")\\n+            pass\\n+        pass\\n+    return True\\n+pass\\n',\n",
       " '@@ -12,7 +12,8 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .loader import FastLanguageModel\\n-from .llama import FastLlamaModel\\n+from .loader  import FastLanguageModel\\n+from .llama   import FastLlamaModel\\n from .mistral import FastMistralModel\\n-from .dpo import PatchDPOTrainer\\n+from .qwen2   import FastQwen2Model\\n+from .dpo     import PatchDPOTrainer\\n',\n",
       " '@@ -30,7 +30,7 @@ import numpy as np\\n import os\\n import psutil\\n \\n-__version__ = \"2024.4\"\\n+__version__ = \"2024.5\"\\n \\n # Get Flash Attention v2 if Ampere (RTX 30xx, A100)\\n major_version, minor_version = torch.cuda.get_device_capability()\\n',\n",
       " '@@ -1605,6 +1605,7 @@ class FastLlamaModel:\\n \\n         if   model_type == \"llama\":   apply_lora_mlp = apply_lora_mlp_swiglu\\n         elif model_type == \"mistral\": apply_lora_mlp = apply_lora_mlp_swiglu\\n+        elif model_type == \"qwen2\":   apply_lora_mlp = apply_lora_mlp_swiglu\\n         elif model_type == \"gemma\":   apply_lora_mlp = apply_lora_mlp_geglu_approx\\n         else:\\n             raise NotImplementedError(f\"Unsloth: {model_type} is not yet implemented!\")\\n',\n",
       " '@@ -14,6 +14,7 @@\\n \\n from .llama import FastLlamaModel, logger\\n from .mistral import FastMistralModel\\n+from .qwen2 import FastQwen2Model\\n from transformers import AutoConfig\\n from transformers import __version__ as transformers_version\\n from peft import PeftConfig, PeftModel\\n@@ -119,6 +120,8 @@ class FastLanguageModel(FastLlamaModel):\\n                     f\"to obtain the latest transformers build, then restart this session.\"\\\\\\n                 )\\n             dispatch_model = FastGemmaModel\\n+        elif model_type == \"qwen2\":\\n+            dispatch_model = FastQwen2Model\\n         else:\\n             raise NotImplementedError(\\n                 f\"Unsloth: {model_name} not supported yet!\\\\n\"\\\\\\n',\n",
       " '@@ -343,7 +343,7 @@ class FastMistralModel(FastLlamaModel):\\n         # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n         if max_seq_length > model_max_seq_length:\\n             raise RuntimeError(\\n-                \"Unsloth: Unfortunately Mistral type models do not support RoPE scaling!\\\\n\"\\\\\\n+                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n                 f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n             )\\n         pass\\n',\n",
       " '@@ -0,0 +1,91 @@\\n+# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\\n+#\\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\\n+# you may not use this file except in compliance with the License.\\n+# You may obtain a copy of the License at\\n+#\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\n+#\\n+# Unless required by applicable law or agreed to in writing, software\\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n+# See the License for the specific language governing permissions and\\n+# limitations under the License.\\n+\\n+from .llama import *\\n+from .mistral import FastMistralModel\\n+import os\\n+from ._utils import __version__\\n+\\n+from transformers.models.qwen2.modeling_qwen2 import (\\n+    Qwen2Attention,\\n+    Qwen2DecoderLayer,\\n+    Qwen2Model,\\n+    Qwen2ForCausalLM,\\n+)\\n+# For Pytorch 2.1.1\\n+try:\\n+    from transformers.models.qwen2.modeling_qwen2 import (\\n+        Qwen2SdpaAttention,\\n+        Qwen2FlashAttention2,\\n+    )\\n+except:\\n+    Qwen2SdpaAttention   = Qwen2Attention\\n+    Qwen2FlashAttention2 = Qwen2Attention\\n+pass\\n+\\n+\\n+class FastQwen2Model(FastLlamaModel):\\n+\\n+    @staticmethod\\n+    def pre_patch():\\n+        Qwen2Attention      .forward = LlamaAttention_fast_forward\\n+        Qwen2SdpaAttention  .forward = LlamaAttention_fast_forward\\n+        Qwen2FlashAttention2.forward = LlamaAttention_fast_forward\\n+        Qwen2DecoderLayer   .forward = LlamaDecoderLayer_fast_forward\\n+        Qwen2Model          .forward = LlamaModel_fast_forward\\n+        Qwen2ForCausalLM    .forward = CausalLM_fast_forward(LlamaModel_fast_forward_inference)\\n+        PeftModelForCausalLM.forward = PeftModelForCausalLM_fast_forward\\n+\\n+        # Solves https://github.com/unslothai/unsloth/issues/168\\n+        # Static KV Cache was introduced in 4.38.0, causing training to be much slower.\\n+        # Inferene can now be CUDAGraphed, but we shall retain the old rotary embeddings.\\n+        # https://github.com/huggingface/transformers/pull/27931\\n+        # https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py\\n+        import transformers.models.qwen2.modeling_qwen2\\n+        transformers.models.qwen2.modeling_qwen2.Qwen2RotaryEmbedding = LlamaRotaryEmbedding\\n+        return\\n+    pass\\n+\\n+\\n+    @staticmethod\\n+    def from_pretrained(\\n+        model_name     = \"Qwen/Qwen1.5-7B\",\\n+        max_seq_length = 4096,\\n+        dtype          = None,\\n+        load_in_4bit   = True,\\n+        token          = None,\\n+        device_map     = \"sequential\",\\n+        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer  = True,\\n+        model_patcher  = None,\\n+        tokenizer_name = None,\\n+        trust_remote_code = False,\\n+        **kwargs,\\n+    ):\\n+        return FastMistralModel.from_pretrained(\\n+            model_name     = model_name,\\n+            max_seq_length = max_seq_length,\\n+            dtype          = dtype,\\n+            load_in_4bit   = load_in_4bit,\\n+            token          = token,\\n+            device_map     = device_map,\\n+            rope_scaling   = rope_scaling,\\n+            fix_tokenizer  = fix_tokenizer,\\n+            model_patcher  = FastQwen2Model,\\n+            tokenizer_name = tokenizer_name,\\n+            trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n+        )\\n+    pass\\n+pass\\n',\n",
       " '@@ -27,6 +27,7 @@ import subprocess\\n import psutil\\n import re\\n from transformers.models.llama.modeling_llama import logger\\n+from .tokenizer_utils import fix_sentencepiece_gguf\\n \\n __all__ = [\\n     \"print_quantization_methods\",\\n@@ -774,7 +775,7 @@ def install_llama_cpp_old(version = -10):\\n         f\"make all -j{psutil.cpu_count()*2} -C llama.cpp\",\\n     ]\\n     for command in commands:\\n-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n@@ -806,7 +807,7 @@ def install_llama_cpp_blocking(use_cuda = True):\\n     if os.path.exists(\"llama.cpp\"): return\\n \\n     for command in commands:\\n-        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, bufsize = 1) as sp:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n@@ -865,11 +866,11 @@ def save_to_gguf(\\n     first_conversion     : str = \"f16\",\\n     _run_installer = None, # Non blocking install of llama.cpp\\n ):\\n-    logger.warning(\\n-        \"NOTICE: llama.cpp GGUF conversion is currently unstable, since llama.cpp is\\\\n\"\\\\\\n-        \"undergoing some major bug fixes as at 5th of May 2024. This is not an Unsloth issue.\\\\n\"\\\\\\n-        \"Please be patient - GGUF saving should still work, but might not work as well.\"\\n-    )\\n+    # logger.warning(\\n+    #     \"NOTICE: llama.cpp GGUF conversion is currently unstable, since llama.cpp is\\\\n\"\\\\\\n+    #     \"undergoing some major bug fixes as at 5th of May 2024. This is not an Unsloth issue.\\\\n\"\\\\\\n+    #     \"Please be patient - GGUF saving should still work, but might not work as well.\"\\n+    # )\\n \\n     if quantization_method.startswith(\"iq2\"):\\n         raise RuntimeError(\"Unsloth: Currently iq2 type quantizations aren\\'t supported yet - sorry!\")\\n@@ -962,6 +963,8 @@ def save_to_gguf(\\n     # We first check if tokenizer.model exists in the model_directory\\n     if os.path.exists(f\"{model_directory}/tokenizer.model\"):\\n         vocab_type = \"spm,hfft,bpe\"\\n+        # Fix Sentencepiece model as well!\\n+        fix_sentencepiece_gguf(model_directory)\\n     else:\\n         vocab_type = \"bpe\"\\n     pass\\n@@ -969,7 +972,7 @@ def save_to_gguf(\\n     if use_fast_convert:\\n         command = f\"python llama.cpp/convert.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n-            f\"--outtype {first_conversion} --concurrency {n_cpus}\"\\n+            f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n         # Need to fix convert-hf-to-gguf.py for some models!\\n         # _fix_gemma_gguf()\\n@@ -979,7 +982,7 @@ def save_to_gguf(\\n             f\"--outtype {first_conversion}\"\\n     pass\\n \\n-    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n+    with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n             print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n@@ -1020,8 +1023,8 @@ def save_to_gguf(\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n-        with subprocess.Popen(command, shell = True, stderr = subprocess.PIPE, bufsize = 1) as sp:\\n-            for line in sp.stderr:\\n+        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n+            for line in sp.stdout:\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n@@ -1073,7 +1076,7 @@ def unsloth_save_pretrained_merged(\\n     save_peft_format     : bool = True,\\n     tags                 : List[str] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n-    maximum_memory_usage : float = 0.85,\\n+    maximum_memory_usage : float = 0.75,\\n ):\\n     \"\"\"\\n         Same as .save_pretrained(...) except 4bit weights are auto\\n@@ -1116,7 +1119,7 @@ def unsloth_push_to_hub_merged(\\n     commit_description   : str = \"Upload model trained with Unsloth 2x faster\",\\n     tags                 : Optional[List[str]] = None,\\n     temporary_location   : str = \"_unsloth_temporary_saved_buffers\",\\n-    maximum_memory_usage : float = 0.85,\\n+    maximum_memory_usage : float = 0.75,\\n ):\\n     \"\"\"\\n         Same as .push_to_hub(...) except 4bit weights are auto\\n',\n",
       " '@@ -26,6 +26,7 @@ __all__ = [\\n     \"fix_sentencepiece_tokenizer\",\\n     \"check_tokenizer\",\\n     \"add_new_tokens\",\\n+    \"fix_sentencepiece_gguf\",\\n ]\\n \\n \\n@@ -267,6 +268,71 @@ def fix_sentencepiece_tokenizer(\\n pass\\n \\n \\n+def fix_sentencepiece_gguf(saved_location):\\n+    \"\"\"\\n+        Fixes sentencepiece tokenizers which did not extend the vocabulary with\\n+        user defined tokens.\\n+        Inspiration from https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py\\n+    \"\"\"\\n+    import numpy as np\\n+    from copy import deepcopy\\n+    from transformers.utils import sentencepiece_model_pb2\\n+    import json\\n+    from enum import IntEnum\\n+    import os\\n+    \\n+    class SentencePieceTokenTypes(IntEnum):\\n+        NORMAL = 1\\n+        UNKNOWN = 2\\n+        CONTROL = 3\\n+        USER_DEFINED = 4\\n+        UNUSED = 5\\n+        BYTE = 6\\n+    pass\\n+\\n+    # Load tokenizer.model\\n+    tokenizer_file = sentencepiece_model_pb2.ModelProto()\\n+    if not os.path.isfile(f\"{saved_location}/tokenizer.model\"): return\\n+    tokenizer_file.ParseFromString(open(f\"{saved_location}/tokenizer.model\", \"rb\").read())\\n+    sentence_piece_size = len(tokenizer_file.pieces)\\n+\\n+    # Load added_tokens_json\\n+    if not os.path.isfile(f\"{saved_location}/added_tokens.json\"): return\\n+    with open(f\"{saved_location}/added_tokens.json\", \"r\", encoding = \"utf-8\") as file:\\n+        added_tokens_json = json.load(file)\\n+    pass\\n+    if len(added_tokens_json) == 0: return\\n+\\n+    added_tokens_json = dict(sorted(added_tokens_json.items(), key = lambda item: item[1]))\\n+\\n+    # Confirm added_tokens_json is correct\\n+    added_tokens_ids = np.array(list(added_tokens_json.values()))\\n+    diff = np.diff(added_tokens_ids)\\n+    if (diff.min() != 1 or diff.max() != 1): return\\n+    if (added_tokens_ids.min() != sentence_piece_size): return\\n+\\n+    # Edit sentence piece tokens with added_tokens_json\\n+    logger.warning(\"Unsloth: Extending tokenizer.model with added_tokens.json!\")\\n+    new_tokens = deepcopy(tokenizer_file.pieces[-len(added_tokens_ids):])\\n+    for new_token, added_token in zip(new_tokens, added_tokens_json.keys()):\\n+        new_token.piece = added_token.encode(\"utf-8\")\\n+        new_token.score = -1000.0\\n+        new_token.type  = SentencePieceTokenTypes.USER_DEFINED\\n+    pass\\n+\\n+    tokenizer_file.pieces.extend(new_tokens)\\n+\\n+    with open(f\"{saved_location}/tokenizer.model\", \"wb\") as file:\\n+        file.write(tokenizer_file.SerializeToString())\\n+    pass\\n+\\n+    # Add padding tokens\\n+    # actual_vocab_size = model.config.vocab_size\\n+    # padding = actual_vocab_size - len(tokenizer_file.pieces)\\n+    return\\n+pass\\n+\\n+\\n def load_correct_tokenizer(\\n     tokenizer_name,\\n     model_max_length = None,\\n',\n",
       " '@@ -180,12 +180,14 @@ def patch_tokenizer(model, tokenizer):\\n             # Try unk_token\\n             possible_pad_token = tokenizer.unk_token\\n         pass\\n+\\n         if possible_pad_token is None:\\n-            # Failure!!\\n-            raise RuntimeError(\\n-                \"Unsloth: Tokenizer\\'s pad_token cannot be = eos_token, and we couldn\\'t find a\\\\n\"\\\\\\n-                \"replacement of either <|reserved... or <|placeholder...\"\\n-            )\\n+            # Failure to find a good replacement!! We shall manually add one!\\n+            new_pad_token = \"<|PAD_TOKEN|>\"\\n+            while new_pad_token in tokenizer.get_vocab():\\n+                new_pad_token += \"#\"\\n+            pass\\n+            possible_pad_token = new_pad_token\\n         pass\\n \\n         name = model.config._name_or_path if model is not None else \"Model\"\\n',\n",
       " '@@ -1005,7 +1005,7 @@ class FastLlamaModel:\\n     @staticmethod\\n     def from_pretrained(\\n         model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = 4096,\\n+        max_seq_length = None,\\n         dtype          = None,\\n         load_in_4bit   = True,\\n         token          = None,\\n@@ -1050,6 +1050,11 @@ class FastLlamaModel:\\n         model_max_seq_length = \\\\\\n             AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n \\n+        # If max_seq_length is not specified, use maximum fron config\\n+        if max_seq_length is None:\\n+            max_seq_length = model_max_seq_length\\n+        pass\\n+\\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n             rope_scaling = max_seq_length / model_max_seq_length\\n             logger.warning_once(\\n',\n",
       " '@@ -67,8 +67,8 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = 4096,\\n+        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length = None,\\n         dtype          = None,\\n         load_in_4bit   = True,\\n         token          = None,\\n',\n",
       " '@@ -290,7 +290,7 @@ class FastMistralModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n         model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = 4096,\\n+        max_seq_length = None,\\n         dtype          = None,\\n         load_in_4bit   = True,\\n         token          = None,\\n@@ -340,6 +340,11 @@ class FastMistralModel(FastLlamaModel):\\n         model_config = AutoConfig.from_pretrained(model_name, token = token)\\n         model_max_seq_length = model_config.max_position_embeddings\\n \\n+        # If max_seq_length is not specified, use maximum fron config\\n+        if max_seq_length is None:\\n+            max_seq_length = model_max_seq_length\\n+        pass\\n+\\n         # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n         if max_seq_length > model_max_seq_length:\\n             raise RuntimeError(\\n',\n",
       " '@@ -304,6 +304,7 @@ def fix_sentencepiece_gguf(saved_location):\\n     if len(added_tokens_json) == 0: return\\n \\n     added_tokens_json = dict(sorted(added_tokens_json.items(), key = lambda item: item[1]))\\n+    new_size = sentence_piece_size + len(added_tokens_json)\\n \\n     # Confirm added_tokens_json is correct\\n     added_tokens_ids = np.array(list(added_tokens_json.values()))\\n@@ -312,7 +313,11 @@ def fix_sentencepiece_gguf(saved_location):\\n     if (added_tokens_ids.min() != sentence_piece_size): return\\n \\n     # Edit sentence piece tokens with added_tokens_json\\n-    logger.warning(\"Unsloth: Extending tokenizer.model with added_tokens.json!\")\\n+    logger.warning(\\n+        f\"Unsloth: Extending {saved_location}/tokenizer.model with added_tokens.json.\\\\n\"\\\\\\n+        f\"Originally tokenizer.model is of size ({sentence_piece_size}).\\\\n\"\\\\\\n+        f\"But we need to extend to sentencepiece vocab size ({new_size}).\"\\n+    )\\n     new_tokens = deepcopy(tokenizer_file.pieces[-len(added_tokens_ids):])\\n     for new_token, added_token in zip(new_tokens, added_tokens_json.keys()):\\n         new_token.piece = added_token.encode(\"utf-8\")\\n@@ -357,7 +362,10 @@ def load_correct_tokenizer(\\n             padding_side      = padding_side,\\n             token             = token,\\n             trust_remote_code = trust_remote_code,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n             use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n             cache_dir         = cache_dir,\\n         )\\n     except:\\n@@ -512,7 +520,10 @@ def check_tokenizer(\\n                     model_max_length = model_max_length,\\n                     padding_side = padding_side,\\n                     token = token,\\n+                    # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n                     use_fast = False,\\n+                    legacy = False,\\n+                    from_slow = True,\\n                     cache_dir = cache_dir,\\n                 )\\n                 return check_tokenizer(\\n@@ -725,7 +736,8 @@ def fix_sft_trainer_tokenizer():\\n         \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n-        \"has_bos_token_already = test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template\\\\n\"\\\\\\n+        \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n+        \"if getattr(tokenizer, \\'bos_token\\', None) is not None else False\\\\n\"\\\\\\n         \"add_special_tokens = False if has_bos_token_already else add_special_tokens\\\\n\\\\n\"\\n \\n         check_text = check_text.split(\"\\\\n\")\\n',\n",
       " '@@ -15,11 +15,12 @@\\n import torch\\n from typing import Union, Optional, List, Any, Callable\\n import warnings\\n-warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"torch\")\\n-warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"huggingface_hub\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning,    module = \"torch\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning,    module = \"huggingface_hub\")\\n warnings.filterwarnings(action = \"ignore\", category = RuntimeWarning, module = \"subprocess\")\\n-warnings.filterwarnings(action = \"ignore\", category = UserWarning, module = \"transformers\")\\n-warnings.filterwarnings(action = \"ignore\", category = FutureWarning, module = \"accelerate\")\\n+warnings.filterwarnings(action = \"ignore\", category = UserWarning,    module = \"transformers\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning,  module = \"accelerate\")\\n+warnings.filterwarnings(action = \"ignore\", category = FutureWarning,  module = \"huggingface_hub\")\\n import bitsandbytes as bnb\\n from transformers.models.llama.modeling_llama import logger\\n from transformers import AutoTokenizer\\n@@ -388,3 +389,35 @@ class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):\\n     pass\\n pass\\n \\n+\\n+\"\"\"\\n+    Remove warnings about missing kwargs\\n+\"\"\"\\n+try:\\n+    from transformers.utils.quantization_config import BitsAndBytesConfig, QuantizationMethod\\n+    from inspect import getsource\\n+    import re\\n+    BitsAndBytesConfig__init__ = getsource(BitsAndBytesConfig.__init__)\\n+    BitsAndBytesConfig__init__ = re.sub(\\n+        r\"if[\\\\s]{1,}kwargs\\\\:[\\\\s]{1,}.+?\\\\n\",\\n+        \"\",\\n+        BitsAndBytesConfig__init__,\\n+        flags = re.MULTILINE,\\n+    )\\n+    BitsAndBytesConfig__init__ = BitsAndBytesConfig__init__.split(\"\\\\n\")\\n+    length_spaces = len(re.match(r\"[\\\\s]{1,}\", BitsAndBytesConfig__init__[0]).group(0))\\n+    BitsAndBytesConfig__init__ = \"\\\\n\".join(x[length_spaces:] for x in BitsAndBytesConfig__init__)\\n+    BitsAndBytesConfig__init__ = BitsAndBytesConfig__init__.replace(\\n+        \"__init__\",\\n+        \"_BitsAndBytesConfig__init__\",\\n+    )\\n+    exec(BitsAndBytesConfig__init__, globals())\\n+    \\n+    import transformers.utils.quantization_config\\n+    transformers.utils.quantization_config.BitsAndBytesConfig.__init__ = _BitsAndBytesConfig__init__\\n+except:\\n+    logger.warning_once(\\n+        \"Unsloth unsuccessfully patched bitsandbytes. Please file a bug report.\\\\n\"\\\\\\n+        \"Luckily, your training run will still work in the meantime!\"\\n+    )\\n+pass\\n',\n",
       " '@@ -71,7 +71,7 @@ def GemmaDecoderLayer_fast_forward(\\n     padding_mask:         Optional[torch.LongTensor] = None,\\n     *args, **kwargs,\\n ):\\n-    if use_cache: #past_key_value is not None:\\n+    if use_cache and hasattr(self, \"_flag_for_generation\"): #past_key_value is not None:\\n         out_weight = torch.empty(self.input_layernorm.weight.shape, dtype = torch.float32, device = \"cuda\")\\n \\n         # Self Attention\\n',\n",
       " '@@ -407,7 +407,7 @@ def LlamaDecoderLayer_fast_forward(\\n             (see `past_key_values`).\\n         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\\n     \"\"\"\\n-    if use_cache:\\n+    if use_cache and hasattr(self, \"_flag_for_generation\"):\\n         residual = hidden_states\\n         hidden_states = fast_rms_layernorm_inference(self.input_layernorm, hidden_states)\\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\\n@@ -789,7 +789,7 @@ def CausalLM_fast_forward(fast_forward_inference):\\n         return_dict: Optional[bool] = None,\\n         *args, **kwargs,\\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\\n-\\n+        \\n         if past_key_values is not None:\\n             outputs = fast_forward_inference(\\n                 self,\\n@@ -968,12 +968,34 @@ class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\\n pass\\n \\n \\n-def _wrap_fast_inference(generate, device_type, dtype):\\n+def _wrap_fast_inference(generate, device_type, dtype, model):\\n     # Wraps inference with bfloat16 / float16\\n     @torch.inference_mode\\n     def _fast_generate(*args, **kwargs):\\n+\\n+        # Set a flag for generation!\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            internal_model._flag_for_generation = True\\n+            internal_model = internal_model.model\\n+        pass\\n+        internal_model._flag_for_generation = True\\n+\\n+        # Autocasted\\n         with torch.autocast(device_type = device_type, dtype = dtype):\\n-            return generate(*args, **kwargs)\\n+            output = generate(*args, **kwargs)\\n+        pass\\n+\\n+        # Unset a flag for generation!\\n+        internal_model = model\\n+        while hasattr(internal_model, \"model\"):\\n+            if hasattr(internal_model, \"_flag_for_generation\"): del internal_model._flag_for_generation\\n+            internal_model = internal_model.model\\n+        pass\\n+        if hasattr(internal_model, \"_flag_for_generation\"): del internal_model._flag_for_generation\\n+\\n+        return output\\n+    pass\\n     return _fast_generate\\n pass\\n \\n@@ -1787,7 +1809,7 @@ class FastLlamaModel:\\n \\n         # Wrap model.generate\\n         model._unwrapped_old_generate = model.generate\\n-        model.generate = _wrap_fast_inference(model.generate, device_type, dtype)\\n+        model.generate = _wrap_fast_inference(model.generate, device_type, dtype, model)\\n \\n         # Patch tokenizer to pad to the left\\n         internal_model = model\\n',\n",
       " '@@ -369,10 +369,11 @@ def load_correct_tokenizer(\\n             cache_dir         = cache_dir,\\n         )\\n     except:\\n-        print(\\n-            f\"Unsloth: {tokenizer_name} has no tokenizer.model file.\\\\n\"\\\\\\n-            \"Just informing you about this - this is not a critical error.\"\\n-        )\\n+        pass\\n+        # print(\\n+        #     f\"Unsloth: {tokenizer_name} has no tokenizer.model file.\\\\n\"\\\\\\n+        #     \"Just informing you about this - this is not a critical error.\"\\n+        # )\\n     pass\\n \\n     fast_tokenizer = AutoTokenizer.from_pretrained(\\n',\n",
       " '@@ -43,7 +43,7 @@ huggingface = [\\n     \"numpy\",\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n-    \"peft>=0.7.1\",\\n+    \"peft>=0.7.1,<0.11.0\",\\n     \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n',\n",
       " '@@ -777,6 +777,8 @@ def install_llama_cpp_old(version = -10):\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                if \"undefined reference\" in line:\\n+                    raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n@@ -809,6 +811,8 @@ def install_llama_cpp_blocking(use_cuda = True):\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                if \"undefined reference\" in line:\\n+                    raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n@@ -984,6 +988,8 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n+            if \"undefined reference\" in line:\\n+                raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n             print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n@@ -1025,6 +1031,8 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                if \"undefined reference\" in line:\\n+                    raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n',\n",
       " '@@ -43,7 +43,7 @@ huggingface = [\\n     \"numpy\",\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n-    \"peft>=0.7.1\",\\n+    \"peft>=0.7.1,<0.11.0\",\\n     \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n',\n",
       " '@@ -777,6 +777,8 @@ def install_llama_cpp_old(version = -10):\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                if \"undefined reference\" in line:\\n+                    raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n@@ -809,6 +811,8 @@ def install_llama_cpp_blocking(use_cuda = True):\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                if \"undefined reference\" in line:\\n+                    raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         pass\\n     pass\\n@@ -984,6 +988,8 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n+            if \"undefined reference\" in line:\\n+                raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n             print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n@@ -1025,6 +1031,8 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                if \"undefined reference\" in line:\\n+                    raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n                 print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n',\n",
       " '@@ -777,9 +777,10 @@ def install_llama_cpp_old(version = -10):\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                line = line.decode(\"utf-8\", errors = \"replace\")\\n                 if \"undefined reference\" in line:\\n                     raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n-                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n+                print(line, flush = True, end = \"\")\\n         pass\\n     pass\\n     # Check if successful\\n@@ -811,9 +812,10 @@ def install_llama_cpp_blocking(use_cuda = True):\\n     for command in commands:\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                line = line.decode(\"utf-8\", errors = \"replace\")\\n                 if \"undefined reference\" in line:\\n                     raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n-                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n+                print(line, flush = True, end = \"\")\\n         pass\\n     pass\\n pass\\n@@ -988,9 +990,10 @@ def save_to_gguf(\\n \\n     with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n         for line in sp.stdout:\\n+            line = line.decode(\"utf-8\", errors = \"replace\")\\n             if \"undefined reference\" in line:\\n                 raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n-            print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n+            print(line, flush = True, end = \"\")\\n         if sp.returncode is not None and sp.returncode != 0:\\n             raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n     pass\\n@@ -1031,9 +1034,10 @@ def save_to_gguf(\\n         # quantize uses stderr\\n         with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\\n             for line in sp.stdout:\\n+                line = line.decode(\"utf-8\", errors = \"replace\")\\n                 if \"undefined reference\" in line:\\n                     raise RuntimeError(\"Failed compiling llama.cpp. Please report this ASAP!\")\\n-                print(line.decode(\"utf-8\", errors = \"replace\"), flush = True, end = \"\")\\n+                print(line, flush = True, end = \"\")\\n             if sp.returncode is not None and sp.returncode != 0:\\n                 raise subprocess.CalledProcessError(sp.returncode, sp.args)\\n         pass\\n',\n",
       " '@@ -43,7 +43,7 @@ huggingface = [\\n     \"numpy\",\\n     \"accelerate>=0.26.1\",\\n     \"trl>=0.7.9\",\\n-    \"peft>=0.7.1,<0.11.0\",\\n+    \"peft>=0.7.1,!=0.11.0\",\\n     \"protobuf<4.0.0\",\\n ]\\n cu118only = [\\n',\n",
       " '@@ -31,3 +31,9 @@ from .fast_lora import (\\n \\tapply_lora_o,\\n )\\n from .utils import fast_dequantize, fast_gemv, QUANT_STATE, fast_linear_forward, matmul_lora\\n+\\n+try:\\n+\\tprint(\"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\")\\n+except:\\n+\\tprint(\"Unsloth: Will patch your computer to enable 2x faster free finetuning.\")\\n+pass\\n',\n",
       " '@@ -10,7 +10,7 @@\\n <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n \\n-### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!\\n+### Finetune Llama 3, Mistral, Phi-3 & Gemma 2-5x faster with 80% less memory!\\n \\n ![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n \\n@@ -24,24 +24,24 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n |-----------|---------|--------|----------|\\n | **Llama 3 (8B)**      | [▶️ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n | **Mistral v3 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing)               | 2.2x faster | 73% less |\\n-| **Mistral v1 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n+| **Phi-3 (medium)** | [▶️ Start for free](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)               | 2x faster | 50% less |\\n+| **Phi-3 (mini)** | [▶️ Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |\\n | **Gemma (7B)**      | [▶️ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n | **ORPO**     | [▶️ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n | **DPO Zephyr**     | [▶️ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n-| **Phi-3 (3.8B)** | [▶️ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |\\n | **TinyLlama**  | [▶️ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |\\n \\n-- Benchmarking compared to FA2 + Hugging Face combined.\\n-- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n-- Also [Llama-3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing). [Mistral 7b v1 ChatML](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing). [Mistral 7b v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing).\\n-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.\\n+- **Kaggle Notebooks** for [Llama 3 8B](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7B](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7B](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n+- Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n+\\n \\n ## 🦥 Unsloth.ai News\\n-- 📣 NEW! Mistral v3 Base and Instruct now supported! 2x faster, 70% less VRAM notebooks for the [base model](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [instruct with ShareGPT](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- 📣 NEW! [Phi-3 medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) and [Phi-3 mini](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing) support is here!\\n+- 📣 NEW! [Mistral v3 Base](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [Mistral v3 Instruct](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing) support is here!\\n - 📣 NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly\\'s PR [#428](https://github.com/unslothai/unsloth/pull/428)\\n - 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n - 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n-- 📣 NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!\\n - 📣 NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you\\'re using our notebooks. To enable, simply change 1 line:\\n ```python\\n model = FastLanguageModel.get_peft_model(\\n@@ -195,15 +195,15 @@ dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\\n \\n # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\\n fourbit_models = [\\n+    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\\n+    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\\n+    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\\n+    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\\n+    \"unsloth/llama-3-70b-bnb-4bit\",\\n+    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\\n+    \"unsloth/Phi-3-medium-4k-instruct\",\\n     \"unsloth/mistral-7b-bnb-4bit\",\\n-    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\\n-    \"unsloth/gemma-2b-bnb-4bit\",\\n-    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\\n-    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\\n-    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\\n+    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\\n ] # More models at https://huggingface.co/unsloth\\n \\n model, tokenizer = FastLanguageModel.from_pretrained(\\n',\n",
       " '@@ -10,7 +10,7 @@\\n <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n \\n-### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!\\n+### Finetune Llama 3, Mistral, Phi-3 & Gemma 2-5x faster with 80% less memory!\\n \\n ![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n \\n@@ -24,24 +24,24 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n |-----------|---------|--------|----------|\\n | **Llama 3 (8B)**      | [▶️ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n | **Mistral v3 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing)               | 2.2x faster | 73% less |\\n-| **Mistral v1 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n+| **Phi-3 (medium)** | [▶️ Start for free](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)               | 2x faster | 50% less |\\n+| **Phi-3 (mini)** | [▶️ Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |\\n | **Gemma (7B)**      | [▶️ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n | **ORPO**     | [▶️ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n | **DPO Zephyr**     | [▶️ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n-| **Phi-3 (3.8B)** | [▶️ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |\\n | **TinyLlama**  | [▶️ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |\\n \\n-- Benchmarking compared to FA2 + Hugging Face combined.\\n-- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n-- Also [Llama-3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing). [Mistral 7b v1 ChatML](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing). [Mistral 7b v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing).\\n-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.\\n+- **Kaggle Notebooks** for [Llama 3 8B](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7B](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7B](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n+- Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n+\\n \\n ## 🦥 Unsloth.ai News\\n-- 📣 NEW! Mistral v3 Base and Instruct now supported! 2x faster, 70% less VRAM notebooks for the [base model](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [instruct with ShareGPT](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- 📣 NEW! [Phi-3 medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) and [Phi-3 mini](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing) support is here!\\n+- 📣 NEW! [Mistral v3 Base](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [Mistral v3 Instruct](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing) support is here!\\n - 📣 NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly\\'s PR [#428](https://github.com/unslothai/unsloth/pull/428)\\n - 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n - 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n-- 📣 NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!\\n - 📣 NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you\\'re using our notebooks. To enable, simply change 1 line:\\n ```python\\n model = FastLanguageModel.get_peft_model(\\n@@ -195,15 +195,15 @@ dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\\n \\n # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\\n fourbit_models = [\\n+    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\\n+    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\\n+    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\\n+    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\\n+    \"unsloth/llama-3-70b-bnb-4bit\",\\n+    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\\n+    \"unsloth/Phi-3-medium-4k-instruct\",\\n     \"unsloth/mistral-7b-bnb-4bit\",\\n-    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\\n-    \"unsloth/gemma-2b-bnb-4bit\",\\n-    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\\n-    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\\n-    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\\n+    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\\n ] # More models at https://huggingface.co/unsloth\\n \\n model, tokenizer = FastLanguageModel.from_pretrained(\\n',\n",
       " '@@ -10,7 +10,7 @@\\n <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n \\n-### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!\\n+### Finetune Llama 3, Mistral, Phi-3 & Gemma 2-5x faster with 80% less memory!\\n \\n ![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n \\n@@ -24,24 +24,24 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n |-----------|---------|--------|----------|\\n | **Llama 3 (8B)**      | [▶️ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n | **Mistral v3 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing)               | 2.2x faster | 73% less |\\n-| **Mistral v1 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n+| **Phi-3 (medium)** | [▶️ Start for free](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)               | 2x faster | 50% less |\\n+| **Phi-3 (mini)** | [▶️ Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |\\n | **Gemma (7B)**      | [▶️ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n | **ORPO**     | [▶️ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n | **DPO Zephyr**     | [▶️ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n-| **Phi-3 (3.8B)** | [▶️ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |\\n | **TinyLlama**  | [▶️ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |\\n \\n-- Benchmarking compared to FA2 + Hugging Face combined.\\n-- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n-- Also [Llama-3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing). [Mistral 7b v1 ChatML](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing). [Mistral 7b v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing).\\n-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.\\n+- **Kaggle Notebooks** for [Llama 3 8B](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7B](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7B](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n+- Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n+\\n \\n ## 🦥 Unsloth.ai News\\n-- 📣 NEW! Mistral v3 Base and Instruct now supported! 2x faster, 70% less VRAM notebooks for the [base model](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [instruct with ShareGPT](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- 📣 NEW! [Phi-3 medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) and [Phi-3 mini](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing) support is here!\\n+- 📣 NEW! [Mistral v3 Base](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [Mistral v3 Instruct](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing) support is here!\\n - 📣 NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly\\'s PR [#428](https://github.com/unslothai/unsloth/pull/428)\\n - 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n - 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n-- 📣 NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!\\n - 📣 NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you\\'re using our notebooks. To enable, simply change 1 line:\\n ```python\\n model = FastLanguageModel.get_peft_model(\\n@@ -195,15 +195,15 @@ dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\\n \\n # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\\n fourbit_models = [\\n+    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\\n+    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\\n+    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\\n+    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\\n+    \"unsloth/llama-3-70b-bnb-4bit\",\\n+    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\\n+    \"unsloth/Phi-3-medium-4k-instruct\",\\n     \"unsloth/mistral-7b-bnb-4bit\",\\n-    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\\n-    \"unsloth/gemma-2b-bnb-4bit\",\\n-    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\\n-    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\\n-    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\\n+    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\\n ] # More models at https://huggingface.co/unsloth\\n \\n model, tokenizer = FastLanguageModel.from_pretrained(\\n',\n",
       " '@@ -10,7 +10,7 @@\\n <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\\n <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png\" height=\"48\"></a>\\n \\n-### Finetune Llama 3, Mistral & Gemma 2-5x faster with 80% less memory!\\n+### Finetune Llama 3, Mistral, Phi-3 & Gemma 2-5x faster with 80% less memory!\\n \\n ![](https://i.ibb.co/sJ7RhGG/image-41.png)\\n \\n@@ -24,24 +24,24 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n |-----------|---------|--------|----------|\\n | **Llama 3 (8B)**      | [▶️ Start for free](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2x faster | 60% less |\\n | **Mistral v3 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing)               | 2.2x faster | 73% less |\\n-| **Mistral v1 (7B)**    | [▶️ Start for free](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 73% less |\\n+| **Phi-3 (medium)** | [▶️ Start for free](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)               | 2x faster | 50% less |\\n+| **Phi-3 (mini)** | [▶️ Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |\\n | **Gemma (7B)**      | [▶️ Start for free](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 71% less |\\n | **ORPO**     | [▶️ Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |\\n | **DPO Zephyr**     | [▶️ Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |\\n-| **Phi-3 (3.8B)** | [▶️ Start for free](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing)               | 2x faster | 50% less |\\n | **TinyLlama**  | [▶️ Start for free](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)               | 3.9x faster | 74% less |\\n \\n-- Benchmarking compared to FA2 + Hugging Face combined.\\n-- **Kaggle Notebooks** for [Llama-3 8b](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7b](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7b](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n-- Also [Llama-3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing). [Mistral 7b v1 ChatML](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing). [Mistral 7b v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing).\\n-- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text.\\n+- **Kaggle Notebooks** for [Llama 3 8B](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook), [Gemma 7B](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral 7B](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\\n+- Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n+\\n \\n ## 🦥 Unsloth.ai News\\n-- 📣 NEW! Mistral v3 Base and Instruct now supported! 2x faster, 70% less VRAM notebooks for the [base model](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [instruct with ShareGPT](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n+- 📣 NEW! [Phi-3 medium](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing) and [Phi-3 mini](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing) support is here!\\n+- 📣 NEW! [Mistral v3 Base](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing) and [Mistral v3 Instruct](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing) support is here!\\n - 📣 NEW! Qwen1.5-7B, Qwen1.5-14B, Qwen1.5-32B, Qwen1.5-72B now work, courtesy of Firefly\\'s PR [#428](https://github.com/unslothai/unsloth/pull/428)\\n - 📣 NEW! [Llama-3 8b](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing) now works! Llama-3 70b also works (change the model name in the notebook).\\n - 📣 NEW! [ORPO support](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing) is here!\\n-- 📣 NEW! [Phi-3 3.8b support](https://colab.research.google.com/drive/1NvkBmkHfucGO3Ve9s1NKZvMNlw5p83ym?usp=sharing) is here!\\n - 📣 NEW! We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support fine-tuning of LLMs with [4x longer context windows](https://unsloth.ai/blog/long-context)! No change required if you\\'re using our notebooks. To enable, simply change 1 line:\\n ```python\\n model = FastLanguageModel.get_peft_model(\\n@@ -195,15 +195,15 @@ dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\\n \\n # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\\n fourbit_models = [\\n+    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\\n+    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\\n+    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\\n+    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\\n+    \"unsloth/llama-3-70b-bnb-4bit\",\\n+    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\\n+    \"unsloth/Phi-3-medium-4k-instruct\",\\n     \"unsloth/mistral-7b-bnb-4bit\",\\n-    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\\n-    \"unsloth/llama-2-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-bnb-4bit\",\\n-    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\\n-    \"unsloth/gemma-2b-bnb-4bit\",\\n-    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\\n-    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\\n-    \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\\n+    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\\n ] # More models at https://huggingface.co/unsloth\\n \\n model, tokenizer = FastLanguageModel.from_pretrained(\\n',\n",
       " '@@ -176,7 +176,7 @@ def patch_tokenizer(model, tokenizer):\\n \\n     if bad_pad_token:\\n         # Find a better pad token\\n-        aadded_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n+        added_tokens = [str(x) for x in tokenizer.added_tokens_decoder.values()]\\n         possible_pad_token = None\\n         n_possible_pad_tokens = 0\\n         for added_token in added_tokens[::-1]:\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -0,0 +1,87 @@\\n+## LoraConfig Parameters\\r\\n+\\r\\n+Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n+\\r\\n+**r**\\r\\n+- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Retains more information, increases computational load.\\r\\n+  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n+\\r\\n+\\r\\n+**lora_alpha**\\r\\n+- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n+  - **Lower**: Subtler effect, may require more training steps.\\r\\n+\\r\\n+**lora_dropout**\\r\\n+- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n+  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n+\\r\\n+**loftq_config**\\r\\n+- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n+- **Impact**:\\r\\n+  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n+  - **None**: LoftQ quantization is not applied.\\r\\n+  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n+\\r\\n+\\r\\n+**use_rslora**\\r\\n+- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n+- **Impact**:\\r\\n+  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n+  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n+\\r\\n+**gradient_accumulation_steps**\\r\\n+- **Default**: 1\\r\\n+- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n+- **Impact**: \\r\\n+  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n+  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n+\\r\\n+**weight_decay**\\r\\n+- **Default**: 0.01\\r\\n+- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n+- **Impact**:\\r\\n+  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n+  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n+\\r\\n+**learning_rate**\\r\\n+- **Default**: 2e-4\\r\\n+- **Description**: The rate at which the model updates its parameters during training.\\r\\n+- **Impact**:\\r\\n+  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n+  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n+\\r\\n+## Target Modules \\r\\n+\\r\\n+**q_proj (query projection)**\\r\\n+- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n+- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n+\\r\\n+**k_proj (key projection)**\\r\\n+- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n+- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n+\\r\\n+**v_proj (value projection)**\\r\\n+- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n+- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n+\\r\\n+**o_proj (output projection)**\\r\\n+- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n+- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n+\\r\\n+**gate_proj (gate projection)**\\r\\n+- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n+- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n+\\r\\n+**up_proj (up projection)**\\r\\n+- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n+- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n+\\r\\n+**down_proj (down projection)**\\r\\n+- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n+- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -14,8 +14,20 @@\\n import os\\n import warnings\\n import importlib\\n+import sys\\n+from packaging.version import Version\\n \\n-# Currently only supports 1 GPU, or else seg faults will occur.\\n+# Define a list of modules to check\\n+MODULES_TO_CHECK = [\"peft\", \"bitsandbytes\"]\\n+\\n+# Check if any of the modules in the list have been imported\\n+for module in MODULES_TO_CHECK:\\n+    if module in sys.modules:\\n+        raise ImportError(f\"Unsloth: Please import Unsloth before {module}.\")\\n+    pass\\n+pass\\n+\\n+# Currently only supports 1 GPU, or else seg faults will occur.    \\n if \"CUDA_VISIBLE_DEVICES\" in os.environ:\\n     os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\\n     devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\\n@@ -66,8 +78,14 @@ pass\\n \\n # Try loading bitsandbytes and triton\\n import bitsandbytes as bnb\\n+\\n import triton\\n-from triton.common.build import libcuda_dirs\\n+libcuda_dirs = lambda: None\\n+if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+    try: from triton.backends.nvidia.driver import libcuda_dirs\\n+    except: pass\\n+else: from triton.common.build import libcuda_dirs\\n+\\n import os\\n import re\\n import numpy as np\\n@@ -103,8 +121,11 @@ except:\\n     importlib.reload(bnb)\\n     importlib.reload(triton)\\n     try:\\n-        import bitsandbytes as bnb\\n-        from triton.common.build import libcuda_dirs\\n+        libcuda_dirs = lambda: None\\n+        if Version(triton.__version__) >= Version(\"3.0.0\"):\\n+            try: from triton.backends.nvidia.driver import libcuda_dirs\\n+            except: pass\\n+        else: from triton.common.build import libcuda_dirs\\n         cdequantize_blockwise_fp32 = bnb.functional.lib.cdequantize_blockwise_fp32\\n         libcuda_dirs()\\n     except:\\n',\n",
       " '@@ -1286,7 +1286,7 @@ def test_hf_gguf_equivalence(tokenizer, gguf_model = \"./model-unsloth.F16.gguf\")\\n     pass\\n     \\n     for prompt in prompts:\\n-        command = f\"./llama.cpp/main -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n+        command = f\"./llama.cpp/llama-cli -m {gguf_model} -n 0 --temp 0.0 --verbose-prompt \"\\\\\\n             f\"--check-tensors -p \\'{prompt}\\'\"\\n \\n         datas = []\\n',\n",
       " '@@ -24,6 +24,7 @@ from .geglu import (\\n )\\n from .fast_lora import (\\n \\tget_lora_parameters,\\n+\\tget_lora_parameters_bias,\\n \\tapply_lora_mlp_swiglu,\\n \\tapply_lora_mlp_geglu_exact,\\n \\tapply_lora_mlp_geglu_approx,\\n',\n",
       " '@@ -13,7 +13,13 @@\\n # limitations under the License.\\n \\n import torch\\n-from .utils import fast_dequantize, QUANT_STATE, get_lora_parameters, matmul_lora\\n+from .utils import (\\n+    fast_dequantize,\\n+    QUANT_STATE,\\n+    get_lora_parameters,\\n+    get_lora_parameters_bias,\\n+    matmul_lora,\\n+)\\n \\n \\n class LoRA_MLP(torch.autograd.Function):\\n',\n",
       " '@@ -33,11 +33,8 @@ del major, minor\\n \\n def _get_model_name(model_name, load_in_4bit = True):\\n \\n-    # First try replacing lowercase \\'b\\' with uppercase \\'B\\'\\n-    model_name = model_name.lower()\\n-\\n     if not SUPPORTS_FOURBIT and model_name in INT_TO_FLOAT_MAPPER:\\n-        model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         logger.warning_once(\\n             f\"Unsloth: Your transformers version of {transformers_version} does not support native \"\\\\\\n             f\"4bit loading.\\\\nThe minimum required version is 4.37.\\\\n\"\\\\\\n@@ -47,7 +44,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         )\\n     \\n     elif not load_in_4bit and model_name in INT_TO_FLOAT_MAPPER:\\n-        new_model_name = INT_TO_FLOAT_MAPPER[model_name]\\n+        new_model_name = INT_TO_FLOAT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` which is a 4bit model, yet you set\\\\n\"\\\\\\n         #     f\"`load_in_4bit = False`. We shall load `{new_model_name}` instead.\"\\n@@ -55,7 +52,7 @@ def _get_model_name(model_name, load_in_4bit = True):\\n         model_name = new_model_name\\n \\n     elif load_in_4bit and SUPPORTS_FOURBIT and model_name in FLOAT_TO_INT_MAPPER:\\n-        new_model_name = FLOAT_TO_INT_MAPPER[model_name]\\n+        new_model_name = FLOAT_TO_INT_MAPPER[model_name.lower()]\\n         # logger.warning_once(\\n         #     f\"Unsloth: You passed in `{model_name}` and `load_in_4bit = True`.\\\\n\"\\\\\\n         #     f\"We shall load `{new_model_name}` for 4x faster loading.\"\\n@@ -70,17 +67,18 @@ pass\\n class FastLanguageModel(FastLlamaModel):\\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-3-8b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        trust_remote_code = False,\\n-        use_gradient_checkpointing = True,\\n-        resize_model_vocab = None,\\n+        model_name                 = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length             = None,\\n+        dtype                      = None,\\n+        load_in_4bit               = True,\\n+        token                      = None,\\n+        device_map                 = \"sequential\",\\n+        rope_scaling               = None,\\n+        fix_tokenizer              = True,\\n+        trust_remote_code          = False,\\n+        use_gradient_checkpointing = \"unsloth\",\\n+        resize_model_vocab         = None,\\n+        revision                   = None,\\n         *args, **kwargs,\\n     ):\\n         if token is None and \"HF_TOKEN\" in os.environ:\\n@@ -95,12 +93,12 @@ class FastLanguageModel(FastLlamaModel):\\n         # First check if it\\'s a normal model via AutoConfig\\n         is_peft = False\\n         try:\\n-            model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+            model_config = AutoConfig.from_pretrained(model_name, token = token, revision = revision)\\n             is_peft = False\\n         except:\\n             try:\\n                 # Most likely a PEFT model\\n-                peft_config = PeftConfig.from_pretrained(model_name, token = token)\\n+                peft_config = PeftConfig.from_pretrained(model_name, token = token, revision = revision)\\n             except:\\n                 raise RuntimeError(f\"Unsloth: `{model_name}` is not a full model or a PEFT model.\")\\n             \\n@@ -143,22 +141,24 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         model, tokenizer = dispatch_model.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = dispatch_model,\\n-            tokenizer_name = tokenizer_name,\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = dispatch_model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            revision          = revision if not is_peft else None,\\n             *args, **kwargs,\\n         )\\n         \\n         if resize_model_vocab is not None:\\n             model.resize_token_embeddings(resize_model_vocab)\\n+        pass\\n \\n         # In case the model supports tagging, add the unsloth tag.\\n         if hasattr(model, \"add_model_tags\"):\\n@@ -188,8 +188,16 @@ class FastLanguageModel(FastLlamaModel):\\n         pass\\n \\n         if is_peft:\\n+            # From https://github.com/huggingface/peft/issues/184\\n             # Now add PEFT adapters\\n-            model = PeftModel.from_pretrained(model, old_model_name, token = token)\\n+            model.enable_input_require_grads()\\n+            model = PeftModel.from_pretrained(\\n+                model,\\n+                old_model_name,\\n+                token = token,\\n+                revision = revision,\\n+                is_trainable = True,\\n+            )\\n             # Patch it as well!\\n             model = dispatch_model.patch_peft_model(model, use_gradient_checkpointing)\\n         pass\\n',\n",
       " '@@ -186,6 +186,9 @@ __INT_TO_FLOAT_MAPPER = \\\\\\n     \"unsloth/Qwen2-70B-Instruct-bnb-4bit\" : (\\n         \"Qwen/Qwen2-70B-Instruct\",\\n     ),\\n+    \"mistralai/Codestral-22B-v0.1\" : (\\n+        \"mistral-community/Codestral-22B-v0.1\",\\n+    ),\\n }\\n \\n INT_TO_FLOAT_MAPPER = {}\\n',\n",
       " '@@ -22,7 +22,7 @@ import shutil\\n import pickle\\n import gc\\n from transformers.models.llama.modeling_llama import logger\\n-from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters\\n+from .kernels import fast_dequantize, QUANT_STATE, get_lora_parameters_bias\\n import subprocess\\n import psutil\\n import re\\n@@ -132,9 +132,10 @@ pass\\n \\n def _merge_lora(layer, name):\\n \\n+    bias = None\\n     if isinstance(layer, (Bnb_Linear4bit, Peft_Linear4bit, Peft_Linear)):\\n         # Is LoRA so we need to merge!\\n-        W, quant_state, A, B, s = get_lora_parameters(layer)\\n+        W, quant_state, A, B, s, bias = get_lora_parameters_bias(layer)\\n         if quant_state is not None:\\n             dtype = quant_state.dtype if type(quant_state) is not list else quant_state[2]\\n             W = fast_dequantize(W, quant_state)\\n@@ -156,7 +157,7 @@ def _merge_lora(layer, name):\\n         W = W.t().to(dtype)\\n     else:\\n         W = layer.weight\\n-    return W\\n+    return W, bias\\n pass\\n \\n \\n@@ -527,7 +528,12 @@ def unsloth_save_model(\\n         for item in LLAMA_WEIGHTS:\\n             proj = eval(f\"layer.{item}\")\\n             name = f\"model.layers.{j}.{item}.weight\"\\n-            W = _merge_lora(proj, name)\\n+            W, bias = _merge_lora(proj, name)\\n+\\n+            # Bias term\\n+            if bias is not None:\\n+                state_dict[f\"model.layers.{j}.{item}.bias\"] = bias\\n+            pass\\n \\n             if (torch.cuda.memory_allocated() + W.nbytes) < max_vram:\\n                 # Save to GPU memory\\n@@ -643,7 +649,8 @@ def unsloth_save_model(\\n     model.config = new_config\\n \\n     # Save!\\n-\\n+    \\n+    save_pretrained_settings[\"selected_adapters\"] = None\\n     # Check if pushing to an organization\\n     if save_pretrained_settings[\"push_to_hub\"] and (username != actual_username):\\n         print(f\"Unsloth: Saving to organization with address {new_save_directory}\")\\n@@ -785,7 +792,7 @@ def install_llama_cpp_old(version = -10):\\n         pass\\n     pass\\n     # Check if successful\\n-    if not os.path.exists(\"llama.cpp/quantize\"):\\n+    if not os.path.exists(\"llama.cpp/quantize\") and not os.path.exists(\"llama.cpp/llama-quantize\"):\\n         raise RuntimeError(\\n             \"Unsloth: llama.cpp GGUF seems to be too buggy to install.\\\\n\"\\\\\\n             \"File a report to llama.cpp\\'s main repo since this is not an Unsloth issue.\"\\n@@ -794,7 +801,7 @@ def install_llama_cpp_old(version = -10):\\n pass\\n \\n \\n-def install_llama_cpp_blocking(use_cuda = True):\\n+def install_llama_cpp_blocking(use_cuda = False):\\n     # https://github.com/ggerganov/llama.cpp/issues/7062\\n     # Weirdly GPU conversion for GGUF breaks??\\n     # use_cuda = \"LLAMA_CUDA=1\" if use_cuda else \"\"\\n@@ -822,49 +829,6 @@ def install_llama_cpp_blocking(use_cuda = True):\\n pass\\n \\n \\n-def _fix_gemma_gguf():\\n-    # Fixes Gemma saving to GGUF to float32 instead of float16!\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"rb\") as file:\\n-        text = file.read()\\n-    pass\\n-\\n-    gemma_start = text.find(b\"class GemmaModel(Model):\")\\n-    if gemma_start == -1: return\\n-\\n-    gemma_end   = text.find(b\"self.gguf_writer.add_tensor(new_name, data)\", gemma_start)\\n-    if gemma_end == -1: return\\n-\\n-    gemma_text = text[gemma_start : gemma_end]\\n-    bad_text = \\\\\\n-b\"\"\"         data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    good_text = \\\\\\n-b\"\"\"         # if f32 desired, convert any float16 to float32\\n-            if self.ftype == 0 and data_dtype == np.float16:\\n-                data = data.astype(np.float32)\\n-\\n-            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\\n-            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\\n-                data = data.astype(np.float32)\\n-\\n-            # if f16 desired, convert any float32 2-dim weight tensors to float16\\n-            if self.ftype == 1 and data_dtype == np.float32 and name.endswith(\".weight\") and n_dims == 2:\\n-                data = data.astype(np.float16)\"\"\"\\n-    find_bad = gemma_text.find(bad_text)\\n-    if find_bad == -1: return\\n-\\n-    gemma_text = gemma_text[:find_bad] + good_text + gemma_text[find_bad + len(bad_text):]\\n-    text = text[:gemma_start] + gemma_text + text[gemma_end:]\\n-\\n-    with open(\"llama.cpp/convert-hf-to-gguf.py\", \"w+b\") as file:\\n-        file.write(text)\\n-    pass\\n-pass\\n-\\n-\\n def save_to_gguf(\\n     model_type           : str,\\n     model_dtype          : str,\\n@@ -930,7 +894,7 @@ def save_to_gguf(\\n \\n     # Check first_conversion format\\n     if   first_conversion == \"f16\"  : pass\\n-    if   first_conversion == \"bf16\" : pass\\n+    elif first_conversion == \"bf16\" : pass\\n     elif first_conversion == \"f32\"  : pass\\n     elif first_conversion == \"q8_0\" : pass\\n     else:\\n@@ -946,8 +910,20 @@ def save_to_gguf(\\n         error = 0\\n         install_llama_cpp_blocking()\\n     pass\\n+\\n     # Check if successful. If not install 10th latest release\\n-    if error != 0 or not os.path.exists(\"llama.cpp/quantize\"):\\n+\\n+    # Careful llama.cpp/quantize changed to llama.cpp/llama-quantize\\n+    # and llama.cpp/main changed to llama.cpp/llama-cli\\n+    # See https://github.com/ggerganov/llama.cpp/pull/7809\\n+    quantize_location = None\\n+    if os.path.exists(\"llama.cpp/quantize\"):\\n+        quantize_location = \"llama.cpp/quantize\"\\n+    elif os.path.exists(\"llama.cpp/llama-quantize\"):\\n+        quantize_location = \"llama.cpp/llama-quantize\"\\n+    pass\\n+\\n+    if error != 0 or quantize_location is None:\\n         print(f\"Unsloth: llama.cpp error code = {error}.\")\\n         install_llama_cpp_old(-10)\\n     pass\\n@@ -1017,9 +993,6 @@ def save_to_gguf(\\n             f\"--outfile {final_location} --vocab-type {vocab_type} \"\\\\\\n             f\"--outtype {first_conversion} --concurrency {n_cpus} --pad-vocab\"\\n     else:\\n-        # Need to fix convert-hf-to-gguf.py for some models!\\n-        # _fix_gemma_gguf()\\n-\\n         command = f\"python llama.cpp/convert-hf-to-gguf.py {model_directory} \"\\\\\\n             f\"--outfile {final_location} \"\\\\\\n             f\"--outtype {first_conversion}\"\\n@@ -1065,7 +1038,7 @@ def save_to_gguf(\\n         print(f\"Unsloth: [2] Converting GGUF 16bit into {quantization_method}. This will take 20 minutes...\")\\n         final_location = f\"./{model_directory}-unsloth.{quantization_method.upper()}.gguf\"\\n \\n-        command = f\"./llama.cpp/quantize {old_location} \"\\\\\\n+        command = f\"./{quantize_location} {old_location} \"\\\\\\n             f\"{final_location} {quantization_method} {n_cpus}\"\\n         \\n         # quantize uses stderr\\n@@ -1654,6 +1627,140 @@ def unsloth_push_to_hub_gguf(\\n     pass\\n pass\\n \\n+# Corrected function to save LoRA to a custom directory\\n+def save_lora_to_custom_dir(model, tokenizer, save_directory):\\n+    # Create the custom directory if it doesn\\'t exist\\n+    os.makedirs(save_directory, exist_ok=True)\\n+\\n+    # Call the unsloth_save_model function with the custom directory\\n+    unsloth_save_model(\\n+        model,\\n+        tokenizer,\\n+        save_directory=save_directory,\\n+        save_method=\"lora\",\\n+        push_to_hub=False,\\n+    )\\n+\\n+# Corrected method within the model class to convert LoRA to GGML and push to Hugging Face Hub\\n+def unsloth_convert_lora_to_ggml_and_push_to_hub(\\n+    self,\\n+    tokenizer,\\n+    repo_id: str,\\n+    use_temp_dir: Optional[bool] = None,\\n+    commit_message: Optional[str] = \"Converted LoRA to GGML with Unsloth\",\\n+    private: Optional[bool] = None,\\n+    token: Union[bool, str, None] = None,\\n+    create_pr: bool = False,\\n+    revision: str = None,\\n+    commit_description: str = \"Convert LoRA to GGML format using Unsloth\",\\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    lora_directory_push = \"lora-to-ggml-push\"\\n+    save_lora_to_custom_dir(self, tokenizer, lora_directory_push)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(lora_directory_push, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {lora_directory_push} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {lora_directory_push} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+\\n+    print(\"Unsloth: Uploading GGML file to Hugging Face Hub...\")\\n+    username = upload_to_huggingface(\\n+        self, repo_id, token,\\n+        \"GGML converted LoRA\", \"ggml\", output_file, None, private,\\n+    )\\n+    link = f\"{repo_id.lstrip(\\'/\\')}\"\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Converted LoRA to GGML and uploaded to https://huggingface.co/{link}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n+\\n+def unsloth_convert_lora_to_ggml_and_save_locally(\\n+    self,\\n+    save_directory: str, # Added parameter for the folder name \\n+    tokenizer, \\n+    temporary_location: str = \"_unsloth_temporary_saved_buffers\",\\n+    maximum_memory_usage: float = 0.85,\\n+):\\n+    if not os.path.exists(\"llama.cpp\"):\\n+        if IS_KAGGLE_ENVIRONMENT:\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            python_install.wait()\\n+            install_llama_cpp_blocking(use_cuda=False)\\n+            makefile = None\\n+        else:\\n+            git_clone = install_llama_cpp_clone_non_blocking()\\n+            python_install = install_python_non_blocking([\"protobuf\"])\\n+            git_clone.wait()\\n+            makefile = install_llama_cpp_make_non_blocking()\\n+            python_install.wait()\\n+    else:\\n+        makefile = None\\n+\\n+    for _ in range(3):\\n+        gc.collect()\\n+\\n+    # Use the provided save_directory for local saving\\n+    save_lora_to_custom_dir(self, tokenizer, save_directory)\\n+\\n+    model_type = self.config.model_type\\n+    output_file = os.path.join(save_directory, \"ggml-adapter-model.bin\")\\n+\\n+    print(f\"Unsloth: Converting auto-saved LoRA adapters at {save_directory} to GGML format.\")\\n+    print(f\"The output file will be {output_file}\")\\n+\\n+    command = f\"python3 llama.cpp/convert-lora-to-ggml.py {save_directory} {output_file} llama\"\\n+\\n+    try:\\n+        with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1, universal_newlines=True) as sp:\\n+            for line in sp.stdout:\\n+                print(line, end=\"\", flush=True)\\n+            for line in sp.stderr:\\n+                print(line, end=\"\", flush=True)\\n+            sp.wait()\\n+            if sp.returncode != 0:\\n+                raise subprocess.CalledProcessError(sp.returncode, command)\\n+    except subprocess.CalledProcessError as e:\\n+        print(f\"Error: Conversion failed with return code {e.returncode}\")\\n+        return\\n+    print(\"Unsloth: Done.\")\\n+    print(f\"Unsloth: Conversion completed! Output file: {output_file}\")\\n+    print(\"\\\\nThis GGML making function was made by Maheswar. Ping him @Maheswar on the Unsloth Discord or on HuggingFace (@mahiatlinux) if you like this!\")\\n \\n def patch_saving_functions(model):\\n     import inspect\\n@@ -1746,10 +1853,12 @@ def patch_saving_functions(model):\\n     # Add saving methods to top level model\\n     if hasattr(model, \"config\"):\\n         # Counteract tokenizers\\n-        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,     model)\\n-        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged, model)\\n-        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,       model)\\n-        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,   model)\\n+        model.push_to_hub_merged     = types.MethodType(unsloth_push_to_hub_merged,                    model)\\n+        model.save_pretrained_merged = types.MethodType(unsloth_save_pretrained_merged,                model)\\n+        model.push_to_hub_gguf       = types.MethodType(unsloth_push_to_hub_gguf,                      model)\\n+        model.save_pretrained_gguf   = types.MethodType(unsloth_save_pretrained_gguf,                  model)\\n+        model.push_to_hub_ggml       = types.MethodType(unsloth_convert_lora_to_ggml_and_push_to_hub,  model)\\n+        model.save_pretrained_ggml   = types.MethodType(unsloth_convert_lora_to_ggml_and_save_locally, model)\\n     pass\\n     return model\\n pass\\n',\n",
       " '@@ -232,62 +232,6 @@ llama_template = \\\\\\n         \"{% endif %}\"\\\\\\n     \"{% endfor %}\"\\n pass\\n-    \\n-\\n-def select_correct_slow_tokenizer(\\n-    tokenizer_name,\\n-    model_max_length = None,\\n-    padding_side = \"right\",\\n-    token = None,\\n-    trust_remote_code = False,\\n-    cache_dir = \"huggingface_tokenizers_cache\",\\n-):\\n-    \"\"\"\\n-    Returns \\'correct\\' tokenizer by checking if the chat templates are\\n-    actually tokenized correctly.\\n-    \"\"\"\\n-    messages = [\\n-        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\\n-        {\"role\": \"assistant\", \"content\": \"It\\'s 4.\"},\\n-    ]\\n-    \\n-    settings = (\\n-        (False, False, True,),\\n-        (False, True,  True,),\\n-        (True,  False, True,),\\n-        (True,  False, False,),\\n-    )\\n-\\n-    for (use_fast, legacy, from_slow,) in settings:\\n-        # Default as mentioned by Arthur from HF:\\n-        slow_tokenizer = AutoTokenizer.from_pretrained(\\n-            tokenizer_name,\\n-            model_max_length  = model_max_length,\\n-            padding_side      = padding_side,\\n-            token             = token,\\n-            trust_remote_code = trust_remote_code,\\n-            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n-            use_fast          = use_fast,\\n-            legacy            = legacy,\\n-            from_slow         = from_slow,\\n-            cache_dir         = cache_dir,\\n-        )\\n-        slow_tokenizer_chat_template = slow_tokenizer.chat_template\\n-\\n-        slow_tokenizer.chat_template = llama_template\\n-        result1 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-        slow_tokenizer.chat_template = mistral_template\\n-        result2 = slow_tokenizer.decode(slow_tokenizer.apply_chat_template(messages))\\n-\\n-        # If 2 spaces seen, normally wrong!\\n-        if \" \"*2 not in result1 and \" \"*2 not in result2:\\n-            slow_tokenizer.chat_template = slow_tokenizer_chat_template\\n-            return slow_tokenizer\\n-        pass\\n-    pass\\n-    # Return fast version as default\\n-    return slow_tokenizer\\n-pass\\n \\n \\n def assert_same_tokenization(slow_tokenizer, fast_tokenizer):\\n@@ -508,13 +452,17 @@ def load_correct_tokenizer(\\n     # Mainly to solve Deepseek models with no tokenizer.model file\\n     slow_tokenizer = None\\n     try:\\n-        slow_tokenizer = select_correct_slow_tokenizer(\\n+        slow_tokenizer = AutoTokenizer.from_pretrained(\\n             tokenizer_name,\\n-            model_max_length = model_max_length,\\n-            padding_side = padding_side,\\n-            token = token,\\n+            model_max_length  = model_max_length,\\n+            padding_side      = padding_side,\\n+            token             = token,\\n             trust_remote_code = trust_remote_code,\\n-            cache_dir = cache_dir,\\n+            # Cannot just use use_fast = False as per https://twitter.com/danielhanchen/status/1789659394302718373\\n+            use_fast          = False,\\n+            legacy            = False,\\n+            from_slow         = True,\\n+            cache_dir         = cache_dir,\\n         )\\n     except:\\n         pass\\n@@ -786,7 +734,7 @@ def fix_untrained_tokens(model, tokenizer, train_dataset, eps = 1e-16):\\n     pass\\n \\n     # Count all the possible bad tokens\\n-    final_counts = np.zeros(len(tokenizer), dtype = np.int64)\\n+    final_counts = np.zeros(max(len(tokenizer), embedding_matrix.shape[0]), dtype = np.int64)\\n     def mapping(examples):\\n         input_ids = examples[\"input_ids\"]\\n         counter = np.fromiter(itertools.chain.from_iterable(input_ids), dtype = np.int32)\\n@@ -972,7 +920,7 @@ def patch_sft_trainer_tokenizer():\\n \\n         check_text = \\\\\\n         \"\\\\n\"\\\\\\n-        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])\\\\n\"\\\\\\n+        \"test_text = dataset[0][dataset_text_field] if (formatting_func is None or not use_formatting_func) else formatting_func(dataset[0])[0]\\\\n\"\\\\\\n         \"chat_template = getattr(tokenizer, \\'chat_template\\', None)\\\\n\"\\\\\\n         \"chat_template = \\'\\' if chat_template is None else chat_template\\\\n\"\\\\\\n         \"has_bos_token_already = (test_text.startswith(tokenizer.bos_token) or tokenizer.bos_token in chat_template) \"\\\\\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " \"@@ -1,87 +0,0 @@\\n-## LoraConfig Parameters\\r\\n-\\r\\n-Adjusting the `LoraConfig` parameters allows you to balance model performance and computational efficiency in Low-Rank Adaptation (LoRA). Here’s a concise breakdown of key parameters:\\r\\n-\\r\\n-**r**\\r\\n-- **Description**: Rank of the low-rank decomposition for factorizing weight matrices.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Retains more information, increases computational load.\\r\\n-  - **Lower**: Fewer parameters, more efficient training, potential performance drop if too small.\\r\\n-\\r\\n-\\r\\n-**lora_alpha**\\r\\n-- **Description**: Scaling factor for the low-rank matrices' contribution.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Increases influence, speeds up convergence, risks instability or overfitting.\\r\\n-  - **Lower**: Subtler effect, may require more training steps.\\r\\n-\\r\\n-**lora_dropout**\\r\\n-- **Description**: Probability of zeroing out elements in low-rank matrices for regularization.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: More regularization, prevents overfitting, may slow training and degrade performance.\\r\\n-  - **Lower**: Less regularization, may speed up training, risks overfitting.\\r\\n-\\r\\n-**loftq_config**\\r\\n-- **Description**: Configuration for LoftQ, a quantization method for the backbone weights and initialization of LoRA layers.\\r\\n-- **Impact**:\\r\\n-  - **Not None**: If specified, LoftQ will quantize the backbone weights and initialize the LoRA layers. It requires setting `init_lora_weights='loftq'`.\\r\\n-  - **None**: LoftQ quantization is not applied.\\r\\n-  - **Note**: Do not pass an already quantized model when using LoftQ as LoftQ handles the quantization process itself.\\r\\n-\\r\\n-\\r\\n-**use_rslora**\\r\\n-- **Description**: Enables Rank-Stabilized LoRA (RSLora).\\r\\n-- **Impact**:\\r\\n-  - **True**: Uses Rank-Stabilized LoRA, setting the adapter scaling factor to `lora_alpha/math.sqrt(r)`, which has been proven to work better as per the [Rank-Stabilized LoRA paper](https://doi.org/10.48550/arXiv.2312.03732).\\r\\n-  - **False**: Uses the original default scaling factor `lora_alpha/r`.\\r\\n-\\r\\n-**gradient_accumulation_steps**\\r\\n-- **Default**: 1\\r\\n-- **Description**: The number of steps to accumulate gradients before performing a backpropagation update.\\r\\n-- **Impact**: \\r\\n-  - **Higher**: Accumulate gradients over multiple steps, effectively increasing the batch size without requiring additional memory. This can improve training stability and convergence, especially with large models and limited hardware.\\r\\n-  - **Lower**: Faster updates but may require more memory per step and can be less stable.\\r\\n-\\r\\n-**weight_decay**\\r\\n-- **Default**: 0.01\\r\\n-- **Description**: Regularization technique that applies a small penalty to the weights during training.\\r\\n-- **Impact**:\\r\\n-  - **Non-zero Value (e.g., 0.01)**: Adds a penalty proportional to the magnitude of the weights to the loss function, helping to prevent overfitting by discouraging large weights.\\r\\n-  - **Zero**: No weight decay is applied, which can lead to overfitting, especially in large models or with small datasets.\\r\\n-\\r\\n-**learning_rate**\\r\\n-- **Default**: 2e-4\\r\\n-- **Description**: The rate at which the model updates its parameters during training.\\r\\n-- **Impact**:\\r\\n-  - **Higher**: Faster convergence but risks overshooting optimal parameters and causing instability in training.\\r\\n-  - **Lower**: More stable and precise updates but may slow down convergence, requiring more training steps to achieve good performance.\\r\\n-\\r\\n-## Target Modules \\r\\n-\\r\\n-**q_proj (query projection)**\\r\\n-- **Description**: Part of the attention mechanism in transformer models, responsible for projecting the input into the query space.\\r\\n-- **Impact**: Transforms the input into query vectors that are used to compute attention scores.\\r\\n-\\r\\n-**k_proj (key projection)**\\r\\n-- **Description**: Projects the input into the key space in the attention mechanism.\\r\\n-- **Impact**: Produces key vectors that are compared with query vectors to determine attention weights.\\r\\n-\\r\\n-**v_proj (value projection)**\\r\\n-- **Description**: Projects the input into the value space in the attention mechanism.\\r\\n-- **Impact**: Produces value vectors that are weighted by the attention scores and combined to form the output.\\r\\n-\\r\\n-**o_proj (output projection)**\\r\\n-- **Description**: Projects the output of the attention mechanism back into the original space.\\r\\n-- **Impact**: Transforms the combined weighted value vectors back to the input dimension, integrating attention results into the model.\\r\\n-\\r\\n-**gate_proj (gate projection)**\\r\\n-- **Description**: Typically used in gated mechanisms within neural networks, such as gating units in gated recurrent units (GRUs) or other gating mechanisms.\\r\\n-- **Impact**: Controls the flow of information through the gate, allowing selective information passage based on learned weights.\\r\\n-\\r\\n-**up_proj (up projection)**\\r\\n-- **Description**: Used for up-projection, typically increasing the dimensionality of the input.\\r\\n-- **Impact**: Expands the input to a higher-dimensional space, often used in feedforward layers or when transitioning between different layers with differing dimensionalities.\\r\\n-\\r\\n-**down_proj (down projection)**\\r\\n-- **Description**: Used for down-projection, typically reducing the dimensionality of the input.\\r\\n-- **Impact**: Compresses the input to a lower-dimensional space, useful for reducing computational complexity and controlling the model size.\\r\\n\",\n",
       " '@@ -35,7 +35,7 @@ All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and\\n - Run [Llama 3 conversational notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) and [Mistral 7B v3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)\\n - This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text\\n - This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language\\n-\\n+- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.\\n \\n ## 🦥 Unsloth.ai News\\n - 📣 NEW! Continued Pretraining [notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) for other languages like Korean!\\n@@ -76,7 +76,7 @@ model = FastLanguageModel.get_peft_model(\\n \\n \\n ## 🥇 Performance Benchmarking\\n-- For the full list of **reproducable** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n+- For the full list of **reproducible** benchmarking tables, [go to our website](https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables)\\n \\n | 1 A100 40GB  | 🤗Hugging Face | Flash Attention | 🦥Unsloth Open Source | 🦥[Unsloth Pro](https://unsloth.ai/pricing) |\\n |--------------|--------------|-----------------|---------------------|-----------------|\\n@@ -100,14 +100,16 @@ model = FastLanguageModel.get_peft_model(\\n ### Conda Installation\\n Select either `pytorch-cuda=11.8` for CUDA 11.8 or `pytorch-cuda=12.1` for CUDA 12.1. If you have `mamba`, use `mamba` instead of `conda` for faster solving. See this [Github issue](https://github.com/unslothai/unsloth/issues/73) for help on debugging Conda installs.\\n ```bash\\n-conda create --name unsloth_env python=3.10\\n+conda create --name unsloth_env \\\\\\n+    python=3.10 \\\\\\n+    pytorch-cuda=<11.8/12.1> \\\\\\n+    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\\\\n+    -y\\n conda activate unsloth_env\\n \\n-conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\\n-\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n \\n-pip install --no-deps trl peft accelerate bitsandbytes\\n+pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n \\n ### Pip Installation\\n@@ -162,7 +164,7 @@ pip install --no-deps packaging ninja einops flash-attn xformers trl peft accele\\n \\n # Pre Ampere RTX 2080, T4, GTX 1080 GPUs:\\n pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\\n-pip install --no-deps xformers trl peft accelerate bitsandbytes\\n+pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\\n ```\\n 7. For Pytorch 2.3.0: Use the `\"ampere\"` path for newer RTX 30xx GPUs or higher.\\n ```bash\\n@@ -257,7 +259,7 @@ trainer.train()\\n # (1) Saving to GGUF / merging to 16bit for vLLM\\n # (2) Continued training from a saved LoRA adapter\\n # (3) Adding an evaluation loop / OOMs\\n-# (4) Cutomized chat templates\\n+# (4) Customized chat templates\\n ```\\n \\n <a name=\"DPO\"></a>\\n',\n",
       " '@@ -512,7 +512,7 @@ class FastMistralModel(FastLlamaModel):\\n         if \"n_total_devices >\" not in inner_training_loop:\\n             raise RuntimeError(\\n                 \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports four GPUs - please obtain a commercial license from our website.\\\\n\"\\n+                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n                 \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n                 \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n             )\\n@@ -521,6 +521,7 @@ class FastMistralModel(FastLlamaModel):\\n             \"is_sagemaker_mp_enabled()\",\\n             \"False\",\\n         )\\n+        exec(inner_training_loop, globals())\\n         Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save max_seq_length\\n@@ -560,6 +561,7 @@ class FastMistralModel(FastLlamaModel):\\n \\n         # Add save modules\\n         patch_saving_functions(model)\\n+        Trainer._inner_training_loop = _fast_inner_training_loop\\n \\n         # Save tokenizer for inference purposes\\n         tokenizer.padding_side = \"left\" # Force inference\\n',\n",
       " '@@ -12,9 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .llama import *\\n-import os\\n-from ._utils import __version__\\n+from .mistral import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -34,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastLlamaModel):\\n+class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -72,7 +70,7 @@ class FastQwen2Model(FastLlamaModel):\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastLlamaModel.from_pretrained(\\n+        return FastMistralModel.from_pretrained(\\n             model_name     = model_name,\\n             max_seq_length = max_seq_length,\\n             dtype          = dtype,\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " '@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/mistral-7b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        if token is None and \"HF_TOKEN\" in os.environ:\\n-            token = os.environ[\"HF_TOKEN\"]\\n-\\n-        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n-            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n-\\n-        if model_patcher is None: model_patcher = FastMistralModel\\n-        # Mistral does NOT support RoPE Scaling!\\n-        if rope_scaling is not None:\\n-            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-        pass\\n-\\n-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-        statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n-           f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        print(statistics)\\n-        model_patcher.pre_patch()\\n-        # get_statistics()\\n-\\n-        if dtype is None:\\n-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:\\n-            logger.warning_once(\"Device does not support bfloat16. Will change to float16.\")\\n-            dtype = torch.float16\\n-\\n-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n-\\n-        # Check max sequence length\\n-        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n-        model_max_seq_length = model_config.max_position_embeddings\\n-\\n-        # If max_seq_length is not specified, use maximum fron config\\n-        if max_seq_length is None:\\n-            max_seq_length = model_max_seq_length\\n-        pass\\n-\\n-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n-        if max_seq_length > model_max_seq_length:\\n-            raise RuntimeError(\\n-                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n-                f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n-            )\\n-        pass\\n-\\n-        bnb_config = None\\n-        if load_in_4bit:\\n-            bnb_config = BitsAndBytesConfig(\\n-                load_in_4bit              = True,\\n-                bnb_4bit_use_double_quant = True,\\n-                bnb_4bit_quant_type       = \"nf4\",\\n-                bnb_4bit_compute_dtype    = dtype,\\n-            )\\n-\\n-        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map          = device_map,\\n-            torch_dtype         = dtype,\\n-            quantization_config = bnb_config,\\n-            token               = token,\\n-            # rope_scaling      = rope_scaling,\\n-            trust_remote_code   = trust_remote_code,\\n-            **kwargs,\\n-        )\\n-\\n-        # Counteract saved tokenizers\\n-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = load_correct_tokenizer(\\n-            tokenizer_name,\\n-            model_max_length  = max_position_embeddings,\\n-            padding_side      = \"right\",\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n             token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastMistralModel,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n         )\\n-\\n-        model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = model_patcher.post_patch(model)\\n-\\n-        # Patch up QKV / O and MLP\\n-        for idx, layer in enumerate(model.model.layers):\\n-            layer.self_attn.apply_qkv = original_apply_qkv\\n-            layer.self_attn.apply_o   = original_apply_o\\n-        pass\\n-\\n-        # Patch Trainer\\n-        from transformers.trainer import Trainer\\n-        try:\\n-            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n-                Trainer._original_training_loop = inner_training_loop\\n-            else:\\n-                inner_training_loop = Trainer._original_training_loop\\n-        except:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-\\n-        import transformers.trainer\\n-        items_in_trainer = dir(transformers.trainer)\\n-        good_items = []\\n-        for item in items_in_trainer:\\n-            # TODO: Support Deepspeed\\n-            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n-            if item in inner_training_loop: good_items.append(item)\\n-        pass\\n-        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n-\\n-        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n-        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n-        original_debug = inner_training_loop[start:end]\\n-        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n-        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n-\\n-        debug_info = \"\"\"debug_info = \\\\\\\\\\n-        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n-        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n-        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n-        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n-        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n-        logger.warning(debug_info)\\n-        import subprocess, re, gc\\n-        output = subprocess.check_output(\\n-            \\'nvidia-smi --query-gpu=memory.used --format=csv\\', shell = True)\\n-        output = re.findall(rb\\'([\\\\\\\\d]{1,})[\\\\\\\\s]{1,}M\\', output)\\n-        output = sum(int(x.decode(\\'utf-8\\'))/1024 > 4 for x in output)\\n-        if output > 1: raise RuntimeError(\\n-            \\'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.\\')\\n-        for _ in range(3):\\n-            gc.collect()\\n-            torch.cuda.empty_cache()\"\"\"\\n-\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n-\\n-        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n-            args.gradient_accumulation_steps // self._train_batch_size\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        debug_info =\"\"\"\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n-\\n-        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n-        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n-            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"self.accelerator.free_memory()\",\\n-            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n-            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n-            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n-        )\\n-\\n-        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n-        ga  = args.gradient_accumulation_steps\\n-        bsz = self._train_batch_size\\n-        total_batches = bsz * ga * args.world_size\\n-        n_total_devices = total_batches // ga // bsz\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-            divisor = n_total_devices / 1\\n-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n-            if total_batches // ga // bsz > 1:\\n-                divisor = n_total_devices / 1\\n-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n-        check_batches = check_batches.split(\\'\\\\n\\')\\n-        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = self.get_train_dataloader()\",\\n-            check_batches, 1,\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"_inner_training_loop\",\\n-            \"_fast_inner_training_loop\", 1,\\n-        )\\n-        exec(inner_training_loop, globals())\\n-\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_torch_tpu_available()\",\\n-            \"False\",\\n-        )\\n-        if \"n_total_devices >\" not in inner_training_loop:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_sagemaker_mp_enabled()\",\\n-            \"False\",\\n-        )\\n-        exec(inner_training_loop, globals())\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save max_seq_length\\n-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n-        model.max_seq_length = max_position_embeddings\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model.max_seq_length = max_position_embeddings\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model.max_seq_length = max_position_embeddings\\n-\\n-        # We check the tokenizer first for errors\\n-        if fix_tokenizer:\\n-            tokenizer = check_tokenizer(\\n-                model            = model,\\n-                tokenizer        = tokenizer,\\n-                model_name       = model_name,\\n-                model_max_length = max_position_embeddings,\\n-                padding_side     = \"right\",\\n-                token            = token,\\n-            )\\n-        pass\\n-        patch_saving_functions(tokenizer)\\n-\\n-        # Fix up config for transformers uploading PEFT\\n-        # Not necessary anymore since we require transformers>=4.37\\n-        if False:\\n-            name = model.config._name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.config.update({\"_name_or_path\" : name})\\n-            pass\\n-        \\n-        # Log Unsloth version for future fastpaths for inference\\n-        model.config.update({\"unsloth_version\" : __version__})\\n-\\n-        # Add save modules\\n-        patch_saving_functions(model)\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save tokenizer for inference purposes\\n-        tokenizer.padding_side = \"left\" # Force inference\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model._saved_temp_tokenizer = tokenizer\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model._saved_temp_tokenizer = tokenizer\\n-        \\n-        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -12,7 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .mistral import *\\n+from .llama import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -32,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastMistralModel):\\n+class FastQwen2Model(FastLlamaModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"Qwen/Qwen2-7B\",\\n-        max_seq_length = 4096,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"Qwen/Qwen2-7B\",\\n+        max_seq_length    = 4096,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastMistralModel.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = FastQwen2Model,\\n-            tokenizer_name = tokenizer_name,\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastQwen2Model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n             **kwargs,\\n         )\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " '@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/mistral-7b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        if token is None and \"HF_TOKEN\" in os.environ:\\n-            token = os.environ[\"HF_TOKEN\"]\\n-\\n-        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n-            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n-\\n-        if model_patcher is None: model_patcher = FastMistralModel\\n-        # Mistral does NOT support RoPE Scaling!\\n-        if rope_scaling is not None:\\n-            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-        pass\\n-\\n-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-        statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n-           f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        print(statistics)\\n-        model_patcher.pre_patch()\\n-        # get_statistics()\\n-\\n-        if dtype is None:\\n-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:\\n-            logger.warning_once(\"Device does not support bfloat16. Will change to float16.\")\\n-            dtype = torch.float16\\n-\\n-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n-\\n-        # Check max sequence length\\n-        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n-        model_max_seq_length = model_config.max_position_embeddings\\n-\\n-        # If max_seq_length is not specified, use maximum fron config\\n-        if max_seq_length is None:\\n-            max_seq_length = model_max_seq_length\\n-        pass\\n-\\n-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n-        if max_seq_length > model_max_seq_length:\\n-            raise RuntimeError(\\n-                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n-                f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n-            )\\n-        pass\\n-\\n-        bnb_config = None\\n-        if load_in_4bit:\\n-            bnb_config = BitsAndBytesConfig(\\n-                load_in_4bit              = True,\\n-                bnb_4bit_use_double_quant = True,\\n-                bnb_4bit_quant_type       = \"nf4\",\\n-                bnb_4bit_compute_dtype    = dtype,\\n-            )\\n-\\n-        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map          = device_map,\\n-            torch_dtype         = dtype,\\n-            quantization_config = bnb_config,\\n-            token               = token,\\n-            # rope_scaling      = rope_scaling,\\n-            trust_remote_code   = trust_remote_code,\\n-            **kwargs,\\n-        )\\n-\\n-        # Counteract saved tokenizers\\n-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = load_correct_tokenizer(\\n-            tokenizer_name,\\n-            model_max_length  = max_position_embeddings,\\n-            padding_side      = \"right\",\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n             token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastMistralModel,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n         )\\n-\\n-        model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = model_patcher.post_patch(model)\\n-\\n-        # Patch up QKV / O and MLP\\n-        for idx, layer in enumerate(model.model.layers):\\n-            layer.self_attn.apply_qkv = original_apply_qkv\\n-            layer.self_attn.apply_o   = original_apply_o\\n-        pass\\n-\\n-        # Patch Trainer\\n-        from transformers.trainer import Trainer\\n-        try:\\n-            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n-                Trainer._original_training_loop = inner_training_loop\\n-            else:\\n-                inner_training_loop = Trainer._original_training_loop\\n-        except:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-\\n-        import transformers.trainer\\n-        items_in_trainer = dir(transformers.trainer)\\n-        good_items = []\\n-        for item in items_in_trainer:\\n-            # TODO: Support Deepspeed\\n-            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n-            if item in inner_training_loop: good_items.append(item)\\n-        pass\\n-        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n-\\n-        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n-        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n-        original_debug = inner_training_loop[start:end]\\n-        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n-        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n-\\n-        debug_info = \"\"\"debug_info = \\\\\\\\\\n-        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n-        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n-        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n-        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n-        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n-        logger.warning(debug_info)\\n-        import subprocess, re, gc\\n-        output = subprocess.check_output(\\n-            \\'nvidia-smi --query-gpu=memory.used --format=csv\\', shell = True)\\n-        output = re.findall(rb\\'([\\\\\\\\d]{1,})[\\\\\\\\s]{1,}M\\', output)\\n-        output = sum(int(x.decode(\\'utf-8\\'))/1024 > 4 for x in output)\\n-        if output > 1: raise RuntimeError(\\n-            \\'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.\\')\\n-        for _ in range(3):\\n-            gc.collect()\\n-            torch.cuda.empty_cache()\"\"\"\\n-\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n-\\n-        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n-            args.gradient_accumulation_steps // self._train_batch_size\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        debug_info =\"\"\"\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n-\\n-        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n-        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n-            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"self.accelerator.free_memory()\",\\n-            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n-            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n-            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n-        )\\n-\\n-        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n-        ga  = args.gradient_accumulation_steps\\n-        bsz = self._train_batch_size\\n-        total_batches = bsz * ga * args.world_size\\n-        n_total_devices = total_batches // ga // bsz\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-            divisor = n_total_devices / 1\\n-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n-            if total_batches // ga // bsz > 1:\\n-                divisor = n_total_devices / 1\\n-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n-        check_batches = check_batches.split(\\'\\\\n\\')\\n-        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = self.get_train_dataloader()\",\\n-            check_batches, 1,\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"_inner_training_loop\",\\n-            \"_fast_inner_training_loop\", 1,\\n-        )\\n-        exec(inner_training_loop, globals())\\n-\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_torch_tpu_available()\",\\n-            \"False\",\\n-        )\\n-        if \"n_total_devices >\" not in inner_training_loop:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_sagemaker_mp_enabled()\",\\n-            \"False\",\\n-        )\\n-        exec(inner_training_loop, globals())\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save max_seq_length\\n-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n-        model.max_seq_length = max_position_embeddings\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model.max_seq_length = max_position_embeddings\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model.max_seq_length = max_position_embeddings\\n-\\n-        # We check the tokenizer first for errors\\n-        if fix_tokenizer:\\n-            tokenizer = check_tokenizer(\\n-                model            = model,\\n-                tokenizer        = tokenizer,\\n-                model_name       = model_name,\\n-                model_max_length = max_position_embeddings,\\n-                padding_side     = \"right\",\\n-                token            = token,\\n-            )\\n-        pass\\n-        patch_saving_functions(tokenizer)\\n-\\n-        # Fix up config for transformers uploading PEFT\\n-        # Not necessary anymore since we require transformers>=4.37\\n-        if False:\\n-            name = model.config._name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.config.update({\"_name_or_path\" : name})\\n-            pass\\n-        \\n-        # Log Unsloth version for future fastpaths for inference\\n-        model.config.update({\"unsloth_version\" : __version__})\\n-\\n-        # Add save modules\\n-        patch_saving_functions(model)\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save tokenizer for inference purposes\\n-        tokenizer.padding_side = \"left\" # Force inference\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model._saved_temp_tokenizer = tokenizer\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model._saved_temp_tokenizer = tokenizer\\n-        \\n-        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -12,7 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .mistral import *\\n+from .llama import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -32,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastMistralModel):\\n+class FastQwen2Model(FastLlamaModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"Qwen/Qwen2-7B\",\\n-        max_seq_length = 4096,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"Qwen/Qwen2-7B\",\\n+        max_seq_length    = 4096,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastMistralModel.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = FastQwen2Model,\\n-            tokenizer_name = tokenizer_name,\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastQwen2Model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n             **kwargs,\\n         )\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " '@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/mistral-7b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        if token is None and \"HF_TOKEN\" in os.environ:\\n-            token = os.environ[\"HF_TOKEN\"]\\n-\\n-        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n-            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n-\\n-        if model_patcher is None: model_patcher = FastMistralModel\\n-        # Mistral does NOT support RoPE Scaling!\\n-        if rope_scaling is not None:\\n-            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-        pass\\n-\\n-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-        statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n-           f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        print(statistics)\\n-        model_patcher.pre_patch()\\n-        # get_statistics()\\n-\\n-        if dtype is None:\\n-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:\\n-            logger.warning_once(\"Device does not support bfloat16. Will change to float16.\")\\n-            dtype = torch.float16\\n-\\n-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n-\\n-        # Check max sequence length\\n-        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n-        model_max_seq_length = model_config.max_position_embeddings\\n-\\n-        # If max_seq_length is not specified, use maximum fron config\\n-        if max_seq_length is None:\\n-            max_seq_length = model_max_seq_length\\n-        pass\\n-\\n-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n-        if max_seq_length > model_max_seq_length:\\n-            raise RuntimeError(\\n-                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n-                f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n-            )\\n-        pass\\n-\\n-        bnb_config = None\\n-        if load_in_4bit:\\n-            bnb_config = BitsAndBytesConfig(\\n-                load_in_4bit              = True,\\n-                bnb_4bit_use_double_quant = True,\\n-                bnb_4bit_quant_type       = \"nf4\",\\n-                bnb_4bit_compute_dtype    = dtype,\\n-            )\\n-\\n-        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map          = device_map,\\n-            torch_dtype         = dtype,\\n-            quantization_config = bnb_config,\\n-            token               = token,\\n-            # rope_scaling      = rope_scaling,\\n-            trust_remote_code   = trust_remote_code,\\n-            **kwargs,\\n-        )\\n-\\n-        # Counteract saved tokenizers\\n-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = load_correct_tokenizer(\\n-            tokenizer_name,\\n-            model_max_length  = max_position_embeddings,\\n-            padding_side      = \"right\",\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n             token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastMistralModel,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n         )\\n-\\n-        model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = model_patcher.post_patch(model)\\n-\\n-        # Patch up QKV / O and MLP\\n-        for idx, layer in enumerate(model.model.layers):\\n-            layer.self_attn.apply_qkv = original_apply_qkv\\n-            layer.self_attn.apply_o   = original_apply_o\\n-        pass\\n-\\n-        # Patch Trainer\\n-        from transformers.trainer import Trainer\\n-        try:\\n-            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n-                Trainer._original_training_loop = inner_training_loop\\n-            else:\\n-                inner_training_loop = Trainer._original_training_loop\\n-        except:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-\\n-        import transformers.trainer\\n-        items_in_trainer = dir(transformers.trainer)\\n-        good_items = []\\n-        for item in items_in_trainer:\\n-            # TODO: Support Deepspeed\\n-            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n-            if item in inner_training_loop: good_items.append(item)\\n-        pass\\n-        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n-\\n-        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n-        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n-        original_debug = inner_training_loop[start:end]\\n-        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n-        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n-\\n-        debug_info = \"\"\"debug_info = \\\\\\\\\\n-        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n-        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n-        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n-        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n-        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n-        logger.warning(debug_info)\\n-        import subprocess, re, gc\\n-        output = subprocess.check_output(\\n-            \\'nvidia-smi --query-gpu=memory.used --format=csv\\', shell = True)\\n-        output = re.findall(rb\\'([\\\\\\\\d]{1,})[\\\\\\\\s]{1,}M\\', output)\\n-        output = sum(int(x.decode(\\'utf-8\\'))/1024 > 4 for x in output)\\n-        if output > 1: raise RuntimeError(\\n-            \\'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.\\')\\n-        for _ in range(3):\\n-            gc.collect()\\n-            torch.cuda.empty_cache()\"\"\"\\n-\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n-\\n-        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n-            args.gradient_accumulation_steps // self._train_batch_size\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        debug_info =\"\"\"\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n-\\n-        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n-        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n-            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"self.accelerator.free_memory()\",\\n-            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n-            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n-            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n-        )\\n-\\n-        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n-        ga  = args.gradient_accumulation_steps\\n-        bsz = self._train_batch_size\\n-        total_batches = bsz * ga * args.world_size\\n-        n_total_devices = total_batches // ga // bsz\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-            divisor = n_total_devices / 1\\n-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n-            if total_batches // ga // bsz > 1:\\n-                divisor = n_total_devices / 1\\n-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n-        check_batches = check_batches.split(\\'\\\\n\\')\\n-        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = self.get_train_dataloader()\",\\n-            check_batches, 1,\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"_inner_training_loop\",\\n-            \"_fast_inner_training_loop\", 1,\\n-        )\\n-        exec(inner_training_loop, globals())\\n-\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_torch_tpu_available()\",\\n-            \"False\",\\n-        )\\n-        if \"n_total_devices >\" not in inner_training_loop:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_sagemaker_mp_enabled()\",\\n-            \"False\",\\n-        )\\n-        exec(inner_training_loop, globals())\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save max_seq_length\\n-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n-        model.max_seq_length = max_position_embeddings\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model.max_seq_length = max_position_embeddings\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model.max_seq_length = max_position_embeddings\\n-\\n-        # We check the tokenizer first for errors\\n-        if fix_tokenizer:\\n-            tokenizer = check_tokenizer(\\n-                model            = model,\\n-                tokenizer        = tokenizer,\\n-                model_name       = model_name,\\n-                model_max_length = max_position_embeddings,\\n-                padding_side     = \"right\",\\n-                token            = token,\\n-            )\\n-        pass\\n-        patch_saving_functions(tokenizer)\\n-\\n-        # Fix up config for transformers uploading PEFT\\n-        # Not necessary anymore since we require transformers>=4.37\\n-        if False:\\n-            name = model.config._name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.config.update({\"_name_or_path\" : name})\\n-            pass\\n-        \\n-        # Log Unsloth version for future fastpaths for inference\\n-        model.config.update({\"unsloth_version\" : __version__})\\n-\\n-        # Add save modules\\n-        patch_saving_functions(model)\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save tokenizer for inference purposes\\n-        tokenizer.padding_side = \"left\" # Force inference\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model._saved_temp_tokenizer = tokenizer\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model._saved_temp_tokenizer = tokenizer\\n-        \\n-        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -12,7 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .mistral import *\\n+from .llama import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -32,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastMistralModel):\\n+class FastQwen2Model(FastLlamaModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"Qwen/Qwen2-7B\",\\n-        max_seq_length = 4096,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"Qwen/Qwen2-7B\",\\n+        max_seq_length    = 4096,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastMistralModel.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = FastQwen2Model,\\n-            tokenizer_name = tokenizer_name,\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastQwen2Model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n             **kwargs,\\n         )\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " '@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/mistral-7b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        if token is None and \"HF_TOKEN\" in os.environ:\\n-            token = os.environ[\"HF_TOKEN\"]\\n-\\n-        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n-            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n-\\n-        if model_patcher is None: model_patcher = FastMistralModel\\n-        # Mistral does NOT support RoPE Scaling!\\n-        if rope_scaling is not None:\\n-            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-        pass\\n-\\n-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-        statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n-           f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        print(statistics)\\n-        model_patcher.pre_patch()\\n-        # get_statistics()\\n-\\n-        if dtype is None:\\n-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:\\n-            logger.warning_once(\"Device does not support bfloat16. Will change to float16.\")\\n-            dtype = torch.float16\\n-\\n-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n-\\n-        # Check max sequence length\\n-        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n-        model_max_seq_length = model_config.max_position_embeddings\\n-\\n-        # If max_seq_length is not specified, use maximum fron config\\n-        if max_seq_length is None:\\n-            max_seq_length = model_max_seq_length\\n-        pass\\n-\\n-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n-        if max_seq_length > model_max_seq_length:\\n-            raise RuntimeError(\\n-                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n-                f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n-            )\\n-        pass\\n-\\n-        bnb_config = None\\n-        if load_in_4bit:\\n-            bnb_config = BitsAndBytesConfig(\\n-                load_in_4bit              = True,\\n-                bnb_4bit_use_double_quant = True,\\n-                bnb_4bit_quant_type       = \"nf4\",\\n-                bnb_4bit_compute_dtype    = dtype,\\n-            )\\n-\\n-        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map          = device_map,\\n-            torch_dtype         = dtype,\\n-            quantization_config = bnb_config,\\n-            token               = token,\\n-            # rope_scaling      = rope_scaling,\\n-            trust_remote_code   = trust_remote_code,\\n-            **kwargs,\\n-        )\\n-\\n-        # Counteract saved tokenizers\\n-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = load_correct_tokenizer(\\n-            tokenizer_name,\\n-            model_max_length  = max_position_embeddings,\\n-            padding_side      = \"right\",\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n             token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastMistralModel,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n         )\\n-\\n-        model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = model_patcher.post_patch(model)\\n-\\n-        # Patch up QKV / O and MLP\\n-        for idx, layer in enumerate(model.model.layers):\\n-            layer.self_attn.apply_qkv = original_apply_qkv\\n-            layer.self_attn.apply_o   = original_apply_o\\n-        pass\\n-\\n-        # Patch Trainer\\n-        from transformers.trainer import Trainer\\n-        try:\\n-            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n-                Trainer._original_training_loop = inner_training_loop\\n-            else:\\n-                inner_training_loop = Trainer._original_training_loop\\n-        except:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-\\n-        import transformers.trainer\\n-        items_in_trainer = dir(transformers.trainer)\\n-        good_items = []\\n-        for item in items_in_trainer:\\n-            # TODO: Support Deepspeed\\n-            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n-            if item in inner_training_loop: good_items.append(item)\\n-        pass\\n-        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n-\\n-        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n-        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n-        original_debug = inner_training_loop[start:end]\\n-        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n-        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n-\\n-        debug_info = \"\"\"debug_info = \\\\\\\\\\n-        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n-        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n-        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n-        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n-        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n-        logger.warning(debug_info)\\n-        import subprocess, re, gc\\n-        output = subprocess.check_output(\\n-            \\'nvidia-smi --query-gpu=memory.used --format=csv\\', shell = True)\\n-        output = re.findall(rb\\'([\\\\\\\\d]{1,})[\\\\\\\\s]{1,}M\\', output)\\n-        output = sum(int(x.decode(\\'utf-8\\'))/1024 > 4 for x in output)\\n-        if output > 1: raise RuntimeError(\\n-            \\'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.\\')\\n-        for _ in range(3):\\n-            gc.collect()\\n-            torch.cuda.empty_cache()\"\"\"\\n-\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n-\\n-        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n-            args.gradient_accumulation_steps // self._train_batch_size\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        debug_info =\"\"\"\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n-\\n-        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n-        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n-            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"self.accelerator.free_memory()\",\\n-            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n-            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n-            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n-        )\\n-\\n-        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n-        ga  = args.gradient_accumulation_steps\\n-        bsz = self._train_batch_size\\n-        total_batches = bsz * ga * args.world_size\\n-        n_total_devices = total_batches // ga // bsz\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-            divisor = n_total_devices / 1\\n-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n-            if total_batches // ga // bsz > 1:\\n-                divisor = n_total_devices / 1\\n-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n-        check_batches = check_batches.split(\\'\\\\n\\')\\n-        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = self.get_train_dataloader()\",\\n-            check_batches, 1,\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"_inner_training_loop\",\\n-            \"_fast_inner_training_loop\", 1,\\n-        )\\n-        exec(inner_training_loop, globals())\\n-\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_torch_tpu_available()\",\\n-            \"False\",\\n-        )\\n-        if \"n_total_devices >\" not in inner_training_loop:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_sagemaker_mp_enabled()\",\\n-            \"False\",\\n-        )\\n-        exec(inner_training_loop, globals())\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save max_seq_length\\n-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n-        model.max_seq_length = max_position_embeddings\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model.max_seq_length = max_position_embeddings\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model.max_seq_length = max_position_embeddings\\n-\\n-        # We check the tokenizer first for errors\\n-        if fix_tokenizer:\\n-            tokenizer = check_tokenizer(\\n-                model            = model,\\n-                tokenizer        = tokenizer,\\n-                model_name       = model_name,\\n-                model_max_length = max_position_embeddings,\\n-                padding_side     = \"right\",\\n-                token            = token,\\n-            )\\n-        pass\\n-        patch_saving_functions(tokenizer)\\n-\\n-        # Fix up config for transformers uploading PEFT\\n-        # Not necessary anymore since we require transformers>=4.37\\n-        if False:\\n-            name = model.config._name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.config.update({\"_name_or_path\" : name})\\n-            pass\\n-        \\n-        # Log Unsloth version for future fastpaths for inference\\n-        model.config.update({\"unsloth_version\" : __version__})\\n-\\n-        # Add save modules\\n-        patch_saving_functions(model)\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save tokenizer for inference purposes\\n-        tokenizer.padding_side = \"left\" # Force inference\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model._saved_temp_tokenizer = tokenizer\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model._saved_temp_tokenizer = tokenizer\\n-        \\n-        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -12,7 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .mistral import *\\n+from .llama import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -32,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastMistralModel):\\n+class FastQwen2Model(FastLlamaModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"Qwen/Qwen2-7B\",\\n-        max_seq_length = 4096,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"Qwen/Qwen2-7B\",\\n+        max_seq_length    = 4096,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastMistralModel.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = FastQwen2Model,\\n-            tokenizer_name = tokenizer_name,\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastQwen2Model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n             **kwargs,\\n         )\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " '@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/mistral-7b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        if token is None and \"HF_TOKEN\" in os.environ:\\n-            token = os.environ[\"HF_TOKEN\"]\\n-\\n-        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n-            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n-\\n-        if model_patcher is None: model_patcher = FastMistralModel\\n-        # Mistral does NOT support RoPE Scaling!\\n-        if rope_scaling is not None:\\n-            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-        pass\\n-\\n-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-        statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n-           f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        print(statistics)\\n-        model_patcher.pre_patch()\\n-        # get_statistics()\\n-\\n-        if dtype is None:\\n-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:\\n-            logger.warning_once(\"Device does not support bfloat16. Will change to float16.\")\\n-            dtype = torch.float16\\n-\\n-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n-\\n-        # Check max sequence length\\n-        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n-        model_max_seq_length = model_config.max_position_embeddings\\n-\\n-        # If max_seq_length is not specified, use maximum fron config\\n-        if max_seq_length is None:\\n-            max_seq_length = model_max_seq_length\\n-        pass\\n-\\n-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n-        if max_seq_length > model_max_seq_length:\\n-            raise RuntimeError(\\n-                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n-                f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n-            )\\n-        pass\\n-\\n-        bnb_config = None\\n-        if load_in_4bit:\\n-            bnb_config = BitsAndBytesConfig(\\n-                load_in_4bit              = True,\\n-                bnb_4bit_use_double_quant = True,\\n-                bnb_4bit_quant_type       = \"nf4\",\\n-                bnb_4bit_compute_dtype    = dtype,\\n-            )\\n-\\n-        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map          = device_map,\\n-            torch_dtype         = dtype,\\n-            quantization_config = bnb_config,\\n-            token               = token,\\n-            # rope_scaling      = rope_scaling,\\n-            trust_remote_code   = trust_remote_code,\\n-            **kwargs,\\n-        )\\n-\\n-        # Counteract saved tokenizers\\n-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = load_correct_tokenizer(\\n-            tokenizer_name,\\n-            model_max_length  = max_position_embeddings,\\n-            padding_side      = \"right\",\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n             token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastMistralModel,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n         )\\n-\\n-        model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = model_patcher.post_patch(model)\\n-\\n-        # Patch up QKV / O and MLP\\n-        for idx, layer in enumerate(model.model.layers):\\n-            layer.self_attn.apply_qkv = original_apply_qkv\\n-            layer.self_attn.apply_o   = original_apply_o\\n-        pass\\n-\\n-        # Patch Trainer\\n-        from transformers.trainer import Trainer\\n-        try:\\n-            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n-                Trainer._original_training_loop = inner_training_loop\\n-            else:\\n-                inner_training_loop = Trainer._original_training_loop\\n-        except:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-\\n-        import transformers.trainer\\n-        items_in_trainer = dir(transformers.trainer)\\n-        good_items = []\\n-        for item in items_in_trainer:\\n-            # TODO: Support Deepspeed\\n-            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n-            if item in inner_training_loop: good_items.append(item)\\n-        pass\\n-        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n-\\n-        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n-        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n-        original_debug = inner_training_loop[start:end]\\n-        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n-        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n-\\n-        debug_info = \"\"\"debug_info = \\\\\\\\\\n-        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n-        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n-        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n-        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n-        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n-        logger.warning(debug_info)\\n-        import subprocess, re, gc\\n-        output = subprocess.check_output(\\n-            \\'nvidia-smi --query-gpu=memory.used --format=csv\\', shell = True)\\n-        output = re.findall(rb\\'([\\\\\\\\d]{1,})[\\\\\\\\s]{1,}M\\', output)\\n-        output = sum(int(x.decode(\\'utf-8\\'))/1024 > 4 for x in output)\\n-        if output > 1: raise RuntimeError(\\n-            \\'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.\\')\\n-        for _ in range(3):\\n-            gc.collect()\\n-            torch.cuda.empty_cache()\"\"\"\\n-\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n-\\n-        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n-            args.gradient_accumulation_steps // self._train_batch_size\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        debug_info =\"\"\"\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n-\\n-        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n-        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n-            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"self.accelerator.free_memory()\",\\n-            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n-            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n-            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n-        )\\n-\\n-        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n-        ga  = args.gradient_accumulation_steps\\n-        bsz = self._train_batch_size\\n-        total_batches = bsz * ga * args.world_size\\n-        n_total_devices = total_batches // ga // bsz\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-            divisor = n_total_devices / 1\\n-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n-            if total_batches // ga // bsz > 1:\\n-                divisor = n_total_devices / 1\\n-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n-        check_batches = check_batches.split(\\'\\\\n\\')\\n-        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = self.get_train_dataloader()\",\\n-            check_batches, 1,\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"_inner_training_loop\",\\n-            \"_fast_inner_training_loop\", 1,\\n-        )\\n-        exec(inner_training_loop, globals())\\n-\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_torch_tpu_available()\",\\n-            \"False\",\\n-        )\\n-        if \"n_total_devices >\" not in inner_training_loop:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_sagemaker_mp_enabled()\",\\n-            \"False\",\\n-        )\\n-        exec(inner_training_loop, globals())\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save max_seq_length\\n-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n-        model.max_seq_length = max_position_embeddings\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model.max_seq_length = max_position_embeddings\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model.max_seq_length = max_position_embeddings\\n-\\n-        # We check the tokenizer first for errors\\n-        if fix_tokenizer:\\n-            tokenizer = check_tokenizer(\\n-                model            = model,\\n-                tokenizer        = tokenizer,\\n-                model_name       = model_name,\\n-                model_max_length = max_position_embeddings,\\n-                padding_side     = \"right\",\\n-                token            = token,\\n-            )\\n-        pass\\n-        patch_saving_functions(tokenizer)\\n-\\n-        # Fix up config for transformers uploading PEFT\\n-        # Not necessary anymore since we require transformers>=4.37\\n-        if False:\\n-            name = model.config._name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.config.update({\"_name_or_path\" : name})\\n-            pass\\n-        \\n-        # Log Unsloth version for future fastpaths for inference\\n-        model.config.update({\"unsloth_version\" : __version__})\\n-\\n-        # Add save modules\\n-        patch_saving_functions(model)\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save tokenizer for inference purposes\\n-        tokenizer.padding_side = \"left\" # Force inference\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model._saved_temp_tokenizer = tokenizer\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model._saved_temp_tokenizer = tokenizer\\n-        \\n-        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -12,7 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .mistral import *\\n+from .llama import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -32,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastMistralModel):\\n+class FastQwen2Model(FastLlamaModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"Qwen/Qwen2-7B\",\\n-        max_seq_length = 4096,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"Qwen/Qwen2-7B\",\\n+        max_seq_length    = 4096,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastMistralModel.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = FastQwen2Model,\\n-            tokenizer_name = tokenizer_name,\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastQwen2Model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n             **kwargs,\\n         )\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " '@@ -289,289 +289,32 @@ class FastMistralModel(FastLlamaModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/mistral-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Mistral does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/mistral-7b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Mistral does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        if token is None and \"HF_TOKEN\" in os.environ:\\n-            token = os.environ[\"HF_TOKEN\"]\\n-\\n-        if token is None and \"HUGGINGFACE_TOKEN\" in os.environ:\\n-            token = os.environ[\"HUGGINGFACE_TOKEN\"]\\n-\\n-        if model_patcher is None: model_patcher = FastMistralModel\\n-        # Mistral does NOT support RoPE Scaling!\\n-        if rope_scaling is not None:\\n-            logger.warning_once(\"Unsloth: Mistral models do not support RoPE scaling.\")\\n-        pass\\n-\\n-        SUPPORTS_BFLOAT16 = is_bfloat16_supported()\\n-        gpu_stats = torch.cuda.get_device_properties(0)\\n-        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\\n-\\n-        statistics = \\\\\\n-           f\"==((====))==  Unsloth: Fast {model_patcher.__name__[4:-5]} patching release {__version__}\\\\n\"\\\\\\n-           f\"   \\\\\\\\\\\\   /|    GPU: {gpu_stats.name}. Max memory: {max_memory} GB. Platform = {platform_system}.\\\\n\"\\\\\\n-           f\"O^O/ \\\\_/ \\\\\\\\    Pytorch: {torch.__version__}. CUDA = {gpu_stats.major}.{gpu_stats.minor}. CUDA Toolkit = {torch.version.cuda}.\\\\n\"\\\\\\n-           f\"\\\\        /    Bfloat16 = {str(SUPPORTS_BFLOAT16).upper()}. Xformers = {xformers_version}. FA = {HAS_FLASH_ATTENTION}.\\\\n\"\\\\\\n-           f\\' \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\\'\\n-        print(statistics)\\n-        model_patcher.pre_patch()\\n-        # get_statistics()\\n-\\n-        if dtype is None:\\n-            dtype = torch.float16 if not SUPPORTS_BFLOAT16 else torch.bfloat16\\n-        elif dtype == torch.bfloat16 and not SUPPORTS_BFLOAT16:\\n-            logger.warning_once(\"Device does not support bfloat16. Will change to float16.\")\\n-            dtype = torch.float16\\n-\\n-        assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n-\\n-        # Check max sequence length\\n-        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n-        model_max_seq_length = model_config.max_position_embeddings\\n-\\n-        # If max_seq_length is not specified, use maximum fron config\\n-        if max_seq_length is None:\\n-            max_seq_length = model_max_seq_length\\n-        pass\\n-\\n-        # Mistral does NOT support RoPE Scaling sadly so we have to error out.\\n-        if max_seq_length > model_max_seq_length:\\n-            raise RuntimeError(\\n-                f\"Unsloth: Unfortunately {model_patcher.__name__[4:-5]} type models do not support RoPE scaling!\\\\n\"\\\\\\n-                f\"The maximum sequence length supported is {model_max_seq_length}.\",\\n-            )\\n-        pass\\n-\\n-        bnb_config = None\\n-        if load_in_4bit:\\n-            bnb_config = BitsAndBytesConfig(\\n-                load_in_4bit              = True,\\n-                bnb_4bit_use_double_quant = True,\\n-                bnb_4bit_quant_type       = \"nf4\",\\n-                bnb_4bit_compute_dtype    = dtype,\\n-            )\\n-\\n-        max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        model = AutoModelForCausalLM.from_pretrained(\\n-            model_name,\\n-            device_map          = device_map,\\n-            torch_dtype         = dtype,\\n-            quantization_config = bnb_config,\\n-            token               = token,\\n-            # rope_scaling      = rope_scaling,\\n-            trust_remote_code   = trust_remote_code,\\n-            **kwargs,\\n-        )\\n-\\n-        # Counteract saved tokenizers\\n-        tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n-        tokenizer = load_correct_tokenizer(\\n-            tokenizer_name,\\n-            model_max_length  = max_position_embeddings,\\n-            padding_side      = \"right\",\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n             token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastMistralModel,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n+            **kwargs,\\n         )\\n-\\n-        model, tokenizer = patch_tokenizer(model, tokenizer)\\n-        model = model_patcher.post_patch(model)\\n-\\n-        # Patch up QKV / O and MLP\\n-        for idx, layer in enumerate(model.model.layers):\\n-            layer.self_attn.apply_qkv = original_apply_qkv\\n-            layer.self_attn.apply_o   = original_apply_o\\n-        pass\\n-\\n-        # Patch Trainer\\n-        from transformers.trainer import Trainer\\n-        try:\\n-            if Trainer._inner_training_loop.__name__ != \"_fast_inner_training_loop\":\\n-                inner_training_loop = inspect.getsource(Trainer._inner_training_loop)\\n-                Trainer._original_training_loop = inner_training_loop\\n-            else:\\n-                inner_training_loop = Trainer._original_training_loop\\n-        except:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-\\n-        import transformers.trainer\\n-        items_in_trainer = dir(transformers.trainer)\\n-        good_items = []\\n-        for item in items_in_trainer:\\n-            # TODO: Support Deepspeed\\n-            if item.startswith((\"deepspeed\", \"xm\", \"met\", \"smp\")): continue\\n-            if item in inner_training_loop: good_items.append(item)\\n-        pass\\n-        exec(\"from transformers.trainer import (\" + \", \".join(x for x in good_items) + \")\", globals())\\n-\\n-        start = re.search(\\'logger\\\\.info\\\\([\\\\\"\\\\\\'].+?Running training\\', inner_training_loop).span(0)[0]\\n-        end = inner_training_loop.find(\"\\\\n\\\\n\", start)\\n-        original_debug = inner_training_loop[start:end]\\n-        spaces = re.search(\\'\\\\n([\\\\s\\\\t]{1,})\\', original_debug).group(0)[1:]\\n-        front_spaces = re.match(\\'([\\\\s\\\\t]{1,})\\', inner_training_loop).group(0)\\n-\\n-        debug_info = \"\"\"debug_info = \\\\\\\\\\n-        f\"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = {args.world_size}\\\\\\\\n\"\\\\\\\\\\n-        f\"   \\\\\\\\\\\\\\\\\\\\\\\\   /|    Num examples = {num_examples:,} | Num Epochs = {num_train_epochs:,}\\\\\\\\n\"\\\\\\\\\\n-        f\"O^O/ \\\\\\\\_/ \\\\\\\\    Batch size per device = {self._train_batch_size:,} | Gradient Accumulation steps = {args.gradient_accumulation_steps}\\\\\\\\n\"\\\\\\\\\\n-        f\"\\\\\\\\        /    Total batch size = {total_train_batch_size:,} | Total steps = {max_steps:,}\\\\\\\\n\"\\\\\\\\\\n-        f\\' \"-____-\"     Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\\'\\n-        logger.warning(debug_info)\\n-        import subprocess, re, gc\\n-        output = subprocess.check_output(\\n-            \\'nvidia-smi --query-gpu=memory.used --format=csv\\', shell = True)\\n-        output = re.findall(rb\\'([\\\\\\\\d]{1,})[\\\\\\\\s]{1,}M\\', output)\\n-        output = sum(int(x.decode(\\'utf-8\\'))/1024 > 4 for x in output)\\n-        if output > 1: raise RuntimeError(\\n-            \\'Error: More than 1 GPUs have a lot of VRAM usage. Please obtain a commercial license.\\')\\n-        for _ in range(3):\\n-            gc.collect()\\n-            torch.cuda.empty_cache()\"\"\"\\n-\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(original_debug, debug_info)\\n-\\n-        debug_info = \"\"\"n_total_devices = total_train_batch_size // \\\\\\\\\\n-            args.gradient_accumulation_steps // self._train_batch_size\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        debug_info =\"\"\"\\n-        debug_info = debug_info.split(\\'\\\\n\\')\\n-        debug_info = \"\\\\n\".join([debug_info[0]] + [spaces + x[8:] for x in debug_info[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\"debug_info =\", debug_info, 1)\\n-\\n-        front_spaces = re.match(r\"[\\\\t\\\\s]{1,}\", inner_training_loop).group(0)\\n-        inner_training_loop = re.sub(r\"^\" + front_spaces, \"\", inner_training_loop, flags = re.MULTILINE)\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = tpu_spmd_dataloader(train_dataloader)\",\\n-            \"raise RuntimeError(\\'Unsloth: TPUs are not yet supported!\\')\"\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"self.accelerator.free_memory()\",\\n-            \"self.accelerator.free_memory()\\\\n\" + \\\\\\n-            front_spaces + \"if self.is_deepspeed_enabled:\"\\\\\\n-            \"raise RuntimeError(\\'Unsloth: Deepspeed is not yet supported!\\')\\\\n\", 1,\\n-        )\\n-\\n-        check_batches = \"\"\"train_dataloader = self.get_train_dataloader()\\n-        ga  = args.gradient_accumulation_steps\\n-        bsz = self._train_batch_size\\n-        total_batches = bsz * ga * args.world_size\\n-        n_total_devices = total_batches // ga // bsz\\n-        if n_total_devices > 1:\\n-            logger.warning_once(\\n-                \"* Our OSS was designed for people with few GPU resources to level the playing field.\\\\\\\\n\"\\n-                \"* The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\\\\\n\"\\n-                \"* We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\\\\\n\"\\n-                \"* If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-            divisor = n_total_devices / 1\\n-            bsz = self._train_batch_size = max(int(bsz / divisor), 1)\\n-            if total_batches // ga // bsz > 1:\\n-                divisor = n_total_devices / 1\\n-                ga = args.gradient_accumulation_steps = max(int(ga / divisor), 1)\"\"\"\\n-        check_batches = check_batches.split(\\'\\\\n\\')\\n-        check_batches = \"\\\\n\".join([check_batches[0]] + [front_spaces + x[8:] for x in check_batches[1:]])\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"train_dataloader = self.get_train_dataloader()\",\\n-            check_batches, 1,\\n-        )\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"_inner_training_loop\",\\n-            \"_fast_inner_training_loop\", 1,\\n-        )\\n-        exec(inner_training_loop, globals())\\n-\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_torch_tpu_available()\",\\n-            \"False\",\\n-        )\\n-        if \"n_total_devices >\" not in inner_training_loop:\\n-            raise RuntimeError(\\n-                \"Our OSS was designed for people with few GPU resources to level the playing field.\\\\n\"\\n-                \"The OSS Apache 2 license only supports one GPU - please obtain a commercial license.\\\\n\"\\n-                \"We\\'re a 2 person team, so we still have to fund our development costs - thanks!\\\\n\"\\n-                \"If you don\\'t, please consider at least sponsoring us through Ko-fi! Appreciate it!\",\\n-            )\\n-        pass\\n-        inner_training_loop = inner_training_loop.replace(\\n-            \"is_sagemaker_mp_enabled()\",\\n-            \"False\",\\n-        )\\n-        exec(inner_training_loop, globals())\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save max_seq_length\\n-        max_position_embeddings = max(max_seq_length, model.config.max_position_embeddings)\\n-        model.max_seq_length = max_position_embeddings\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model.max_seq_length = max_position_embeddings\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model.max_seq_length = max_position_embeddings\\n-\\n-        # We check the tokenizer first for errors\\n-        if fix_tokenizer:\\n-            tokenizer = check_tokenizer(\\n-                model            = model,\\n-                tokenizer        = tokenizer,\\n-                model_name       = model_name,\\n-                model_max_length = max_position_embeddings,\\n-                padding_side     = \"right\",\\n-                token            = token,\\n-            )\\n-        pass\\n-        patch_saving_functions(tokenizer)\\n-\\n-        # Fix up config for transformers uploading PEFT\\n-        # Not necessary anymore since we require transformers>=4.37\\n-        if False:\\n-            name = model.config._name_or_path\\n-            if name.startswith(\"unsloth/\") and name.endswith(\"-bnb-4bit\"):\\n-                name = name[:len(name) - len(\"-bnb-4bit\")]\\n-                model.config.update({\"_name_or_path\" : name})\\n-            pass\\n-        \\n-        # Log Unsloth version for future fastpaths for inference\\n-        model.config.update({\"unsloth_version\" : __version__})\\n-\\n-        # Add save modules\\n-        patch_saving_functions(model)\\n-        Trainer._inner_training_loop = _fast_inner_training_loop\\n-\\n-        # Save tokenizer for inference purposes\\n-        tokenizer.padding_side = \"left\" # Force inference\\n-        internal_model = model\\n-        while hasattr(internal_model, \"model\"):\\n-            internal_model._saved_temp_tokenizer = tokenizer\\n-            internal_model = internal_model.model\\n-        pass\\n-        internal_model._saved_temp_tokenizer = tokenizer\\n-        \\n-        return model, tokenizer\\n     pass\\n pass\\n',\n",
       " '@@ -12,7 +12,7 @@\\n # See the License for the specific language governing permissions and\\n # limitations under the License.\\n \\n-from .mistral import *\\n+from .llama import *\\n \\n from transformers.models.qwen2.modeling_qwen2 import (\\n     Qwen2Attention,\\n@@ -32,7 +32,7 @@ except:\\n pass\\n \\n \\n-class FastQwen2Model(FastMistralModel):\\n+class FastQwen2Model(FastLlamaModel):\\n \\n     @staticmethod\\n     def pre_patch():\\n@@ -57,30 +57,30 @@ class FastQwen2Model(FastMistralModel):\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"Qwen/Qwen2-7B\",\\n-        max_seq_length = 4096,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None, # Qwen2 does not support RoPE scaling\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"Qwen/Qwen2-7B\",\\n+        max_seq_length    = 4096,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None, # Qwen2 does not support RoPE scaling\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n-        return FastMistralModel.from_pretrained(\\n-            model_name     = model_name,\\n-            max_seq_length = max_seq_length,\\n-            dtype          = dtype,\\n-            load_in_4bit   = load_in_4bit,\\n-            token          = token,\\n-            device_map     = device_map,\\n-            rope_scaling   = rope_scaling,\\n-            fix_tokenizer  = fix_tokenizer,\\n-            model_patcher  = FastQwen2Model,\\n-            tokenizer_name = tokenizer_name,\\n+        return FastLlamaModel.from_pretrained(\\n+            model_name        = model_name,\\n+            max_seq_length    = max_seq_length,\\n+            dtype             = dtype,\\n+            load_in_4bit      = load_in_4bit,\\n+            token             = token,\\n+            device_map        = device_map,\\n+            rope_scaling      = rope_scaling,\\n+            fix_tokenizer     = fix_tokenizer,\\n+            model_patcher     = FastQwen2Model,\\n+            tokenizer_name    = tokenizer_name,\\n             trust_remote_code = trust_remote_code,\\n             **kwargs,\\n         )\\n',\n",
       " '@@ -51,6 +51,7 @@ except:\\n pass\\n \\n from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\\n+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\\n from transformers import set_seed as transformers_set_seed\\n from peft import LoraConfig, TaskType, get_peft_model as _get_peft_model\\n from peft import PeftModelForCausalLM\\n@@ -1028,16 +1029,16 @@ class FastLlamaModel:\\n \\n     @staticmethod\\n     def from_pretrained(\\n-        model_name     = \"unsloth/llama-2-7b-bnb-4bit\",\\n-        max_seq_length = None,\\n-        dtype          = None,\\n-        load_in_4bit   = True,\\n-        token          = None,\\n-        device_map     = \"sequential\",\\n-        rope_scaling   = None,\\n-        fix_tokenizer  = True,\\n-        model_patcher  = None,\\n-        tokenizer_name = None,\\n+        model_name        = \"unsloth/llama-3-8b-bnb-4bit\",\\n+        max_seq_length    = None,\\n+        dtype             = None,\\n+        load_in_4bit      = True,\\n+        token             = None,\\n+        device_map        = \"sequential\",\\n+        rope_scaling      = None,\\n+        fix_tokenizer     = True,\\n+        model_patcher     = None,\\n+        tokenizer_name    = None,\\n         trust_remote_code = False,\\n         **kwargs,\\n     ):\\n@@ -1070,9 +1071,17 @@ class FastLlamaModel:\\n \\n         assert(dtype == torch.float16 or dtype == torch.bfloat16 or dtype == torch.float32)\\n \\n-        # RoPE scaling\\n-        model_max_seq_length = \\\\\\n-            AutoConfig.from_pretrained(model_name, token = token).max_position_embeddings\\n+        # RoPE Scaling\\n+        model_config = AutoConfig.from_pretrained(model_name, token = token)\\n+        model_max_seq_length = model_config.max_position_embeddings\\n+\\n+        # Check if RoPE Scaling is even allowed\\n+        model_function = MODEL_FOR_CAUSAL_LM_MAPPING[model_config.__class__]\\n+        has_rope_scaling = False\\n+        try:\\n+            with open(inspect.getfile(model_function), \"r\") as file:\\n+                has_rope_scaling = \"self.config.rope_scaling\" in file.read()\\n+        except: pass\\n \\n         # If max_seq_length is not specified, use maximum fron config\\n         if max_seq_length is None:\\n@@ -1080,14 +1089,28 @@ class FastLlamaModel:\\n         pass\\n \\n         if (rope_scaling is None) and (max_seq_length > model_max_seq_length):\\n+\\n             rope_scaling = max_seq_length / model_max_seq_length\\n+\\n             logger.warning_once(\\n                 f\"Unsloth: {model_name} can only handle sequence lengths of at most \"\\\\\\n                 f\"{model_max_seq_length}.\\\\nBut with kaiokendev\\'s RoPE scaling of \"\\\\\\n                 f\"{round(rope_scaling, 3)}, it can be magically be extended to \"\\\\\\n                 f\"{max_seq_length}!\"\\n             )\\n+\\n+            # Warn RoPE scaling isn\\'t allowed\\n+            if not has_rope_scaling:\\n+                raise RuntimeError(\\n+                    \"However, {model_name} doesn\\'t support RoPE Scaling!\\\\n\"\\\\\\n+                    \"Please file a feature request at https://github.com/unslothai/unsloth.\"\\n+                )\\n+            pass\\n+\\n             rope_scaling = {\"type\": \"linear\", \"factor\": rope_scaling,}\\n+\\n+            # Add to kwargs\\n+            kwargs[\"rope_scaling\"] = rope_scaling\\n         pass\\n \\n         bnb_config = None\\n@@ -1103,39 +1126,16 @@ class FastLlamaModel:\\n         # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/12\\n         # RoPE Scaling\\'s max_position_embeddings must be updated\\n         max_position_embeddings = max(max_seq_length, model_max_seq_length)\\n-        try:\\n-            model = AutoModelForCausalLM.from_pretrained(\\n-                model_name,\\n-                device_map              = device_map,\\n-                torch_dtype             = dtype,\\n-                quantization_config     = bnb_config,\\n-                token                   = token,\\n-                rope_scaling            = rope_scaling,\\n-                max_position_embeddings = max_position_embeddings,\\n-                trust_remote_code       = trust_remote_code,\\n-                **kwargs,\\n-            )\\n-        except Exception as error:\\n-            if \"rope_scaling\" in str(error):\\n-                if rope_scaling is not None:\\n-                    raise TypeError(\"Unsloth: {model_name} does not support rope_scaling.\")\\n-                pass\\n-\\n-                # Counteract missing rope_scaling\\n-                model = AutoModelForCausalLM.from_pretrained(\\n-                    model_name,\\n-                    device_map              = device_map,\\n-                    torch_dtype             = dtype,\\n-                    quantization_config     = bnb_config,\\n-                    token                   = token,\\n-                    max_position_embeddings = max_position_embeddings,\\n-                    trust_remote_code       = trust_remote_code,\\n-                    **kwargs,\\n-                )\\n-            else:\\n-                raise error\\n-            pass\\n-        pass\\n+        model = AutoModelForCausalLM.from_pretrained(\\n+            model_name,\\n+            device_map              = device_map,\\n+            torch_dtype             = dtype,\\n+            quantization_config     = bnb_config,\\n+            token                   = token,\\n+            max_position_embeddings = max_position_embeddings,\\n+            trust_remote_code       = trust_remote_code,\\n+            **kwargs,\\n+        )\\n \\n         # Counteract saved tokenizers\\n         tokenizer_name = model_name if tokenizer_name is None else tokenizer_name\\n@@ -1423,7 +1423,6 @@ class FastLlamaModel:\\n \\n         if loftq_config is None: loftq_config = {}\\n \\n-        import inspect\\n         signature = str(inspect.signature(LoraConfig))\\n         SUPPORTS_LOFTQ  = \"loftq_config\" in signature\\n         SUPPORTS_RSLORA = \"use_rslora\"   in signature\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = df2['Diff'].to_list()\n",
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bc2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "['add more info about nvidia gpu',\n",
    " 'distro binary files',\n",
    " 'test the image',\n",
    " 'add missing missing nodes in skeleton skeleton',\n",
    " 'add missing missing nodes in skeleton skeleton',\n",
    " 'test binary file',\n",
    " 'add missing colab',\n",
    " 'add missing deprecation warning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eececb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad057cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"commit_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b554a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e67b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"commit_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df78c8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Hashes of parents</th>\n",
       "      <th>Is a merge commit?</th>\n",
       "      <th>List of modified files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>['3aa16bb452ab82d7a2b2987ec3bfb47c6812582c']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>['3aa16bb452ab82d7a2b2987ec3bfb47c6812582c']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4b97a810b509c93f44be4c037c7aa18fb8922884</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Pre-release 2023 December version (Mistral, Pr...</td>\n",
       "      <td>['3aa16bb452ab82d7a2b2987ec3bfb47c6812582c']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2d5d88487463e76f75002be3b2704267ec96e68a</td>\n",
       "      <td>Daniel Han-Chen</td>\n",
       "      <td>tokenizer pad fix</td>\n",
       "      <td>['28f3b971d21e469fb985f384db10a03982c4ce12']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>f380cc1170447800c112dc8568bdff3dd34c79a3</td>\n",
       "      <td>Daniel Han-Chen</td>\n",
       "      <td>Fix Mistral\\n\\nBlockDiagonalCausalMask fix cou...</td>\n",
       "      <td>['399f8ed56f40df0919208d1ffdee64a31a1b22c8']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1014</td>\n",
       "      <td>17688b54ea608946853cfb8d10e63e3f8ae7a839</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Nightly (#3102)\\n\\n* Update synthetic.py\\n\\n* ...</td>\n",
       "      <td>['5b14b8fbd2e8aaa9eb560f23c42de10db45b45b3']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1015</td>\n",
       "      <td>17688b54ea608946853cfb8d10e63e3f8ae7a839</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Nightly (#3102)\\n\\n* Update synthetic.py\\n\\n* ...</td>\n",
       "      <td>['5b14b8fbd2e8aaa9eb560f23c42de10db45b45b3']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1016</td>\n",
       "      <td>17688b54ea608946853cfb8d10e63e3f8ae7a839</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>Nightly (#3102)\\n\\n* Update synthetic.py\\n\\n* ...</td>\n",
       "      <td>['5b14b8fbd2e8aaa9eb560f23c42de10db45b45b3']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>['76be074f266e0710c3b8b293859a53dd09b683dc']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1018</td>\n",
       "      <td>f1508c9259f91e33f5c7fdf95d971a309196471c</td>\n",
       "      <td>Daniel Han</td>\n",
       "      <td>GPT OSS fixes</td>\n",
       "      <td>['76be074f266e0710c3b8b293859a53dd09b683dc']</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;pydriller.domain.commit.ModifiedFile object ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1019 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      Hash           Author  \\\n",
       "0              0  4b97a810b509c93f44be4c037c7aa18fb8922884       Daniel Han   \n",
       "1              1  4b97a810b509c93f44be4c037c7aa18fb8922884       Daniel Han   \n",
       "2              2  4b97a810b509c93f44be4c037c7aa18fb8922884       Daniel Han   \n",
       "3              3  2d5d88487463e76f75002be3b2704267ec96e68a  Daniel Han-Chen   \n",
       "4              4  f380cc1170447800c112dc8568bdff3dd34c79a3  Daniel Han-Chen   \n",
       "...          ...                                       ...              ...   \n",
       "1014        1014  17688b54ea608946853cfb8d10e63e3f8ae7a839       Daniel Han   \n",
       "1015        1015  17688b54ea608946853cfb8d10e63e3f8ae7a839       Daniel Han   \n",
       "1016        1016  17688b54ea608946853cfb8d10e63e3f8ae7a839       Daniel Han   \n",
       "1017        1017  f1508c9259f91e33f5c7fdf95d971a309196471c       Daniel Han   \n",
       "1018        1018  f1508c9259f91e33f5c7fdf95d971a309196471c       Daniel Han   \n",
       "\n",
       "                                                Message  \\\n",
       "0     Pre-release 2023 December version (Mistral, Pr...   \n",
       "1     Pre-release 2023 December version (Mistral, Pr...   \n",
       "2     Pre-release 2023 December version (Mistral, Pr...   \n",
       "3                                     tokenizer pad fix   \n",
       "4     Fix Mistral\\n\\nBlockDiagonalCausalMask fix cou...   \n",
       "...                                                 ...   \n",
       "1014  Nightly (#3102)\\n\\n* Update synthetic.py\\n\\n* ...   \n",
       "1015  Nightly (#3102)\\n\\n* Update synthetic.py\\n\\n* ...   \n",
       "1016  Nightly (#3102)\\n\\n* Update synthetic.py\\n\\n* ...   \n",
       "1017                                      GPT OSS fixes   \n",
       "1018                                      GPT OSS fixes   \n",
       "\n",
       "                                 Hashes of parents  Is a merge commit?  \\\n",
       "0     ['3aa16bb452ab82d7a2b2987ec3bfb47c6812582c']               False   \n",
       "1     ['3aa16bb452ab82d7a2b2987ec3bfb47c6812582c']               False   \n",
       "2     ['3aa16bb452ab82d7a2b2987ec3bfb47c6812582c']               False   \n",
       "3     ['28f3b971d21e469fb985f384db10a03982c4ce12']               False   \n",
       "4     ['399f8ed56f40df0919208d1ffdee64a31a1b22c8']               False   \n",
       "...                                            ...                 ...   \n",
       "1014  ['5b14b8fbd2e8aaa9eb560f23c42de10db45b45b3']               False   \n",
       "1015  ['5b14b8fbd2e8aaa9eb560f23c42de10db45b45b3']               False   \n",
       "1016  ['5b14b8fbd2e8aaa9eb560f23c42de10db45b45b3']               False   \n",
       "1017  ['76be074f266e0710c3b8b293859a53dd09b683dc']               False   \n",
       "1018  ['76be074f266e0710c3b8b293859a53dd09b683dc']               False   \n",
       "\n",
       "                                 List of modified files  \n",
       "0     [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "1     [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "2     [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "3     [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "4     [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "...                                                 ...  \n",
       "1014  [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "1015  [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "1016  [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "1017  [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "1018  [<pydriller.domain.commit.ModifiedFile object ...  \n",
       "\n",
       "[1019 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09614de5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m      5\u001b[0m author_name \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList of modified files\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]:\n\u001b[0;32m      7\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHash\u001b[39m\u001b[38;5;124m'\u001b[39m: commit_hash,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMessage\u001b[39m\u001b[38;5;124m'\u001b[39m: commit_msg,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m\"\u001b[39m : author_name,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m'\u001b[39m : mod\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChange Type\u001b[39m\u001b[38;5;124m'\u001b[39m: mod\u001b[38;5;241m.\u001b[39mchange_type\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m---> 13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource Code (before)\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_code_before\u001b[49m,\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource Code (current)\u001b[39m\u001b[38;5;124m'\u001b[39m : mod\u001b[38;5;241m.\u001b[39msource_code,\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiff\u001b[39m\u001b[38;5;124m'\u001b[39m : mod\u001b[38;5;241m.\u001b[39mdiff\n\u001b[0;32m     16\u001b[0m     })\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydriller\\domain\\commit.py:234\u001b[0m, in \u001b[0;36mModifiedFile.source_code_before\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msource_code_before\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent_before\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_before, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_decoded_content(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_before)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydriller\\domain\\commit.py:220\u001b[0m, in \u001b[0;36mModifiedFile.content_before\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontent_before\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_undecoded_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_blob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydriller\\domain\\commit.py:223\u001b[0m, in \u001b[0;36mModifiedFile._get_undecoded_content\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_undecoded_content\u001b[39m(\u001b[38;5;28mself\u001b[39m, blob: Optional[IndexObject]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_stream\u001b[49m\u001b[38;5;241m.\u001b[39mread() \u001b[38;5;28;01mif\u001b[39;00m blob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\git\\objects\\base.py:201\u001b[0m, in \u001b[0;36mObject.data_stream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOStream\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        File-object compatible stream to the uncompressed raw data of the object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m        Returned streams must be read in order.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43modb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinsha\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\git\\db.py:46\u001b[0m, in \u001b[0;36mGitCmdObjectDB.stream\u001b[1;34m(self, binsha)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\u001b[38;5;28mself\u001b[39m, binsha: \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OStream:\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get git object data as a stream supporting ``read()`` (using git itself).\"\"\"\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     hexsha, typename, size, stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_git\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_object_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbin_to_hex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinsha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OStream(hex_to_bin(hexsha), typename, size, stream)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\git\\cmd.py:1706\u001b[0m, in \u001b[0;36mGit.stream_object_data\u001b[1;34m(self, ref)\u001b[0m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Similar to :meth:`get_object_data`, but returns the data as a stream.\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m \n\u001b[0;32m   1698\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;124;03m    instance per thread to be safe!\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_persistent_cmd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_file_all\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1706\u001b[0m hexsha, typename, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_object_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m cmd_stdout \u001b[38;5;241m=\u001b[39m cmd\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (hexsha, typename, size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCatFileContentStream(size, cmd_stdout))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\git\\cmd.py:1662\u001b[0m, in \u001b[0;36mGit.__get_object_header\u001b[1;34m(self, cmd, ref)\u001b[0m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mstdin \u001b[38;5;129;01mand\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[0;32m   1661\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_ref(ref))\n\u001b[1;32m-> 1662\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m   1663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_object_header(cmd\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mreadline())\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for i in range(len(df)):\n",
    "    commit_hash = df.iloc[i]['Hash']\n",
    "    commit_msg = df.iloc[i]['Message']\n",
    "    author_name = df.iloc[i]['Author']\n",
    "    for mod in df['List of modified files'].iloc[i]:\n",
    "        rows.append({\n",
    "            'Hash': commit_hash,\n",
    "            'Message': commit_msg,\n",
    "            \"Author\" : author_name,\n",
    "            'Filename' : mod.filename,\n",
    "            'Change Type': mod.change_type.name,\n",
    "            'Source Code (before)' : mod.source_code_before,\n",
    "            'Source Code (current)' : mod.source_code,\n",
    "            'Diff' : mod.diff\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1adec981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global --add safe.directory \"C:\\Users\\Vedant\\AppData\\Local\\Temp\\tmpnq9rzntx\\unsloth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da734b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash</th>\n",
       "      <th>Message</th>\n",
       "      <th>Author</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Change Type</th>\n",
       "      <th>Source Code (before)</th>\n",
       "      <th>Source Code (current)</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>644362e5c630f3c7710ec678705980bc3277d01d</td>\n",
       "      <td>Adding Assignment Questions</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>README.md</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td># Assignment 2 \\n\\n**Total marks: 10 (This ass...</td>\n",
       "      <td>@@ -0,0 +1,95 @@\\n+# Assignment 2 \\n+\\n+**Tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>644362e5c630f3c7710ec678705980bc3277d01d</td>\n",
       "      <td>Adding Assignment Questions</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>1colour.jpg</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>\u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000`\u0000`\u0000\u0000\u0000C\u0000\u0003\u0002\u0002\u0002\u0002\u0002\u0003\u0002\u0002\u0002\u0003\u0003\u0003\u0003\u0004\u0006\u0004\u0004\u0004\u0004\u0004\b\u0006\u0006\u0005\u0006\\...</td>\n",
       "      <td>Binary files /dev/null and b/sample_images/1co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>644362e5c630f3c7710ec678705980bc3277d01d</td>\n",
       "      <td>Adding Assignment Questions</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>2-3_colours.jpg</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>\u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000`\u0000`\u0000\u0000\u0000C\u0000\u0003\u0002\u0002\u0002\u0002\u0002\u0003\u0002\u0002\u0002\u0003\u0003\u0003\u0003\u0004\u0006\u0004\u0004\u0004\u0004\u0004\b\u0006\u0006\u0005\u0006\\...</td>\n",
       "      <td>Binary files /dev/null and b/sample_images/2-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>644362e5c630f3c7710ec678705980bc3277d01d</td>\n",
       "      <td>Adding Assignment Questions</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>multiple_colours.jpg</td>\n",
       "      <td>ADD</td>\n",
       "      <td>None</td>\n",
       "      <td>\u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000`\u0000`\u0000\u0000\u0000C\u0000\u0003\u0002\u0002\u0002\u0002\u0002\u0003\u0002\u0002\u0002\u0003\u0003\u0003\u0003\u0004\u0006\u0004\u0004\u0004\u0004\u0004\b\u0006\u0006\u0005\u0006\\...</td>\n",
       "      <td>Binary files /dev/null and b/sample_images/mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>457dc952c512f1960152a571c7a5c2837084a258</td>\n",
       "      <td>Simplifying the asignmnet</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>README.md</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td># Assignment 2 \\n\\n**Total marks: 10 (This ass...</td>\n",
       "      <td># Assignment 2 \\n\\n**Total marks: 10 (This ass...</td>\n",
       "      <td>@@ -16,63 +16,61 @@ eps = np.random.randn(num_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>24069667cf494dffff47c0fe730a60d3264c2207</td>\n",
       "      <td>Finalized task 1 Resolved all mistakes</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Task1.ipynb</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n...</td>\n",
       "      <td>{\"cells\":[{\"cell_type\":\"code\",\"execution_count...</td>\n",
       "      <td>@@ -1,1053 +1 @@\\n-{\\n- \"cells\": [\\n-  {\\n-   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2a22e90cf1cd4b2a668c6aa36e841f5a5ad3161f</td>\n",
       "      <td>Added md for momentum</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task1.ipynb</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>{\"cells\":[{\"cell_type\":\"code\",\"execution_count...</td>\n",
       "      <td>{\"cells\":[{\"cell_type\":\"code\",\"execution_count...</td>\n",
       "      <td>@@ -1 +1 @@\\n-{\"cells\":[{\"cell_type\":\"code\",\"e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1d0f7855a94bce5fa5543d0d1924f5dc2e99da34</td>\n",
       "      <td>Finalizing Task 2_Q2 &amp; Task 4_Q1</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Task2_Q2.ipynb</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>{\"metadata\":{\"kernelspec\":{\"language\":\"python\"...</td>\n",
       "      <td>{\"metadata\":{\"kernelspec\":{\"language\":\"python\"...</td>\n",
       "      <td>@@ -1 +1 @@\\n-{\"metadata\":{\"kernelspec\":{\"lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1d0f7855a94bce5fa5543d0d1924f5dc2e99da34</td>\n",
       "      <td>Finalizing Task 2_Q2 &amp; Task 4_Q1</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Task4_Q1.ipynb</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>{\\n  \"cells\": [\\n    {\\n      \"cell_type\": \"co...</td>\n",
       "      <td>{\\n  \"cells\": [\\n    {\\n      \"cell_type\": \"co...</td>\n",
       "      <td>@@ -210,6 +210,13 @@\\n         \"    return rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2ac9c31c47a4a260c7db08321b4129cbe47e6701</td>\n",
       "      <td>Final task2</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Task2_Q2.ipynb</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>{\"metadata\":{\"kernelspec\":{\"language\":\"python\"...</td>\n",
       "      <td>{\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{...</td>\n",
       "      <td>@@ -1 +1 @@\\n-{\"metadata\":{\"kernelspec\":{\"lang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Hash  \\\n",
       "0    644362e5c630f3c7710ec678705980bc3277d01d   \n",
       "1    644362e5c630f3c7710ec678705980bc3277d01d   \n",
       "2    644362e5c630f3c7710ec678705980bc3277d01d   \n",
       "3    644362e5c630f3c7710ec678705980bc3277d01d   \n",
       "4    457dc952c512f1960152a571c7a5c2837084a258   \n",
       "..                                        ...   \n",
       "109  24069667cf494dffff47c0fe730a60d3264c2207   \n",
       "110  2a22e90cf1cd4b2a668c6aa36e841f5a5ad3161f   \n",
       "111  1d0f7855a94bce5fa5543d0d1924f5dc2e99da34   \n",
       "112  1d0f7855a94bce5fa5543d0d1924f5dc2e99da34   \n",
       "113  2ac9c31c47a4a260c7db08321b4129cbe47e6701   \n",
       "\n",
       "                                    Message             Author  \\\n",
       "0               Adding Assignment Questions  Ayush Shrivastava   \n",
       "1               Adding Assignment Questions  Ayush Shrivastava   \n",
       "2               Adding Assignment Questions  Ayush Shrivastava   \n",
       "3               Adding Assignment Questions  Ayush Shrivastava   \n",
       "4                 Simplifying the asignmnet  Ayush Shrivastava   \n",
       "..                                      ...                ...   \n",
       "109  Finalized task 1 Resolved all mistakes        haarit19058   \n",
       "110                   Added md for momentum    AnuragSingh0000   \n",
       "111        Finalizing Task 2_Q2 & Task 4_Q1     Vedant Acharya   \n",
       "112        Finalizing Task 2_Q2 & Task 4_Q1     Vedant Acharya   \n",
       "113                             Final task2     Vedant Acharya   \n",
       "\n",
       "                 Filename Change Type  \\\n",
       "0               README.md         ADD   \n",
       "1             1colour.jpg         ADD   \n",
       "2         2-3_colours.jpg         ADD   \n",
       "3    multiple_colours.jpg         ADD   \n",
       "4               README.md      MODIFY   \n",
       "..                    ...         ...   \n",
       "109           Task1.ipynb      MODIFY   \n",
       "110           Task1.ipynb      MODIFY   \n",
       "111        Task2_Q2.ipynb      MODIFY   \n",
       "112        Task4_Q1.ipynb      MODIFY   \n",
       "113        Task2_Q2.ipynb      MODIFY   \n",
       "\n",
       "                                  Source Code (before)  \\\n",
       "0                                                 None   \n",
       "1                                                 None   \n",
       "2                                                 None   \n",
       "3                                                 None   \n",
       "4    # Assignment 2 \\n\\n**Total marks: 10 (This ass...   \n",
       "..                                                 ...   \n",
       "109  {\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n...   \n",
       "110  {\"cells\":[{\"cell_type\":\"code\",\"execution_count...   \n",
       "111  {\"metadata\":{\"kernelspec\":{\"language\":\"python\"...   \n",
       "112  {\\n  \"cells\": [\\n    {\\n      \"cell_type\": \"co...   \n",
       "113  {\"metadata\":{\"kernelspec\":{\"language\":\"python\"...   \n",
       "\n",
       "                                 Source Code (current)  \\\n",
       "0    # Assignment 2 \\n\\n**Total marks: 10 (This ass...   \n",
       "1    \u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000`\u0000`\u0000\u0000\u0000C\u0000\u0003\u0002\u0002\u0002\u0002\u0002\u0003\u0002\u0002\u0002\u0003\u0003\u0003\u0003\u0004\u0006\u0004\u0004\u0004\u0004\u0004\b\u0006\u0006\u0005\u0006\\...   \n",
       "2    \u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000`\u0000`\u0000\u0000\u0000C\u0000\u0003\u0002\u0002\u0002\u0002\u0002\u0003\u0002\u0002\u0002\u0003\u0003\u0003\u0003\u0004\u0006\u0004\u0004\u0004\u0004\u0004\b\u0006\u0006\u0005\u0006\\...   \n",
       "3    \u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000`\u0000`\u0000\u0000\u0000C\u0000\u0003\u0002\u0002\u0002\u0002\u0002\u0003\u0002\u0002\u0002\u0003\u0003\u0003\u0003\u0004\u0006\u0004\u0004\u0004\u0004\u0004\b\u0006\u0006\u0005\u0006\\...   \n",
       "4    # Assignment 2 \\n\\n**Total marks: 10 (This ass...   \n",
       "..                                                 ...   \n",
       "109  {\"cells\":[{\"cell_type\":\"code\",\"execution_count...   \n",
       "110  {\"cells\":[{\"cell_type\":\"code\",\"execution_count...   \n",
       "111  {\"metadata\":{\"kernelspec\":{\"language\":\"python\"...   \n",
       "112  {\\n  \"cells\": [\\n    {\\n      \"cell_type\": \"co...   \n",
       "113  {\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{...   \n",
       "\n",
       "                                                  Diff  \n",
       "0    @@ -0,0 +1,95 @@\\n+# Assignment 2 \\n+\\n+**Tota...  \n",
       "1    Binary files /dev/null and b/sample_images/1co...  \n",
       "2    Binary files /dev/null and b/sample_images/2-3...  \n",
       "3    Binary files /dev/null and b/sample_images/mul...  \n",
       "4    @@ -16,63 +16,61 @@ eps = np.random.randn(num_...  \n",
       "..                                                 ...  \n",
       "109  @@ -1,1053 +1 @@\\n-{\\n- \"cells\": [\\n-  {\\n-   ...  \n",
       "110  @@ -1 +1 @@\\n-{\"cells\":[{\"cell_type\":\"code\",\"e...  \n",
       "111  @@ -1 +1 @@\\n-{\"metadata\":{\"kernelspec\":{\"lang...  \n",
       "112  @@ -210,6 +210,13 @@\\n         \"    return rec...  \n",
       "113  @@ -1 +1 @@\\n-{\"metadata\":{\"kernelspec\":{\"lang...  \n",
       "\n",
       "[114 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_df = pd.DataFrame(rows)\n",
    "diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7faeec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pydriller.domain.commit.ModifiedFile at 0x2366a2288d0>,\n",
       " <pydriller.domain.commit.ModifiedFile at 0x2366a58b6d0>,\n",
       " <pydriller.domain.commit.ModifiedFile at 0x2366e2d0690>,\n",
       " <pydriller.domain.commit.ModifiedFile at 0x2366e2d0a50>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['List of modified files'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d143c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash</th>\n",
       "      <th>Author</th>\n",
       "      <th>Message</th>\n",
       "      <th>Hashes of parents</th>\n",
       "      <th>Is a merge commit?</th>\n",
       "      <th>List of modified files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>644362e5c630f3c7710ec678705980bc3277d01d</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>Adding Assignment Questions</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[README.md, sample_images\\1colour.jpg, sample_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>457dc952c512f1960152a571c7a5c2837084a258</td>\n",
       "      <td>Ayush Shrivastava</td>\n",
       "      <td>Simplifying the asignmnet</td>\n",
       "      <td>[644362e5c630f3c7710ec678705980bc3277d01d]</td>\n",
       "      <td>False</td>\n",
       "      <td>[README.md]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38bead3ce526071586bafc87c2e282eb4f095ecf</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>gd sgd added</td>\n",
       "      <td>[457dc952c512f1960152a571c7a5c2837084a258]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f141d4d8184df386d4dc3d76f12406ef83973496</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Structured the directory</td>\n",
       "      <td>[38bead3ce526071586bafc87c2e282eb4f095ecf]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task 1\\Task1.ipynb, Task 2\\assets\\baboon.png]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fd9b75816847896c031b3793327b777aff2642af</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Completed Task2_Q1</td>\n",
       "      <td>[f141d4d8184df386d4dc3d76f12406ef83973496]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task 2\\Task2_Q1.ipynb, Task 2\\assets\\baboon.p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a9c48c0361ec0f26c86146a6403e33c1764d08de</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Corrected image loading</td>\n",
       "      <td>[fd9b75816847896c031b3793327b777aff2642af]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task 2\\Task2_Q1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>447be57e4b68903e934ed24b36e1a06910e67c55</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task 1 part 3 and 4 done</td>\n",
       "      <td>[38bead3ce526071586bafc87c2e282eb4f095ecf]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4138e9533c2e52745677ad1bc8807e26abe03160</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Merge branch 'main' of https://github.com/adi7...</td>\n",
       "      <td>[447be57e4b68903e934ed24b36e1a06910e67c55, a9c...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac1f9e86c95d6109eca69470520abba7d4c858be</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task 5 completed</td>\n",
       "      <td>[4138e9533c2e52745677ad1bc8807e26abe03160]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb, Task_2\\Task2_Q1.ipynb, Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>120d8b374ac6b9bff4875c34a4a02abef88b087e</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Added Task 4</td>\n",
       "      <td>[ac1f9e86c95d6109eca69470520abba7d4c858be]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_4\\Task_4.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fb86e73f7e56e7e016bfce453c9d8bb6bb8dbddd</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task 5 fully completed</td>\n",
       "      <td>[120d8b374ac6b9bff4875c34a4a02abef88b087e]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_5\\Task_5.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>81dd372b94cc7e393b7c2050767f37dcfb53c664</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Rectified the task name</td>\n",
       "      <td>[fb86e73f7e56e7e016bfce453c9d8bb6bb8dbddd]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb, Task_3\\Task_3.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>d5e983a98c7a079902c91ba37a0b41d16a127573</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Completed Task_4_Q2</td>\n",
       "      <td>[81dd372b94cc7e393b7c2050767f37dcfb53c664]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_4\\Task4_Q2.ipynb, Task_4\\assets\\fashion....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9ddc8cb5f621c98110aec1a121a96472e607eac4</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Fixed a minor error in observations</td>\n",
       "      <td>[d5e983a98c7a079902c91ba37a0b41d16a127573]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_4\\Task4_Q2.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6f4401a26b4feb94b96bdf0bff64ca41085542ab</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Check commit</td>\n",
       "      <td>[9ddc8cb5f621c98110aec1a121a96472e607eac4]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_5\\Task_5.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6552f19e74ede6c9c2967c37924150e5c287007c</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Task 4 Q1 done</td>\n",
       "      <td>[6f4401a26b4feb94b96bdf0bff64ca41085542ab]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_4\\Task4_Q1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>520a9b5f3c436a405cc809587f266514fb4ed8c1</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Updated Task4_Q2</td>\n",
       "      <td>[6552f19e74ede6c9c2967c37924150e5c287007c]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_4\\Task4_Q2.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>04c5fdf6239b2bc1beb7dcf49286ea8e275ad9c0</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Added slider in task3 and explanation for task1</td>\n",
       "      <td>[520a9b5f3c436a405cc809587f266514fb4ed8c1]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb, Task_3\\Task_3.ipynb, Task...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>158c9d1b769fe88c9a5a94992be0e064c6a92251</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task 5 final push</td>\n",
       "      <td>[04c5fdf6239b2bc1beb7dcf49286ea8e275ad9c0]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_5\\Task_5.ipynb, Task_5\\patch_factorizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23d334a91ba480b38ced0f91089d91bbc9d12f18</td>\n",
       "      <td>Aditya Borate</td>\n",
       "      <td>Rewritten the explanation for clarity</td>\n",
       "      <td>[158c9d1b769fe88c9a5a94992be0e064c6a92251]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5706bf62c31d2da7086b8cfd6c3395ad8874b921</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Completed Task 1</td>\n",
       "      <td>[158c9d1b769fe88c9a5a94992be0e064c6a92251]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fd6072c49a8f6cbb0dd3b84cb33fc7248f0f1c5b</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Merge branch 'main' of https://github.com/adi7...</td>\n",
       "      <td>[5706bf62c31d2da7086b8cfd6c3395ad8874b921, 23d...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>72f15f164c6e2f17927432da8a1430089c2e9843</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Task 1 Part 1 added</td>\n",
       "      <td>[fd6072c49a8f6cbb0dd3b84cb33fc7248f0f1c5b]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1_Part1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dfc57607159613aeaa793c90c3693cff33dfe59d</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Updated the explanation</td>\n",
       "      <td>[72f15f164c6e2f17927432da8a1430089c2e9843]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>489ff2e3da907811cdc506fce2b65042df2a5eeb</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Final Task 4 400x400</td>\n",
       "      <td>[dfc57607159613aeaa793c90c3693cff33dfe59d]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_3\\Task_3.ipynb, Task_3\\dog.jpg, Task_3\\i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ab115cdcbbb4f7b1d6cd72f265e817e1b07413d2</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Added markdown menus in Task 1</td>\n",
       "      <td>[489ff2e3da907811cdc506fce2b65042df2a5eeb]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1_Part1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>459c2743eea91671ee83a56d93972577084ce115</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Task 2 Q2 added</td>\n",
       "      <td>[ab115cdcbbb4f7b1d6cd72f265e817e1b07413d2]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_2\\Task2_Q2.ipynb, Task_2\\assets\\Taqdeer....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>e6cfddeccb66cceb4d1164e1a96979c49aa90442</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task 1 corrected</td>\n",
       "      <td>[459c2743eea91671ee83a56d93972577084ce115]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ffb3ae078f017ba4f822c1f299d2c2e4f1ae5027</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Finalising Task 1</td>\n",
       "      <td>[e6cfddeccb66cceb4d1164e1a96979c49aa90442]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>621e4d6be25b682c9d7f737d6b0693d0385d634e</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>added theta ols</td>\n",
       "      <td>[ffb3ae078f017ba4f822c1f299d2c2e4f1ae5027]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>98fdaa182971cbb6c9c06bc5b3e9127bf4395709</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>edited the task 3</td>\n",
       "      <td>[621e4d6be25b682c9d7f737d6b0693d0385d634e]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_3\\Task_3.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>322873e37df54d7089a9fd4b52250fb31be750b1</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Task 1 Finalised</td>\n",
       "      <td>[98fdaa182971cbb6c9c06bc5b3e9127bf4395709]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>248b45e59338a572aae44f76a89e32ee96b5cf8f</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Finalizing Task 2</td>\n",
       "      <td>[322873e37df54d7089a9fd4b52250fb31be750b1]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1_Part1.ipynb, Task_2\\Task2_Q2.ipy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>82f62ae0dba1bf27915ad9ede5862098ede0c868</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Final TAsk 1</td>\n",
       "      <td>[322873e37df54d7089a9fd4b52250fb31be750b1]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>e57c20b7a5294ee8dd27407e2a84a0687bc8989a</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Merge branch 'main' of https://github.com/adi7...</td>\n",
       "      <td>[82f62ae0dba1bf27915ad9ede5862098ede0c868, 248...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>24069667cf494dffff47c0fe730a60d3264c2207</td>\n",
       "      <td>haarit19058</td>\n",
       "      <td>Finalized task 1 Resolved all mistakes</td>\n",
       "      <td>[e57c20b7a5294ee8dd27407e2a84a0687bc8989a]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2a22e90cf1cd4b2a668c6aa36e841f5a5ad3161f</td>\n",
       "      <td>AnuragSingh0000</td>\n",
       "      <td>Added md for momentum</td>\n",
       "      <td>[24069667cf494dffff47c0fe730a60d3264c2207]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_1\\Task1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1d0f7855a94bce5fa5543d0d1924f5dc2e99da34</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Finalizing Task 2_Q2 &amp; Task 4_Q1</td>\n",
       "      <td>[2a22e90cf1cd4b2a668c6aa36e841f5a5ad3161f]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_2\\Task2_Q2.ipynb, Task_4\\Task4_Q1.ipynb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2ac9c31c47a4a260c7db08321b4129cbe47e6701</td>\n",
       "      <td>Vedant Acharya</td>\n",
       "      <td>Final task2</td>\n",
       "      <td>[1d0f7855a94bce5fa5543d0d1924f5dc2e99da34]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Task_2\\Task2_Q2.ipynb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Hash             Author  \\\n",
       "0   644362e5c630f3c7710ec678705980bc3277d01d  Ayush Shrivastava   \n",
       "1   457dc952c512f1960152a571c7a5c2837084a258  Ayush Shrivastava   \n",
       "2   38bead3ce526071586bafc87c2e282eb4f095ecf        haarit19058   \n",
       "3   f141d4d8184df386d4dc3d76f12406ef83973496      Aditya Borate   \n",
       "4   fd9b75816847896c031b3793327b777aff2642af      Aditya Borate   \n",
       "5   a9c48c0361ec0f26c86146a6403e33c1764d08de      Aditya Borate   \n",
       "6   447be57e4b68903e934ed24b36e1a06910e67c55    AnuragSingh0000   \n",
       "7   4138e9533c2e52745677ad1bc8807e26abe03160    AnuragSingh0000   \n",
       "8   ac1f9e86c95d6109eca69470520abba7d4c858be    AnuragSingh0000   \n",
       "9   120d8b374ac6b9bff4875c34a4a02abef88b087e        haarit19058   \n",
       "10  fb86e73f7e56e7e016bfce453c9d8bb6bb8dbddd    AnuragSingh0000   \n",
       "11  81dd372b94cc7e393b7c2050767f37dcfb53c664        haarit19058   \n",
       "12  d5e983a98c7a079902c91ba37a0b41d16a127573      Aditya Borate   \n",
       "13  9ddc8cb5f621c98110aec1a121a96472e607eac4      Aditya Borate   \n",
       "14  6f4401a26b4feb94b96bdf0bff64ca41085542ab    AnuragSingh0000   \n",
       "15  6552f19e74ede6c9c2967c37924150e5c287007c     Vedant Acharya   \n",
       "16  520a9b5f3c436a405cc809587f266514fb4ed8c1      Aditya Borate   \n",
       "17  04c5fdf6239b2bc1beb7dcf49286ea8e275ad9c0        haarit19058   \n",
       "18  158c9d1b769fe88c9a5a94992be0e064c6a92251    AnuragSingh0000   \n",
       "19  23d334a91ba480b38ced0f91089d91bbc9d12f18      Aditya Borate   \n",
       "20  5706bf62c31d2da7086b8cfd6c3395ad8874b921    AnuragSingh0000   \n",
       "21  fd6072c49a8f6cbb0dd3b84cb33fc7248f0f1c5b    AnuragSingh0000   \n",
       "22  72f15f164c6e2f17927432da8a1430089c2e9843     Vedant Acharya   \n",
       "23  dfc57607159613aeaa793c90c3693cff33dfe59d        haarit19058   \n",
       "24  489ff2e3da907811cdc506fce2b65042df2a5eeb        haarit19058   \n",
       "25  ab115cdcbbb4f7b1d6cd72f265e817e1b07413d2     Vedant Acharya   \n",
       "26  459c2743eea91671ee83a56d93972577084ce115     Vedant Acharya   \n",
       "27  e6cfddeccb66cceb4d1164e1a96979c49aa90442    AnuragSingh0000   \n",
       "28  ffb3ae078f017ba4f822c1f299d2c2e4f1ae5027    AnuragSingh0000   \n",
       "29  621e4d6be25b682c9d7f737d6b0693d0385d634e        haarit19058   \n",
       "30  98fdaa182971cbb6c9c06bc5b3e9127bf4395709        haarit19058   \n",
       "31  322873e37df54d7089a9fd4b52250fb31be750b1    AnuragSingh0000   \n",
       "32  248b45e59338a572aae44f76a89e32ee96b5cf8f     Vedant Acharya   \n",
       "33  82f62ae0dba1bf27915ad9ede5862098ede0c868    AnuragSingh0000   \n",
       "34  e57c20b7a5294ee8dd27407e2a84a0687bc8989a    AnuragSingh0000   \n",
       "35  24069667cf494dffff47c0fe730a60d3264c2207        haarit19058   \n",
       "36  2a22e90cf1cd4b2a668c6aa36e841f5a5ad3161f    AnuragSingh0000   \n",
       "37  1d0f7855a94bce5fa5543d0d1924f5dc2e99da34     Vedant Acharya   \n",
       "38  2ac9c31c47a4a260c7db08321b4129cbe47e6701     Vedant Acharya   \n",
       "\n",
       "                                              Message  \\\n",
       "0                         Adding Assignment Questions   \n",
       "1                           Simplifying the asignmnet   \n",
       "2                                        gd sgd added   \n",
       "3                            Structured the directory   \n",
       "4                                  Completed Task2_Q1   \n",
       "5                             Corrected image loading   \n",
       "6                            Task 1 part 3 and 4 done   \n",
       "7   Merge branch 'main' of https://github.com/adi7...   \n",
       "8                                    Task 5 completed   \n",
       "9                                        Added Task 4   \n",
       "10                             Task 5 fully completed   \n",
       "11                            Rectified the task name   \n",
       "12                                Completed Task_4_Q2   \n",
       "13                Fixed a minor error in observations   \n",
       "14                                       Check commit   \n",
       "15                                     Task 4 Q1 done   \n",
       "16                                   Updated Task4_Q2   \n",
       "17    Added slider in task3 and explanation for task1   \n",
       "18                                  Task 5 final push   \n",
       "19              Rewritten the explanation for clarity   \n",
       "20                                   Completed Task 1   \n",
       "21  Merge branch 'main' of https://github.com/adi7...   \n",
       "22                                Task 1 Part 1 added   \n",
       "23                            Updated the explanation   \n",
       "24                               Final Task 4 400x400   \n",
       "25                     Added markdown menus in Task 1   \n",
       "26                                    Task 2 Q2 added   \n",
       "27                                   Task 1 corrected   \n",
       "28                                  Finalising Task 1   \n",
       "29                                    added theta ols   \n",
       "30                                  edited the task 3   \n",
       "31                                   Task 1 Finalised   \n",
       "32                                  Finalizing Task 2   \n",
       "33                                       Final TAsk 1   \n",
       "34  Merge branch 'main' of https://github.com/adi7...   \n",
       "35             Finalized task 1 Resolved all mistakes   \n",
       "36                              Added md for momentum   \n",
       "37                   Finalizing Task 2_Q2 & Task 4_Q1   \n",
       "38                                        Final task2   \n",
       "\n",
       "                                    Hashes of parents  Is a merge commit?  \\\n",
       "0                                                  []               False   \n",
       "1          [644362e5c630f3c7710ec678705980bc3277d01d]               False   \n",
       "2          [457dc952c512f1960152a571c7a5c2837084a258]               False   \n",
       "3          [38bead3ce526071586bafc87c2e282eb4f095ecf]               False   \n",
       "4          [f141d4d8184df386d4dc3d76f12406ef83973496]               False   \n",
       "5          [fd9b75816847896c031b3793327b777aff2642af]               False   \n",
       "6          [38bead3ce526071586bafc87c2e282eb4f095ecf]               False   \n",
       "7   [447be57e4b68903e934ed24b36e1a06910e67c55, a9c...                True   \n",
       "8          [4138e9533c2e52745677ad1bc8807e26abe03160]               False   \n",
       "9          [ac1f9e86c95d6109eca69470520abba7d4c858be]               False   \n",
       "10         [120d8b374ac6b9bff4875c34a4a02abef88b087e]               False   \n",
       "11         [fb86e73f7e56e7e016bfce453c9d8bb6bb8dbddd]               False   \n",
       "12         [81dd372b94cc7e393b7c2050767f37dcfb53c664]               False   \n",
       "13         [d5e983a98c7a079902c91ba37a0b41d16a127573]               False   \n",
       "14         [9ddc8cb5f621c98110aec1a121a96472e607eac4]               False   \n",
       "15         [6f4401a26b4feb94b96bdf0bff64ca41085542ab]               False   \n",
       "16         [6552f19e74ede6c9c2967c37924150e5c287007c]               False   \n",
       "17         [520a9b5f3c436a405cc809587f266514fb4ed8c1]               False   \n",
       "18         [04c5fdf6239b2bc1beb7dcf49286ea8e275ad9c0]               False   \n",
       "19         [158c9d1b769fe88c9a5a94992be0e064c6a92251]               False   \n",
       "20         [158c9d1b769fe88c9a5a94992be0e064c6a92251]               False   \n",
       "21  [5706bf62c31d2da7086b8cfd6c3395ad8874b921, 23d...                True   \n",
       "22         [fd6072c49a8f6cbb0dd3b84cb33fc7248f0f1c5b]               False   \n",
       "23         [72f15f164c6e2f17927432da8a1430089c2e9843]               False   \n",
       "24         [dfc57607159613aeaa793c90c3693cff33dfe59d]               False   \n",
       "25         [489ff2e3da907811cdc506fce2b65042df2a5eeb]               False   \n",
       "26         [ab115cdcbbb4f7b1d6cd72f265e817e1b07413d2]               False   \n",
       "27         [459c2743eea91671ee83a56d93972577084ce115]               False   \n",
       "28         [e6cfddeccb66cceb4d1164e1a96979c49aa90442]               False   \n",
       "29         [ffb3ae078f017ba4f822c1f299d2c2e4f1ae5027]               False   \n",
       "30         [621e4d6be25b682c9d7f737d6b0693d0385d634e]               False   \n",
       "31         [98fdaa182971cbb6c9c06bc5b3e9127bf4395709]               False   \n",
       "32         [322873e37df54d7089a9fd4b52250fb31be750b1]               False   \n",
       "33         [322873e37df54d7089a9fd4b52250fb31be750b1]               False   \n",
       "34  [82f62ae0dba1bf27915ad9ede5862098ede0c868, 248...                True   \n",
       "35         [e57c20b7a5294ee8dd27407e2a84a0687bc8989a]               False   \n",
       "36         [24069667cf494dffff47c0fe730a60d3264c2207]               False   \n",
       "37         [2a22e90cf1cd4b2a668c6aa36e841f5a5ad3161f]               False   \n",
       "38         [1d0f7855a94bce5fa5543d0d1924f5dc2e99da34]               False   \n",
       "\n",
       "                               List of modified files  \n",
       "0   [README.md, sample_images\\1colour.jpg, sample_...  \n",
       "1                                         [README.md]  \n",
       "2                                       [Task1.ipynb]  \n",
       "3      [Task 1\\Task1.ipynb, Task 2\\assets\\baboon.png]  \n",
       "4   [Task 2\\Task2_Q1.ipynb, Task 2\\assets\\baboon.p...  \n",
       "5                             [Task 2\\Task2_Q1.ipynb]  \n",
       "6                                       [Task1.ipynb]  \n",
       "7                                                  []  \n",
       "8   [Task_1\\Task1.ipynb, Task_2\\Task2_Q1.ipynb, Ta...  \n",
       "9                               [Task_4\\Task_4.ipynb]  \n",
       "10                              [Task_5\\Task_5.ipynb]  \n",
       "11          [Task_1\\Task1.ipynb, Task_3\\Task_3.ipynb]  \n",
       "12  [Task_4\\Task4_Q2.ipynb, Task_4\\assets\\fashion....  \n",
       "13                            [Task_4\\Task4_Q2.ipynb]  \n",
       "14                              [Task_5\\Task_5.ipynb]  \n",
       "15                            [Task_4\\Task4_Q1.ipynb]  \n",
       "16                            [Task_4\\Task4_Q2.ipynb]  \n",
       "17  [Task_1\\Task1.ipynb, Task_3\\Task_3.ipynb, Task...  \n",
       "18  [Task_5\\Task_5.ipynb, Task_5\\patch_factorizati...  \n",
       "19                               [Task_1\\Task1.ipynb]  \n",
       "20                               [Task_1\\Task1.ipynb]  \n",
       "21                                                 []  \n",
       "22                         [Task_1\\Task1_Part1.ipynb]  \n",
       "23                               [Task_1\\Task1.ipynb]  \n",
       "24  [Task_3\\Task_3.ipynb, Task_3\\dog.jpg, Task_3\\i...  \n",
       "25                         [Task_1\\Task1_Part1.ipynb]  \n",
       "26  [Task_2\\Task2_Q2.ipynb, Task_2\\assets\\Taqdeer....  \n",
       "27                               [Task_1\\Task1.ipynb]  \n",
       "28                               [Task_1\\Task1.ipynb]  \n",
       "29                               [Task_1\\Task1.ipynb]  \n",
       "30                              [Task_3\\Task_3.ipynb]  \n",
       "31                               [Task_1\\Task1.ipynb]  \n",
       "32  [Task_1\\Task1_Part1.ipynb, Task_2\\Task2_Q2.ipy...  \n",
       "33                               [Task_1\\Task1.ipynb]  \n",
       "34                                                 []  \n",
       "35                               [Task_1\\Task1.ipynb]  \n",
       "36                               [Task_1\\Task1.ipynb]  \n",
       "37     [Task_2\\Task2_Q2.ipynb, Task_4\\Task4_Q1.ipynb]  \n",
       "38                            [Task_2\\Task2_Q2.ipynb]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f61736fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hash': '2ac9c31c47a4a260c7db08321b4129cbe47e6701',\n",
       " 'Message': 'Final task2',\n",
       " 'Hashes of parents': ['1d0f7855a94bce5fa5543d0d1924f5dc2e99da34'],\n",
       " 'Is a merge commit?': False,\n",
       " 'List of modified files': [<pydriller.domain.commit.ModifiedFile at 0x2366e9ef5d0>]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf896821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitignore\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "for i in data['List of modified files']:\n",
    "    print(i.new_path or i.old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13636fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hash': '43badf217d1ccfaf486e2cbb1b3567226b5e95bf',\n",
       " 'Message': 'Initial commit',\n",
       " 'Hashes of parents': [],\n",
       " 'Is a merge commit?': False,\n",
       " 'List of modified files': [<pydriller.domain.commit.ModifiedFile at 0x2366a5b1010>,\n",
       "  <pydriller.domain.commit.ModifiedFile at 0x2366e73f4d0>]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a863d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
