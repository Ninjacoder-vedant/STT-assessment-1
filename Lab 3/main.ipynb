{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:39:52.261052Z",
     "iopub.status.busy": "2025-08-22T16:39:52.260782Z",
     "iopub.status.idle": "2025-08-22T16:39:52.264715Z",
     "shell.execute_reply": "2025-08-22T16:39:52.264015Z",
     "shell.execute_reply.started": "2025-08-22T16:39:52.261029Z"
    }
   },
   "source": [
    "### Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T16:05:24.645026Z",
     "iopub.status.busy": "2025-09-07T16:05:24.644821Z",
     "iopub.status.idle": "2025-09-07T16:05:30.974535Z",
     "shell.execute_reply": "2025-09-07T16:05:30.973618Z",
     "shell.execute_reply.started": "2025-09-07T16:05:24.645008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydriller\n",
      "  Downloading PyDriller-2.9-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from pydriller) (3.1.44)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from pydriller) (2025.2)\n",
      "Requirement already satisfied: types-pytz in /usr/local/lib/python3.11/dist-packages (from pydriller) (2025.2.0.20250516)\n",
      "Collecting lizard (from pydriller)\n",
      "  Downloading lizard-1.17.31-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->pydriller) (4.0.12)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from lizard->pydriller) (2.19.2)\n",
      "Collecting pathspec (from lizard->pydriller)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->pydriller) (5.0.2)\n",
      "Downloading PyDriller-2.9-py3-none-any.whl (36 kB)\n",
      "Downloading lizard-1.17.31-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: pathspec, lizard, pydriller\n",
      "Successfully installed lizard-1.17.31 pathspec-0.12.1 pydriller-2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pydriller\n",
    "import pandas as pd\n",
    "from pydriller import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:53:41.407836Z",
     "iopub.status.busy": "2025-09-05T06:53:41.407524Z",
     "iopub.status.idle": "2025-09-05T06:53:44.890588Z",
     "shell.execute_reply": "2025-09-05T06:53:44.889635Z",
     "shell.execute_reply.started": "2025-09-05T06:53:41.407814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting radon\n",
      "  Downloading radon-6.0.1-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting mando<0.8,>=0.6 (from radon)\n",
      "  Downloading mando-0.7.1-py2.py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: colorama>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from radon) (0.4.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from mando<0.8,>=0.6->radon) (1.17.0)\n",
      "Downloading radon-6.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mando-0.7.1-py2.py3-none-any.whl (28 kB)\n",
      "Installing collected packages: mando, radon\n",
      "Successfully installed mando-0.7.1 radon-6.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install radon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "analysis_df = pd.read_csv('Lab_2_analysis.csv')\n",
    "\n",
    "# 1. Total number of commits and files\n",
    "total_commits = analysis_df[\"Hash\"].nunique()\n",
    "total_files = analysis_df[\"Filename\"].nunique()\n",
    "\n",
    "# 2. Average number of modified files per commit\n",
    "files_per_commit = analysis_df.groupby(\"Hash\")[\"Filename\"].nunique()\n",
    "avg_files_per_commit = files_per_commit.mean()\n",
    "\n",
    "fix_type_distribution = analysis_df[\"LLM_inference\"].value_counts().reset_index()\n",
    "fix_type_distribution.columns = ['Fix type','Count']\n",
    "top_files = analysis_df[\"Filename\"].value_counts().reset_index()\n",
    "top_files.columns = ['Filename','Count']\n",
    "analysis_df[\"Extension\"] = analysis_df[\"Filename\"].str.split(\".\").str[-1]\n",
    "top_extensions = analysis_df[\"Extension\"].value_counts().reset_index()\n",
    "top_extensions.columns = ['Extension','Count']\n",
    "\n",
    "# Display results\n",
    "print(\"Total commits:\", total_commits)\n",
    "print(\"Total files:\", total_files)\n",
    "print(\"Average modified files per commit:\", round(avg_files_per_commit,2))\n",
    "print(\"\\nFix type distribution:\\n\", fix_type_distribution)\n",
    "print(\"\\nMost frequently modified files:\\n\", top_files.head(10))\n",
    "print(\"\\nMost frequently modified extensions:\\n\", top_extensions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:06:31.689648Z",
     "iopub.status.busy": "2025-09-05T07:06:31.688874Z",
     "iopub.status.idle": "2025-09-05T07:11:39.173545Z",
     "shell.execute_reply": "2025-09-05T07:11:39.172739Z",
     "shell.execute_reply.started": "2025-09-05T07:06:31.689624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "from radon.raw import analyze\n",
    "\n",
    "\n",
    "def analyze_code(source_code: str):\n",
    "    \"\"\"Extract MI, CC, and LOC from given source code string.\"\"\"\n",
    "    try:\n",
    "        # Maintainability Index\n",
    "        mi_score = mi_visit(source_code, True)\n",
    "\n",
    "        # Average Cyclomatic Complexity\n",
    "        cc_blocks = cc_visit(source_code)\n",
    "        cc_score =  sum(block.complexity for block in cc_blocks)/len(cc_blocks)\n",
    "\n",
    "        # Lines of Code\n",
    "        raw_metrics = analyze(source_code)\n",
    "        loc = raw_metrics.loc\n",
    "\n",
    "        return mi_score, cc_score, loc\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"Run radon analysis on Source Code (before/current) for each row.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        mi_before, cc_before, loc_before = analyze_code(str(row[\"Source Code (before)\"]))\n",
    "        mi_after, cc_after, loc_after = analyze_code(str(row[\"Source Code (current)\"]))\n",
    "\n",
    "        results.append({\n",
    "            \"MI_Change\": (mi_after - mi_before) if mi_before is not None and mi_after is not None else None,\n",
    "            \"CC_Change\": (cc_after - cc_before) if cc_before is not None and cc_after is not None else None,\n",
    "            \"LOC_Change\": (loc_after - loc_before) if loc_before is not None and loc_after is not None else None\n",
    "        })\n",
    "\n",
    "    # Merging with original dataframe\n",
    "    return pd.concat([df, pd.DataFrame(results)], axis=1)\n",
    "\n",
    "\n",
    "df_processed = process_dataframe(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:22:15.274746Z",
     "iopub.status.busy": "2025-09-05T06:22:15.274113Z",
     "iopub.status.idle": "2025-09-05T06:23:01.256424Z",
     "shell.execute_reply": "2025-09-05T06:23:01.255457Z",
     "shell.execute_reply.started": "2025-09-05T06:22:15.274722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fd6dfd32794571b22d380480e93419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186fc069ad0d41ca87c66b359b8484e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bddf2f2f284d8693c056904610ee24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ae8851a2de429ca6d0dbd69c62fc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7da355e963406e9d9cb0203237fc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 06:22:41.207153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757053361.557522      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757053361.654838      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb89e6d9c13c4be188be37dca65504e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93f24fec70e4f169f9fa43251b3e528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\",device_map=\"auto\")\n",
    "\n",
    "def semantic_similarity(code1, code2):\n",
    "    inputs = tokenizer([code1, code2], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Mean pooling over tokens\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Cosine similarity\n",
    "    sim = cosine_similarity([embeddings[0].numpy()], [embeddings[1].numpy()])[0][0]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:30:19.508146Z",
     "iopub.status.busy": "2025-09-05T06:30:19.507844Z",
     "iopub.status.idle": "2025-09-05T06:30:19.651426Z",
     "shell.execute_reply": "2025-09-05T06:30:19.650688Z",
     "shell.execute_reply.started": "2025-09-05T06:30:19.508121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "# Initialize BLEU scorer\n",
    "bleu = BLEU()\n",
    "\n",
    "def token_similarity_bleu(before_code, after_code):\n",
    "    sys = [after_code]\n",
    "    refs = [[before_code]]\n",
    "    \n",
    "    score = bleu.corpus_score(sys, refs)\n",
    "    return score.score / 100.0      # sacrebleu returns score out of 100, normalize to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:46:01.509000Z",
     "iopub.status.busy": "2025-09-05T07:46:01.508718Z",
     "iopub.status.idle": "2025-09-05T07:48:17.168007Z",
     "shell.execute_reply": "2025-09-05T07:48:17.167135Z",
     "shell.execute_reply.started": "2025-09-05T07:46:01.508979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1136/1136 [01:40<00:00, 11.27it/s]\n",
      "/tmp/ipykernel_36/4121688861.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[\"Semantic_Similarity\"] = results_df.progress_apply(\n",
      "100%|██████████| 1136/1136 [00:31<00:00, 35.57it/s]\n",
      "/tmp/ipykernel_36/4121688861.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[\"Token_Similarity\"] = results_df.progress_apply(\n",
      "/tmp/ipykernel_36/4121688861.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[\"Semantic_Class\"] = results_df[\"Semantic_Similarity\"].apply(classify_semantic)\n",
      "/tmp/ipykernel_36/4121688861.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[\"Token_Class\"] = results_df[\"Token_Similarity\"].apply(classify_token)\n",
      "/tmp/ipykernel_36/4121688861.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[\"Classes_Agree\"] = results_df.apply(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Wrap apply with tqdm\n",
    "tqdm.pandas()\n",
    "results_df = df_processed.dropna(subset=[\"Source Code (before)\", \"Source Code (current)\"])\n",
    "results_df[\"Semantic_Similarity\"] = results_df.progress_apply(\n",
    "    lambda row: semantic_similarity(str(row[\"Source Code (before)\"]), str(row[\"Source Code (current)\"])), axis=1\n",
    ")\n",
    "\n",
    "results_df[\"Token_Similarity\"] = results_df.progress_apply(\n",
    "    lambda row: token_similarity_bleu(str(row[\"Source Code (before)\"]), str(row[\"Source Code (current)\"])), axis=1\n",
    ")\n",
    "\n",
    "def classify_semantic(sim):\n",
    "    return \"Minor\" if sim >= 0.80 else \"Major\"\n",
    "\n",
    "def classify_token(sim):\n",
    "    return \"Minor\" if sim >= 0.75 else \"Major\"\n",
    "\n",
    "results_df[\"Semantic_Class\"] = results_df[\"Semantic_Similarity\"].apply(classify_semantic)\n",
    "results_df[\"Token_Class\"] = results_df[\"Token_Similarity\"].apply(classify_token)\n",
    "results_df[\"Classes_Agree\"] = results_df.apply(\n",
    "    lambda row: \"YES\" if row[\"Semantic_Class\"] == row[\"Token_Class\"] else \"NO\", axis=1\n",
    ")\n",
    "results_df.to_csv(\"Lab3_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8181547,
     "sourceId": 12977521,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
